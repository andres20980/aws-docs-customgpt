Creative Commons Attribution-ShareAlike 4.0 International Public License

By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License ("Public License"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.

Section 1 – Definitions.
	
     a.	Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.
	
     b.	Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.
	
     c.	BY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.
	
     d.	Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.
	
     e.	Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.
	
     f.	Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.
	
     g.	License Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.
	
     h.	Licensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.
	
     i.	Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.
	
     j.	Licensor means the individual(s) or entity(ies) granting rights under this Public License.
	
     k.	Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.
	
     l.	Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.
	
     m.	You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.

Section 2 – Scope.
	
     a.	License grant.
	
          1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:

               A. reproduce and Share the Licensed Material, in whole or in part; and	

               B. produce, reproduce, and Share Adapted Material.
	
          2. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.
	
          3. Term. The term of this Public License is specified in Section 6(a).
	
          4. Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.
	
          5. Downstream recipients.

               A. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.
	
               B. Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.
	
               C. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.
	
          6. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).
	
     b.	Other rights.
	
          1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.
	
          2. Patent and trademark rights are not licensed under this Public License.
	
          3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.

Section 3 – License Conditions.

Your exercise of the Licensed Rights is expressly made subject to the following conditions.
	
     a.	Attribution.
	
          1. If You Share the Licensed Material (including in modified form), You must:

               A. retain the following if it is supplied by the Licensor with the Licensed Material:

                    i.	identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);

                    ii.	a copyright notice;

                    iii. a notice that refers to this Public License;

                    iv.	a notice that refers to the disclaimer of warranties;

                    v.	a URI or hyperlink to the Licensed Material to the extent reasonably practicable;

               B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and

               C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.
	
          2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.
	
          3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.
	
     b.	ShareAlike.In addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.
	
          1. The Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.
	
          2. You must include the text of, or the URI or hyperlink to, the Adapter's License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.
	
          3. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter's License You apply.

Section 4 – Sui Generis Database Rights.

Where the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:
	
     a.	for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;
	
     b.	if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and
	
     c.	You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.
For the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.

Section 5 – Disclaimer of Warranties and Limitation of Liability.
	
     a.	Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.
	
     b.	To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.
	
     c.	The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.

Section 6 – Term and Termination.
	
     a.	This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.
	
     b.	Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:
	
          1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or
	
          2. upon express reinstatement by the Licensor.
	
     c.	For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.
	
     d.	For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.
	
     e.	Sections 1, 5, 6, 7, and 8 survive termination of this Public License.

Section 7 – Other Terms and Conditions.
	
     a.	The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.
	
     b.	Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.

Section 8 – Interpretation.
	
     a.	For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.
	
     b.	To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.
	
     c.	No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.
	
     d.	Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.



gitdir: ../../../.git/modules/repos/awsdocs/amazon-s3-developer-guide



# Error code<a name="ErrorCode"></a>

The error code is a string that uniquely identifies an error condition\. It is meant to be read and understood by programs that detect and handle errors by type\. Many error codes are common across SOAP and REST APIs, but some are API\-specific\. For example, NoSuchKey is universal, but UnexpectedContent can occur only in response to an invalid REST request\. In all cases, SOAP fault codes carry a prefix as indicated in the table of error codes, so that a NoSuchKey error is actually returned in SOAP as Client\.NoSuchKey\.

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 


# Specifying server\-side encryption using the AWS SDK for PHP<a name="SSEUsingPHPSDK"></a>

This topic shows how to use classes from version 3 of the AWS SDK for PHP to add server\-side encryption to objects that you upload to Amazon Simple Storage Service \(Amazon S3\)\. It assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\.

To upload an object to Amazon S3, use the [Aws\\S3\\S3Client::putObject\(\)](https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html#putobject) method\. To add the `x-amz-server-side-encryption` request header to your upload request, specify the `ServerSideEncryption` parameter with the value `AES256`, as shown in the following code example\. For information about server\-side encryption requests, see [Specifying Server\-Side Encryption Using the REST API](SSEUsingRESTAPI.md)\.

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';

// $filepath should be an absolute path to a file on disk.
$filepath = '*** Your File Path ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Upload a file with server-side encryption.
$result = $s3->putObject([
    'Bucket'               => $bucket,
    'Key'                  => $keyname,
    'SourceFile'           => $filepath,
    'ServerSideEncryption' => 'AES256',
]);
```

In response, Amazon S3 returns the `x-amz-server-side-encryption` header with the value of the encryption algorithm that was used to encrypt your object's data\. 

When you upload large objects using the multipart upload API, you can specify server\-side encryption for the objects that you are uploading, as follows: 
+ When using the low\-level multipart upload API, specify server\-side encryption when you call the [ Aws\\S3\\S3Client::createMultipartUpload\(\)](https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html#createmultipartupload) method\. To add the `x-amz-server-side-encryption` request header to your request, specify the `array` parameter's `ServerSideEncryption` key with the value `AES256`\. For more information about the low\-level multipart upload API, see [Using the AWS PHP SDK for multipart upload \(low\-level API\)](usingLLmpuPHP.md)\.
+ When using the high\-level multipart upload API, specify server\-side encryption using the `ServerSideEncryption` parameter of the [CreateMultipartUpload](https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html#createmultipartupload) method\. For an example of using the `setOption()` method with the high\-level multipart upload API, see [Using the AWS PHP SDK for multipart upload](usingHLmpuPHP.md)\.

## Determining encryption algorithm used<a name="DeterminingEncryptionAlgorithmUsed04"></a>

To determine the encryption state of an existing object, retrieve the object metadata by calling the [Aws\\S3\\S3Client::headObject\(\)](https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html#headobject) method as shown in the following PHP code example\.

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
            
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Check which server-side encryption algorithm is used.
$result = $s3->headObject([
    'Bucket' => $bucket,
    'Key'    => $keyname,
]);
echo $result['ServerSideEncryption'];
```

## Changing server\-side encryption of an existing object \(copy operation\)<a name="ChangingServer-SideEncryptionofanExistingObjectCopyOperation04"></a>

To change the encryption state of an existing object, make a copy of the object using the [Aws\\S3\\S3Client::copyObject\(\)](https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html#copyobject) method and delete the source object\. By default, `copyObject()` does not encrypt the target unless you explicitly request server\-side encryption of the destination object using the `ServerSideEncryption` parameter with the value `AES256`\. The following PHP code example makes a copy of an object and adds server\-side encryption to the copied object\.

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;

$sourceBucket = '*** Your Source Bucket Name ***';
$sourceKeyname = '*** Your Source Object Key ***';

$targetBucket = '*** Your Target Bucket Name ***';
$targetKeyname = '*** Your Target Object Key ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Copy an object and add server-side encryption.
$s3->copyObject([
    'Bucket'               => $targetBucket,
    'Key'                  => $targetKeyname,
    'CopySource'           => "{$sourceBucket}/{$sourceKeyname}",
    'ServerSideEncryption' => 'AES256',
]);
```

### Related resources<a name="RelatedResources-ChangingServer-SideEncryptionofanExistingObjectCopyOperation04"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Managing objects in a versioning\-enabled bucket<a name="manage-objects-versioned-bucket"></a>

**Topics**
+ [Adding objects to versioning\-enabled buckets](AddingObjectstoVersioningEnabledBuckets.md)
+ [Listing objects in a versioning\-enabled bucket](list-obj-version-enabled-bucket.md)
+ [Retrieving object versions](RetrievingObjectVersions.md)
+ [Deleting object versions](DeletingObjectVersions.md)
+ [Transitioning object versions](transitioning-object-versions.md)
+ [Restoring previous versions](RestoringPreviousVersions.md)
+ [Versioned object permissions](VersionedObjectPermissionsandACLs.md)

Objects that are stored in your bucket before you set the versioning state have a version ID of `null`\. When you enable versioning, existing objects in your bucket do not change\. What changes is how Amazon S3 handles the objects in future requests\. The topics in this section explain various object operations in a versioning\-enabled bucket\.


# Abort a multipart upload<a name="LLAbortMPUphp"></a>

This topic describes how to use a class from version 3 of the AWS SDK for PHP to stop a multipart upload that is in progress\. It assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\.

The following PHP example shows how to stop an in\-progress multipart upload using the `abortMultipartUpload()` method\. For information about running the PHP examples in this guide, see [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\.

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
$uploadId = '*** Upload ID of upload to Abort ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Abort the multipart upload.
$s3->abortMultipartUpload([
    'Bucket'   => $bucket,
    'Key'      => $keyname,
    'UploadId' => $uploadId,
]);
```

## Related resources<a name="RelatedResources-LLAbortMPUphp"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [ Amazon S3 Multipart Uploads](https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-multipart-upload.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# SQL Functions<a name="s3-glacier-select-sql-reference-sql-functions"></a>

Amazon S3 Select and S3 Glacier Select support several SQL functions\.

**Topics**
+ [Aggregate Functions \(Amazon S3 Select only\)](s3-glacier-select-sql-reference-aggregate.md)
+ [Conditional Functions](s3-glacier-select-sql-reference-conditional.md)
+ [Conversion Functions](s3-glacier-select-sql-reference-conversion.md)
+ [Date Functions](s3-glacier-select-sql-reference-date.md)
+ [String Functions](s3-glacier-select-sql-reference-string.md)


# Troubleshooting Amazon S3<a name="troubleshooting"></a>

This section describes how to troubleshoot Amazon S3 and explains how to get request IDs that you'll need when you contact AWS Support\.

**Topics**
+ [Troubleshooting Amazon S3 by Symptom](troubleshooting-by-symptom.md)
+ [Getting Amazon S3 Request IDs for AWS Support](#get-request-ids)
+ [Related Topics](#related-troubleshooting-topics)

For other troubleshooting and support topics, see the following:
+ [Troubleshooting CORS issues](cors-troubleshooting.md)
+ [Handling REST and SOAP errors](HandlingErrors.md)
+ [AWS Support Documentation](https://aws.amazon.com/documentation/aws-support/)

For troubleshooting information regarding third\-party tools, see [Getting Amazon S3 request IDs](https://forums.aws.amazon.com/thread.jspa?threadID=182409) in the AWS Developer Forums\.

## Getting Amazon S3 Request IDs for AWS Support<a name="get-request-ids"></a>

Whenever you need to contact AWS Support due to encountering errors or unexpected behavior in Amazon S3, you will need to get the request IDs associated with the failed action\. Getting these request IDs enables AWS Support to help you resolve the problems you're experiencing\. Request IDs come in pairs, are returned in every response that Amazon S3 processes \(even the erroneous ones\), and can be accessed through verbose logs\. There are a number of common methods for getting your request IDs including, S3 access logs and CloudTrail events/data events\.

After you've recovered these logs, copy and retain those two values, because you'll need them when you contact AWS Support\. For information about contacting AWS Support, see [Contact Us](https://aws.amazon.com/contact-us/)\.

**Topics**
+ [Using HTTP to Obtain Request IDs](#http-request-id)
+ [Using a Web Browser to Obtain Request IDs](#browser-request-id)
+ [Using AWS SDKs to Obtain Request IDs](#sdk-request-ids)
+ [Using the AWS CLI to Obtain Request IDs](#cli-request-id)

### Using HTTP to Obtain Request IDs<a name="http-request-id"></a>

You can obtain your request IDs, `x-amz-request-id` and `x-amz-id-2` by logging the bits of an HTTP request before it reaches the target application\. There are a variety of third\-party tools that can be used to recover verbose logs for HTTP requests\. Choose one you trust, and run the tool, listening on the port that your Amazon S3 traffic travels on, as you send out another Amazon S3 HTTP request\.

For HTTP requests, the pair of request IDs will look like the following examples\.

```
x-amz-request-id: 79104EXAMPLEB723 
x-amz-id-2: IOWQ4fDEXAMPLEQM+ey7N9WgVhSnQ6JEXAMPLEZb7hSQDASK+Jd1vEXAMPLEa3Km
```

**Note**  
HTTPS requests are encrypted and hidden in most packet captures\.

### Using a Web Browser to Obtain Request IDs<a name="browser-request-id"></a>

Most web browsers have developer tools that allow you to view request headers\.

For web browser\-based requests that return an error, the pair of requests IDs will look like the following examples\.

```
<Error><Code>AccessDenied</Code><Message>Access Denied</Message>
<RequestId>79104EXAMPLEB723</RequestId><HostId>IOWQ4fDEXAMPLEQM+ey7N9WgVhSnQ6JEXAMPLEZb7hSQDASK+Jd1vEXAMPLEa3Km</HostId></Error>
```

For obtaining the request ID pair from successful requests, you'll need to use the developer tools to look at the HTTP response headers\. For information about developer tools for specific browsers, see **Amazon S3 Troubleshooting \- How to recover your S3 request IDs** in the AWS Developer Forums\.

### Using AWS SDKs to Obtain Request IDs<a name="sdk-request-ids"></a>

The following sections include information for configuring logging using an AWS SDK\. While you can enable verbose logging on every request and response, you should not enable logging in production systems since large requests/responses can cause significant slowdown in an application\.

For AWS SDK requests, the pair of request IDs will look like the following examples\.

```
Status Code: 403, AWS Service: Amazon S3, AWS Request ID: 79104EXAMPLEB723  
AWS Error Code: AccessDenied  AWS Error Message: Access Denied  
S3 Extended Request ID: IOWQ4fDEXAMPLEQM+ey7N9WgVhSnQ6JEXAMPLEZb7hSQDASK+Jd1vEXAMPLEa3Km
```

#### Using the SDK for PHP to Obtain Request IDs<a name="php-request-id"></a>

You can configure logging using PHP\. For more information, see [How can I see what data is sent over the wire?](https://docs.aws.amazon.com/aws-sdk-php/guide/latest/faq.html#how-can-i-see-what-data-is-sent-over-the-wire) in the FAQ for the *AWS SDK for PHP*\.

#### Using the SDK for Java to Obtain Request IDs<a name="java-request-id"></a>

You can enable logging for specific requests or responses, allowing you to catch and return only the relevant headers\. To do this, import the `com.amazonaws.services.s3.S3ResponseMetadata` class\. Afterwards, you can store the request in a variable before performing the actual request\. Call `getCachedResponseMetadata(AmazonWebServiceRequest request).getRequestID()` to get the logged request or response\.

**Example**  

```
PutObjectRequest req = new PutObjectRequest(bucketName, key, createSampleFile());
s3.putObject(req);
S3ResponseMetadata md = s3.getCachedResponseMetadata(req);
System.out.println("Host ID: " + md.getHostId() + " RequestID: " + md.getRequestId());
```

Alternatively, you can use verbose logging of every Java request and response\. For more information, see [Verbose Wire Logging](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/java-dg-logging.html#sdk-net-logging-verbose) in the Logging AWS SDK for Java Calls topic in the *AWS SDK for Java Developer Guide*\.

#### Using the AWS SDK for \.NET to Obtain Request IDs<a name="net-request-id"></a>

You can configure logging in AWS SDK for \.NET using the built\-in `System.Diagnostics` logging tool\. For more information, see the [ Logging with the AWS SDK for \.NET](https://aws.amazon.com/blogs/developer/logging-with-the-aws-sdk-for-net/) AWS Developer Blog post\.

**Note**  
By default, the returned log contains only error information\. The config file needs to have `AWSLogMetrics` \(and optionally, `AWSResponseLogging`\) added to get the request IDs\.

#### Using the SDK for Python \(Boto3\) to Obtain Request IDs<a name="python-request-id"></a>

With SDK for Python \(Boto3\), you can log specific responses, which enables you to capture only the relevant headers\. The following code shows you how to log parts of the response to a file:

```
import logging
import boto3
logging.basicConfig(filename='logfile.txt', level=logging.INFO)
logger = logging.getLogger(__name__)
s3 = boto3.resource('s3')
response = s3.Bucket(bucket_name).Object(object_key).put()
logger.info("HTTPStatusCode: %s", response['ResponseMetadata']['HTTPStatusCode'])
logger.info("RequestId: %s", response['ResponseMetadata']['RequestId'])
logger.info("HostId: %s", response['ResponseMetadata']['HostId'])
logger.info("Date: %s", response['ResponseMetadata']['HTTPHeaders']['date'])
```

You can also catch exceptions and log relevant information when an exception is raised\. For details, see [Discerning useful information from error responses](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/error-handling.html#discerning-useful-information-from-error-responses) in the *Boto3 developer guide*

Additionally, you can configure Boto3 to output verbose debugging logs using the following code:

```
import boto3
boto3.set_stream_logger('', logging.DEBUG)
```

For more information, see [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/boto3.html#boto3.set_stream_logger](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/boto3.html#boto3.set_stream_logger) in the *Boto3 reference*\.

#### Using the SDK for Ruby to Obtain Request IDs<a name="ruby-request-id"></a>

You can get your request IDs using either the SDK for Ruby \- Version 1, Version 2, or Version 3\.
+ **Using the SDK for Ruby \- Version 1**– You can enable HTTP wire logging globally with the following line of code\.

  ```
  s3 = AWS::S3.new(:logger => Logger.new($stdout), :http_wire_trace => true)
  ```
+ **Using the SDK for Ruby \- Version 2 or Version 3**– You can enable HTTP wire logging globally with the following line of code\.

  ```
  s3 = Aws::S3::Client.new(:logger => Logger.new($stdout), :http_wire_trace => true)
  ```

### Using the AWS CLI to Obtain Request IDs<a name="cli-request-id"></a>

You can get your request IDs in the AWS CLI by adding `--debug` to your command\.

## Related Topics<a name="related-troubleshooting-topics"></a>

For other troubleshooting and support topics, see the following:
+ [Troubleshooting CORS issues](cors-troubleshooting.md)
+ [Handling REST and SOAP errors](HandlingErrors.md)
+ [AWS Support Documentation](https://aws.amazon.com/documentation/aws-support/)

For troubleshooting information regarding third\-party tools, see [Getting Amazon S3 request IDs](https://forums.aws.amazon.com/thread.jspa?threadID=182409) in the AWS Developer Forums\.


# Using AWS Identity and Access Management with Amazon S3 on Outposts<a name="S3OutpostsIAM"></a>

AWS Identity and Access Management \(IAM\) is an AWS service that administrators can use to securely control access to AWS Outposts resources\. To allow IAM users to manage AWS Outposts resources, you create an IAM policy that explicitly grants them permissions\.You then attach the policy to the IAM users or groups that require those permissions\. For more information, see [Identity and Access Management for AWS Outposts](https://docs.aws.amazon.com/outposts/latest/userguide/identity-access-management.html) in the *AWS Outposts User Guide*\. 

Amazon S3 on Outposts supports both bucket and access point policies\. S3 on Outposts policies use a different IAM actions namespace from S3 \(`s3-outposts:*` vs\. `s3:*`\) to provide you with distinct controls for data stored on your Outpost\.

Requests made to S3 on Outposts control API in an AWS Region are authenticated using IAM and authorized against the `s3-outposts:*` IAM namespace\. Requests made to the object API endpoints on the Outpost are authenticated\.

 Configure your IAM users and authorize them against the `s3-outposts:*` IAM namespace\. Access point policies that are configured on the Outpost access point control authorization of object API requests in addition to IAM user policies\.

**Note**  
S3 on Outposts defaults to the bucket owner as object owner, to help ensure that the owner of a bucket can't be prevented from accessing or deleting objects\.
S3 on Outposts always has S3 Block Public Access enabled to help ensure that objects can never have public access\.
S3 on Outposts uses the service prefix `s3-outposts:<ACTION>`\. For more information, see [Actions, resources, and condition keys for Amazon S3](https://docs.aws.amazon.com/IAM/latest/UserGuide/list_amazons3.html) in the *IAM User Guide*\.

## ARNS for Amazon S3 on Outposts<a name="S3OutpostsARN"></a>

 S3 on Outposts have different Amazon Resource Names \(ARN\) then Amazon S3\. The following is the ARN format for S3 on Outposts buckets\. You must use this ARN format to access and perform actions on your Outposts buckets and objects\.


****  

| Amazon S3 on Outposts ARN | ARN format | Example | 
| --- | --- | --- | 
| Bucket ARN | arn:<partition>:s3\-outposts:<region>:<account\_id>:outpost/<outpost\_id>/bucket/<bucket\_name | arn:aws:s3\-outposts:us\-west\-2:123456789012:outpost/op\-01ac5d28a6a232904/bucket/DOC\-EXAMPLE\-BUCKET1 | 
| accesspoint ARN | arn:<partition>:s3\-outposts:<region>:<account\_id>:outpost/<outpost\_id>/accesspoint/<accesspoint\_name> | arn:aws:s3\-outposts:us\-west\-2:123456789012:outpost/op\-01ac5d28a6a232904/accesspoint/example\-access\-point | 
| Object ARN | arn:<partition>:s3\-outposts:<region>:<account\_id>:outpost/<outpost\_id>/bucket/<bucket\_name>/object/<object\_key> | arn:aws:s3\-outposts:us\-west\-2:123456789012:outpost/op\-01ac5d28a6a232904/bucket/DOC\-EXAMPLE\-BUCKET1/object/myobject | 
| S3 on Outposts AP object ARN \(used in policies\) | arn:<partition>:s3\-outposts:<region>:<account\_id>:outpost/<outpost\_id>/accesspoint/<accesspoint\_name>/object/<object\_key> | arn:aws:s3\-outposts:us\-west\-2:123456789012:outpost/op\-01ac5d28a6a232904/accesspoint/example\-access\-point/object/myobject | 
| S3 on Outposts ARN | arn:<partition>:s3\-outposts:<region>:<account\_id>:outpost/<outpost\_id> | arn:aws:s3\-outposts:us\-west\-2:123456789012:outpost/op\-01ac5d28a6a232904 | 


# Deleting an object using the AWS SDK for PHP<a name="DeletingOneObjectUsingPHPSDK"></a>

This topic shows how to use classes from version 3 of the AWS SDK for PHP to delete an object from a non\-versioned bucket\. For information on deleting an object from a versioned bucket, see [Deleting an object using the REST API](DeletingAnObjectsUsingREST.md)\. 

This topic assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\.

The following PHP example deletes an object from a bucket\. Because this example shows how to delete objects from non\-versioned buckets, it provides only the bucket name and object key \(not a version ID\) in the delete request\. \. For information about running the PHP examples in this guide, see [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\.

```
<?php

require 'vendor/autoload.php';

use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// 1. Delete the object from the bucket.
try
{
    echo 'Attempting to delete ' . $keyname . '...' . PHP_EOL;

    $result = $s3->deleteObject([
        'Bucket' => $bucket,
        'Key'    => $keyname
    ]);

    if ($result['DeleteMarker'])
    {
        echo $keyname . ' was deleted or does not exist.' . PHP_EOL;
    } else {
        exit('Error: ' . $keyname . ' was not deleted.' . PHP_EOL);
    }
}
catch (S3Exception $e) {
    exit('Error: ' . $e->getAwsErrorMessage() . PHP_EOL);
}

// 2. Check to see if the object was deleted.
try
{
    echo 'Checking to see if ' . $keyname . ' still exists...' . PHP_EOL;

    $result = $s3->getObject([
        'Bucket' => $bucket,
        'Key'    => $keyname
    ]);

    echo 'Error: ' . $keyname . ' still exists.';
}
catch (S3Exception $e) {
    exit($e->getAwsErrorMessage());
}
```

## Related resources<a name="RelatedResources-DeletingOneObjectUsingPHPSDK"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Get an object Using the AWS SDK for Java<a name="RetrievingObjectUsingJava"></a>

When you download an object through the AWS SDK for Java, Amazon S3 returns all of the object's metadata and an input stream from which to read the object's contents\.

To retrieve an object, you do the following:
+ Run the `AmazonS3Client.getObject()` method, providing the bucket name and object key in the request\.
+ Run one of the `S3Object` instance methods to process the input stream\.

**Note**  
Your network connection remains open until you read all of the data or close the input stream\. We recommend that you read the content of the stream as quickly as possible\.

The following are some variations you might use:
+ Instead of reading the entire object, you can read only a portion of the object data by specifying the byte range that you want in the request\.
+ You can optionally override the response header values \(see [Getting objects](GettingObjectsUsingAPIs.md)\) by using a `ResponseHeaderOverrides` object and setting the corresponding request property\. For example, you can use this feature to indicate that the object should be downloaded into a file with a different file name than the object key name\.

The following example retrieves an object from an Amazon S3 bucket three ways: first, as a complete object, then as a range of bytes from the object, then as a complete object with overridden response header values\. For more information about getting objects from Amazon S3, see [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html)\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.GetObjectRequest;
import com.amazonaws.services.s3.model.ResponseHeaderOverrides;
import com.amazonaws.services.s3.model.S3Object;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;

public class GetObject2 {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String key = "*** Object key ***";

        S3Object fullObject = null, objectPortion = null, headerOverrideObject = null;
        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

            // Get an object and print its contents.
            System.out.println("Downloading an object");
            fullObject = s3Client.getObject(new GetObjectRequest(bucketName, key));
            System.out.println("Content-Type: " + fullObject.getObjectMetadata().getContentType());
            System.out.println("Content: ");
            displayTextInputStream(fullObject.getObjectContent());

            // Get a range of bytes from an object and print the bytes.
            GetObjectRequest rangeObjectRequest = new GetObjectRequest(bucketName, key)
                    .withRange(0, 9);
            objectPortion = s3Client.getObject(rangeObjectRequest);
            System.out.println("Printing bytes retrieved.");
            displayTextInputStream(objectPortion.getObjectContent());

            // Get an entire object, overriding the specified response headers, and print the object's content.
            ResponseHeaderOverrides headerOverrides = new ResponseHeaderOverrides()
                    .withCacheControl("No-cache")
                    .withContentDisposition("attachment; filename=example.txt");
            GetObjectRequest getObjectRequestHeaderOverride = new GetObjectRequest(bucketName, key)
                    .withResponseHeaders(headerOverrides);
            headerOverrideObject = s3Client.getObject(getObjectRequestHeaderOverride);
            displayTextInputStream(headerOverrideObject.getObjectContent());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        } finally {
            // To ensure that the network connection doesn't remain open, close any open input streams.
            if (fullObject != null) {
                fullObject.close();
            }
            if (objectPortion != null) {
                objectPortion.close();
            }
            if (headerOverrideObject != null) {
                headerOverrideObject.close();
            }
        }
    }

    private static void displayTextInputStream(InputStream input) throws IOException {
        // Read the text input stream one line at a time and display each line.
        BufferedReader reader = new BufferedReader(new InputStreamReader(input));
        String line = null;
        while ((line = reader.readLine()) != null) {
            System.out.println(line);
        }
        System.out.println();
    }
}
```


# SELECT Command<a name="s3-glacier-select-sql-reference-select"></a>

Amazon S3 Select and S3 Glacier Select support only the `SELECT` SQL command\. The following ANSI standard clauses are supported for `SELECT`: 


+ `SELECT` list
+ `FROM` clause 
+ `WHERE` clause
+ `LIMIT` clause \(Amazon S3 Select only\)

**Note**  
Amazon S3 Select and S3 Glacier Select queries currently do not support subqueries or joins\.

## SELECT List<a name="s3-glacier-select-sql-reference-select-list"></a>

The `SELECT` list names the columns, functions, and expressions that you want the query to return\. The list represents the output of the query\. 

```
SELECT *
SELECT projection [ AS column_alias | column_alias ] [, ...]
```

The first form with `*` \(asterisk\) returns every row that passed the `WHERE` clause, as\-is\. The second form creates a row with user\-defined output scalar expressions ***projection*** for each column\.

## FROM Clause<a name="s3-glacier-select-sql-reference-from"></a>

Amazon S3 Select and S3 Glacier Select support the following forms of the `FROM` clause:

```
FROM table_name
FROM table_name alias
FROM table_name AS alias
```

Where `table_name` is one of `S3Object` \(for Amazon S3 Select\) or `ARCHIVE` or `OBJECT` \(for S3 Glacier Select\) referring to the archive being queried over\. Users coming from traditional relational databases can think of this as a database schema that contains multiple views over a table\.

Following standard SQL, the `FROM` clause creates rows that are filtered in the `WHERE` clause and projected in the `SELECT` list\. 

For JSON objects that are stored in Amazon S3 Select, you can also use the following forms of the `FROM` clause:

```
FROM S3Object[*].path
FROM S3Object[*].path alias
FROM S3Object[*].path AS alias
```

Using this form of the `FROM` clause, you can select from arrays or objects within a JSON object\. You can specify `path` using one of the following forms:
+ By name \(in an object\): `.name` or `['name']`
+ By index \(in an array\): `[index]`
+ By wildcard \(in an object\): `.*`
+ By wildcard \(in an array\): `[*]`

**Note**  
This form of the `FROM` clause works only with JSON objects\.
Wildcards always emit at least one record\. If no record matches, then Amazon S3 Select emits the value `MISSING`\. During output serialization \(after the query is complete\), Amazon S3 Select replaces `MISSING` values with empty records\.
Aggregate functions \(`AVG`, `COUNT`, `MAX`, `MIN`, and `SUM`\) skip `MISSING` values\.
If you don't provide an alias when using a wildcard, you can refer to the row using the last element in the path\. For example, you could select all prices from a list of books using the query `SELECT price FROM S3Object[*].books[*].price`\. If the path ends in a wildcard rather than a name, then you can use the value `_1` to refer to the row\. For example, instead of `SELECT price FROM S3Object[*].books[*].price`, you could use the query `SELECT _1.price FROM S3Object[*].books[*]`\.
Amazon S3 Select always treats a JSON document as an array of root\-level values\. Thus, even if the JSON object that you are querying has only one root element, the `FROM` clause must begin with `S3Object[*]`\. However, for compatibility reasons, Amazon S3 Select allows you to omit the wildcard if you don't include a path\. Thus, the complete clause `FROM S3Object` is equivalent to `FROM S3Object[*] as S3Object`\. If you include a path, you must also use the wildcard\. So `FROM S3Object` and `FROM S3Object[*].path` are both valid clauses, but `FROM S3Object.path` is not\.

**Example**  
**Examples:**  
*Example \#1*  
This example shows results using the following dataset and query:  

```
{ "Rules": [ {"id": "1"}, {"expr": "y > x"}, {"id": "2", "expr": "z = DEBUG"} ]}
{ "created": "June 27", "modified": "July 6" }
```

```
SELECT id FROM S3Object[*].Rules[*].id
```

```
{"id":"1"}
{}
{"id":"2"}
{}
```
Amazon S3 Select produces each result for the following reasons:  
+ *\{"id":"id\-1"\}* — S3Object\[0\]\.Rules\[0\]\.id produced a match\.
+ *\{\}* — S3Object\[0\]\.Rules\[1\]\.id did not match a record, so Amazon S3 Select emitted `MISSING`, which was then changed to an empty record during output serialization and returned\.
+ *\{"id":"id\-2"\}* — S3Object\[0\]\.Rules\[2\]\.id produced a match\.
+ *\{\}* — S3Object\[1\] did not match on `Rules`, so Amazon S3 Select emitted `MISSING`, which was then changed to an empty record during output serialization and returned\.
If you don't want Amazon S3 Select to return empty records when it doesn't find a match, you can test for the value `MISSING`\. The following query returns the same results as the previous query, but with the empty values omitted:  

```
SELECT id FROM S3Object[*].Rules[*].id WHERE id IS NOT MISSING
```

```
{"id":"1"}
{"id":"2"}
```
*Example \#2*  
This example shows results using the following dataset and queries:  

```
{ "created": "936864000", "dir_name": "important_docs", "files": [ { "name": "." }, { "name": ".." }, { "name": ".aws" }, { "name": "downloads" } ], "owner": "AWS S3" }
{ "created": "936864000", "dir_name": "other_docs", "files": [ { "name": "." }, { "name": ".." }, { "name": "my stuff" }, { "name": "backup" } ], "owner": "User" }
```

```
SELECT d.dir_name, d.files FROM S3Object[*] d
```

```
{"dir_name":"important_docs","files":[{"name":"."},{"name":".."},{"name":".aws"},{"name":"downloads"}]}
{"dir_name":"other_docs","files":[{"name":"."},{"name":".."},{"name":"my stuff"},{"name":"backup"}]}
```

```
SELECT _1.dir_name, _1.owner FROM S3Object[*]
```

```
{"dir_name":"important_docs","owner":"AWS S3"}
{"dir_name":"other_docs","owner":"User"}
```

## WHERE Clause<a name="s3-glacier-select-sql-reference-where"></a>

The `WHERE` clause follows this syntax: 

```
WHERE condition
```

The `WHERE` clause filters rows based on the *condition*\. A condition is an expression that has a Boolean result\. Only rows for which the condition evaluates to `TRUE` are returned in the result\.

## LIMIT Clause \(Amazon S3 Select only\)<a name="s3-glacier-select-sql-reference-limit"></a>

The `LIMIT` clause follows this syntax: 

```
LIMIT number
```

The `LIMIT` clause limits the number of records that you want the query to return based on *number*\.

**Note**  
S3 Glacier Select does not support the `LIMIT` clause\.

## Attribute Access<a name="s3-glacier-select-sql-reference-attribute-access"></a>

The `SELECT` and `WHERE` clauses can refer to record data using one of the methods in the following sections, depending on whether the file that is being queried is in CSV or JSON format\.

### CSV<a name="s3-glacier-select-sql-reference-attribute-access-csv"></a>
+ **Column Numbers** – You can refer to the *Nth* column of a row with the column name `_N`, where N is the column position\. The position count starts at 1\. For example, the first column is named `_1` and the second column is named `_2`\.

  You can refer to a column as `_N` or `alias._N`\. For example, `_2` and `myAlias._2` are both valid ways to refer to a column in the `SELECT` list and `WHERE` clause\.
+ **Column Headers** – For objects in CSV format that have a header row, the headers are available to the `SELECT` list and `WHERE` clause\. In particular, as in traditional SQL, within `SELECT` and `WHERE` clause expressions, you can refer to the columns by `alias.column_name` or `column_name`\.

### JSON \(Amazon S3 Select only\)<a name="s3-glacier-select-sql-reference-attribute-access-json"></a>
+ **Document** – You can access JSON document fields as `alias.name`\. Nested fields can also be accessed; for example, `alias.name1.name2.name3`\.
+ **List** – You can access elements in a JSON list using zero\-based indexes with the `[]` operator\. For example, you can access the second element of a list as `alias[1]`\. Accessing list elements can be combined with fields as `alias.name1.name2[1].name3`\.
+ **Examples:** Consider this JSON object as a sample dataset:

  ```
  {"name": "Susan Smith",
  "org": "engineering",
  "projects":
      [
       {"project_name":"project1", "completed":false},
       {"project_name":"project2", "completed":true}
      ]
  }
  ```

  *Example \#1*

  The following query returns these results:

  ```
  Select s.name from S3Object s
  ```

  ```
  {"name":"Susan Smith"}
  ```

  *Example \#2*

  The following query returns these results:

  ```
  Select s.projects[0].project_name from S3Object s
  ```

  ```
  {"project_name":"project1"}
  ```

## Case Sensitivity of Header/Attribute Names<a name="s3-glacier-select-sql-reference-case-sensitivity"></a>

With Amazon S3 Select and S3 Glacier Select, you can use double quotation marks to indicate that column headers \(for CSV objects\) and attributes \(for JSON objects\) are case sensitive\. Without double quotation marks, object headers/attributes are case insensitive\. An error is thrown in cases of ambiguity\.

The following examples are either 1\) Amazon S3 or S3 Glacier objects in CSV format with the specified column header\(s\), and with `FileHeaderInfo` set to "Use" for the query request; or 2\) Amazon S3 objects in JSON format with the specified attributes\.

*Example \#1:* The object being queried has header/attribute "NAME"\.
+ The following expression successfully returns values from the object \(no quotation marks: case insensitive\):

  ```
  SELECT s.name from S3Object s
  ```
+ The following expression results in a 400 error `MissingHeaderName` \(quotation marks: case sensitive\):

  ```
  SELECT s."name" from S3Object s
  ```

*Example \#2:* The Amazon S3 object being queried has one header/attribute with "NAME" and another header/attribute with "name"\.
+ The following expression results in a 400 error `AmbiguousFieldName` \(no quotation marks: case insensitive, but there are two matches\):

  ```
  SELECT s.name from S3Object s
  ```
+ The following expression successfully returns values from the object \(quotation marks: case sensitive, so it resolves the ambiguity\)\.

  ```
  SELECT s."NAME" from S3Object s
  ```

## Using Reserved Keywords as User\-Defined Terms<a name="s3-glacier-select-sql-reference-using-keywords"></a>

Amazon S3 Select and S3 Glacier Select have a set of reserved keywords that are needed to run the SQL expressions used to query object content\. Reserved keywords include function names, data types, operators, and so on\. In some cases, user\-defined terms like the column headers \(for CSV files\) or attributes \(for JSON object\) may clash with a reserved keyword\. When this happens, you must use double quotation marks to indicate that you are intentionally using a user\-defined term that clashes with a reserved keyword\. Otherwise a 400 parse error will result\.

For the full list of reserved keywords see [Reserved Keywords](s3-glacier-select-sql-reference-keyword-list.md)\.

The following example is either 1\) an Amazon S3 or S3 Glacier object in CSV format with the specified column headers, with `FileHeaderInfo` set to "Use" for the query request, or 2\) an Amazon S3 object in JSON format with the specified attributes\.

*Example:* The object being queried has header/attribute named "CAST", which is a reserved keyword\.
+ The following expression successfully returns values from the object \(quotation marks: use user\-defined header/attribute\):

  ```
  SELECT s."CAST" from S3Object s
  ```
+ The following expression results in a 400 parse error \(no quotation marks: clash with reserved keyword\):

  ```
  SELECT s.CAST from S3Object s
  ```

## Scalar Expressions<a name="s3-glacier-select-sql-reference-scalar"></a>

Within the `WHERE` clause and the `SELECT` list, you can have SQL *scalar expressions*, which are expressions that return scalar values\. They have the following form:
+ ***literal*** 

  An SQL literal\. 
+ ***column\_reference*** 

  A reference to a column in the form *column\_name* or *alias\.column\_name*\. 
+ ***unary\_op*** ***expression*** 

  Where ***unary\_op*** unary is an SQL unary operator\.
+ ***expression*** ***binary\_op*** ***expression*** 

   Where ****binary\_op**** is an SQL binary operator\. 
+ **func\_name** 

   Where **func\_name** is the name of a scalar function to invoke\. 

  
+ ***expression*** `[ NOT ] BETWEEN` ***expression*** `AND` ***expression***

  
+ ***expression*** `LIKE` ***expression*** \[ `ESCAPE` ***expression*** \]

  


# Copy an Object Using the AWS SDK for PHP<a name="CopyingObjectUsingPHP"></a>

 This topic guides you through using classes from version 3 of the AWS SDK for PHP to copy a single object and multiple objects within Amazon S3, from one bucket to another or within the same bucket\.

 This topic assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\.

The following tasks guide you through using PHP SDK classes to copy an object that is already stored in Amazon S3\.

The following tasks guide you through using PHP classes to make multiple copies of an object within Amazon S3\. 


**Copying Objects**  

|  |  | 
| --- |--- |
|  1  |  Create an instance of an Amazon S3 client by using the `Aws\S3\S3Client` class constructor\.  | 
|  2  |  To make multiple copies of an object, you run a batch of calls to the Amazon S3 client [getCommand\(\)](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.AwsClientInterface.html#_getCommand) method, which is inherited from the [Aws\\CommandInterface](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.CommandInterface.html) class\. You provide the `CopyObject` command as the first argument and an array containing the source bucket, source key name, target bucket, and target key name as the second argument\.   | 

**Example of Copying Objects within Amazon S3**  
The following PHP example illustrates the use of the `copyObject()` method to copy a single object within Amazon S3 and using a batch of calls to `CopyObject` using the `getcommand()` method to make multiple copies of an object\.  

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;

$sourceBucket = '*** Your Source Bucket Name ***';
$sourceKeyname = '*** Your Source Object Key ***';
$targetBucket = '*** Your Target Bucket Name ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Copy an object.
$s3->copyObject([
    'Bucket'     => $targetBucket,
    'Key'        => "{$sourceKeyname}-copy",
    'CopySource' => "{$sourceBucket}/{$sourceKeyname}",
]);

// Perform a batch of CopyObject operations.
$batch = array();
for ($i = 1; $i <= 3; $i++) {
    $batch[] = $s3->getCommand('CopyObject', [
        'Bucket'     => $targetBucket,
        'Key'        => "{targetKeyname}-{$i}",
        'CopySource' => "{$sourceBucket}/{$sourceKeyname}",
    ]);
}
try {
    $results = CommandPool::batch($s3, $batch);
    foreach($results as $result) {
        if ($result instanceof ResultInterface) {
            // Result handling here
        }
        if ($result instanceof AwsException) {
            // AwsException handling here
        }
    }
} catch (\Exception $e) {
    // General error handling here
}
```

## Related Resources<a name="RelatedResources-CopyingObjectUsingPHP"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Understanding your AWS billing and usage reports for Amazon S3<a name="aws-usage-report-understand"></a>

Amazon S3 billing and usage reports use codes and abbreviations\. For example, for usage type, which is defined in the following table, *region* is replaced with one of the following abbreviations:
+ **APE1:** Asia Pacific \(Hong Kong\)
+ **APN1:** Asia Pacific \(Tokyo\)
+ **APN2:** Asia Pacific \(Seoul\)
+ **APS1:** Asia Pacific \(Singapore\)
+ **APS2:** Asia Pacific \(Sydney\)
+ **APS3:** Asia Pacific \(Mumbai\)
+ **CAN1:** Canada \(Central\)
+ **CPT:** Africa \(Cape Town\)
+ **EUN1:** Europe \(Stockholm\)
+ **EUC1:** Europe \(Frankfurt\)
+ **EU:** Europe \(Ireland\)
+ **EUW2:** Europe \(London\)
+ **EUW3:** Europe \(Paris\)
+ **MES1:** Middle East \(Bahrain\)
+ **SAE1:** South America \(São Paulo\)
+ **UGW1:** AWS GovCloud \(US\-West\)
+ **UGE1:** AWS GovCloud \(US\-East\)
+ **USE1 \(or no prefix\):** US East \(N\. Virginia\)
+ **USE2:** US East \(Ohio\)
+ **USW1:** US West \(N\. California\)
+ **USW2:** US West \(Oregon\)

For information about pricing by AWS Region, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\.

The first column in the following table lists usage types that appear in your billing and usage reports\.


**Usage Types**  

| Usage Type | Units | Granularity | Description | 
| --- | --- | --- | --- | 
|  *region1*\-*region2*\-AWS\-In\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred in to AWS Region1 from AWS Region2  | 
|  *region1*\-*region2*\-AWS\-Out\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred from AWS Region1 to AWS Region2  | 
|  *region*\-BatchOperations\-Jobs   |  Count   |  Hourly  |  The number of S3 Batch Operations jobs performed\.  | 
|  *region*\-BatchOperations\-Objects   |  Count   |  Hourly  |  The number of object operations performed by S3 Batch Operations\.  | 
|  *region*\-DataTransfer\-In\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred into Amazon S3 from the internet  | 
|  *region*\-DataTransfer\-Out\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred from Amazon S3 to the internet1  | 
|  *region*\-C3DataTransfer\-In\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred into Amazon S3 from Amazon EC2 within the same AWS Region  | 
|  *region*\-C3DataTransfer\-Out\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred from Amazon S3 to Amazon EC2 within the same AWS Region  | 
|  *region*\-S3G\-DataTransfer\-In\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred into Amazon S3 to restore objects from S3 Glacier or S3 Glacier Deep Archive storage  | 
|  *region*\-S3G\-DataTransfer\-Out\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred from Amazon S3 to transition objects to S3 Glacier or S3 Glacier Deep Archive storage  | 
|  *region*\-DataTransfer\-Regional\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred from Amazon S3 to AWS resources within the same AWS Region  | 
|  StorageObjectCount  |  Count  |  Daily  |  The number of objects stored within a given bucket  | 
|  *region*\-CloudFront\-In\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred into an AWS Region from a CloudFront distribution  | 
|  *region*\-CloudFront\-Out\-Bytes  |  Bytes  |  Hourly  |  The amount of data transferred from an AWS Region to a CloudFront distribution  | 
|  *region*\-EarlyDelete\-ByteHrs  |  Byte\-Hours2  |  Hourly  |  Prorated storage usage for objects deleted from, S3 Glacier storage before the 90\-day minimum commitment ended3  | 
|  *region*\-EarlyDelete\-GDA  |  Byte\-Hours2  |  Hourly  |  Prorated storage usage for objects deleted from S3 Glacier Deep Archive storage before the 180\-day minimum commitment ended 3  | 
|  *region*\-EarlyDelete\-SIA  |  Byte\-Hours  |  Hourly  |  Prorated storage usage for objects deleted from STANDARD\_IA before the 30\-day minimum commitment ended4  | 
|  *region*\-EarlyDelete\-ZIA  |  Byte\-Hours  |  Hourly  |  Prorated storage usage for objects deleted from ONEZONE\_IA before the 30\-day minimum commitment ended4  | 
|  *region*\-EarlyDelete\-SIA\-SmObjects  |  Byte\-Hours  |  Hourly  |  Prorated storage usage for small objects \(smaller than 128 KB\) that were deleted from STANDARD\_IA before the 30\-day minimum commitment ended4  | 
|  *region*\-EarlyDelete\-ZIA\-SmObjects  |  Byte\-Hours  |  Hourly  |  Prorated storage usage for small objects \(smaller than 128 KB\) that were deleted from ONEZONE\_IA before the 30\-day minimum commitment ended4  | 
|  *region*\-Inventory\-ObjectsListed  |  Objects  |  Hourly  |  The number of objects listed for an object group \(objects are grouped by bucket or prefix\) with an inventory list  | 
|  *region*\-Requests\-S3 Glacier\-Tier1  |  Count  |  Hourly  |  The number of PUT, COPY, POST, InitiateMultipartUpload, UploadPart, or CompleteMultipartUpload requests on S3 Glacier objects  | 
|  *region*\-Requests\-S3 Glacier\-Tier2  |  Count  |  Hourly  | The number of GET and all other requests not listed on S3 Glacier objects | 
|  *region*\-Requests\-SIA\-Tier1  |  Count  |  Hourly  |  The number of PUT, COPY, POST, or LIST requests on STANDARD\_IA objects  | 
|  *region*\-Requests\-ZIA\-Tier1  |  Count  |  Hourly  |  The number of PUT, COPY, POST, or LIST requests on ONEZONE\_IA objects  | 
|  *region*\-Requests\-SIA\-Tier2  |  Count  |  Hourly  |  The number of GET and all other non\-SIA\-Tier1 requests on STANDARD\_IA objects  | 
|  *region*\-Requests\-ZIA\-Tier2  |  Count  |  Hourly  |  The number of GET and all other non\-ZIA\-Tier1 requests on ONEZONE\_IA objects  | 
|  *region*\-Requests\-Tier1  |  Count  |  Hourly  |  The number of PUT, COPY, POST, or LIST requests for STANDARD, RRS, and tags  | 
|  *region*\-Requests\-Tier2  |  Count  |  Hourly  |  The number of GET and all other non\-Tier1 requests  | 
|  *region*\-Requests\-Tier3  |  Count  |  Hourly  |  The number of lifecycle requests to S3 Glacier or S3 Glacier Deep Archive and standard S3 Glacier restore requests  | 
|  *region*\-Requests\-Tier4  |  Count  |  Hourly  |  The number of lifecycle transitions to INTELLIGENT\_TIERING, STANDARD\_IA, or ONEZONE\_IA storage  | 
|  *region*\-Requests\-Tier5  |  Count  |  Hourly  |  The number of Bulk S3 Glacier restore requests  | 
|  *region*\-Requests\-GDA\-Tier1  |  Count  |  Hourly  |  The number of PUT, COPY, POST, InitiateMultipartUpload, UploadPart, or CompleteMultipartUpload requests on DEEP Archive objects  | 
|  *region*\-Requests\-GDA\-Tier2  |  Count  |  Hourly  |  The number of GET, HEAD, and LIST requests  | 
|  *region*\-Requests\-GDA\-Tier3  |  Count  |  Hourly  |  The number of S3 Glacier Deep Archive standard restore requests  | 
|  *region*\-Requests\-GDA\-Tier5  |  Count  |  Hourly  |  The number of Bulk S3 Glacier Deep Archive restore requests  | 
|  *region*\-Requests\-Tier6  |  Count  |  Hourly  |  The number of Expedited S3 Glacier restore requests  | 
|  *region*\-Bulk\-Retrieval\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data retrieved with Bulk S3 Glacier or S3 Glacier Deep Archive requests  | 
|  *region*\-Requests\-INT\-Tier1  |  Count  |  Hourly  |  The number of PUT, COPY, POST, or LIST requests on INTELLIGENT\_TIERING objects  | 
|  *region*\-Requests\-INT\-Tier2  |  Count  |  Hourly  |  The number of GET and all other non\-Tier1 requests for INTELLIGENT\_TIERING objects  | 
|  *region*\-Select\-Returned\-INT\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data returned with Select requests from INTELLIGENT\_TIERING storage  | 
|  *region*\-Select\-Scanned\-INT\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data scanned with Select requests from INTELLIGENT\_TIERING storage  | 
|  *region*\-EarlyDelete\-INT  |  Byte\-Hours  |  Hourly  |  Prorated storage usage for objects deleted from INTELLIGENT\_TIERING before the 30\-day minimum commitment ended  | 
|  *region*\-Monitoring\-Automation\-INT  |  Objects  |  Hourly  |  The number of unique objects monitored and auto\-tiered in the INTELLIGENT\_TIERING storage class  | 
|  *region*\-Expedited\-Retrieval\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data retrieved with Expedited S3 Glacier requests  | 
|  *region*\-Standard\-Retrieval\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data retrieved with standard S3 Glacier or S3 Glacier Deep Archive requests  | 
|  *region*\-Retrieval\-SIA  |  Bytes  |  Hourly  |  The number of bytes of data retrieved from STANDARD\_IA storage  | 
|  *region*\-Retrieval\-ZIA  |  Bytes  |  Hourly  |  The number of bytes of data retrieved from ONEZONE\_IA storage  | 
|  *region*\-StorageAnalytics\-ObjCount  |  Objects  |  Hourly  |  The number of unique objects in each object group \(where objects are grouped by bucket or prefix\) tracked by storage analytics  | 
|  *region*\-Select\-Scanned\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data scanned with Select requests from STANDARD storage  | 
|  *region*\-Select\-Scanned\-SIA\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data scanned with Select requests from STANDARD\_IA storage  | 
|  *region*\-Select\-Scanned\-ZIA\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data scanned with Select requests from ONEZONE\_IA storage  | 
|  *region*\-Select\-Returned\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data returned with Select requests from STANDARD storage  | 
|  *region*\-Select\-Returned\-SIA\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data returned with Select requests from STANDARD\_IA storage  | 
|  *region*\-Select\-Returned\-ZIA\-Bytes  |  Bytes  |  Hourly  |  The number of bytes of data returned with Select requests from ONEZONE\_IA storage  | 
|  *region*\-TagStorage\-TagHrs  |  Tag\-Hours  |  Daily  |  The total of tags on all objects in the bucket reported by hour  | 
|  *region*\-TimedStorage\-ByteHrs  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in STANDARD storage  | 
|  *region*\-TimedStorage\-S3 GlacierByteHrs  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in S3 Glacier storage  | 
|  *region*\-TimedStorage\-GlacierStaging  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in S3 Glacier staging storage  | 
|  *region*\-TimedStorage\-GDA\-ByteHrs  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in S3 Glacier Deep Archive storage  | 
|  *region*\-TimedStorage\-GDA\-Staging  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in S3 Glacier Deep Archive staging storage  | 
|  *region*\-TimedStorage\-INT\-FA\-ByteHrs  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in the frequent access tier of INTELLIGENT\_TIERING storage  | 
|  *region*\-TimedStorage\-INT\-IA\-ByteHrs  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in the infrequent access tier of INTELLIGENT\_TIERING storage  | 
|  *region*\-TimedStorage\-RRS\-ByteHrs  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in Reduced Redundancy Storage \(RRS\) storage  | 
|  *region*\-TimedStorage\-SIA\-ByteHrs  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in STANDARD\_IA storage  | 
|  *region*\-TimedStorage\-ZIA\-ByteHrs  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that data was stored in ONEZONE\_IA storage  | 
|  *region*\-TimedStorage\-SIA\-SmObjects  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that small objects \(smaller than 128 KB\) were stored in STANDARD\_IA storage  | 
|  *region*\-TimedStorage\-ZIA\-SmObjects  |  Byte\-Hours  |  Daily  |  The number of byte\-hours that small objects \(smaller than 128 KB\) were stored in ONEZONE\_IA storage  | 

**Notes:**

1. If you terminate a transfer before completion, the amount of data that is transferred might exceed the amount of data that your application receives\. This discrepancy can occur because a transfer termination request cannot be executed instantaneously, and some amount of data might be in transit pending execution of the termination request\. This data in transit is billed as data transferred “out\.”

1. For more information on the byte\-hours unit, see [Converting Usage Byte\-Hours to Billed GB\-Months](#aws-usage-report-understand-converting-byte-hours)\.

1. When objects that are archived to the S3 Glacier or S3 Glacier Deep Archive storage class are deleted, overwritten, or transitioned to a different storage class before the minimum storage commitment has passed, which is 90 days for S3 Glacier or 180\-days for S3 Glacier Deep Archive, there is a prorated charge per gigabyte for the remaining days\.

1. For objects that are in INTELLIGENT\_TIERING, STANDARD\_IA, or ONEZONE\_IA storage, when they are deleted, overwritten, or transitioned to a different storage class prior to 30 days, there is a prorated charge per gigabyte for the remaining days\.

1. For small objects \(smaller than 128 KB\) that are in STANDARD\_IA or ONEZONE\_IA storage, when they are deleted, overwritten, or transitioned to a different storage class prior to 30 days, there is a prorated charge per gigabyte for the remaining days\.

1. There is no minimum billable object size for objects in the INTELLIGENT\_TIERING storage class, but objects that are smaller than 128 KB are not eligible for auto\-tiering and are always charged at the rate for the INTELLIGENT\_TIERING frequent access tier\. 

## Tracking Operations in Your Usage Reports<a name="aws-usage-report-understand-operations"></a>

Operations describe the action taken on your AWS object or bucket by the specified usage type\. Operations are indicated by self\-explanatory codes, such as `PutObject` or `ListBucket`\. To see which actions on your bucket generated a specific type of usage, use these codes\. When you create a usage report, you can choose to include **All Operations**, or a specific operation, for example, **GetObject**, to report on\.

## Converting Usage Byte\-Hours to Billed GB\-Months<a name="aws-usage-report-understand-converting-byte-hours"></a>

The volume of storage that we bill you for each month is based on the average amount of storage you used throughout the month\. You are billed for all of the object data and metadata stored in buckets that you created under your AWS account\. For more information about metadata, see [Object key and metadata](UsingMetadata.md)\. 

We measure your storage usage in TimedStorage\-ByteHrs, which are totaled up at the end of the month to generate your monthly charges\. The usage report reports your storage usage in byte\-hours and the billing reports report storage usage in GB\-months\. To correlate your usage report to your billing reports, you need to convert byte\-hours into GB\-months\.

For example, if you store 100 GB \(107,374,182,400 bytes\) of STANDARD Amazon S3 storage data in your bucket for the first 15 days in March, and 100 TB \(109,951,162,777,600 bytes\) of STANDARD Amazon S3 storage data for the final 16 days in March, you will have used 42,259,901,212,262,400 byte\-hours\.

First, calculate the total byte\-hour usage:

```
[107,374,182,400 bytes x 15 days x (24 hours/day)] 
    + [109,951,162,777,600 bytes x 16 days x (24 hours/day)]
    = 42,259,901,212,262,400 byte-hours
```

Then convert the byte\-hours to GB\-Months:

```
42,259,901,212,262,400 byte-hours/1,073,741,824 bytes per GB/24 hours per day
     /31 days in March 
     =52,900 GB-Months
```

## More Info<a name="aws-usage-report-understand-more-info"></a>
+ [AWS usage report for Amazon S3](aws-usage-report.md)
+ [AWS Billing reports for Amazon S3](aws-billing-reports.md)
+ [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)
+ [Amazon S3 FAQ](https://aws.amazon.com/s3/faqs/#billing)
+ [S3 Glacier Pricing](https://aws.amazon.com/glacier/pricing/)
+ [S3 Glacier FAQs](https://aws.amazon.com/glacier/faqs/)


# List multipart uploads using the low\-level AWS SDK for PHP API<a name="LLlistMPuploadsPHP"></a>

This topic shows how to use the low\-level API classes from version 3 of the AWS SDK for PHP to list all in\-progress multipart uploads on a bucket\. It assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\. 

The following PHP example demonstrates listing all in\-progress multipart uploads on a bucket\.

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Retrieve a list of the current multipart uploads.
$result = $s3->listMultipartUploads([
    'Bucket' => $bucket
]);

// Write the list of uploads to the page.
print_r($result->toArray());
```

## Related resources<a name="RelatedResources-LLlistMPuploadsPHP"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [ Amazon S3 Multipart Uploads](https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-multipart-upload.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Managing S3 Batch Operations jobs<a name="batch-ops-managing-jobs"></a>

Amazon S3 provides a robust set of tools to help you manage your Batch Operations jobs after you create them\. This section describes the operations you can use to manage your jobs\. You can perform all of the operations listed in this section using the AWS Management Console, AWS CLI, AWS SDKs, or REST APIs\.

**Topics**
+ [Listing jobs](#batch-ops-list-jobs)
+ [Viewing job details](#batch-ops-job-details)
+ [Controlling access and labeling jobs using tags](#batch-ops-job-tags)
+ [Assigning job priority](#batch-ops-job-priority)
+ [Job status](#batch-ops-job-status)
+ [Tracking job failure](#batch-ops-job-status-failure)
+ [Notifications and logging](#batch-ops-notifications)
+ [Completion reports](#batch-ops-completion-report)

The following video briefly describes how you can use the Amazon S3 console to manage your S3 Batch Operations jobs\.

[![AWS Videos](http://img.youtube.com/vi/https://www.youtube.com/embed/CuMDH6c0zm4//0.jpg)](http://www.youtube.com/watch?v=https://www.youtube.com/embed/CuMDH6c0zm4/)

## Listing jobs<a name="batch-ops-list-jobs"></a>

You can retrieve a list of your S3 Batch Operations jobs\. The list includes jobs that haven't yet finished and jobs that finished within the last 90 days\. The job list includes information for each job, such as its ID, description, priority, current status, and the number of tasks that have succeeded and failed\. You can filter your job list by status\. When you retrieve a job list through the console, you can also search your jobs by description or ID and filter them by AWS Region\.

## Viewing job details<a name="batch-ops-job-details"></a>

If you want more information about a job than you can retrieve by listing jobs, you can view all of the details for a single job\. In addition to the information returned in a job list, a single job's details include other items\. This information includes the operation parameters, details about the manifest, information about the completion report \(if you configured one when you created the job\), and the Amazon Resource Name \(ARN\) of the user role that you assigned to run the job\. By viewing an individual job's details, you can access a job's entire configuration\. 

## Controlling access and labeling jobs using tags<a name="batch-ops-job-tags"></a>

You can label and control access to your S3 Batch Operations jobs by adding *tags*\. Tags can be used to identify who is responsible for a Batch Operations job\. The presence of job tags can grant or limit a user's ability to cancel a job, activate a job in the confirmation state, or change a job's priority level\. You can create jobs with tags attached to them, and you can add tags to jobs after they are created\. Each tag is a key\-value pair that can be included when you create the job or updated later\.

**Warning**  
Job tags should not contain any confidential information or personal data\.

Consider the following tagging example: Suppose that you want your Finance department to create a Batch Operations job\. You could write an AWS Identity and Access Management \(IAM\) policy that allows a user to invoke `CreateJob`, provided that the job is created with the `Department` tag assigned the value `Finance`\. Furthermore, you could attach that policy to all users who are members of the Finance department\.

Continuing with this example, you could write a policy that allows a user to update the priority of any job that has the desired tags, or cancel any job that has those tags\. For more information, see [Example: Using job tags to control permissions for S3 Batch Operations](batch-ops-job-tags-examples.md)\.

You can add tags to new S3 Batch Operations jobs when you create them, or you can add them to existing jobs\. 

Note the following tag restrictions:
+ You can associate up to 50 tags with a job as long as they have unique tag keys\.
+ A tag key can be up to 128 Unicode characters in length, and tag values can be up to 256 Unicode characters in length\.
+ The key and values are case sensitive\.

For more information about tag restrictions, see [User\-Defined Tag Restrictions](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocation-tag-restrictions.html) in the *AWS Billing and Cost Management User Guide*\.

### API operations related to S3 Batch Operations job tagging<a name="batch-ops-job-tags-api"></a>

Amazon S3 supports the following API operations that are specific to S3 Batch Operations job tagging:
+ [GetJobTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_GetJobTagging.html) — Returns the tag set associated with a Batch Operations job\. 
+ [PutJobTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_PutJobTagging.html) — Replaces the tag set associated with a job\. There are two distinct scenarios for S3 Batch Operations job tag management using this API action:
  + Job has no tags — You can add a set of tags to a job \(the job has no prior tags\)\.
  + Job has a set of existing tags — To modify the existing tag set, you can either replace the existing tag set entirely, or make changes within the existing tag set by retrieving the existing tag set using [GetJobTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_GetJobTagging.html), modify that tag set, and use this API action to replace the tag set with the one you have modified\.
**Note**  
If you send this request with an empty tag set, S3 Batch Operations deletes the existing tag set on the object\. If you use this method, you are charged for a Tier 1 Request \(PUT\)\. For more information, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing)\.  
To delete existing tags for your Batch Operations job, the `DeleteJobTagging` action is preferred because it achieves the same result without incurring charges\.
+ [DeleteJobTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_DeleteJobTagging.html) — Deletes the tag set associated with a Batch Operations job\. 

## Assigning job priority<a name="batch-ops-job-priority"></a>

You can assign each job a numeric priority, which can be any positive integer\. S3 Batch Operations prioritize jobs according to the assigned priority\. Jobs with a higher priority \(or a higher numeric value for the priority parameter\) are evaluated first\. Priority is determined in descending order\. For example, a job queue with a priority value of 10 is given scheduling preference over a job queue with a priority value of 1\. 

You can change a job's priority while it is running\. If you submit a new job with a higher priority while a job is running, the lower\-priority job can pause to allow the higher\-priority job to run\.

**Note**  
S3 Batch Operations honor job priorities on a best\-effort basis\. Although jobs with higher priorities generally take precedence over jobs with lower priorities, Amazon S3 does not guarantee strict ordering of jobs\.

## Job status<a name="batch-ops-job-status"></a>

After you create a job, it progresses through a series of statuses\. The following table describes the statuses that jobs can have and the possible transitions between job statuses\.


****  

| Status | Description | Transitions | 
| --- | --- | --- | 
| New | A job begins in the New state when you create it\. | A job automatically moves to the Preparing state when Amazon S3 begins processing the manifest object\. | 
| Preparing | Amazon S3 is processing the manifest object and other job parameters to set up and run the job\. | A job automatically moves to the Ready state after Amazon S3 finishes processing the manifest and other parameters\. It is then ready to begin running the specified operation on the objects listed in the manifest\.If the job requires confirmation before running, such as when you create a job using the Amazon S3 console, then the job transitions from `Preparing` to `Suspended`\. It remains in the `Suspended` state until you confirm that you want to run it\. | 
| Suspended | The job requires confirmation, but you have not yet confirmed that you want to run it\. Only jobs that you create using the Amazon S3 console require confirmation\. A job that is created using the console enters the Suspended state immediately after Preparing\. After you confirm that you want to run the job and the job becomes Ready, it never returns to the Suspended state\. | After you confirm that you want to run the job, its status changes to Ready\. | 
| Ready | Amazon S3 is ready to begin running the requested object operations\. | A job automatically moves to Active when Amazon S3 begins to run it\. The amount of time that a job remains in the Ready state depends on whether you have higher\-priority jobs running already and how long those jobs take to complete\. | 
| Active | Amazon S3 is executing the requested operation on the objects listed in the manifest\. While a job is Active, you can monitor its progress using the Amazon S3 console or the DescribeJob operation through the REST API, AWS CLI, or AWS SDKs\. | A job moves out of the Active state when it is no longer running operations on objects\. This can happen automatically, such as when a job completes successfully or fails\. Or it can occur as a result of user actions, such as canceling a job\. The state that the job moves to depends on the reason for the transition\. | 
| Pausing | The job is transitioning to Paused from another state\. | A job automatically moves to Paused when the Pausing stage is finished\. | 
| Paused | A job can become Paused if you submit another job with a higher priority while the current job is running\. | A Paused job automatically returns to Active after any higher\-priority jobs that are blocking the job's' execution complete, fail, or are suspended\. | 
| Complete | The job has finished executing the requested operation on all objects in the manifest\. The operation might have succeeded or failed for each object\. If you configured the job to generate a completion report, the report is available as soon as the job is Complete\. | Complete is a terminal state\. Once a job reaches Complete, it does not transition to any other state\. | 
| Cancelling | The job is transitioning to the Cancelled state\. | A job automatically moves to Cancelled when the Cancelling stage is finished\. | 
| Cancelled | You requested that the job be cancelled, and S3 Batch Operations has successfully cancelled the job\. The job will not submit any new requests to Amazon S3\. | Cancelled is a terminal state\. After a job reaches Cancelled, it will not transition to any other state\. | 
| Failing | The job is transitioning to the Failed state\. | A job automatically moves to Failed once the Failing stage is finished\. | 
| Failed | The job has failed and is no longer running\. For more information about job failures, see [Tracking job failure](#batch-ops-job-status-failure)\. | Failed is a terminal state\. After a job reaches Failed, it will not transition to any other state\. | 

## Tracking job failure<a name="batch-ops-job-status-failure"></a>

If an S3 Batch Operations job encounters a problem that prevents it from running successfully, such as not being able to read the specified manifest, the job fails\. When a job fails, it generates one or more failure codes or failure reasons\. S3 Batch Operations store the failure codes and reasons with the job so that you can view them by requesting the job's details\. If you requested a completion report for the job, the failure codes and reasons also appear there\.

To prevent jobs from running a large number of unsuccessful operations, Amazon S3 imposes a task\-failure threshold on every Batch Operations job\. When a job has run at least 1,000 tasks, Amazon S3 monitors the task failure rate\. At any point, if the failure rate \(the number of tasks that have failed as a proportion of the total number of tasks that have run\) exceeds 50 percent, the job fails\. If your job fails because it exceeded the task\-failure threshold, you can identify the cause of the failures\. For example, you might have accidentally included some objects in the manifest that don't exist in the specified bucket\. After fixing the errors, you can resubmit the job\.

**Note**  
S3 Batch Operations operate asynchronously and the tasks don't necessarily run in the order that the objects are listed in the manifest\. Therefore, you can't use the manifest ordering to determine which objects' tasks succeeded and which ones failed\. Instead, you can examine the job's completion report \(if you requested one\) or view your AWS CloudTrail event logs to help determine the source of the failures\.

## Notifications and logging<a name="batch-ops-notifications"></a>

 In addition to requesting completion reports, you can also capture, review, and audit Batch Operations activity using AWS CloudTrail\. Because Batch Operations use existing Amazon S3 APIs to perform tasks, those tasks also emit the same events that they would if you called them directly\. Thus, you can track and record the progress of your job and all of its tasks using the same notification, logging, and auditing tools and processes that you already use with Amazon S3\. 

For more information about Amazon S3 events, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\. 

## Completion reports<a name="batch-ops-completion-report"></a>

When you create a job, you can request a completion report\. As long as S3 Batch Operations successfully invoke at least one task, Amazon S3 generates a completion report after it finishes running tasks, fails, or is canceled\. You can configure the completion report to include all tasks or only failed tasks\. 

The completion report includes the job configuration and status and information for each task, including the object key and version, status, error codes, and descriptions of any errors\. Completion reports provide an easy way to view the results of your tasks in a consolidated format with no additional setup required\. For an example of a completion report, see [Example: Requesting S3 Batch Operations completion reports](batch-ops-examples-reports.md)\. 

If you don't configure a completion report, you can still monitor and audit your job and its tasks using CloudTrail and Amazon CloudWatch For more information, see [Example: Tracking an S3 Batch Operations job in Amazon EventBridge through AWS CloudTrail](batch-ops-examples-event-bridge-cloud-trail.md)\.


# Request routing<a name="UsingRouting"></a>

**Topics**
+ [Request redirection and the REST API](Redirects.md)
+ [DNS considerations](DNSConsiderations.md)

Programs that make requests against buckets created using the <CreateBucketConfiguration> API must support redirects\. Additionally, some clients that do not respect DNS TTLs might encounter issues\.

This section describes routing and DNS issues to consider when designing your service or application for use with Amazon S3\.


# Specifying server\-side encryption with customer\-provided encryption keys using the AWS SDK for \.NET<a name="sse-c-using-dot-net-sdk"></a>

The following C\# example shows how server\-side encryption with customer\-provided keys \(SSE\-C\) works\. The example performs the following operations\. Each operation shows how to specify SSE\-C–related headers in the request\.
+ **Put object**—Uploads an object and requests server\-side encryption using customer\-provided encryption keys\. 
+ **Get object**—Downloads the object that was uploaded in the previous step\. The request provides the same encryption information that was provided when the object was uploaded\. Amazon S3 needs this information to decrypt the object and return it to you\.
+ **Get object metadata**—Provides the same encryption information used when the object was created to retrieve the object's metadata\.
+ **Copy object**—Makes a copy of the uploaded object\. Because the source object is stored using SSE\-C, the copy request must provide encryption information\. By default, Amazon S3 does not encrypt a copy of an object\. The code directs Amazon S3 to encrypt the copied object using SSE\-C by providing encryption\-related information for the target\. It also stores the target\.

**Note**  
For examples of uploading large objects using the multipart upload API, see [Using the AWS SDK for \.NET for multipart upload \(high\-level API\)](usingHLmpuDotNet.md) and [Using the AWS SDK for \.NET for multipart upload \(low\-level API\)](usingLLmpuDotNet.md)\.

For information about SSE\-C, see [Protecting data using server\-side encryption with customer\-provided encryption keys \(SSE\-C\)](ServerSideEncryptionCustomerKeys.md)\)\. For information about creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

**Example**  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.IO;
using System.Security.Cryptography;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class SSEClientEncryptionKeyObjectOperationsTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string keyName = "*** key name for new object created ***"; 
        private const string copyTargetKeyName = "*** key name for object copy ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            ObjectOpsUsingClientEncryptionKeyAsync().Wait();
        }
        private static async Task ObjectOpsUsingClientEncryptionKeyAsync()
        {
            try
            {
                // Create an encryption key.
                Aes aesEncryption = Aes.Create();
                aesEncryption.KeySize = 256;
                aesEncryption.GenerateKey();
                string base64Key = Convert.ToBase64String(aesEncryption.Key);

                // 1. Upload the object.
                PutObjectRequest putObjectRequest = await UploadObjectAsync(base64Key);
                // 2. Download the object and verify that its contents matches what you uploaded.
                await DownloadObjectAsync(base64Key, putObjectRequest);
                // 3. Get object metadata and verify that the object uses AES-256 encryption.
                await GetObjectMetadataAsync(base64Key);
                // 4. Copy both the source and target objects using server-side encryption with 
                //    a customer-provided encryption key.
                await CopyObjectAsync(aesEncryption, base64Key);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        private static async Task<PutObjectRequest> UploadObjectAsync(string base64Key)
        {
            PutObjectRequest putObjectRequest = new PutObjectRequest
            {
                BucketName = bucketName,
                Key = keyName,
                ContentBody = "sample text",
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = base64Key
            };
            PutObjectResponse putObjectResponse = await client.PutObjectAsync(putObjectRequest);
            return putObjectRequest;
        }
        private static async Task DownloadObjectAsync(string base64Key, PutObjectRequest putObjectRequest)
        {
            GetObjectRequest getObjectRequest = new GetObjectRequest
            {
                BucketName = bucketName,
                Key = keyName,
                // Provide encryption information for the object stored in Amazon S3.
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = base64Key
            };

            using (GetObjectResponse getResponse = await client.GetObjectAsync(getObjectRequest))
            using (StreamReader reader = new StreamReader(getResponse.ResponseStream))
            {
                string content = reader.ReadToEnd();
                if (String.Compare(putObjectRequest.ContentBody, content) == 0)
                    Console.WriteLine("Object content is same as we uploaded");
                else
                    Console.WriteLine("Error...Object content is not same.");

                if (getResponse.ServerSideEncryptionCustomerMethod == ServerSideEncryptionCustomerMethod.AES256)
                    Console.WriteLine("Object encryption method is AES256, same as we set");
                else
                    Console.WriteLine("Error...Object encryption method is not the same as AES256 we set");

                // Assert.AreEqual(putObjectRequest.ContentBody, content);
                // Assert.AreEqual(ServerSideEncryptionCustomerMethod.AES256, getResponse.ServerSideEncryptionCustomerMethod);
            }
        }
        private static async Task GetObjectMetadataAsync(string base64Key)
        {
            GetObjectMetadataRequest getObjectMetadataRequest = new GetObjectMetadataRequest
            {
                BucketName = bucketName,
                Key = keyName,

                // The object stored in Amazon S3 is encrypted, so provide the necessary encryption information.
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = base64Key
            };

            GetObjectMetadataResponse getObjectMetadataResponse = await client.GetObjectMetadataAsync(getObjectMetadataRequest);
            Console.WriteLine("The object metadata show encryption method used is: {0}", getObjectMetadataResponse.ServerSideEncryptionCustomerMethod);
            // Assert.AreEqual(ServerSideEncryptionCustomerMethod.AES256, getObjectMetadataResponse.ServerSideEncryptionCustomerMethod);
        }
        private static async Task CopyObjectAsync(Aes aesEncryption, string base64Key)
        {
            aesEncryption.GenerateKey();
            string copyBase64Key = Convert.ToBase64String(aesEncryption.Key);

            CopyObjectRequest copyRequest = new CopyObjectRequest
            {
                SourceBucket = bucketName,
                SourceKey = keyName,
                DestinationBucket = bucketName,
                DestinationKey = copyTargetKeyName,
                // Information about the source object's encryption.
                CopySourceServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                CopySourceServerSideEncryptionCustomerProvidedKey = base64Key,
                // Information about the target object's encryption.
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = copyBase64Key
            };
            await client.CopyObjectAsync(copyRequest);
        }
    }
}
```

## Other Amazon S3 operations and SSE\-C<a name="sse-c-dotnet-other-api"></a>

The example in the preceding section shows how to request server\-side encryption with customer\-provided key \(SSE\-C\) in the PUT, GET, Head, and Copy operations\. This section describes other Amazon S3 APIs that support SSE\-C\.

To upload large objects, you can use multipart upload API \(see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\)\. AWS SDK for \.NET provides both high\-level or low\-level APIs to upload large objects\. These APIs support encryption\-related headers in the request\.
+ When using high\-level `Transfer-Utility `API, you provide the encryption\-specific headers in the `TransferUtilityUploadRequest` as shown\. For code examples, see [Using the AWS SDK for \.NET for multipart upload \(high\-level API\)](usingHLmpuDotNet.md)\.

  ```
  TransferUtilityUploadRequest request = new TransferUtilityUploadRequest()
  {
      FilePath = filePath,
      BucketName = existingBucketName,
      Key = keyName,
      // Provide encryption information.
      ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
      ServerSideEncryptionCustomerProvidedKey = base64Key,
  };
  ```
+ When using the low\-level API, you provide encryption\-related information in the initiate multipart upload request, followed by identical encryption information in the subsequent upload part requests\. You do not need to provide any encryption\-specific headers in your complete multipart upload request\. For examples, see [Using the AWS SDK for \.NET for multipart upload \(low\-level API\)](usingLLmpuDotNet.md)\.

  The following is a low\-level multipart upload example that makes a copy of an existing large object\. In the example, the object to be copied is stored in Amazon S3 using SSE\-C, and you want to save the target object also using SSE\-C\. In the example, you do the following:
  + Initiate a multipart upload request by providing an encryption key and related information\.
  + Provide source and target object encryption keys and related information in the `CopyPartRequest`\.
  + Obtain the size of the source object to be copied by retrieving the object metadata\.
  + Upload the objects in 5 MB parts\.  
**Example**  

  ```
  using Amazon;
  using Amazon.S3;
  using Amazon.S3.Model;
  using System;
  using System.Collections.Generic;
  using System.IO;
  using System.Security.Cryptography;
  using System.Threading.Tasks;
  
  namespace Amazon.DocSamples.S3
  {
      class SSECLowLevelMPUcopyObjectTest
      {
          private const string existingBucketName = "*** bucket name ***";
          private const string sourceKeyName      = "*** source object key name ***"; 
          private const string targetKeyName      = "*** key name for the target object ***";
          private const string filePath           = @"*** file path ***";
          // Specify your bucket region (an example region is shown).
          private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
          private static IAmazonS3 s3Client;
          static void Main()
          {
              s3Client = new AmazonS3Client(bucketRegion);
              CopyObjClientEncryptionKeyAsync().Wait();
          }
  
          private static async Task CopyObjClientEncryptionKeyAsync()
          {
              Aes aesEncryption = Aes.Create();
              aesEncryption.KeySize = 256;
              aesEncryption.GenerateKey();
              string base64Key = Convert.ToBase64String(aesEncryption.Key);
  
              await CreateSampleObjUsingClientEncryptionKeyAsync(base64Key, s3Client);
  
              await CopyObjectAsync(s3Client, base64Key);
          }
          private static async Task CopyObjectAsync(IAmazonS3 s3Client, string base64Key)
          {
              List<CopyPartResponse> uploadResponses = new List<CopyPartResponse>();
  
              // 1. Initialize.
              InitiateMultipartUploadRequest initiateRequest = new InitiateMultipartUploadRequest
              {
                  BucketName = existingBucketName,
                  Key = targetKeyName,
                  ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                  ServerSideEncryptionCustomerProvidedKey = base64Key,
              };
  
              InitiateMultipartUploadResponse initResponse =
                  await s3Client.InitiateMultipartUploadAsync(initiateRequest);
  
              // 2. Upload Parts.
              long partSize = 5 * (long)Math.Pow(2, 20); // 5 MB
              long firstByte = 0;
              long lastByte = partSize;
  
              try
              {
                  // First find source object size. Because object is stored encrypted with
                  // customer provided key you need to provide encryption information in your request.
                  GetObjectMetadataRequest getObjectMetadataRequest = new GetObjectMetadataRequest()
                  {
                      BucketName = existingBucketName,
                      Key = sourceKeyName,
                      ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                      ServerSideEncryptionCustomerProvidedKey = base64Key // " * **source object encryption key ***"
                  };
  
                  GetObjectMetadataResponse getObjectMetadataResponse = await s3Client.GetObjectMetadataAsync(getObjectMetadataRequest);
  
                  long filePosition = 0;
                  for (int i = 1; filePosition < getObjectMetadataResponse.ContentLength; i++)
                  {
                      CopyPartRequest copyPartRequest = new CopyPartRequest
                      {
                          UploadId = initResponse.UploadId,
                          // Source.
                          SourceBucket = existingBucketName,
                          SourceKey = sourceKeyName,
                          // Source object is stored using SSE-C. Provide encryption information.
                          CopySourceServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                          CopySourceServerSideEncryptionCustomerProvidedKey = base64Key, //"***source object encryption key ***",
                          FirstByte = firstByte,
                          // If the last part is smaller then our normal part size then use the remaining size.
                          LastByte = lastByte > getObjectMetadataResponse.ContentLength ?
                              getObjectMetadataResponse.ContentLength - 1 : lastByte,
  
                          // Target.
                          DestinationBucket = existingBucketName,
                          DestinationKey = targetKeyName,
                          PartNumber = i,
                          // Encryption information for the target object.
                          ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                          ServerSideEncryptionCustomerProvidedKey = base64Key
                      };
                      uploadResponses.Add(await s3Client.CopyPartAsync(copyPartRequest));
                      filePosition += partSize;
                      firstByte += partSize;
                      lastByte += partSize;
                  }
  
                  // Step 3: complete.
                  CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest
                  {
                      BucketName = existingBucketName,
                      Key = targetKeyName,
                      UploadId = initResponse.UploadId,
                  };
                  completeRequest.AddPartETags(uploadResponses);
  
                  CompleteMultipartUploadResponse completeUploadResponse =
                      await s3Client.CompleteMultipartUploadAsync(completeRequest);
              }
              catch (Exception exception)
              {
                  Console.WriteLine("Exception occurred: {0}", exception.Message);
                  AbortMultipartUploadRequest abortMPURequest = new AbortMultipartUploadRequest
                  {
                      BucketName = existingBucketName,
                      Key = targetKeyName,
                      UploadId = initResponse.UploadId
                  };
                  s3Client.AbortMultipartUpload(abortMPURequest);
              }
          }
          private static async Task CreateSampleObjUsingClientEncryptionKeyAsync(string base64Key, IAmazonS3 s3Client)
          {
              // List to store upload part responses.
              List<UploadPartResponse> uploadResponses = new List<UploadPartResponse>();
  
              // 1. Initialize.
              InitiateMultipartUploadRequest initiateRequest = new InitiateMultipartUploadRequest
              {
                  BucketName = existingBucketName,
                  Key = sourceKeyName,
                  ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                  ServerSideEncryptionCustomerProvidedKey = base64Key
              };
  
              InitiateMultipartUploadResponse initResponse =
                 await s3Client.InitiateMultipartUploadAsync(initiateRequest);
  
              // 2. Upload Parts.
              long contentLength = new FileInfo(filePath).Length;
              long partSize = 5 * (long)Math.Pow(2, 20); // 5 MB
  
              try
              {
                  long filePosition = 0;
                  for (int i = 1; filePosition < contentLength; i++)
                  {
                      UploadPartRequest uploadRequest = new UploadPartRequest
                      {
                          BucketName = existingBucketName,
                          Key = sourceKeyName,
                          UploadId = initResponse.UploadId,
                          PartNumber = i,
                          PartSize = partSize,
                          FilePosition = filePosition,
                          FilePath = filePath,
                          ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                          ServerSideEncryptionCustomerProvidedKey = base64Key
                      };
  
                      // Upload part and add response to our list.
                      uploadResponses.Add(await s3Client.UploadPartAsync(uploadRequest));
  
                      filePosition += partSize;
                  }
  
                  // Step 3: complete.
                  CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest
                  {
                      BucketName = existingBucketName,
                      Key = sourceKeyName,
                      UploadId = initResponse.UploadId,
                      //PartETags = new List<PartETag>(uploadResponses)
  
                  };
                  completeRequest.AddPartETags(uploadResponses);
  
                  CompleteMultipartUploadResponse completeUploadResponse =
                      await s3Client.CompleteMultipartUploadAsync(completeRequest);
  
              }
              catch (Exception exception)
              {
                  Console.WriteLine("Exception occurred: {0}", exception.Message);
                  AbortMultipartUploadRequest abortMPURequest = new AbortMultipartUploadRequest
                  {
                      BucketName = existingBucketName,
                      Key = sourceKeyName,
                      UploadId = initResponse.UploadId
                  };
                  await s3Client.AbortMultipartUploadAsync(abortMPURequest);
              }
          }
      }
  }
  ```


# Example walkthroughs: Managing access to your Amazon S3 resources<a name="example-walkthroughs-managing-access"></a>

This topic provides the following introductory walkthrough examples for granting access to Amazon S3 resources\. These examples use the AWS Management Console to create resources \(buckets, objects, users\) and grant them permissions\. The examples then show you how to verify permissions using the command line tools, so you don't have to write any code\. We provide commands using both the AWS Command Line Interface \(CLI\) and the AWS Tools for Windows PowerShell\.
+ [Example 1: Bucket owner granting its users bucket permissions ](example-walkthroughs-managing-access-example1.md)

  The IAM users you create in your account have no permissions by default\. In this exercise, you grant a user permission to perform bucket and object operations\.
+ [Example 2: Bucket owner granting cross\-account bucket permissions ](example-walkthroughs-managing-access-example2.md)

  In this exercise, a bucket owner, Account A, grants cross\-account permissions to another AWS account, Account B\. Account B then delegates those permissions to users in its account\. 
+ **Managing object permissions when the object and bucket owners are not the same**

  The example scenarios in this case are about a bucket owner granting object permissions to others, but not all objects in the bucket are owned by the bucket owner\. What permissions does the bucket owner need, and how can it delegate those permissions?

  The AWS account that creates a bucket is called the bucket owner\. The owner can grant other AWS accounts permission to upload objects, and the AWS accounts that create objects own them\. The bucket owner has no permissions on those objects created by other AWS accounts\. If the bucket owner writes a bucket policy granting access to objects, the policy does not apply to objects that are owned by other accounts\. 

  In this case, the object owner must first grant permissions to the bucket owner using an object ACL\. The bucket owner can then delegate those object permissions to others, to users in its own account, or to another AWS account, as illustrated by the following examples\.
  + [Example 3: Bucket owner granting its users permissions to objects it does not own ](example-walkthroughs-managing-access-example3.md)

    In this exercise, the bucket owner first gets permissions from the object owner\. The bucket owner then delegates those permissions to users in its own account\.
  + [Example 4: Bucket owner granting cross\-account permission to objects it does not own](example-walkthroughs-managing-access-example4.md)

    After receiving permissions from the object owner, the bucket owner cannot delegate permission to other AWS accounts because cross\-account delegation is not supported \(see [Permission delegation](access-policy-alternatives-guidelines.md#permission-delegation)\)\. Instead, the bucket owner can create an IAM role with permissions to perform specific operations \(such as get object\) and allow another AWS account to assume that role\. Anyone who assumes the role can then access objects\. This example shows how a bucket owner can use an IAM role to enable this cross\-account delegation\. 

## Before you try the example walkthroughs<a name="before-you-try-example-walkthroughs-manage-access"></a>

These examples use the AWS Management Console to create resources and grant permissions\. And to test permissions, the examples use the command line tools, AWS Command Line Interface \(CLI\) and AWS Tools for Windows PowerShell, so you don't need to write any code\. To test permissions you will need to set up one of these tools\. For more information, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\. 

In addition, when creating resources these examples don't use root credentials of an AWS account\. Instead, you create an administrator user in these accounts to perform these tasks\. 

### About using an administrator user to create resources and grant permissions<a name="about-using-root-credentials"></a>

AWS Identity and Access Management \(IAM\) recommends not using the root credentials of your AWS account to make requests\. Instead, create an IAM user, grant that user full access, and then use that user's credentials to interact with AWS\. We refer to this user as an administrator user\. For more information, go to [Root Account Credentials vs\. IAM User Credentials](https://docs.aws.amazon.com/general/latest/gr/root-vs-iam.html) in the *AWS General Reference* and [IAM Best Practices](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html) in the *IAM User Guide*\.

All example walkthroughs in this section use the administrator user credentials\. If you have not created an administrator user for your AWS account, the topics show you how\. 

Note that to sign in to the AWS Management Console using the user credentials, you will need to use the IAM User Sign\-In URL\. The IAM console provides this URL for your AWS account\. The topics show you how to get the URL\.


# Making requests using IAM user temporary credentials<a name="AuthUsingTempSessionToken"></a>

**Topics**
+ [Making requests using IAM user temporary credentials \- AWS SDK for Java](AuthUsingTempSessionTokenJava.md)
+ [Making requests using IAM user temporary credentials \- AWS SDK for \.NET](AuthUsingTempSessionTokenDotNet.md)
+ [Making requests using AWS account or IAM user temporary credentials \- AWS SDK for PHP](AuthUsingTempSessionTokenPHP.md)
+ [Making requests using IAM user temporary credentials \- AWS SDK for Ruby](AuthUsingTempSessionTokenRuby.md)

 An AWS Account or an IAM user can request temporary security credentials and use them to send authenticated requests to Amazon S3\. This section provides examples of how to use the AWS SDK for Java, \.NET, and PHP to obtain temporary security credentials and use them to authenticate your requests to Amazon S3\.


# Managing Access with ACLs<a name="S3_ACLs_UsingACLs"></a>

**Topics**
+ [Access Control List \(ACL\) Overview](acl-overview.md)
+ [Managing ACLs](managing-acls.md)

 Access control lists \(ACLs\) are one of the resource\-based access policy options \(see [Overview of managing access](access-control-overview.md)\) that you can use to manage access to your buckets and objects\. You can use ACLs to grant basic read/write permissions to other AWS accounts\. There are limits to managing permissions using ACLs\. For example, you can grant permissions only to other AWS accounts; you cannot grant permissions to users in your account\. You cannot grant conditional permissions, nor can you explicitly deny permissions\. ACLs are suitable for specific scenarios\. For example, if a bucket owner allows other AWS accounts to upload objects, permissions to these objects can only be managed using object ACL by the AWS account that owns the object\.

The following introductory topics explain the basic concepts and options that are available for you to manage access to your Amazon S3 resources, and provide guidelines for when to use which access policy options\. 
+ [Introduction to managing access to Amazon S3 resources](s3-access-control.md#intro-managing-access-s3-resources)
+ [Guidelines for using the available access policy options](access-policy-alternatives-guidelines.md)


# What is an S3 Storage Lens export manifest?<a name="storage_lens_whatis_metrics_export_manifest"></a>

Given the large amount of data aggregated, an S3 Storage Lens daily metrics export can be split into multiple files\. The manifest file `manifest.json` describes where the metrics export files for that day are located\. Whenever a new export is delivered, it is accompanied by a new manifest\. Each manifest contained in the `manifest.json` file provides metadata and other basic information about the export\. 

The manifest information includes the following properties:
+ `sourceAccountId` – The account ID of the configuration owner\.
+ `configId` – A unique identifier for the dashboard\.
+ `destinationBucket` – The destination bucket Amazon Resource Name \(ARN\) that the metrics export is placed in\.
+ `reportVersion` – The version of the export\.
+ `reportDate` – The date of the report\.
+ `reportFormat` – The format of the report\.
+ `reportSchema` – The schema of the report\.
+ `reportFiles` – The actual list of the export report files that are in the destination bucket\.



The following is an example of a manifest in a `manifest.json` file for a CSV\-formatted export\.

```
{
   "sourceAccountId":"123456789012",
   "configId":"my-dashboard-configuration-id",
   "destinationBucket":"arn:aws:s3:::destination-bucket",
   "reportVersion":"V_1",
   "reportDate":"2020-11-03",
   "reportFormat":"CSV",
   "reportSchema":"version_number,configuration_id,report_date,aws_account_number,aws_region,storage_class,record_type,record_value,bucket_name,metric_name,metric_value",
   "reportFiles":[
      {
         "key":"DestinationPrefix/StorageLens/123456789012/my-dashboard-configuration-id/V_1/reports/dt=2020-11-03/a38f6bc4-2e3d-4355-ac8a-e2fdcf3de158.csv",
         "size":1603959,
         "md5Checksum":"2177e775870def72b8d84febe1ad3574"
      }
}
```



The following is an example of a manifest in a `manifest.json` file for a Parquet\-formatted export\.

```
{
   "sourceAccountId":"123456789012",
   "configId":"my-dashboard-configuration-id",
   "destinationBucket":"arn:aws:s3:::destination-bucket",
   "reportVersion":"V_1",
   "reportDate":"2020-11-03",
   "reportFormat":"Parquet",
   "reportSchema":"message s3.storage.lens { required string version_number; required string configuration_id; required string report_date; required string aws_account_number; required string aws_region; required string storage_class; required string record_type; required string record_value; required string bucket_name; required string metric_name; required long metric_value; }",
   "reportFiles":[
      {
         "key":"DestinationPrefix/StorageLens/123456789012/my-dashboard-configuration-id/V_1/reports/dt=2020-11-03/bd23de7c-b46a-4cf4-bcc5-b21aac5be0f5.par",
         "size":14714,
         "md5Checksum":"b5c741ee0251cd99b90b3e8eff50b944"
      }
}
```

You can configure your metrics export to be generated as part of your dashboard configuration in the Amazon S3 console or by using the Amazon S3 REST API, AWS CLI, and SDKs\.


# Deleting or emptying a bucket<a name="delete-or-empty-bucket"></a>

In some situations, you may need to delete or empty a bucket that contains objects\. In this section, we'll explain how to delete objects in an unversioned bucket, and how to delete object versions and delete markers in a bucket that has versioning enabled\. For more information about versioning, see [Using versioning](Versioning.md)\. In some situations, you may choose to empty a bucket instead of deleting it\. This section explains various options you can use to delete or empty a bucket that contains objects\.

**Topics**
+ [Delete a bucket](#delete-bucket)
+ [Empty a bucket](#empty-bucket)

## Delete a bucket<a name="delete-bucket"></a>

You can delete a bucket and its content programmatically using the AWS SDKs\. You can also use lifecycle configuration on a bucket to empty its content and then delete the bucket\. There are additional options, such as using Amazon S3 console and AWS CLI, but there are limitations on these methods based on the number of objects in your bucket and the bucket's versioning status\.

**Topics**
+ [Delete a bucket: Using the Amazon S3 console](#delete-bucket-console)
+ [Delete a bucket: Using the AWS CLI](#delete-bucket-awscli)
+ [Delete a bucket: Using the AWS SDKs](#delete-bucket-awssdks)

### Delete a bucket: Using the Amazon S3 console<a name="delete-bucket-console"></a>

The Amazon S3 console supports deleting a bucket that may or may not be empty\. For information about using the Amazon S3 console to delete a bucket, see [How Do I Delete an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/delete-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\.

### Delete a bucket: Using the AWS CLI<a name="delete-bucket-awscli"></a>

You can delete a bucket that contains objects using the AWS CLI only if the bucket does not have versioning enabled\. If your bucket does not have versioning enabled, you can use the `rb` \(remove bucket\) AWS CLI command with `--force` parameter to remove a non\-empty bucket\. This command deletes all objects first and then deletes the bucket\.

```
$ aws s3 rb s3://bucket-name --force  
```

For more information, see [Using High\-Level S3 Commands with the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html) in the AWS Command Line Interface User Guide\.

### Delete a bucket: Using the AWS SDKs<a name="delete-bucket-awssdks"></a>

You can use the AWS SDKs to delete a bucket\. The following sections provide examples of how to delete a bucket using the AWS SDK for Java and \.NET\. First, the code deletes objects in the bucket and then it deletes the bucket\. For information about other AWS SDKs, see [Tools for Amazon Web Services](https://aws.amazon.com/tools/)\.

#### Delete a bucket using the AWS SDK for Java<a name="delete-bucket-sdk-java"></a>

The following Java example deletes a bucket that contains objects\. The example deletes all objects, and then it deletes the bucket\. The example works for buckets with or without versioning enabled\.

**Note**  
For buckets without versioning enabled, you can delete all objects directly and then delete the bucket\. For buckets with versioning enabled, you must delete all object versions before deleting the bucket\.

For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.util.Iterator;

public class DeleteBucket2 {

    public static void main(String[] args) {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Delete all objects from the bucket. This is sufficient
            // for unversioned buckets. For versioned buckets, when you attempt to delete objects, Amazon S3 inserts
            // delete markers for all objects, but doesn't delete the object versions.
            // To delete objects from versioned buckets, delete all of the object versions before deleting
            // the bucket (see below for an example).
            ObjectListing objectListing = s3Client.listObjects(bucketName);
            while (true) {
                Iterator<S3ObjectSummary> objIter = objectListing.getObjectSummaries().iterator();
                while (objIter.hasNext()) {
                    s3Client.deleteObject(bucketName, objIter.next().getKey());
                }

                // If the bucket contains many objects, the listObjects() call
                // might not return all of the objects in the first listing. Check to
                // see whether the listing was truncated. If so, retrieve the next page of objects 
                // and delete them.
                if (objectListing.isTruncated()) {
                    objectListing = s3Client.listNextBatchOfObjects(objectListing);
                } else {
                    break;
                }
            }

            // Delete all object versions (required for versioned buckets).
            VersionListing versionList = s3Client.listVersions(new ListVersionsRequest().withBucketName(bucketName));
            while (true) {
                Iterator<S3VersionSummary> versionIter = versionList.getVersionSummaries().iterator();
                while (versionIter.hasNext()) {
                    S3VersionSummary vs = versionIter.next();
                    s3Client.deleteVersion(bucketName, vs.getKey(), vs.getVersionId());
                }

                if (versionList.isTruncated()) {
                    versionList = s3Client.listNextBatchOfVersions(versionList);
                } else {
                    break;
                }
            }

            // After all objects and object versions are deleted, delete the bucket.
            s3Client.deleteBucket(bucketName);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client couldn't
            // parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Empty a bucket<a name="empty-bucket"></a>

You can empty a bucket's content \(that is, delete all content, but keep the bucket\) programmatically using the AWS SDK\. You can also specify lifecycle configuration on a bucket to expire objects so that Amazon S3 can delete them\. There are additional options, such as using Amazon S3 console and AWS CLI, but there are limitations on this method based on the number of objects in your bucket and the bucket's versioning status\.

**Topics**
+ [Empty a bucket: Using the Amazon S3 console](#empty-bucket-console)
+ [Empty a bucket: Using the AWS CLI](#empty-bucket-awscli)
+ [Empty a bucket: Using lifecycle configuration](#empty-bucket-lifecycle)
+ [Empty a bucket: Using the AWS SDKs](#empty-bucket-awssdks)

### Empty a bucket: Using the Amazon S3 console<a name="empty-bucket-console"></a>

For information about using the Amazon S3 console to empty a bucket, see [How Do I Empty an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/empty-bucket.html) in the *Amazon Simple Storage Service Console User Guide*

### Empty a bucket: Using the AWS CLI<a name="empty-bucket-awscli"></a>

You can empty a bucket using the AWS CLI only if the bucket does not have versioning enabled\. If your bucket does not have versioning enabled, you can use the `rm` \(remove\) AWS CLI command with the `--recursive` parameter to empty a bucket \(or remove a subset of objects with a specific key name prefix\)\. 

The following `rm` command removes objects with key name prefix `doc`, for example, `doc/doc1` and `doc/doc2`\.

```
$ aws s3 rm s3://bucket-name/doc --recursive
```

Use the following command to remove all objects without specifying a prefix\.

```
$ aws s3 rm s3://bucket-name --recursive
```

For more information, see [Using High\-Level S3 Commands with the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html) in the AWS Command Line Interface User Guide\.

**Note**  
You cannot remove objects from a bucket with versioning enabled\. Amazon S3 adds a delete marker when you delete an object, which is what this command will do\. For more information about versioning, see [Using versioning](Versioning.md)\.

### Empty a bucket: Using lifecycle configuration<a name="empty-bucket-lifecycle"></a>

You can configure lifecycle on your bucket to expire objects and request that Amazon S3 delete expired objects\. You can add lifecycle configuration rules to expire all or a subset of objects with a specific key name prefix\. For example, to remove all objects in a bucket, you can set lifecycle rule to expire objects one day after creation\.

If your bucket has versioning enabled, you can also configure the rule to expire noncurrent objects\. To fully empty the contents of a versioning enabled bucket, you will need to configure an expiration policy on both current and noncurrent objects in the bucket\.

For more information, see [Object lifecycle management](object-lifecycle-mgmt.md) and [Understanding object expiration](lifecycle-expire-general-considerations.md)\.

### Empty a bucket: Using the AWS SDKs<a name="empty-bucket-awssdks"></a>

You can use the AWS SDKs to empty a bucket or remove a subset of objects with a specific key name prefix\.

For an example of how to empty a bucket using AWS SDK for Java, see [Delete a bucket using the AWS SDK for Java](#delete-bucket-sdk-java)\. The code deletes all objects, regardless of whether the bucket has versioning enabled or not, and then it deletes the bucket\. To just empty the bucket, make sure you remove the statement that deletes the bucket\. 

For more information about using other AWS SDKs, see [Tools for Amazon Web Services](https://aws.amazon.com/tools/)\.


# Managing S3 Object Lock legal hold<a name="batch-ops-legal-hold"></a>

S3 Object Lock enables you to place a legal hold on an object version\. Like setting a retention period, a legal hold prevents an object version from being overwritten or deleted\. However, a legal hold doesn't have an associated retention period and remains in effect until removed\. 

You can use S3 Batch Operations with Object Lock to add legal holds to *many* Amazon S3 objects at once \. You can do this by listing the target objects in your manifest and submitting that list to Batch Operations\. Your S3 Batch Operations job with Object Lock legal hold runs until completion, until cancellation, or until a failure state is reached\.

S3 Batch Operations verifies that Object Lock is enabled on your S3 bucket before processing any keys in the manifest\. To perform the object operations and bucket level validation, S3 Batch Operations needs `s3:GetBucketObjectLockConfiguration`, and `s3:PutObjectLegalHold` in an IAM role allowing S3 Batch Operations to call S3 Object Lock on your behalf\. 

When you create the S3 Batch Operations job to remove the legal hold, you just need to specify *Off* as the legal hold status\. For more information, see [Managing Amazon S3 object locks](object-lock-managing.md)\.

For information about how to use this operation with the REST API, see `S3PutObjectRetention` in the [CreateJob](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_CreateJob.html) operation in the *Amazon Simple Storage Service API Reference*\. 

For an AWS Command Line Interface example of using this operation, see [Use S3 Batch Operations with S3 Object Lock legal hold](batch-ops-examples-java.md#batch-ops-examples-java-object-lock-legalhold)\. 

For an AWS SDK for Java example, see [S3 Batch Operations examples using the AWS SDK for Java](batch-ops-examples-java.md)\.

## Restrictions and limitations<a name="batch-ops-legal-hold-restrictions"></a>
+ S3 Batch Operations does not make any bucket level changes\.
+ All objects listed in the manifest must be in the same bucket\.
+ Versioning and S3 Object Lock must be configured on the bucket where the job is performed\.
+ The operation works on the latest version of the object unless a version is explicitly specified in the manifest\.
+ `s3:PutObjectLegalHold` permission is required in your IAM role to add or remove legal hold from objects\.
+ `s3:GetBucketObjectLockConfiguration` IAM permission is required to confirm that S3 Object Lock is enabled for the S3 bucket\. 


# Security Best Practices for Amazon S3<a name="security-best-practices"></a>

Amazon S3 provides a number of security features to consider as you develop and implement your own security policies\. The following best practices are general guidelines and don’t represent a complete security solution\. Because these best practices might not be appropriate or sufficient for your environment, treat them as helpful considerations rather than prescriptions\. 

**Topics**
+ [Amazon S3 Preventative Security Best Practices](#security-best-practices-prevent)
+ [Amazon S3 Monitoring and Auditing Best Practices](#security-best-practices-detect)

## Amazon S3 Preventative Security Best Practices<a name="security-best-practices-prevent"></a>

The following best practices for Amazon S3 can help prevent security incidents\.

**Ensure that your Amazon S3 buckets use the correct policies and are not publicly accessible**  
Unless you explicitly require anyone on the internet to be able to read or write to your S3 bucket, you should ensure that your S3 bucket is not public\. The following are some of the steps you can take:  
+ Use Amazon S3 block public access\. With Amazon S3 block public access, account administrators and bucket owners can easily set up centralized controls to limit public access to their Amazon S3 resources that are enforced regardless of how the resources are created\. For more information, see [Using Amazon S3 block public access](access-control-block-public-access.md)\.
+ Identify Amazon S3 bucket policies that allow a wildcard identity such as Principal “\*” \(which effectively means “anyone”\) or allows a wildcard action “\*” \(which effectively allows the user to perform any action in the Amazon S3 bucket\)\.
+ Similarly, note Amazon S3 bucket access control lists \(ACLs\) that provide read, write, or full\-access to “Everyone” or “Any authenticated AWS user\.” 
+ Use the `ListBuckets` API to scan all of your Amazon S3 buckets\. Then use `GetBucketAcl`, `GetBucketWebsite`, and `GetBucketPolicy` to determine whether the bucket has compliant access controls and configuration\.
+ Use [AWS Trusted Advisor](https://docs.aws.amazon.com/awssupport/latest/user/getting-started.html#trusted-advisor) to inspect your Amazon S3 implementation\.
+ Consider implementing on\-going detective controls using the [s3\-bucket\-public\-read\-prohibited](https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-public-read-prohibited.html) and [s3\-bucket\-public\-write\-prohibited](https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-public-write-prohibited.html) managed AWS Config Rules\.
For more information, see [Setting Bucket and Object Access Permissions](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-permissions.html) in the *Amazon Simple Storage Service Console User Guide*\. 

**Implement least privilege access**  
When granting permissions, you decide who is getting what permissions to which Amazon S3 resources\. You enable specific actions that you want to allow on those resources\. Therefore you should grant only the permissions that are required to perform a task\. Implementing least privilege access is fundamental in reducing security risk and the impact that could result from errors or malicious intent\.   
The following tools are available to implement least privilege access:  
+ [IAM user policies](https://docs.aws.amazon.com/AmazonS3/latest/dev/using-with-s3-actions.html) and [Permissions Boundaries for IAM Entities](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html)
+ [Amazon S3 bucket policies](https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html)
+ [Amazon S3 access control lists \(ACLs\)](https://docs.aws.amazon.com/AmazonS3/latest/dev/S3_ACLs_UsingACLs.html)
+ [Service Control Policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html)
For guidance on what to consider when choosing one or more of the preceding mechanisms, see [Introduction to managing access to Amazon S3 resources](s3-access-control.md#intro-managing-access-s3-resources)\.

**Use IAM roles for applications and AWS services that require Amazon S3 access**  
 For applications on Amazon EC2 or other AWS services to access Amazon S3 resources, they must include valid AWS credentials in their AWS API requests\. You should not store AWS credentials directly in the application or Amazon EC2 instance\. These are long\-term credentials that are not automatically rotated and could have a significant business impact if they are compromised\.  
Instead, you should use an IAM role to manage temporary credentials for applications or services that need to access Amazon S3\. When you use a role, you don't have to distribute long\-term credentials \(such as a user name and password or access keys\) to an Amazon EC2 instance or AWS service such as AWS Lambda\. The role supplies temporary permissions that applications can use when they make calls to other AWS resources\.  
For more information, see the following topics in the *IAM User Guide*:  
+ [IAM Roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html)
+ [Common Scenarios for Roles: Users, Applications, and Services](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios.html)

   

**Enable multi\-factor authentication \(MFA\) Delete**  
MFA Delete can help prevent accidental bucket deletions\. If MFA Delete is not enabled, any user with the password of a sufficiently privileged root or IAM user could permanently delete an Amazon S3 object\.  
MFA Delete requires additional authentication for either of the following operations:  
+ Changing the versioning state of your bucket
+ Permanently deleting an object version
For more information, see [MFA delete](Versioning.md#MultiFactorAuthenticationDelete)\.

**Consider encryption of data at rest**  
You have the following options for protecting data at rest in Amazon S3:  
+ **Server\-Side Encryption** – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects\. Server\-side encryption can help reduce risk to your data by encrypting the data with a key that is stored in a different mechanism than the mechanism that stores the data itself\. 

  Amazon S3 provides these server\-side encryption options:
  + Server\-side encryption with Amazon S3‐managed keys \(SSE\-S3\)\.
  + Server\-side encryption with customer master keys stored in AWS Key Management Service \(SSE\-KMS\)\.
  + Server\-side encryption with customer\-provided keys \(SSE\-C\)\.

  For more information, see [Protecting data using server\-side encryption](serv-side-encryption.md)\.
+ **Client\-Side Encryption** – Encrypt data client\-side and upload the encrypted data to Amazon S3\. In this case, you manage the encryption process, the encryption keys, and related tools\. As with server\-side encryption, client\-side encryption can help reduce risk by encrypting the data with a key that is stored in a different mechanism than the mechanism that stores the data itself\. 

  Amazon S3 provides multiple client\-side encryption options\. For more information, see [Protecting data using client\-side encryption](UsingClientSideEncryption.md)\.

**Enforce encryption of data in transit**  
You can use HTTPS \(TLS\) to help prevent potential attackers from eavesdropping on or manipulating network traffic using person\-in\-the\-middle or similar attacks\. You should allow only encrypted connections over HTTPS \(TLS\) using the [aws:SecureTransport](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition_operators.html#Conditions_Boolean) condition on Amazon S3 bucket policies\.  
Also consider implementing on\-going detective controls using the [s3\-bucket\-ssl\-requests\-only](https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html) managed AWS Config rule\. 

**Consider S3 Object Lock**  
[S3 Object Lock](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html) enables you to store objects using a "Write Once Read Many" \(WORM\) model\. S3 Object Lock can help prevent accidental or inappropriate deletion of data\. For example, you could use S3 Object Lock to help protect your AWS CloudTrail logs\.

**Enable versioning**  
Versioning is a means of keeping multiple variants of an object in the same bucket\. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket\. With versioning, you can easily recover from both unintended user actions and application failures\.   
Also consider implementing on\-going detective controls using the [s3\-bucket\-versioning\-enabled](https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-versioning-enabled.html) managed AWS Config rule\.  
For more information, see [Using versioning](Versioning.md)\. 

**Consider Amazon S3 cross\-region replication**  
Although Amazon S3 stores your data across multiple geographically diverse Availability Zones by default, compliance requirements might dictate that you store data at even greater distances\. Cross\-region replication \(CRR\) allows you to replicate data between distant AWS Regions to help satisfy these requirements\. CRR enables automatic, asynchronous copying of objects across buckets in different AWS Regions\. For more information, see [Replication](replication.md)\.  
CRR requires that both source and destination S3 buckets have versioning enabled\.
Also consider implementing on\-going detective controls using the [s3\-bucket\-replication\-enabled](https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-replication-enabled.html) managed AWS Config rule\.

**Consider VPC endpoints for Amazon S3 access**  
A VPC endpoint for Amazon S3 is a logical entity within an virtual private cloud \(VPC\) that allows connectivity only to Amazon S3\. You can use Amazon S3 bucket policies to control access to buckets from specific VPC endpoints, or specific VPCs\. A VPC endpoint can help prevent traffic from potentially traversing the open internet and being subject to open internet environment\.  
VPC endpoints for Amazon S3 provide multiple ways to control access to your Amazon S3 data:  
+ You can control the requests, users, or groups that are allowed through a specific VPC endpoint\.
+ You can control which VPCs or VPC endpoints have access to your S3 buckets by using S3 bucket policies\.
+ You can help prevent data exfiltration by using a VPC that does not have an internet gateway\.
For more information, see [Example Bucket Policies for VPC Endpoints for Amazon S3](example-bucket-policies-vpc-endpoint.md)\. 

## Amazon S3 Monitoring and Auditing Best Practices<a name="security-best-practices-detect"></a>

The following best practices for Amazon S3 can help detect potential security weaknesses and incidents\.

**Identify and audit all your Amazon S3 buckets**  
Identification of your IT assets is a crucial aspect of governance and security\. You need to have visibility of all your Amazon S3 resources to assess their security posture and take action on potential areas of weakness\.  
Use Tag Editor to identify security\-sensitive or audit\-sensitive resources, then use those tags when you need to search for these resources\. For more information, see [Searching for Resources to Tag](https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html)\.   
Use Amazon S3 inventory to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs\. For more information, see [ Amazon S3 inventory](storage-inventory.md)\.  
Create resource groups for your Amazon S3 resources\. For more information, see [What Is AWS Resource Groups?](https://docs.aws.amazon.com/ARG/latest/userguide/welcome.html) 

**Implement monitoring using AWS monitoring tools**  
Monitoring is an important part of maintaining the reliability, security, availability, and performance of Amazon S3 and your AWS solutions\. AWS provides several tools and services to help you monitor Amazon S3 and your other AWS services\. For example, you can monitor CloudWatch metrics for Amazon S3, particularly `PutRequests`, `GetRequests`, `4xxErrors`, and `DeleteRequests`\. For more information, see [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md) and, [Monitoring Amazon S3](monitoring-overview.md)\.  
For a second example, see [Example: Amazon S3 Bucket Activity](http://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudwatch-alarms-for-cloudtrail.html#cloudwatch-alarms-for-cloudtrail-s3-bucket-activity)\. This example describes how to create an Amazon CloudWatch alarm that is triggered when an Amazon S3 API call is made to PUT or DELETE bucket policy, bucket lifecycle, or bucket replication, or to PUT a bucket ACL\.

**Enable Amazon S3 server access logging**  
Server access logging provides detailed records of the requests that are made to a bucket\. Server access logs can assist you in security and access audits, help you learn about your customer base, and understand your Amazon S3 bill\. For instructions on enabling server access logging, see [Amazon S3 server access logging](ServerLogs.md)\.  
Also consider implementing on\-going detective controls using the [s3\-bucket\-logging\-enabled](https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-logging-enabled.html) AWS Config managed rule\. 

**Use AWS CloudTrail**  
AWS CloudTrail provides a record of actions taken by a user, a role, or an AWS service in Amazon S3\. You can use information collected by CloudTrail to determine the request that was made to Amazon S3, the IP address from which the request was made, who made the request, when it was made, and additional details\. For example, you can identify CloudTrail entries for Put actions that impact data access, in particular `PutBucketAcl`, `PutObjectAcl`, `PutBucketPolicy`, and `PutBucketWebsite`\. When you set up your AWS account, CloudTrail is enabled by default\. You can view recent events in the CloudTrail console\. To create an ongoing record of activity and events for your Amazon S3 buckets, you can create a trail in the CloudTrail console\. For more information, see [Logging Data Events for Trails](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html) in the *AWS CloudTrail User Guide*\.  
When you create a trail, you can configure CloudTrail to log data events\. Data events are records of resource operations performed on or within a resource\. In Amazon S3, data events record object\-level API activity for individual buckets\. CloudTrail supports a subset of Amazon S3 object\-level API operations such as `GetObject`, `DeleteObject`, and `PutObject`\. For more information about how CloudTrail works with Amazon S3, see [Logging Amazon S3 API calls using AWS CloudTrail](cloudtrail-logging.md)\. In the Amazon S3 console, you can also configure your S3 buckets to [enable object\-level logging](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html) for CloudTrail\.  
AWS Config provides a managed rule \(`cloudtrail-s3-dataevents-enabled`\) that you can use to confirm that at least one CloudTrail trail is logging data events for your S3 buckets\. For more information, see [https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-s3-dataevents-enabled.html](https://docs.aws.amazon.com/config/latest/developerguide/cloudtrail-s3-dataevents-enabled.html) in the *AWS Config Developer Guide*\.

**Enable AWS Config**  
Several of the best practices listed in this topic suggest creating AWS Config rules\. AWS Config enables you to assess, audit, and evaluate the configurations of your AWS resources\. AWS Config monitors resource configurations, allowing you to evaluate the recorded configurations against the desired secure configurations\. Using AWS Config, you can review changes in configurations and relationships between AWS resources, investigate detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines\. This can help you simplify compliance auditing, security analysis, change management, and operational troubleshooting\. For more information, see [Setting Up AWS Config with the Console](https://docs.aws.amazon.com/config/latest/developerguide/gs-console.html) in the *AWS Config Developer Guide*\. When specifying the resource types to record, ensure that you include Amazon S3 resources\.  
For an example of how to use AWS Config to monitor for and respond to Amazon S3 buckets that allow public access, see [How to Use AWS Config to Monitor for and Respond to Amazon S3 Buckets Allowing Public Access](https://aws.amazon.com/blogs/security/how-to-use-aws-config-to-monitor-for-and-respond-to-amazon-s3-buckets-allowing-public-access/) on the *AWS Security Blog*\. 

**Consider using Amazon Macie with Amazon S3**  
Macie uses machine learning to automatically discover, classify, and protect sensitive data in AWS\. Macie recognizes sensitive data such as personally identifiable information \(PII\) or intellectual property\. It provides you with dashboards and alerts that give visibility into how this data is being accessed or moved\. For more information, see [What Is Amazon Macie?](https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html)

**Monitor AWS security advisories**  
You should regularly check security advisories posted in Trusted Advisor for your AWS account\. In particular, note warnings about Amazon S3 buckets with “open access permissions\.” You can do this programmatically using [describe\-trusted\-advisor\-checks](https://docs.aws.amazon.com/cli/latest/reference/support/describe-trusted-advisor-checks.html)\.  
Further, actively monitor the primary email address registered to each of your AWS accounts\. AWS will contact you, using this email address, about emerging security issues that might affect you\.  
AWS operational issues with broad impact are posted on the [AWS Service Health Dashboard](https://status.aws.amazon.com/)\. Operational issues are also posted to individual accounts via the Personal Health Dashboard\. For more information, see the [AWS Health Documentation](https://docs.aws.amazon.com/health/)\.


# Meeting compliance requirements using S3 Replication Time Control \(S3 RTC\)<a name="replication-time-control"></a>

S3 Replication Time Control \(S3 RTC\) helps you meet compliance or business requirements for data replication and provides visibility into Amazon S3 replication times\. S3 RTC replicates most objects that you upload to Amazon S3 in seconds, and 99\.99 percent of those objects within 15 minutes\. 

S3 RTC by default includes S3 replication metrics and S3 event notifications, with which you can monitor the total number of S3 API operations that are pending replication, the total size of objects pending replication, and the maximum replication time\. Replication metrics can be enabled independently of S3 RTC, see [Monitoring progress with replication metrics](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-metrics.html)\. Additionally, S3 RTC provides `OperationMissedThreshold` and `OperationReplicatedAfterThreshold` events that notify the bucket owner if object replication exceeds or replicates after the 15\-minute threshold\. 

With S3 RTC, Amazon S3 events can notify you in the rare instance when objects do not replicate within 15 minutes and when those objects replicate successfully to their destination Region\. Amazon S3 events are available through Amazon SQS, Amazon SNS, or AWS Lambda\. For more information, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.

**Topics**
+ [Enabling S3 Replication Time Control](#enabling-replication-time-control)
+ [Replication metrics with S3 RTC](#using-replication-metrics)
+ [Using Amazon S3 event notifications to track replication objects](#using-s3-events-to-track-rtc)
+ [Best practices and guidelines for S3 RTC](rtc-best-practices.md)

## Enabling S3 Replication Time Control<a name="enabling-replication-time-control"></a>

You can start using S3 Replication Time Control \(S3 RTC\) with a new or existing replication rule\. You can choose to apply your replication rule to an entire S3 bucket, or to Amazon S3 objects with a specific prefix or tag\. When you enable S3 RTC, replication metrics are also enabled on your replication rule\. 

If you are using the latest version of the replication configuration \(that is, you specify the `Filter` element in a replication configuration rule\), Amazon S3 does not replicate the delete marker by default\. However you can add delete marker replication to non\-tag\-based rules\.

**Note**  
 Replication metrics are billed at the same rate as Amazon CloudWatch custom metrics\. For information, see [Amazon CloudWatch pricing](https://aws.amazon.com/cloudwatch/pricing/)\. 

For more information about creating a rule with S3 RTC, see [Replication walkthroughs: S3 RTC configuration](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-walkthrough-5.html)\.

## Replication metrics with S3 RTC<a name="using-replication-metrics"></a>

Replication rules with S3 Replication Time Control \(S3 RTC\) enabled publishes replication metrics\. With replication metrics, you can monitor the total number of S3 API operations that are pending replication, the total size of objects pending replication, and the maximum replication time to the destination Region\. You can then monitor each dataset that you replicate separately\.

Replication metrics are available within 15 minutes of enabling S3 RTC\. Replication metrics are available through the [Amazon S3 console](https://console.aws.amazon.com/s3/), the [Amazon S3 API](https://docs.aws.amazon.com/AmazonS3/latest/API/), the [AWS SDKs](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html), the [AWS Command Line Interface \(AWS CLI\)](https://docs.aws.amazon.com/cli/latest/reference/), and [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/)\. For more information, see [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)\.

For more information about finding replication metrics via the Amazon S3 console, see [How do I view replication metrics](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/viewing-replication-metrics.html) in the *Amazon Simple Storage Service Console User Guide*\.

## Using Amazon S3 event notifications to track replication objects<a name="using-s3-events-to-track-rtc"></a>

You can track replication time for objects that did not replicate within 15 minutes by monitoring specific event notifications that S3 Replication Time Control \(S3 RTC\) publishes\. These events are published when an object that was eligible for replication using S3 RTC didn't replicate within 15 minutes, and when that object replicates to the destination Region\. 

Replication events are available within 15 minutes of enabling S3 RTC\. Amazon S3 events are available through Amazon SQS, Amazon SNS, or AWS Lambda\. For more information, see [Configuring Amazon S3 event notifications](https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html)\.


# Introduction to Amazon S3<a name="Introduction"></a>

This introduction to Amazon Simple Storage Service \(Amazon S3\) provides a detailed summary of this web service\. After reading this section, you should have a good idea of what it offers and how it can fit in with your business\.

**Topics**
+ [Overview of Amazon S3 and this guide](#overview)
+ [Advantages of using Amazon S3](#features)
+ [Amazon S3 concepts](#CoreConcepts)
+ [Amazon S3 features](#S3Features)
+ [Amazon S3 application programming interfaces \(API\)](#API)
+ [Paying for Amazon S3](#PayingforStorage)
+ [Related services](#RelatedAmazonWebServices)

## Overview of Amazon S3 and this guide<a name="overview"></a>

Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web\.

This guide describes how you send requests to create buckets, store and retrieve your objects, and manage permissions on your resources\. The guide also describes access control and the authentication process\. Access control defines who can access objects and buckets within Amazon S3, and the type of access \(for example, READ and WRITE\)\. The authentication process verifies the identity of a user who is trying to access Amazon Web Services \(AWS\)\.

## Advantages of using Amazon S3<a name="features"></a>

Amazon S3 is intentionally built with a minimal feature set that focuses on simplicity and robustness\. Following are some of the advantages of using Amazon S3:
+ **Creating buckets** – Create and name a bucket that stores data\. Buckets are the fundamental containers in Amazon S3 for data storage\.
+ **Storing data** – Store an infinite amount of data in a bucket\. Upload as many objects as you like into an Amazon S3 bucket\. Each object can contain up to 5 TB of data\. Each object is stored and retrieved using a unique developer\-assigned key\.
+ **Downloading data** – Download your data or enable others to do so\. Download your data anytime you like, or allow others to do the same\.
+ **Permissions** – Grant or deny access to others who want to upload or download data into your Amazon S3 bucket\. Grant upload and download permissions to three types of users\. Authentication mechanisms can help keep data secure from unauthorized access\.
+ **Standard interfaces** – Use standards\-based REST and SOAP interfaces designed to work with any internet\-development toolkit\.
**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

## Amazon S3 concepts<a name="CoreConcepts"></a>

This section describes key concepts and terminology you need to understand to use Amazon S3 effectively\. They are presented in the order that you will most likely encounter them\.

**Topics**
+ [Buckets](#BasicsBucket)
+ [Objects](#BasicsObjects)
+ [Keys](#BasicsKeys)
+ [Regions](#Regions)
+ [Amazon S3 data consistency model](#ConsistencyModel)

### Buckets<a name="BasicsBucket"></a>

 A bucket is a container for objects stored in Amazon S3\. Every object is contained in a bucket\. For example, if the object named `photos/puppy.jpg` is stored in the `awsexamplebucket1` bucket in the US West \(Oregon\) Region, then it is addressable using the URL `https://awsexamplebucket1.s3.us-west-2.amazonaws.com/photos/puppy.jpg`\.

 Buckets serve several purposes: 
+ They organize the Amazon S3 namespace at the highest level\.
+ They identify the account responsible for storage and data transfer charges\.
+ They play a role in access control\.
+ They serve as the unit of aggregation for usage reporting\.

You can configure buckets so that they are created in a specific AWS Region\. For more information, see [Accessing a Bucket](UsingBucket.md#access-bucket-intro)\. You can also configure a bucket so that every time an object is added to it, Amazon S3 generates a unique version ID and assigns it to the object\. For more information, see [Using Versioning](Versioning.md)\.

 For more information about buckets, see [Working with Amazon S3 Buckets](UsingBucket.md)\. 

### Objects<a name="BasicsObjects"></a>

Objects are the fundamental entities stored in Amazon S3\. Objects consist of object data and metadata\. The data portion is opaque to Amazon S3\. The metadata is a set of name\-value pairs that describe the object\. These include some default metadata, such as the date last modified, and standard HTTP metadata, such as `Content-Type`\. You can also specify custom metadata at the time the object is stored\.

An object is uniquely identified within a bucket by a key \(name\) and a version ID\. For more information, see [Keys](#BasicsKeys) and [Using Versioning](Versioning.md)\.

### Keys<a name="BasicsKeys"></a>

A key is the unique identifier for an object within a bucket\. Every object in a bucket has exactly one key\. The combination of a bucket, key, and version ID uniquely identify each object\. So you can think of Amazon S3 as a basic data map between "bucket \+ key \+ version" and the object itself\. Every object in Amazon S3 can be uniquely addressed through the combination of the web service endpoint, bucket name, key, and optionally, a version\. For example, in the URL `https://doc.s3.amazonaws.com/2006-03-01/AmazonS3.wsdl`, "`doc`" is the name of the bucket and "`2006-03-01/AmazonS3.wsdl`" is the key\.

 For more information about object keys, see [Object Keys](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-keys)\. 

### Regions<a name="Regions"></a>

You can choose the geographical AWS Region where Amazon S3 will store the buckets that you create\. You might choose a Region to optimize latency, minimize costs, or address regulatory requirements\. Objects stored in a Region never leave the Region unless you explicitly transfer them to another Region\. For example, objects stored in the Europe \(Ireland\) Region never leave it\. 

**Note**  
You can only access Amazon S3 and its features in AWS Regions that are enabled for your account\.

 For a list of Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\. 

### Amazon S3 data consistency model<a name="ConsistencyModel"></a>

Amazon S3 provides strong read\-after\-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions\. This applies to both writes to new objects as well as PUTs that overwrite existing objects and DELETEs\. In addition, read operations on Amazon S3 Select, Amazon S3 Access Control Lists, Amazon S3 Object Tags, and object metadata \(e\.g\. HEAD object\) are strongly consistent\. 

Updates to a single key are atomic\. For example, if you PUT to an existing key from one thread and perform a GET on the same key from a second thread concurrently, you will get either the old data or the new data, but never partial or corrupt data\.

Amazon S3 achieves high availability by replicating data across multiple servers within AWS data centers\. If a PUT request is successful, your data is safely stored\. Any read \(GET or LIST\) that is initiated following the receipt of a successful PUT response will return the data written by the PUT\. Here are examples of this behavior: 
+ A process writes a new object to Amazon S3 and immediately lists keys within its bucket\. The new object will appear in the list\.
+ A process replaces an existing object and immediately tries to read it\. Amazon S3 will return the new data\. 
+  A process deletes an existing object and immediately tries to read it\. Amazon S3 will not return any data as the object has been deleted\. 
+  A process deletes an existing object and immediately lists keys within its bucket\. The object will not appear in the listing\. 

**Note**  
Amazon S3 does not support object locking for concurrent writers\. If two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins\. If this is an issue, you will need to build an object\-locking mechanism into your application 
Updates are key\-based\. There is no way to make atomic updates across keys\. For example, you cannot make the update of one key dependent on the update of another key unless you design this functionality into your application\.

Bucket configurations have an eventual consistency model\. Specifically:
+ If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list\.
+ If you enable versioning on a bucket for the first time, it might take a short amount of time for the change to be fully propagated\. We recommend that you wait for 15 minutes after enabling versioning before issuing write operations \(PUT or DELETE\) on objects in the bucket\.

#### Concurrent applications<a name="ApplicationConcurrency"></a>

This section provides examples of behavior to be expected from Amazon S3 when multiple clients are writing to the same items\.

In this example, both W1 \(write 1\) and W2 \(write 2\) complete before the start of R1 \(read 1\) and R2 \(read 2\)\. Because S3 is strongly consistent, R1 and R2 both return `color = ruby`\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/consistency1.png)



In the next example, W2 does not complete before the start of R1\. Therefore, R1 might return `color = ruby` or `color = garnet`\. However, since W1 and W2 finish before the start of R2, R2 returns `color = garnet`\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/consistency2.png)



In the last example, W2 begins before W1 has received an acknowledgement\. Therefore, these writes are considered concurrent\. Amazon S3 internally uses last\-writer\-wins semantics to determine which write takes precedence\. However, the order in which Amazon S3 receives the requests and the order in which applications receive acknowledgements cannot be predicted due to factors such as network latency\. For example, W2 might be initiated by an Amazon EC2 instance in the same region while W1 might be initiated by a host that is further away\. The best way to determine the final value is to perform a read after both writes have been acknowledged\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/consistency3.png)



## Amazon S3 features<a name="S3Features"></a>

This section describes important Amazon S3 features\.

**Topics**
+ [Storage classes](#RRS)
+ [Bucket policies](#BucketPolicies)
+ [AWS identity and access management](#AWSIdentityandAccessManagement)
+ [Access control lists](#S3_ACLs)
+ [Versioning](#Versions)
+ [Operations](#BasicsOperations)

### Storage classes<a name="RRS"></a>

 Amazon S3 offers a range of storage classes designed for different use cases\. These include Amazon S3 STANDARD for general\-purpose storage of frequently accessed data, Amazon S3 STANDARD\_IA for long\-lived, but less frequently accessed data, and S3 Glacier for long\-term archive\.

For more information, see [Amazon S3 storage classes](storage-class-intro.md)\.

### Bucket policies<a name="BucketPolicies"></a>

Bucket policies provide centralized access control to buckets and objects based on a variety of conditions, including Amazon S3 operations, requesters, resources, and aspects of the request \(for example, IP address\)\. The policies are expressed in the *access policy language* and enable centralized management of permissions\. The permissions attached to a bucket apply to all of the bucket's objects that are owned by the bucket owner account\.

Both individuals and companies can use bucket policies\. When companies register with Amazon S3, they create an *account*\. Thereafter, the company becomes synonymous with the account\. Accounts are financially responsible for the AWS resources that they \(and their employees\) create\. Accounts have the power to grant bucket policy permissions and assign employees permissions based on a variety of conditions\. For example, an account could create a policy that gives a user write access:
+ To a particular S3 bucket
+ From an account's corporate network
+ During business hours

An account can grant one user limited read and write access, but allow another to create and delete buckets also\. An account could allow several field offices to store their daily reports in a single bucket\. It could allow each office to write only to a certain set of names \(for example, "Nevada/\*" or "Utah/\*"\) and only from the office's IP address range\.

Unlike access control lists \(described later\), which can add \(grant\) permissions only on individual objects, policies can either add or deny permissions across all \(or a subset\) of objects within a bucket\. With one request, an account can set the permissions of any number of objects in a bucket\. An account can use wildcards \(similar to regular expression operators\) on Amazon Resource Names \(ARNs\) and other values\. The account could then control access to groups of objects that begin with a common [prefix](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix) or end with a given extension, such as *\.html*\.

Only the bucket owner is allowed to associate a policy with a bucket\. Policies \(written in the access policy language\) *allow* or *deny* requests based on the following:
+ Amazon S3 bucket operations \(such as `PUT ?acl`\), and object operations \(such as `PUT Object`, or `GET Object`\)
+ Requester
+ Conditions specified in the policy

An account can control access based on specific Amazon S3 operations, such as `GetObject`, `GetObjectVersion`, `DeleteObject`, or `DeleteBucket`\.

The conditions can be such things as IP addresses, IP address ranges in CIDR notation, dates, user agents, HTTP referrer, and transports \(HTTP and HTTPS\)\. 

For more information, see [Using Bucket Policies and User Policies](using-iam-policies.md)\.

### AWS identity and access management<a name="AWSIdentityandAccessManagement"></a>

You can use AWS Identity and Access Management \(IAM\) to manage access to your Amazon S3 resources\.

For example, you can use IAM with Amazon S3 to control the type of access a user or group of users has to specific parts of an Amazon S3 bucket your AWS account owns\. 

For more information about IAM, see the following:
+ [AWS Identity and Access Management \(IAM\)](https://aws.amazon.com/iam/)
+ [Getting started](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started.html)
+ [IAM User Guide](https://docs.aws.amazon.com/IAM/latest/UserGuide/)

### Access control lists<a name="S3_ACLs"></a>

You can control access to each of your buckets and objects using an access control list \(ACL\)\. For more information, see [Managing Access with ACLs](S3_ACLs_UsingACLs.md)\.

### Versioning<a name="Versions"></a>

You can use *versioning* to keep multiple versions of an object in the same bucket\. For more information, see [Object Versioning](ObjectVersioning.md)\.

### Operations<a name="BasicsOperations"></a>

Following are the most common operations that you'll run through the API\.

**Common operations**
+ **Create a bucket** – Create and name your own bucket in which to store your objects\.
+ **Write an object** – Store data by creating or overwriting an object\. When you write an object, you specify a unique key in the namespace of your bucket\. This is also a good time to specify any access control you want on the object\.
+ **Read an object** – Read data back\. You can download the data via HTTP or BitTorrent\.
+ **Delete an object** – Delete some of your data\.
+ **List keys** – List the keys contained in one of your buckets\. You can filter the key list based on a prefix\.

These operations and all other functionality are described in detail throughout this guide\.

## Amazon S3 application programming interfaces \(API\)<a name="API"></a>

The Amazon S3 architecture is designed to be programming language\-neutral, using AWS supported interfaces to store and retrieve objects\. 

Amazon S3 provides a REST and a SOAP interface\. They are similar, but there are some differences\. For example, in the REST interface, metadata is returned in HTTP headers\. Because we only support HTTP requests of up to 4 KB \(not including the body\), the amount of metadata you can supply is restricted\. 

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

### The REST interface<a name="UsingRESTAPI"></a>

The REST API is an HTTP interface to Amazon S3\. Using REST, you use standard HTTP requests to create, fetch, and delete buckets and objects\.

You can use any toolkit that supports HTTP to use the REST API\. You can even use a browser to fetch objects, as long as they are anonymously readable\.

The REST API uses the standard HTTP headers and status codes, so that standard browsers and toolkits work as expected\. In some areas, we have added functionality to HTTP \(for example, we added headers to support access control\)\. In these cases, we have done our best to add the new functionality in a way that matched the style of standard HTTP usage\.

### The SOAP interface<a name="UsingSOAPAPI"></a>

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

The SOAP API provides a SOAP 1\.1 interface using document literal encoding\. The most common way to use SOAP is to download the WSDL \(see [https://doc\.s3\.amazonaws\.com/2006\-03\-01/AmazonS3\.wsdl](https://doc.s3.amazonaws.com/2006-03-01/AmazonS3.wsdl)\), use a SOAP toolkit such as Apache Axis or Microsoft \.NET to create bindings, and then write code that uses the bindings to call Amazon S3\.

## Paying for Amazon S3<a name="PayingforStorage"></a>

Pricing for Amazon S3 is designed so that you don't have to plan for the storage requirements of your application\. Most storage providers force you to purchase a predetermined amount of storage and network transfer capacity: If you exceed that capacity, your service is shut off or you are charged high overage fees\. If you do not exceed that capacity, you pay as though you used it all\. 

Amazon S3 charges you only for what you actually use, with no hidden fees and no overage charges\. This gives developers a variable\-cost service that can grow with their business while enjoying the cost advantages of the AWS infrastructure\.

Before storing anything in Amazon S3, you must register with the service and provide a payment method that is charged at the end of each month\. There are no setup fees to begin using the service\. At the end of the month, your payment method is automatically charged for that month's usage\.

For information about paying for Amazon S3 storage, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\.

## Related services<a name="RelatedAmazonWebServices"></a>

After you load your data into Amazon S3, you can use it with other AWS services\. The following are the services you might use most frequently:
+ **Amazon Elastic Compute Cloud \(Amazon EC2\)** – This service provides virtual compute resources in the cloud\. For more information, see the [Amazon EC2 product details page](https://aws.amazon.com/ec2/)\.
+ **Amazon EMR** – This service enables businesses, researchers, data analysts, and developers to easily and cost\-effectively process vast amounts of data\. It uses a hosted Hadoop framework running on the web\-scale infrastructure of Amazon EC2 and Amazon S3\. For more information, see the [Amazon EMR product details page](https://aws.amazon.com/elasticmapreduce/)\.
+ **AWS Snowball** – This service accelerates transferring large amounts of data into and out of AWS using physical storage devices, bypassing the internet\. Each AWS Snowball device type can transport data at faster\-than internet speeds\. This transport is done by shipping the data in the devices through a regional carrier\. For more information, see the [AWS Snowball product details page](https://aws.amazon.com/snowball)\. 


# Upload an object using the AWS SDK for \.NET<a name="UploadObjSingleOpNET"></a>

**Example**  
The following C\# code example creates two objects with two `PutObjectRequest` requests:  
+ The first `PutObjectRequest` request saves a text string as sample object data\. It also specifies the bucket and object key names\. 
+ The second `PutObjectRequest` request uploads a file by specifying the file name\. This request also specifies the `ContentType` header and optional object metadata \(a title\)\. 
For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class UploadObjectTest
    {
        private const string bucketName = "*** bucket name ***";
        // For simplicity the example creates two objects from the same file.
        // You specify key names for these objects.
        private const string keyName1 = "*** key name for first object created ***";
        private const string keyName2 = "*** key name for second object created ***";
        private const string filePath = @"*** file path ***";
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.EUWest1; 

        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            WritingAnObjectAsync().Wait();
        }

        static async Task WritingAnObjectAsync()
        {
            try
            {
                // 1. Put object-specify only key name for the new object.
                var putRequest1 = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName1,
                    ContentBody = "sample text"
                };

                PutObjectResponse response1 = await client.PutObjectAsync(putRequest1);

                // 2. Put the object-set ContentType and add metadata.
                var putRequest2 = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName2,
                    FilePath = filePath,
                    ContentType = "text/plain"
                };
                
                putRequest2.Metadata.Add("x-amz-meta-title", "someTitle");
                PutObjectResponse response2 = await client.PutObjectAsync(putRequest2);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine(
                        "Error encountered ***. Message:'{0}' when writing an object"
                        , e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine(
                    "Unknown encountered on server. Message:'{0}' when writing an object"
                    , e.Message);
            }
        }
    }
}
```


# Setting the requestPayment Bucket Configuration<a name="RequesterPaysBucketConfiguration"></a>

Only the bucket owner can set the `RequestPaymentConfiguration.payer` configuration value of a bucket to `BucketOwner`, the default, or `Requester`\. Setting the `requestPayment` resource is optional\. By default, the bucket is not a Requester Pays bucket\.

To revert a Requester Pays bucket to a regular bucket, you use the value `BucketOwner`\. Typically, you would use `BucketOwner` when uploading data to the Amazon S3 bucket, and then you would set the value to `Requester` before publishing the objects in the bucket\.

**To set requestPayment**
+ Use a `PUT` request to set the `Payer` value to `Requester` on a specified bucket\.

  ```
  1. PUT ?requestPayment HTTP/1.1
  2. Host: [BucketName].s3.amazonaws.com
  3. Content-Length: 173
  4. Date: Wed, 01 Mar 2009 12:00:00 GMT
  5. Authorization: AWS [Signature]
  6. 
  7. <RequestPaymentConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  8. <Payer>Requester</Payer>
  9. </RequestPaymentConfiguration>
  ```

If the request succeeds, Amazon S3 returns a response similar to the following\.

```
1. HTTP/1.1 200 OK
2. x-amz-id-2: [id]
3. x-amz-request-id: [request_id]
4. Date: Wed, 01 Mar 2009 12:00:00 GMT
5. Content-Length: 0
6. Connection: close
7. Server: AmazonS3
8. x-amz-request-charged:requester
```

You can set Requester Pays only at the bucket level; you cannot set Requester Pays for specific objects within the bucket\.

You can configure a bucket to be `BucketOwner` or `Requester` at any time\. Realize, however, that there might be a small delay, on the order of minutes, before the new configuration value takes effect\.

**Note**  
Bucket owners who give out presigned URLs should think twice before configuring a bucket to be Requester Pays, especially if the URL has a very long lifetime\. The bucket owner is charged each time the requester uses a presigned URL that uses the bucket owner's credentials\. 


# Protecting data using server\-side encryption with customer\-provided encryption keys \(SSE\-C\)<a name="ServerSideEncryptionCustomerKeys"></a>

Server\-side encryption is about protecting data at rest\. Server\-side encryption encrypts only the object data, not object metadata\. Using server\-side encryption with customer\-provided encryption keys \(SSE\-C\) allows you to set your own encryption keys\. With the encryption key you provide as part of your request, Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects\. Therefore, you don't need to maintain any code to perform data encryption and decryption\. The only thing you do is manage the encryption keys you provide\. 

When you upload an object, Amazon S3 uses the encryption key you provide to apply AES\-256 encryption to your data and removes the encryption key from memory\.  When you retrieve an object, you must provide the same encryption key as part of your request\. Amazon S3 first verifies that the encryption key you provided matches and then decrypts the object before returning the object data to you\. 

**Important**  
Amazon S3 does not store the encryption key you provide\. Instead, it stores a randomly salted HMAC value of the encryption key to validate future requests\. The salted HMAC value cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object\. That means if you lose the encryption key, you lose the object\. 

## SSE\-C overview<a name="sse-c-highlights"></a>

This section provides an overview of SSE\-C:
+  You must use HTTPS\. 
**Important**  
Amazon S3 rejects any requests made over HTTP when using SSE\-C\. For security considerations, we recommend that you consider any key you erroneously send using HTTP to be compromised\. You should discard the key and rotate as appropriate\.
+ The ETag in the response is not the MD5 of the object data\. 
+ You manage a mapping of which encryption key was used to encrypt which object\. Amazon S3 does not store encryption keys\. You are responsible for tracking which encryption key you provided for which object\.
  + If your bucket is versioning\-enabled, each object version you upload using this feature can have its own encryption key\. You are responsible for tracking which encryption key was used for which object version\. 
  + Because you manage encryption keys on the client side, you manage any additional safeguards, such as key rotation, on the client side\.
**Warning**  
If you lose the encryption key, any GET request for an object without its encryption key fails, and you lose the object\.

## Topics<a name="sse-c-topic-list"></a>

For more information, see the following topics:
+ [Specifying server\-side encryption with customer\-provided encryption keys using the AWS SDK for Java](sse-c-using-java-sdk.md)
+ [Specifying server\-side encryption with customer\-provided encryption keys using the AWS SDK for \.NET](sse-c-using-dot-net-sdk.md)
+ [Specifying server\-side encryption with customer\-provided encryption keys using the REST API](ServerSideEncryptionCustomerKeysSSEUsingRESTAPI.md)


# Configuring an S3 Bucket Key at the object level using the REST API, AWS SDKs, or AWS CLI<a name="configuring-bucket-key-object"></a>

When you perform a PUT or COPY operation using the REST API, AWS SDKs, or AWS CLI, you can enable or disable an S3 Bucket Key at the object level\. S3 Bucket Keys reduce the cost of server\-side encryption using AWS Key Management Service \(AWS KMS\) \(SSE\-KMS\) by decreasing request traffic from Amazon S3 to AWS KMS\. For more information, see [Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys](bucket-key.md)\. 

When you configure an S3 Bucket Key for an object using a PUT or COPY operation, Amazon S3 only updates the settings for that object\. The S3 Bucket Key settings for the destination bucket do not change\. If you don't specify an S3 Bucket Key for your object, Amazon S3 applies the S3 Bucket Key settings for the destination bucket to the object\.

**Prerequisite:**  
Before you configure your object to use an S3 Bucket Key, review [Changes to note before enabling an S3 Bucket Key](bucket-key.md#bucket-key-changes)\. 

**Topics**
+ [Using the REST API](#bucket-key-object-rest)
+ [Using AWS SDK \(PutObject\)](#bucket-key-object-sdk)
+ [Using the AWS CLI](#bucket-key-object-cli)

## Using the REST API<a name="bucket-key-object-rest"></a>

When you use SSE\-KMS, you can enable an S3 Bucket Key for an object using the following APIs: 
+ [PutObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html) – When you upload an object, you can specify the `x-amz-server-side-encryption-bucket-key-enabled` request header to enable or disable an S3 Bucket Key at the object level\. 
+ [CopyObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CopyObject.html) – When you copy an object and configure SSE\-KMS, you can specify the `x-amz-server-side-encryption-bucket-key-enabled` request header to enable or disable an S3 Bucket Key for your object\. 
+ [PostObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html) – When you use a POST operation to upload an object and configure SSE\-KMS, you can use the `x-amz-server-side-encryption-bucket-key-enabled` form field to enable or disable an S3 Bucket Key for your object\.
+ [CreateMutlipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateMultipartUpload.html) – When you upload large objects using the multipart upload API and configure SSE\-KMS, you can use the `x-amz-server-side-encryption-bucket-key-enabled` request header to enable or disable an S3 Bucket Key for your object\.

To enable an S3 Bucket Key at the object level, include the `x-amz-server-side-encryption-bucket-key-enabled` request header\. For more information about SSE\-KMS and the REST API, see [Specifying the AWS Key Management Service in Amazon S3 Using the REST API](https://docs.aws.amazon.com/AmazonS3/latest/dev/KMSUsingRESTAPI.html)\.

## Using AWS SDK \(PutObject\)<a name="bucket-key-object-sdk"></a>

You can use the following example to configure an S3 Bucket Key at the object level using the AWS SDK for Java\.

------
#### [ Java ]

```
AmazonS3 s3client = AmazonS3ClientBuilder.standard()
    .withRegion(Regions.DEFAULT_REGION)
    .build();

String bucketName = "bucket name";
String keyName = "key name for object";
String contents = "file contents";

PutObjectRequest putObjectRequest = new PutObjectRequest(bucketName, keyName, contents)
    .withBucketKeyEnabled(true);
    
s3client.putObject(putObjectRequest);
```

------

## Using the AWS CLI<a name="bucket-key-object-cli"></a>

You can use the following AWS CLI example to configure an S3 Bucket Key at the object level as part of a `PutObject` request\.

```
aws s3api put-object --bucket <bucket name> --key <object key name> --server-side-encryption aws:kms --bucket-key-enabled —body <filepath>
```


# DNS considerations<a name="DNSConsiderations"></a>

 One of the design requirements of Amazon S3 is extremely high availability\. One of the ways we meet this requirement is by updating the IP addresses associated with the Amazon S3 endpoint in DNS as needed\. These changes are automatically reflected in short\-lived clients, but not in some long\-lived clients\. Long\-lived clients will need to take special action to re\-resolve the Amazon S3 endpoint periodically to benefit from these changes\. For more information about virtual machines \(VMs\), refer to the following: 
+  For Java, Sun's JVM caches DNS lookups forever by default; go to the "InetAddress Caching" section of [the InetAddress documentation](https://docs.oracle.com/javase/9/docs/api/java/net/InetAddress.html) for information on how to change this behavior\. 
+  For PHP, the persistent PHP VM that runs in the most popular deployment configurations caches DNS lookups until the VM is restarted\. Go to [the getHostByName PHP docs\.](http://us2.php.net/manual/en/function.gethostbyname.php) 


# Amazon S3 Transfer Acceleration<a name="transfer-acceleration"></a>

Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations\. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path\.

When using Transfer Acceleration, additional data transfer charges may apply\. For more information about pricing, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\.

**Topics**
+ [Why Use Amazon S3 Transfer Acceleration?](#transfer-acceleration-why-use)
+ [Getting Started with Amazon S3 Transfer Acceleration](#transfer-acceleration-getting-started)
+ [Requirements for using Amazon S3 Transfer Acceleration](#transfer-acceleration-requirements)
+ [Amazon S3 Transfer Acceleration Examples](transfer-acceleration-examples.md)

## Why Use Amazon S3 Transfer Acceleration?<a name="transfer-acceleration-why-use"></a>

You might want to use Transfer Acceleration on a bucket for various reasons, including the following:
+ You have customers that upload to a centralized bucket from all over the world\.
+ You transfer gigabytes to terabytes of data on a regular basis across continents\.
+ You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3\.

For more information about when to use Transfer Acceleration, see [Amazon S3 FAQs](https://aws.amazon.com/s3/faqs/#s3ta)\.

### Using the Amazon S3 Transfer Acceleration Speed Comparison Tool<a name="transfer-acceleration-speed-comparison"></a>

You can use the [Amazon S3 Transfer Acceleration Speed Comparison tool](https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html) to compare accelerated and non\-accelerated upload speeds across Amazon S3 regions\. The Speed Comparison tool uses multipart uploads to transfer a file from your browser to various Amazon S3 regions with and without using Transfer Acceleration\.

You can access the Speed Comparison tool using either of the following methods:
+ Copy the following URL into your browser window, replacing *region* with the region that you are using \(for example, us\-west\-2\) and *yourBucketName* with the name of the bucket that you want to evaluate: 

  `https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html?region=region&origBucketName=yourBucketName` 

   

  For a list of the regions supported by Amazon S3, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *Amazon Web Services General Reference*\.
+ Use the Amazon S3 console\. For details, see [Enabling Transfer Acceleration](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-transfer-acceleration.html) in the *Amazon Simple Storage Service Console User Guide*\.

## Getting Started with Amazon S3 Transfer Acceleration<a name="transfer-acceleration-getting-started"></a>

To get started using Amazon S3 Transfer Acceleration, perform the following steps:

1. **Enable Transfer Acceleration on a bucket** – For your bucket to work with transfer acceleration, the bucket name must conform to DNS naming requirements and must not contain periods \("\."\)\. 

   You can enable Transfer Acceleration on a bucket any of the following ways:
   + Use the Amazon S3 console\. For more information, see [Enabling Transfer Acceleration](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-transfer-acceleration.html) in the *Amazon Simple Storage Service Console User Guide*\.
   + Use the REST API [PUT Bucket accelerate](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTaccelerate.html) operation\.
   + Use the AWS CLI and AWS SDKs\. For more information, see [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)\. 

1. **Transfer data to and from the acceleration\-enabled bucket by using one of the following s3\-accelerate endpoint domain names**:
   + `bucketname.s3-accelerate.amazonaws.com` – to access an acceleration\-enabled bucket\. 
   + `bucketname.s3-accelerate.dualstack.amazonaws.com` – to access an acceleration\-enabled bucket over IPv6\. Amazon S3 dual\-stack endpoints support requests to S3 buckets over IPv6 and IPv4\. The Transfer Acceleration dual\-stack endpoint only uses the virtual hosted\-style type of endpoint name\. For more information, see [Getting started making requests over IPv6](ipv6-access.md#ipv6-access-getting-started) and [Using Amazon S3 dual\-stack endpoints](dual-stack-endpoints.md)\.
**Note**  
You can continue to use the regular endpoint in addition to the accelerate endpoints\.

   You can point your Amazon S3 PUT object and GET object requests to the s3\-accelerate endpoint domain name after you enable Transfer Acceleration\. For example, let's say you currently have a REST API application using [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) that uses the host name **mybucket\.s3\.us\-east\-1\.amazonaws\.com ** in the `PUT` request\. To accelerate the `PUT` you simply change the host name in your request to **mybucket\.s3\-accelerate\.amazonaws\.com**\. To go back to using the standard upload speed, simply change the name back to **mybucket\.s3\.us\-east\-1\.amazonaws\.com**\.

   After Transfer Acceleration is enabled, it can take up to 20 minutes for you to realize the performance benefit\. However, the accelerate endpoint will be available as soon as you enable Transfer Acceleration\.

   You can use the accelerate endpoint in the AWS CLI, AWS SDKs, and other tools that transfer data to and from Amazon S3\. If you are using the AWS SDKs, some of the supported languages use an accelerate endpoint client configuration flag so you don't need to explicitly set the endpoint for Transfer Acceleration to *bucketname*\.s3\-accelerate\.amazonaws\.com\. For examples of how to use an accelerate endpoint client configuration flag, see [Amazon S3 Transfer Acceleration Examples](transfer-acceleration-examples.md)\.

You can use all of the Amazon S3 operations through the transfer acceleration endpoints, except for the following operations: [GET Service \(list buckets\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTServiceGET.html), [PUT Bucket \(create bucket\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html), and [DELETE Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETE.html)\. Also, Amazon S3 Transfer Acceleration does not support cross region copies using [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)\. 



## Requirements for using Amazon S3 Transfer Acceleration<a name="transfer-acceleration-requirements"></a>

The following are the requirements for using Transfer Acceleration on an S3 bucket:
+ Transfer Acceleration is only supported on virtual\-hosted style requests\. For more information about virtual\-hosted style requests, see [Making requests using the REST API](RESTAPI.md)\. 
+ The name of the bucket used for Transfer Acceleration must be DNS\-compliant and must not contain periods \("\."\)\.
+ Transfer Acceleration must be enabled on the bucket\. After enabling Transfer Acceleration on a bucket it might take up to 20 minutes before the data transfer speed to the bucket increases\.
**Note**  
Transfer Acceleration is currently not supported for buckets located in the following Regions:  
Africa \(Cape Town\) \(af\-south\-1\)
Asia Pacific \(Hong Kong\) \(ap\-east\-1\)
Asia Pacific \(Osaka\-Local\) \(ap\-northeast\-3\)
Europe \(Stockholm\) \(eu\-north\-1\)
Europe \(Milan\) \(eu\-south\-1\)
Middle East \(Bahrain\) \(me\-south\-1\)
+ To access the bucket that is enabled for Transfer Acceleration, you must use the endpoint `bucketname.s3-accelerate.amazonaws.com`\. or the dual\-stack endpoint `bucketname.s3-accelerate.dualstack.amazonaws.com` to connect to the enabled bucket over IPv6\. 
+ You must be the bucket owner to set the transfer acceleration state\. The bucket owner can assign permissions to other users to allow them to set the acceleration state on a bucket\. The `s3:PutAccelerateConfiguration` permission permits users to enable or disable Transfer Acceleration on a bucket\. The `s3:GetAccelerateConfiguration` permission permits users to return the Transfer Acceleration state of a bucket, which is either `Enabled` or `Suspended.` For more information about these permissions, see [Example — Bucket Subresource Operations](using-with-s3-actions.md#using-with-s3-actions-related-to-bucket-subresources) and [Identity and access management in Amazon S3](s3-access-control.md)\.

### More Info<a name="transfer-acceleration-moreinfo"></a>
+ [GET Bucket accelerate](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETaccelerate.html)
+ [PUT Bucket accelerate](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTaccelerate.html)


# Error message<a name="ErrorMessage"></a>

The error message contains a generic description of the error condition in English\. It is intended for a human audience\. Simple programs display the message directly to the end user if they encounter an error condition they don't know how or don't care to handle\. Sophisticated programs with more exhaustive error handling and proper internationalization are more likely to ignore the error message\.


# Troubleshooting CORS issues<a name="cors-troubleshooting"></a>

If you encounter unexpected behavior while accessing buckets set with the CORS configuration, try the following steps to troubleshoot:

1. Verify that the CORS configuration is set on the bucket\. 

   For instructions, see [Editing Bucket Permissions](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/EditingBucketPermissions.html) in the *Amazon Simple Storage Service Console User Guide*\. If the CORS configuration is set, the console displays an **Edit CORS Configuration** link in the **Permissions** section of the **Properties** bucket\.

1. Capture the complete request and response using a tool of your choice\. For each request Amazon S3 receives, there must be a CORS rule that matches the data in your request, as follows:

   1. Verify that the request has the Origin header\. 

      If the header is missing, Amazon S3 doesn't treat the request as a cross\-origin request, and doesn't send CORS response headers in the response\.

   1. Verify that the Origin header in your request matches at least one of the `AllowedOrigin` elements in the specified `CORSRule`\. 

      The scheme, the host, and the port values in the Origin request header must match the `AllowedOrigin` elements in the `CORSRule`\. For example, if you set the `CORSRule` to allow the origin `http://www.example.com`, then both `https://www.example.com` and `http://www.example.com:80` origins in your request don't match the allowed origin in your configuration\.

   1.  Verify that the method in your request \(or in a preflight request, the method specified in the `Access-Control-Request-Method`\) is one of the `AllowedMethod` elements in the same `CORSRule`\. 

   1. For a preflight request, if the request includes an `Access-Control-Request-Headers` header, verify that the `CORSRule` includes the `AllowedHeader` entries for each value in the `Access-Control-Request-Headers` header\. 


# Making requests using federated user temporary credentials \- AWS SDK for PHP<a name="AuthUsingTempFederationTokenPHP"></a>

This topic explains how to use classes from version 3 of the AWS SDK for PHP to request temporary security credentials for federated users and applications and use them to access resources stored in Amazon S3\. It assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\. 

You can provide temporary security credentials to your federated users and applications so they can send authenticated requests to access your AWS resources\. When requesting these temporary credentials, you must provide a user name and an IAM policy that describes the resource permissions that you want to grant\. These credentials expire when the session duration expires\. By default, the session duration is one hour\. You can explicitly set a different value for the duration when requesting the temporary security credentials for federated users and applications\. For more information about temporary security credentials, see [Temporary Security Credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html) in the *IAM User Guide*\. For information about providing temporary security credentials to your federated users and applications, see [Making requests](MakingRequests.md)\.

For added security when requesting temporary security credentials for federated users and applications, we recommend using a dedicated IAM user with only the necessary access permissions\. The temporary user you create can never get more permissions than the IAM user who requested the temporary security credentials\. For information about identity federation, see [AWS Identity and Access Management FAQs](https://aws.amazon.com/iam/faqs/#What_are_the_best_practices_for_using_temporary_security_credentials)\.

For information about running the PHP examples in this guide, see [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\.

**Example**  
The following PHP example lists keys in the specified bucket\. In the example, you obtain temporary security credentials for an hour session for your federated user \(User1\)\. Then you use the temporary security credentials to send authenticated requests to Amazon S3\.   
For added security when requesting temporary credentials for others, you use the security credentials of an IAM user who has permissions to request temporary security credentials\. To ensure that the IAM user grants only the minimum application\-specific permissions to the federated user, you can also limit the access permissions of this IAM user\. This example lists only objects in a specific bucket\. Create an IAM user with the following policy attached:   

```
 1. {
 2.   "Statement":[{
 3.       "Action":["s3:ListBucket",
 4.         "sts:GetFederationToken*"
 5.       ],
 6.       "Effect":"Allow",
 7.       "Resource":"*"
 8.     }
 9.   ]
10. }
```
The policy allows the IAM user to request temporary security credentials and access permission only to list your AWS resources\. For more information about how to create an IAM user, see [Creating Your First IAM User and Administrators Group](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html) in the *IAM User Guide*\.   
You can now use the IAM user security credentials to test the following example\. The example sends an authenticated request to Amazon S3 using temporary security credentials\. When requesting temporary security credentials for the federated user \(User1\), the example specifies the following policy, which restricts access to list objects in a specific bucket\. Update the policy with your bucket name\.  

```
 1. {
 2.   "Statement":[
 3.     {
 4.       "Sid":"1",
 5.       "Action":["s3:ListBucket"],
 6.       "Effect":"Allow", 
 7.       "Resource":"arn:aws:s3:::YourBucketName"
 8.     }
 9.   ]
10. }
```
In the following example, when specifying the policy resource, replace `YourBucketName` with the name of your bucket\.:  

```
 require 'vendor/autoload.php';

use Aws\Sts\StsClient;
use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';

// In real applications, the following code is part of your trusted code. It has
// the security credentials that you use to obtain temporary security credentials.
$sts = new StsClient(
    [
    'version' => 'latest',
    'region' => 'us-east-1']
);

// Fetch the federated credentials.
$sessionToken = $sts->getFederationToken([
    'Name'              => 'User1',
    'DurationSeconds'    => '3600',
    'Policy'            => json_encode([
        'Statement' => [
            'Sid'              => 'randomstatementid' . time(),
            'Action'           => ['s3:ListBucket'],
            'Effect'           => 'Allow',
            'Resource'         => 'arn:aws:s3:::' . $bucket
        ]
    ])
]);

// The following will be part of your less trusted code. You provide temporary
// security credentials so the code can send authenticated requests to Amazon S3.

$s3 = new S3Client([
    'region' => 'us-east-1',
    'version' => 'latest',
    'credentials' => [
        'key'    => $sessionToken['Credentials']['AccessKeyId'],
        'secret' => $sessionToken['Credentials']['SecretAccessKey'],
        'token'  => $sessionToken['Credentials']['SessionToken']
    ]
]);

try {
    $result = $s3->listObjects([
        'Bucket' => $bucket
    ]);
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}
```

## Related resources<a name="RelatedResources-AuthUsingTempFederationTokenPHP"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Using Amazon S3 on Outposts<a name="S3onOutposts"></a>

AWS Outposts is a fully managed service that extends AWS infrastructure, services, APIs, and tools to your premises\. By providing local access to AWS managed infrastructure, AWS Outposts helps you build and run applications on\-premises using the same programming interfaces as in AWS Regions, while using local compute and storage resources for lower latency and local data processing needs\. For more information, see [What is AWS Outposts?](https://docs.aws.amazon.com/outposts/latest/userguide/what-is-outposts.htm) in the *AWS Outposts User Guide*\.

With Amazon S3 on Outposts, you can create S3 buckets on your AWS Outposts and easily store and retrieve objects on\-premises for applications that require local data access, local data processing, and data residency\. S3 on Outposts provides a new storage class, `OUTPOSTS` which uses the S3 APIs, and is designed to durably and redundantly store data across multiple devices and servers on your AWS Outposts\. You communicate with your Outposts bucket using an access point and endpoint connection over a virtual private cloud \(VPC\)\. You can use the same APIs and features on Outposts buckets as you do on Amazon S3, such as access policies, encryption, and tagging\. You can use S3 on Outposts through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\.

You can use S3 on Outposts to to deploy object storage on\-premises and is monitored, patched, and updated by AWS\. With S3 on Outposts you can reduce the time, resources, operational risk, and maintenance downtime required for managing on\-premises storage\. You can process and securely store data locally in your on\-premises environment and transfer data to Amazon S3 in an AWS Region for further processing or archival\. S3 on Outposts enables you to meet data residency or regulatory requirements by keeping data on an Outpost on\-premises within a country, state/province, or location where there is not an AWS Region today\.

For on\-premises applications that require high\-throughput local processing, S3 on Outposts provides on\-premises object storage to minimize data transfers and buffer from network variations, while providing you the ability to easily transfer data between Outposts and AWS Regions\. S3 on Outposts is integrated with AWS DataSync\. So you can automate transferring data between your Outposts and AWS Regions, choosing what to transfer, when to transfer, and how much network bandwidth to use\. For more information about transferring data from your S3 on Outposts buckets using DataSync, see [Getting Started with AWS DataSync](https://docs.aws.amazon.com/datasync/latest/userguide/getting-started.html) in the *AWS DataSync User Guide*\.

**Topics**
+ [Getting started with Amazon S3 on Outposts](S3OutpostsGS.md)
+ [Amazon S3 on Outposts restrictions and limitations](S3OnOutpostsRestrictionsLimitations.md)
+ [Using AWS Identity and Access Management with Amazon S3 on Outposts](S3OutpostsIAM.md)
+ [Working with Amazon S3 on Outposts](WorkingWithS3Outposts.md)
+ [Amazon S3 on Outposts examples](S3OutpostsExamples.md)


# Common SOAP API elements<a name="UsingSOAPOperations"></a>

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

You can interact with Amazon S3 using SOAP 1\.1 over HTTP\. The Amazon S3 WSDL, which describes the Amazon S3 API in a machine\-readable way, is available at: [https://doc\.s3\.amazonaws\.com/2006\-03\-01/AmazonS3\.wsdl](https://doc.s3.amazonaws.com/2006-03-01/AmazonS3.wsdl)\. The Amazon S3 schema is available at [https://doc\.s3\.amazonaws\.com/2006\-03\-01/AmazonS3\.xsd](https://doc.s3.amazonaws.com/2006-03-01/AmazonS3.xsd)\.

Most users will interact with Amazon S3 using a SOAP toolkit tailored for their language and development environment\. Different toolkits will expose the Amazon S3 API in different ways\. Please refer to your specific toolkit documentation to understand how to use it\. This section illustrates the Amazon S3 SOAP operations in a toolkit\-independent way by exhibiting the XML requests and responses as they appear "on the wire\."

## Common elements<a name="SOAPCommon"></a>

You can include the following authorization\-related elements with any SOAP request:
+ `AWSAccessKeyId:` The AWS Access Key ID of the requester
+ `Timestamp:` The current time on your system
+ `Signature:` The signature for the request


# Access Control List \(ACL\) Overview<a name="acl-overview"></a>

**Topics**
+ [Who Is a Grantee?](#specifying-grantee)
+ [What Permissions Can I Grant?](#permissions)
+ [Sample ACL](#sample-acl)
+ [Canned ACL](#canned-acl)
+ [How to Specify an ACL](#setting-acls)

Amazon S3 access control lists \(ACLs\) enable you to manage access to buckets and objects\. Each bucket and object has an ACL attached to it as a subresource\. It defines which AWS accounts or groups are granted access and the type of access\. When a request is received against a resource, Amazon S3 checks the corresponding ACL to verify that the requester has the necessary access permissions\. 

When you create a bucket or an object, Amazon S3 creates a default ACL that grants the resource owner full control over the resource\. This is shown in the following sample bucket ACL \(the default object ACL has the same structure\):

**Example**  

```
 1. <?xml version="1.0" encoding="UTF-8"?>
 2. <AccessControlPolicy xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
 3.   <Owner>
 4.     <ID>*** Owner-Canonical-User-ID ***</ID>
 5.     <DisplayName>owner-display-name</DisplayName>
 6.   </Owner>
 7.   <AccessControlList>
 8.     <Grant>
 9.       <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
10.                xsi:type="Canonical User">
11.         <ID>*** Owner-Canonical-User-ID ***</ID>
12.         <DisplayName>display-name</DisplayName>
13.       </Grantee>
14.       <Permission>FULL_CONTROL</Permission>
15.     </Grant>
16.   </AccessControlList>
17. </AccessControlPolicy>
```

The sample ACL includes an `Owner` element that identifies the owner by the AWS account's canonical user ID\. For instructions on finding your canonical user id, see [Finding an AWS Account Canonical User ID](#finding-canonical-id)\. The `Grant` element identifies the grantee \(either an AWS account or a predefined group\) and the permission granted\. This default ACL has one `Grant` element for the owner\. You grant permissions by adding `Grant` elements, with each grant identifying the grantee and the permission\. 

**Note**  
An ACL can have up to 100 grants\.

## Who Is a Grantee?<a name="specifying-grantee"></a>

A grantee can be an AWS account or one of the predefined Amazon S3 groups\. You grant permission to an AWS account using the email address or the canonical user ID\. However, if you provide an email address in your grant request, Amazon S3 finds the canonical user ID for that account and adds it to the ACL\. The resulting ACLs always contain the canonical user ID for the AWS account, not the AWS account's email address\.

When you grant access rights, you specify each grantee as a type=value pair, where the type is one of the following:
+ `id` – if the value specified is the canonical user ID of an AWS account
+ `uri` – if you are granting permissions to a predefined group
+ `emailAddress` – if the value specified is the email address of an AWS account

**Important**  
Using email addresses to specify a grantee is only supported in the following AWS Regions:  
US East \(N\. Virginia\)
US West \(N\. California\)
US West \(Oregon\)
Asia Pacific \(Singapore\)
Asia Pacific \(Sydney\)
Asia Pacific \(Tokyo\)
Europe \(Ireland\)
South America \(São Paulo\)
For a list of all the Amazon S3 supported regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.

**Example: Email Address**  
For example, the following `x-amz-grant-read` header grants the AWS accounts identified by email addresses permissions to read object data and its metadata:  

```
x-amz-grant-read: emailAddress="xyz@amazon.com", emailAddress="abc@amazon.com"
```

**Warning**  
When you grant other AWS accounts access to your resources, be aware that the AWS accounts can delegate their permissions to users under their accounts\. This is known as *cross\-account access*\. For information about using cross\-account access, see [ Creating a Role to Delegate Permissions to an IAM User](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html) in the *IAM User Guide*\. 

### Finding an AWS Account Canonical User ID<a name="finding-canonical-id"></a>

The canonical user ID is associated with your AWS account\. It is a long string, such as `79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be`\. For information about how to find the canonical user ID for your account, see [Finding Your Account Canonical User ID](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingCanonicalId)\. 

You can also look up the canonical user ID of an AWS account by reading the ACL of a bucket or an object to which the AWS account has access permissions\. When an individual AWS account is granted permissions by a grant request, a grant entry is added to the ACL with the AWS account's canonical user ID\. 

**Note**  
If you make your bucket public \(not recommended\) any unauthenticated user can upload objects to the bucket\. These anonymous users don't have an AWS account\. When an anonymous user uploads an object to your bucket Amazon S3 adds a special canonical user ID \(`65a011a29cdf8ec533ec3d1ccaae921c`\) as the object owner in the ACL\. For more information, see [Amazon S3 bucket and object ownership](access-control-overview.md#about-resource-owner)\.

### Amazon S3 Predefined Groups<a name="specifying-grantee-predefined-groups"></a>

Amazon S3 has a set of predefined groups\. When granting account access to a group, you specify one of our URIs instead of a canonical user ID\. We provide the following predefined groups:
+ ****Authenticated Users group**** – Represented by `http://acs.amazonaws.com/groups/global/AuthenticatedUsers`\.

  This group represents all AWS accounts\. **Access permission to this group allows any AWS account to access the resource\.** However, all requests must be signed \(authenticated\)\.
**Warning**  
When you grant access to the **Authenticated Users group** any AWS authenticated user in the world can access your resource\.
+ ****All Users group**** – Represented by `http://acs.amazonaws.com/groups/global/AllUsers`\.

  **Access permission to this group allows anyone in the world access to the resource\.** The requests can be signed \(authenticated\) or unsigned \(anonymous\)\. Unsigned requests omit the Authentication header in the request\.
**Warning**  
We highly recommend that you never grant the **All Users group** `WRITE`, `WRITE_ACP`, or `FULL_CONTROL` permissions\. For example, `WRITE` permissions allow anyone to store objects in your bucket, for which you are billed\. It also allows others to delete objects that you might want to keep\. For more details about these permissions, see the following section [What Permissions Can I Grant?](#permissions)\.
+ ****Log Delivery group**** – Represented by `http://acs.amazonaws.com/groups/s3/LogDelivery`\.

  WRITE permission on a bucket enables this group to write server access logs \(see [Amazon S3 server access logging](ServerLogs.md)\) to the bucket\.

**Note**  
When using ACLs, a grantee can be an AWS account or one of the predefined Amazon S3 groups\. However, the grantee cannot be an IAM user\. For more information about AWS users and permissions within IAM, go to [Using AWS Identity and Access Management](https://docs.aws.amazon.com/IAM/latest/UserGuide/)\.

## What Permissions Can I Grant?<a name="permissions"></a>

The following table lists the set of permissions that Amazon S3 supports in an ACL\. The set of ACL permissions is the same for an object ACL and a bucket ACL\. However, depending on the context \(bucket ACL or object ACL\), these ACL permissions grant permissions for specific buckets or object operations\. The table lists the permissions and describes what they mean in the context of objects and buckets\. 


| Permission | When granted on a bucket | When granted on an object | 
| --- | --- | --- | 
| READ | Allows grantee to list the objects in the bucket | Allows grantee to read the object data and its metadata | 
| WRITE | Allows grantee to create, overwrite, and delete any object in the bucket | Not applicable | 
| READ\_ACP | Allows grantee to read the bucket ACL | Allows grantee to read the object ACL | 
| WRITE\_ACP | Allows grantee to write the ACL for the applicable bucket | Allows grantee to write the ACL for the applicable object | 
| FULL\_CONTROL | Allows grantee the READ, WRITE, READ\_ACP, and WRITE\_ACP permissions on the bucket | Allows grantee the READ, READ\_ACP, and WRITE\_ACP permissions on the object | 

**Warning**  
Use caution when granting access permissions to your S3 buckets and objects\. For example, granting `WRITE` access to a bucket allows the grantee to create, overwrite, and delete any object in the bucket\. We highly recommend that you read through this entire [Access Control List \(ACL\) Overview](#acl-overview) section before granting permissions\.

### Mapping of ACL Permissions and Access Policy Permissions<a name="acl-access-policy-permission-mapping"></a>

As shown in the preceding table, an ACL allows only a finite set of permissions, compared to the number of permissions you can set in an access policy \(see [Amazon S3 Actions](using-with-s3-actions.md)\)\. Each of these permissions allows one or more Amazon S3 operations\.

The following table shows how each ACL permission maps to the corresponding access policy permissions\. As you can see, access policy allows more permissions than an ACL does\. You use ACLs primarily to grant basic read/write permissions, similar to file system permissions\. For more information about when to use an ACL, see [Guidelines for using the available access policy options](access-policy-alternatives-guidelines.md)\.


| ACL permission | Corresponding access policy permissions when the ACL permission is granted on a bucket  | Corresponding access policy permissions when the ACL permission is granted on an object | 
| --- | --- | --- | 
| READ | s3:ListBucket, s3:ListBucketVersions, and s3:ListBucketMultipartUploads  | s3:GetObject, s3:GetObjectVersion, and s3:GetObjectTorrent | 
| WRITE |  `s3:PutObject` and `s3:DeleteObject`\. In addition, when the grantee is the bucket owner, granting `WRITE` permission in a bucket ACL allows the `s3:DeleteObjectVersion` action to be performed on any version in that bucket\.  | Not applicable | 
| READ\_ACP | s3:GetBucketAcl  | s3:GetObjectAcl and s3:GetObjectVersionAcl | 
| WRITE\_ACP | s3:PutBucketAcl | s3:PutObjectAcl and s3:PutObjectVersionAcl | 
| FULL\_CONTROL | Equivalent to granting READ, WRITE, READ\_ACP, and WRITE\_ACP ACL permissions\. Accordingly, this ACL permission maps to a combination of corresponding access policy permissions\. | Equivalent to granting READ, READ\_ACP, and WRITE\_ACP ACL permissions\. Accordingly, this ACL permission maps to a combination of corresponding access policy permissions\. | 

#### Condition Keys<a name="acl-specific-condition-keys"></a>

When you grant access policy permissions, you can use condition keys to constrain the value for the ACL on an object using a bucket policy\. The context keys below correspond to ACLs\. You can use these context keys to mandate the use of a specific ACL in a request:
+ `s3:x-amz-grant-read` ‐ Require read access\.
+ `s3:x-amz-grant-write` ‐ Require write access\.
+ `s3:x-amz-grant-read-acp` ‐ Require read access to the bucket ACL\.
+ `s3:x-amz-grant-write-acp` ‐ Require write access to the bucket ACL\.
+ `s3:x-amz-grant-full-control` ‐ Require full control\.
+ `s3:x-amz-acl` ‐ Require a [Canned ACL](#canned-acl)\.

For example policies that involves ACL\-specific headers, see [Example 1: Granting s3:PutObject Permission with a Condition Requiring the Bucket Owner to Get Full Control](amazon-s3-policy-keys.md#grant-putobject-conditionally-1)\. For a complete list of Amazon S3‐specific condition keys, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\.

## Sample ACL<a name="sample-acl"></a>

The following sample ACL on a bucket identifies the resource owner and a set of grants\. The format is the XML representation of an ACL in the Amazon S3 REST API\. The bucket owner has `FULL_CONTROL` of the resource\. In addition, the ACL shows how permissions are granted on a resource to two AWS accounts, identified by canonical user ID, and two of the predefined Amazon S3 groups discussed in the preceding section\.

**Example**  

```
 1. <?xml version="1.0" encoding="UTF-8"?>
 2. <AccessControlPolicy xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
 3.   <Owner>
 4.     <ID>Owner-canonical-user-ID</ID>
 5.     <DisplayName>display-name</DisplayName>
 6.   </Owner>
 7.   <AccessControlList>
 8.     <Grant>
 9.       <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="CanonicalUser">
10.         <ID>Owner-canonical-user-ID</ID>
11.         <DisplayName>display-name</DisplayName>
12.       </Grantee>
13.       <Permission>FULL_CONTROL</Permission>
14.     </Grant>
15.     
16.     <Grant>
17.       <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="CanonicalUser">
18.         <ID>user1-canonical-user-ID</ID>
19.         <DisplayName>display-name</DisplayName>
20.       </Grantee>
21.       <Permission>WRITE</Permission>
22.     </Grant>
23. 
24.     <Grant>
25.       <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="CanonicalUser">
26.         <ID>user2-canonical-user-ID</ID>
27.         <DisplayName>display-name</DisplayName>
28.       </Grantee>
29.       <Permission>READ</Permission>
30.     </Grant>
31. 
32.     <Grant>
33.       <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="Group">
34.         <URI>http://acs.amazonaws.com/groups/global/AllUsers</URI> 
35.       </Grantee>
36.       <Permission>READ</Permission>
37.     </Grant>
38.     <Grant>
39.       <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="Group">
40.         <URI>http://acs.amazonaws.com/groups/s3/LogDelivery</URI>
41.       </Grantee>
42.       <Permission>WRITE</Permission>
43.     </Grant>
44. 
45.   </AccessControlList>
46. </AccessControlPolicy>
```

## Canned ACL<a name="canned-acl"></a>

Amazon S3 supports a set of predefined grants, known as *canned ACLs*\. Each canned ACL has a predefined set of grantees and permissions\. The following table lists the set of canned ACLs and the associated predefined grants\. 


| Canned ACL | Applies to | Permissions added to ACL | 
| --- | --- | --- | 
| private | Bucket and object | Owner gets FULL\_CONTROL\. No one else has access rights \(default\)\. | 
| public\-read | Bucket and object | Owner gets FULL\_CONTROL\. The AllUsers group \(see [Who Is a Grantee?](#specifying-grantee)\) gets READ access\.  | 
| public\-read\-write | Bucket and object | Owner gets FULL\_CONTROL\. The AllUsers group gets READ and WRITE access\. Granting this on a bucket is generally not recommended\. | 
| aws\-exec\-read | Bucket and object | Owner gets FULL\_CONTROL\. Amazon EC2 gets READ access to GET an Amazon Machine Image \(AMI\) bundle from Amazon S3\. | 
| authenticated\-read | Bucket and object | Owner gets FULL\_CONTROL\. The AuthenticatedUsers group gets READ access\. | 
| bucket\-owner\-read | Object | Object owner gets FULL\_CONTROL\. Bucket owner gets READ access\. If you specify this canned ACL when creating a bucket, Amazon S3 ignores it\. | 
| bucket\-owner\-full\-control | Object  | Both the object owner and the bucket owner get FULL\_CONTROL over the object\. If you specify this canned ACL when creating a bucket, Amazon S3 ignores it\. | 
| log\-delivery\-write | Bucket  | The LogDelivery group gets WRITE and READ\_ACP permissions on the bucket\. For more information about logs, see \([Amazon S3 server access logging](ServerLogs.md)\)\. | 

**Note**  
You can specify only one of these canned ACLs in your request\.

You specify a canned ACL in your request using the `x-amz-acl` request header\. When Amazon S3 receives a request with a canned ACL in the request, it adds the predefined grants to the ACL of the resource\. 

## How to Specify an ACL<a name="setting-acls"></a>

For more information about specifying an ACL, see these topics:
+ [Managing ACLs in the AWS Management Console](manage-acls-using-console.md)
+ [Managing ACLs Using the AWS SDK for JavaConfiguring ACL Grants on an Existing Object](acl-using-java-sdk.md)
+ [Managing ACLs Using the AWS SDK for \.NET ](acl-using-dot-net-sdk.md)
+ [Managing ACLs Using the REST API](acl-using-rest-api.md)


# Managing websites with the AWS SDK for Java<a name="ConfigWebSiteJava"></a>

The following example shows how to use the AWS SDK for Java to manage website configuration for a bucket\. To add a website configuration to a bucket, you provide a bucket name and a website configuration\. The website configuration must include an index document and can include an optional error document\. These documents must already exist in the bucket\. For more information, see [PUT Bucket website](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTwebsite.html)\. For more information about the Amazon S3 website feature, see [Hosting a static website using Amazon S3](WebsiteHosting.md)\. 

**Example**  
The following example uses the AWS SDK for Java to add a website configuration to a bucket, retrieve and print the configuration, and then delete the configuration and verify the deletion\. For instructions on how to create and test a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketWebsiteConfiguration;

import java.io.IOException;

public class WebsiteConfiguration {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String indexDocName = "*** Index document name ***";
        String errorDocName = "*** Error document name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

            // Print the existing website configuration, if it exists.
            printWebsiteConfig(s3Client, bucketName);

            // Set the new website configuration.
            s3Client.setBucketWebsiteConfiguration(bucketName, new BucketWebsiteConfiguration(indexDocName, errorDocName));

            // Verify that the configuration was set properly by printing it.
            printWebsiteConfig(s3Client, bucketName);

            // Delete the website configuration.
            s3Client.deleteBucketWebsiteConfiguration(bucketName);

            // Verify that the website configuration was deleted by printing it.
            printWebsiteConfig(s3Client, bucketName);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void printWebsiteConfig(AmazonS3 s3Client, String bucketName) {
        System.out.println("Website configuration: ");
        BucketWebsiteConfiguration bucketWebsiteConfig = s3Client.getBucketWebsiteConfiguration(bucketName);
        if (bucketWebsiteConfig == null) {
            System.out.println("No website config.");
        } else {
            System.out.println("Index doc: " + bucketWebsiteConfig.getIndexDocumentSuffix());
            System.out.println("Error doc: " + bucketWebsiteConfig.getErrorDocument());
        }
    }
}
```


# Amazon S3 Actions<a name="using-with-s3-actions"></a>

Amazon S3 defines a set of permissions that you can specify in a policy\. These are keywords, each of which maps to a specific Amazon S3 operation\. For more information about Amazon S3 operations, see [Actions](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations.html) in the *Amazon Simple Storage Service API Reference*\. 

To see how to specify permissions in an Amazon S3 policy, review the following example policies\. For a list of Amazon S3 actions, resources, and condition keys for use in policies, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\. For a complete list of Amazon S3 actions, see [Actions](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations.html)\.

**Topics**
+ [Example — Object Operations](#using-with-s3-actions-related-to-objects)
+ [Example — Bucket Operations](#using-with-s3-actions-related-to-buckets)
+ [Example — Bucket Subresource Operations](#using-with-s3-actions-related-to-bucket-subresources)
+ [Example — Account Operations](#using-with-s3-actions-related-to-accounts)

## Example — Object Operations<a name="using-with-s3-actions-related-to-objects"></a>

The following example bucket policy grants the `s3:PutObject` and the `s3:PutObjectAcl` permissions to a user \(Dave\)\. If you remove the `Principal` element, you can attach the policy to a user\. These are object operations\. Accordingly, the `relative-id` portion of the `Resource` ARN identifies objects \(`awsexamplebucket1/*`\)\. For more information, see [Amazon S3 Resources](s3-arn-format.md)\.

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "statement1",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::12345678901:user/Dave"
            },
            "Action": [
            "s3:PutObject",
            "s3:PutObjectAcl"
            ],
            "Resource": "arn:aws:s3:::awsexamplebucket1/*"
        }
    ]
}
```

**Permissions for All Amazon S3 Actions**  
You can use a wildcard to grant permission for all Amazon S3 actions\.

```
"Action":   "*"
```

## Example — Bucket Operations<a name="using-with-s3-actions-related-to-buckets"></a>

The following example user policy grants the `s3:CreateBucket`, `s3:ListAllMyBuckets`, and the `s3:GetBucketLocation` permissions to a user\. For all these permissions, you set the `relative-id` part of the `Resource` ARN to "\*"\. For all other bucket actions, you must specify a bucket name\. For more information, see [Amazon S3 Resources](s3-arn-format.md)\.

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Sid":"statement1",
         "Effect":"Allow",
         "Action":[
            "s3:CreateBucket", 
            "s3:ListAllMyBuckets", 
            "s3:GetBucketLocation"  
         ],
         "Resource":[
            "arn:aws:s3:::*"
         ]
       }
    ]
}
```

**Policy for Console Access**  
If a user wants to use the AWS Management Console to view buckets and the contents of any of those buckets, the user must have the `s3:ListAllMyBuckets` and `s3:GetBucketLocation` permissions\. For an example, see *Policy for Console Access* in the blog post [Writing IAM Policies: How to Grant Access to an S3 Bucket](https://aws.amazon.com/blogs/security/writing-iam-policies-how-to-grant-access-to-an-amazon-s3-bucket/)\.

## Example — Bucket Subresource Operations<a name="using-with-s3-actions-related-to-bucket-subresources"></a>

The following user policy grants the `s3:GetBucketAcl` permission on the `DOC-EXAMPLE-BUCKET1` bucket to user Dave\.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "statement1",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/Dave"
      },
      "Action": [
        "s3:GetObjectVersion",
        "s3:GetBucketAcl"
      ],
      "Resource": [
        "arn:aws:s3:::DOC-EXAMPLE-BUCKET1",
	 "arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*"
      ]
    }
  ]
}
```

**DELETE Object Permissions**  
You can delete objects either by explicitly calling the DELETE Object API or by configuring its lifecycle \(see [Object lifecycle management](object-lifecycle-mgmt.md)\) so that Amazon S3 can remove the objects when their lifetime expires\. To explicitly block users or accounts from deleting objects, you must explicitly deny them `s3:DeleteObject`, `s3:DeleteObjectVersion`, and `s3:PutLifecycleConfiguration` permissions\. 

**Explicit Deny**  
By default, users have no permissions\. But as you create users, add users to groups, and grant them permissions, they might get certain permissions that you didn't intend to grant\. That is where you can use explicit deny, which supersedes all other permissions a user might have and denies the user permissions for specific actions\.

## Example — Account Operations<a name="using-with-s3-actions-related-to-accounts"></a>

The following example user policy grants the `s3:GetAccountPublicAccessBlock` permission to a user\. For these permissions, you set the `Resource` value to `"*"`\. For more information, see [Amazon S3 Resources](s3-arn-format.md)\.

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Sid":"statement1",
         "Effect":"Allow",
         "Action":[
            "s3:GetAccountPublicAccessBlock" 
         ],
         "Resource":[
            "*"
         ]
       }
    ]
}
```


# Amazon S3 Storage Lens examples and console walk\-through<a name="S3LensExamples"></a>

Amazon S3 Storage Lens aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format\. You can use the dashboard to visualize insights and trends, flag outliers, and provides recommendations for optimizing storage costs and applying data protection best practices\. You can use S3 Storage Lens through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\. 

[![AWS Videos](http://img.youtube.com/vi/https://www.youtube.com/embed/TNmZEvwFiOA/0.jpg)](http://www.youtube.com/watch?v=https://www.youtube.com/embed/TNmZEvwFiOA)

This section contains examples of creating, updating, and viewing S3 Storage Lens configurations and performing operations related to the feature\. If you are using S3 Storage Lens with AWS Organizations, these examples also cover those use cases\. In the examples, replace any variable values with those that are specific to you\.

**Topics**
+ [Amazon S3 Storage Lens examples using the AWS CLI](S3LensCLIExamples.md)
+ [Amazon S3 Storage Lens examples using the SDK for Java](S3LensJavaExamples.md)


# Overview of setting up replication<a name="replication-how-setup"></a>

To enable replication, you simply add a replication configuration to your source bucket\. The configuration tells Amazon S3 to replicate objects as specified\. In the replication configuration, you must provide the following:
+ The destination bucket or buckets — The bucket or buckets where you want Amazon S3 to replicate the objects\.

   
+ The objects that you want to replicate — You can replicate all of the objects in the source bucket or a subset\. You identify a subset by providing a [key name prefix](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix), one or more object tags, or both in the configuration\.
  +  For example, if you configure a replication rule to replicate only objects with the key name prefix `Tax/`, Amazon S3 replicates objects with keys such as `Tax/doc1` or `Tax/doc2`\. But it doesn't replicate an object with the key `Legal/doc3`\. If you specify both prefix and one or more tags, Amazon S3 replicates only objects having the specific key prefix and tags\.

 

In addition to these minimum requirements, you can choose the following options: 
+ By default, Amazon S3 stores object replicas using the same storage class as the source object\. You can specify a different storage class for the replicas\.

   
+ Amazon S3 assumes that an object replica continues to be owned by the owner of the source object\. So when it replicates objects, it also replicates the corresponding object access control list \(ACL\)\. If the source and destination buckets are owned by different AWS accounts, you can configure replication to change the owner of a replica to the AWS account that owns the destination bucket\.

Additional configuration options are available\. For more information, see [Additional replication configurations](replication-additional-configs.md)\.

You can configure replication using the Amazon S3 console, see [How do I add a replication rule to an S3 bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html) in the *Amazon Simple Storage Service Console User Guide*\. This topic provides instructions for creating replication rules when buckets are owned by the same or different AWS accounts\.

Amazon S3 also provides APIs to support setting up replication rules\. For more information, see the following topics in the *Amazon Simple Storage Service API Reference*:
+  [PUT Bucket replication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTreplication.html) 
+  [GET Bucket replication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETreplication.html) 
+  [DELETE Bucket replication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEreplication.html) 

**Topics**
+ [Replication configuration overview](replication-add-config.md)
+ [Setting up permissions for replication](setting-repl-config-perm-overview.md)


# Cross\-origin resource sharing \(CORS\)<a name="cors"></a>

Cross\-origin resource sharing \(CORS\) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain\. With CORS support, you can build rich client\-side web applications with Amazon S3 and selectively allow cross\-origin access to your Amazon S3 resources\. 

This section provides an overview of CORS\. The subtopics describe how you can enable CORS using the Amazon S3 console, or programmatically by using the Amazon S3 REST API and the AWS SDKs\. 

**Topics**
+ [Cross\-origin resource sharing: Use\-case scenarios](#example-scenarios-cors)
+ [How do I configure CORS on my bucket?](#how-do-i-enable-cors)
+ [How does Amazon S3 evaluate the CORS configuration on a bucket?](#cors-eval-criteria)
+ [Enabling cross\-origin resource sharing \(CORS\)](ManageCorsUsing.md)
+ [Troubleshooting CORS issues](cors-troubleshooting.md)

## Cross\-origin resource sharing: Use\-case scenarios<a name="example-scenarios-cors"></a>

The following are example scenarios for using CORS\.

**Scenario 1**  
Suppose that you are hosting a website in an Amazon S3 bucket named `website` as described in [Hosting a static website using Amazon S3](WebsiteHosting.md)\. Your users load the website endpoint:

```
http://website.s3-website.us-east-1.amazonaws.com
```

Now you want to use JavaScript on the webpages that are stored in this bucket to be able to make authenticated GET and PUT requests against the same bucket by using the Amazon S3 API endpoint for the bucket, `website.s3.us-east-1.amazonaws.com`\. A browser would normally block JavaScript from allowing those requests, but with CORS you can configure your bucket to explicitly enable cross\-origin requests from `website.s3-website.us-east-1.amazonaws.com`\.

**Scenario 2**  
Suppose that you want to host a web font from your S3 bucket\. Again, browsers require a CORS check \(also called a preflight check\) for loading web fonts\. You would configure the bucket that is hosting the web font to allow any origin to make these requests\.

## How do I configure CORS on my bucket?<a name="how-do-i-enable-cors"></a>

To configure your bucket to allow cross\-origin requests, you create a CORS configuration\. The CORS configuration is a document with rules that identify the origins that you will allow to access your bucket, the operations \(HTTP methods\) that will support for each origin, and other operation\-specific information\. You can add up to 100 rules to the configuration\. You can add the CORS configuration as the `cors` subresource to the bucket\.

If you are configuring CORS in the S3 console, you must use JSON to create a CORS configuration\. The new S3 console only supports JSON CORS configurations\. 

**Important**  
In the new S3 console, the CORS configuration must be JSON\.

Instead of accessing a website by using an Amazon S3 website endpoint, you can use your own domain, such as `example1.com` to serve your content\. For information about using your own domain, see [Configuring a static website using a custom domain registered with Route 53](website-hosting-custom-domain-walkthrough.md)\. 

**Example 1**  
The following example `cors` configuration has three rules, which are specified as `CORSRule` elements:  
+ The first rule allows cross\-origin PUT, POST, and DELETE requests from the `http://www.example1.com` origin\. The rule also allows all headers in a preflight OPTIONS request through the `Access-Control-Request-Headers` header\. In response to preflight OPTIONS requests, Amazon S3 returns requested headers\.
+ The second rule allows the same cross\-origin requests as the first rule, but the rule applies to another origin, `http://www.example2.com`\. 
+ The third rule allows cross\-origin GET requests from all origins\. The `*` wildcard character refers to all origins\. 

```
<CORSConfiguration>
 <CORSRule>
   <AllowedOrigin>http://www.example1.com</AllowedOrigin>

   <AllowedMethod>PUT</AllowedMethod>
   <AllowedMethod>POST</AllowedMethod>
   <AllowedMethod>DELETE</AllowedMethod>

   <AllowedHeader>*</AllowedHeader>
 </CORSRule>
 <CORSRule>
   <AllowedOrigin>http://www.example2.com</AllowedOrigin>

   <AllowedMethod>PUT</AllowedMethod>
   <AllowedMethod>POST</AllowedMethod>
   <AllowedMethod>DELETE</AllowedMethod>

   <AllowedHeader>*</AllowedHeader>
 </CORSRule>
 <CORSRule>
   <AllowedOrigin>*</AllowedOrigin>
   <AllowedMethod>GET</AllowedMethod>
 </CORSRule>
</CORSConfiguration>
```

```
[
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE"
        ],
        "AllowedOrigins": [
            "http://www.example1.com"
        ],
        "ExposeHeaders": []
    },
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE"
        ],
        "AllowedOrigins": [
            "http://www.example2.com"
        ],
        "ExposeHeaders": []
    },
    {
        "AllowedHeaders": [],
        "AllowedMethods": [
            "GET"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    }
]
```

**Example 2**  
The CORS configuration also allows optional configuration parameters, as shown in the following CORS configuration\. In this example, the CORS configuration allows cross\-origin PUT, POST, and DELETE requests from the `http://www.example.com` origin\.  

```
<CORSConfiguration>
 <CORSRule>
   <AllowedOrigin>http://www.example.com</AllowedOrigin>
   <AllowedMethod>PUT</AllowedMethod>
   <AllowedMethod>POST</AllowedMethod>
   <AllowedMethod>DELETE</AllowedMethod>
   <AllowedHeader>*</AllowedHeader>
  <MaxAgeSeconds>3000</MaxAgeSeconds>
  <ExposeHeader>x-amz-server-side-encryption</ExposeHeader>
  <ExposeHeader>x-amz-request-id</ExposeHeader>
  <ExposeHeader>x-amz-id-2</ExposeHeader>
 </CORSRule>
</CORSConfiguration>
```

```
[
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "PUT",
            "POST",
            "DELETE"
        ],
        "AllowedOrigins": [
            "http://www.example.com"
        ],
        "ExposeHeaders": [
            "x-amz-server-side-encryption",
            "x-amz-request-id",
            "x-amz-id-2"
        ],
        "MaxAgeSeconds": 3000
    }
]
```
The `CORSRule` element in the preceding configuration includes the following optional elements:  
+ `MaxAgeSeconds`—Specifies the amount of time in seconds \(in this example, 3000\) that the browser caches an Amazon S3 response to a preflight OPTIONS request for the specified resource\. By caching the response, the browser does not have to send preflight requests to Amazon S3 if the original request will be repeated\. 
+ `ExposeHeader`—Identifies the response headers \(in this example, `x-amz-server-side-encryption`, `x-amz-request-id`, and `x-amz-id-2`\) that customers are able to access from their applications \(for example, from a JavaScript `XMLHttpRequest` object\)\.

### AllowedMethod element<a name="cors-allowed-methods"></a>

In the CORS configuration, you can specify the following values for the `AllowedMethod` element\.
+ GET
+ PUT
+ POST
+ DELETE
+ HEAD

### AllowedOrigin element<a name="cors-allowed-origin"></a>

In the `AllowedOrigin` element, you specify the origins that you want to allow cross\-domain requests from, for example,` http://www.example.com`\. The origin string can contain only one `*` wildcard character, such as `http://*.example.com`\. You can optionally specify `*` as the origin to enable all the origins to send cross\-origin requests\. You can also specify `https` to enable only secure origins\.

### AllowedHeader element<a name="cors-allowed-headers"></a>

The `AllowedHeader` element specifies which headers are allowed in a preflight request through the `Access-Control-Request-Headers` header\. Each header name in the `Access-Control-Request-Headers` header must match a corresponding entry in the rule\. Amazon S3 will send only the allowed headers in a response that were requested\. For a sample list of headers that can be used in requests to Amazon S3, go to [Common Request Headers](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonRequestHeaders.html) in the *Amazon Simple Storage Service API Reference* guide\.

Each AllowedHeader string in the rule can contain at most one \* wildcard character\. For example, `<AllowedHeader>x-amz-*</AllowedHeader>` will enable all Amazon\-specific headers\.

### ExposeHeader element<a name="cors-expose-headers"></a>

Each `ExposeHeader` element identifies a header in the response that you want customers to be able to access from their applications \(for example, from a JavaScript `XMLHttpRequest` object\)\. For a list of common Amazon S3 response headers, go to [Common Response Headers](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonResponseHeaders.html) in the *Amazon Simple Storage Service API Reference* guide\.

### MaxAgeSeconds element<a name="cors-max-age"></a>

The `MaxAgeSeconds` element specifies the time in seconds that your browser can cache the response for a preflight request as identified by the resource, the HTTP method, and the origin\.

## How does Amazon S3 evaluate the CORS configuration on a bucket?<a name="cors-eval-criteria"></a>

When Amazon S3 receives a preflight request from a browser, it evaluates the CORS configuration for the bucket and uses the first `CORSRule` rule that matches the incoming browser request to enable a cross\-origin request\. For a rule to match, the following conditions must be met:
+ The request's `Origin` header must match an `AllowedOrigin` element\.
+ The request method \(for example, GET or PUT\) or the `Access-Control-Request-Method` header in case of a preflight `OPTIONS` request must be one of the `AllowedMethod` elements\. 
+ Every header listed in the request's `Access-Control-Request-Headers` header on the preflight request must match an `AllowedHeader` element\. 

**Note**  
The ACLs and policies continue to apply when you enable CORS on the bucket\.


# Deleting an object using the AWS SDK for Java<a name="DeletingOneObjectUsingJava"></a>

You can delete an object from a bucket\. If you have S3 Versioning enabled on the bucket, you have the following options:
+ Delete a specific object version by specifying a version ID\.
+ Delete an object without specifying a version ID, in which case S3 adds a delete marker to the object\.

For more information about S3 Versioning, see [Object Versioning](ObjectVersioning.md)\. 

**Example 1: Deleting an object \(non\-versioned bucket\)**  
The following example deletes an object from a bucket\. The example assumes that the bucket is not versioning\-enabled and the object doesn't have any version IDs\. In the delete request, you specify only the object key and not a version ID\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.DeleteObjectRequest;

import java.io.IOException;

public class DeleteObjectNonVersionedBucket {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ****";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            s3Client.deleteObject(new DeleteObjectRequest(bucketName, keyName));
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

**Example 2: Deleting an object \(versioned bucket\)**  
The following example deletes an object from a versioned bucket\. The example deletes a specific object version by specifying the object key name and version ID\. The example does the following:  

1. Adds a sample object to the bucket\. Amazon S3 returns the version ID of the newly added object\. The example uses this version ID in the delete request\.

1. Deletes the object version by specifying both the object key name and a version ID\. If there are no other versions of that object, Amazon S3 deletes the object entirely\. Otherwise, Amazon S3 only deletes the specified version\.
**Note**  
You can get the version IDs of an object by sending a `ListVersions` request\.

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketVersioningConfiguration;
import com.amazonaws.services.s3.model.DeleteVersionRequest;
import com.amazonaws.services.s3.model.PutObjectResult;

import java.io.IOException;

public class DeleteObjectVersionEnabledBucket {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ****";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Check to ensure that the bucket is versioning-enabled.
            String bucketVersionStatus = s3Client.getBucketVersioningConfiguration(bucketName).getStatus();
            if (!bucketVersionStatus.equals(BucketVersioningConfiguration.ENABLED)) {
                System.out.printf("Bucket %s is not versioning-enabled.", bucketName);
            } else {
                // Add an object.
                PutObjectResult putResult = s3Client.putObject(bucketName, keyName, "Sample content for deletion example.");
                System.out.printf("Object %s added to bucket %s\n", keyName, bucketName);

                // Delete the version of the object that we just created.
                System.out.println("Deleting versioned object " + keyName);
                s3Client.deleteVersion(new DeleteVersionRequest(bucketName, keyName, putResult.getVersionId()));
                System.out.printf("Object %s, version %s deleted\n", keyName, putResult.getVersionId());
            }
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Amazon S3 Server Access Log Format<a name="LogFormat"></a>

This section describes the Amazon S3 server access log files\.

**Topics**
+ [Additional Logging for Copy Operations](#AdditionalLoggingforCopyOperations)
+ [Custom Access Log Information](#LogFormatCustom)
+ [Programming Considerations for Extensible Server Access Log Format](#LogFormatExtensible)

The server access log files consist of a sequence of newline\-delimited log records\. Each log record represents one request and consists of space\-delimited fields\. The following is an example log consisting of five log records\. 

```
1. 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be awsexamplebucket1 [06/Feb/2019:00:00:38 +0000] 192.0.2.3 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be 3E57427F3EXAMPLE REST.GET.VERSIONING - "GET /awsexamplebucket1?versioning HTTP/1.1" 200 - 113 - 7 - "-" "S3Console/0.4" - s9lzHYrFp76ZVxRcpX9+5cjAnEH2ROuNkd2BHfIa6UkFVdtjf5mKR3/eTPFvsiP/XV/VLi31234= SigV2 ECDHE-RSA-AES128-GCM-SHA256 AuthHeader awsexamplebucket1.s3.us-west-1.amazonaws.com TLSV1.1
2. 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be awsexamplebucket1 [06/Feb/2019:00:00:38 +0000] 192.0.2.3 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be 891CE47D2EXAMPLE REST.GET.LOGGING_STATUS - "GET /awsexamplebucket1?logging HTTP/1.1" 200 - 242 - 11 - "-" "S3Console/0.4" - 9vKBE6vMhrNiWHZmb2L0mXOcqPGzQOI5XLnCtZNPxev+Hf+7tpT6sxDwDty4LHBUOZJG96N1234= SigV2 ECDHE-RSA-AES128-GCM-SHA256 AuthHeader awsexamplebucket1.s3.us-west-1.amazonaws.com TLSV1.1
3. 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be awsexamplebucket1 [06/Feb/2019:00:00:38 +0000] 192.0.2.3 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be A1206F460EXAMPLE REST.GET.BUCKETPOLICY - "GET /awsexamplebucket1?policy HTTP/1.1" 404 NoSuchBucketPolicy 297 - 38 - "-" "S3Console/0.4" - BNaBsXZQQDbssi6xMBdBU2sLt+Yf5kZDmeBUP35sFoKa3sLLeMC78iwEIWxs99CRUrbS4n11234= SigV2 ECDHE-RSA-AES128-GCM-SHA256 AuthHeader awsexamplebucket1.s3.us-west-1.amazonaws.com TLSV1.1
4. 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be awsexamplebucket1 [06/Feb/2019:00:01:00 +0000] 192.0.2.3 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be 7B4A0FABBEXAMPLE REST.GET.VERSIONING - "GET /awsexamplebucket1?versioning HTTP/1.1" 200 - 113 - 33 - "-" "S3Console/0.4" - Ke1bUcazaN1jWuUlPJaxF64cQVpUEhoZKEG/hmy/gijN/I1DeWqDfFvnpybfEseEME/u7ME1234= SigV2 ECDHE-RSA-AES128-GCM-SHA256 AuthHeader awsexamplebucket1.s3.us-west-1.amazonaws.com TLSV1.1
5. 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be awsexamplebucket1 [06/Feb/2019:00:01:57 +0000] 192.0.2.3 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be DD6CC733AEXAMPLE REST.PUT.OBJECT s3-dg.pdf "PUT /awsexamplebucket1/s3-dg.pdf HTTP/1.1" 200 - - 4406583 41754 28 "-" "S3Console/0.4" - 10S62Zv81kBW7BB6SX4XJ48o6kpcl6LPwEoizZQQxJd5qDSCTLX0TgS37kYUBKQW3+bPdrg1234= SigV4 ECDHE-RSA-AES128-SHA AuthHeader awsexamplebucket1.s3.us-west-1.amazonaws.com TLSV1.1
```

**Note**  
Any field can be set to `-` to indicate that the data was unknown or unavailable, or that the field was not applicable to this request\. 

The following list describes the log record fields\.

**Bucket Owner**  
The canonical user ID of the owner of the source bucket\. The canonical user ID is another form of the AWS account ID\. For more information about the canonical user ID, see [AWS Account Identifiers](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html)\. For information about how to find the canonical user ID for your account, see [Finding Your Account Canonical User ID](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingCanonicalId)\.  
**Example Entry**  

```
79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be
```

**Bucket**  
The name of the bucket that the request was processed against\. If the system receives a malformed request and cannot determine the bucket, the request will not appear in any server access log\.  
**Example Entry**  

```
awsexamplebucket1
```

**Time**  
The time at which the request was received; these dates and times are in Coordinated Universal time \(UTC\)\. The format, using `strftime()` terminology, is as follows: `[%d/%b/%Y:%H:%M:%S %z]`  
**Example Entry**  

```
[06/Feb/2019:00:00:38 +0000]
```

**Remote IP**  
The apparent internet address of the requester\. Intermediate proxies and firewalls might obscure the actual address of the machine making the request\.  
**Example Entry**  

```
192.0.2.3
```

**Requester**  
The canonical user ID of the requester, or a `-` for unauthenticated requests\. If the requester was an IAM user, this field returns the requester's IAM user name along with the AWS root account that the IAM user belongs to\. This identifier is the same one used for access control purposes\.  
**Example Entry**  

```
79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be
```

**Request ID**  
A string generated by Amazon S3 to uniquely identify each request\.  
**Example Entry**  

```
3E57427F33A59F07
```

**Operation**  
The operation listed here is declared as `SOAP.operation`, `REST.HTTP_method.resource_type`, `WEBSITE.HTTP_method.resource_type`, or `BATCH.DELETE.OBJECT`, or `S3.action.resource_type` for [Lifecycle actions](https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-and-other-bucket-config.html#lifecycle-general-considerations-logging)\.  
**Example Entry**  

```
REST.PUT.OBJECT
```

**Key**  
The "key" part of the request, URL encoded, or "\-" if the operation does not take a key parameter\.  
**Example Entry**  

```
/photos/2019/08/puppy.jpg
```

**Request\-URI**  
The Request\-URI part of the HTTP request message\.  
**Example Entry**  

```
"GET /awsexamplebucket1/photos/2019/08/puppy.jpg?x-foo=bar HTTP/1.1"
```

**HTTP status**  
The numeric HTTP status code of the response\.  
**Example Entry**  

```
200
```

**Error Code**  
The Amazon S3 [Error code](ErrorCode.md), or "\-" if no error occurred\.  
**Example Entry**  

```
NoSuchBucket
```

**Bytes Sent**  
The number of response bytes sent, excluding HTTP protocol overhead, or "\-" if zero\.  
**Example Entry**  

```
2662992
```

**Object Size**  
The total size of the object in question\.  
**Example Entry**  

```
3462992
```

**Total Time**  
The number of milliseconds the request was in flight from the server's perspective\. This value is measured from the time your request is received to the time that the last byte of the response is sent\. Measurements made from the client's perspective might be longer due to network latency\.  
**Example Entry**  

```
70
```

**Turn\-Around Time**  
The number of milliseconds that Amazon S3 spent processing your request\. This value is measured from the time the last byte of your request was received until the time the first byte of the response was sent\.  
**Example Entry**  

```
10
```

**Referer**  
The value of the HTTP Referer header, if present\. HTTP user\-agents \(for example, browsers\) typically set this header to the URL of the linking or embedding page when making a request\.  
**Example Entry**  

```
"http://www.amazon.com/webservices"
```

**User\-Agent**  
The value of the HTTP User\-Agent header\.  
**Example Entry**  

```
"curl/7.15.1"
```

**Version Id**  
The version ID in the request, or "\-" if the operation does not take a `versionId` parameter\.  
**Example Entry**  

```
3HL4kqtJvjVBH40Nrjfkd
```

**Host Id**  
The x\-amz\-id\-2 or Amazon S3 extended request ID\.   
**Example Entry**  

```
s9lzHYrFp76ZVxRcpX9+5cjAnEH2ROuNkd2BHfIa6UkFVdtjf5mKR3/eTPFvsiP/XV/VLi31234=
```

**Signature Version**  
The signature version, `SigV2` or `SigV4`, that was used to authenticate the request or a `-` for unauthenticated requests\.  
**Example Entry**  

```
SigV2
```

**Cipher Suite**  
The Secure Sockets Layer \(SSL\) cipher that was negotiated for HTTPS request or a `-` for HTTP\.  
**Example Entry**  

```
ECDHE-RSA-AES128-GCM-SHA256
```

**Authentication Type**  
The type of request authentication used, `AuthHeader` for authentication headers, `QueryString` for query string \(pre\-signed URL\) or a `-` for unauthenticated requests\.  
**Example Entry**  

```
AuthHeader
```

**Host Header**  
The endpoint used to connect to Amazon S3  
**Example Entry**  

```
s3.us-west-2.amazonaws.com
```
Some older Regions support legacy endpoints\. You may see these endpoints in your server access logs or CloudTrail logs\. For more information, see [Legacy Endpoints](VirtualHosting.md#s3-legacy-endpoints)\. For a complete list of Amazon S3 Regions and endpoints, see [Amazon S3 Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\.

**TLS version**  
The Transport Layer Security \(TLS\) version negotiated by the client\. The value is one of following: `TLSv1`, `TLSv1.1`, `TLSv1.2`; or `-` if TLS wasn't used\.  
**Example Entry**  

```
TLSv1.2
```

## Additional Logging for Copy Operations<a name="AdditionalLoggingforCopyOperations"></a>

A copy operation involves a `GET` and a `PUT`\. For that reason, we log two records when performing a copy operation\. The previous table describes the fields related to the `PUT` part of the operation\. The following list describes the fields in the record that relate to the `GET` part of the copy operation\.

**Bucket Owner**  
The canonical user ID of the bucket that stores the object being copied\. The canonical user ID is another form of the AWS account ID\. For more information about the canonical user ID, see [AWS Account Identifiers](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html)\. For information about how to find the canonical user ID for your account, see [Finding Your Account Canonical User ID](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingCanonicalId)\.  
**Example Entry**  

```
79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be
```

**Bucket**  
The name of the bucket that stores the object being copied\.  
**Example Entry**  

```
awsexamplebucket1
```

**Time**  
The time at which the request was received; these dates and times are in Coordinated Universal time \(UTC\)\. The format, using `strftime()` terminology, is as follows: `[%d/%B/%Y:%H:%M:%S %z]`  
**Example Entry**  

```
[06/Feb/2019:00:00:38 +0000]
```

**Remote IP**  
The apparent internet address of the requester\. Intermediate proxies and firewalls might obscure the actual address of the machine making the request\.  
**Example Entry**  

```
192.0.2.3
```

**Requester**  
The canonical user ID of the requester, or a `-` for unauthenticated requests\. If the requester was an IAM user, this field will return the requester's IAM user name along with the AWS root account that the IAM user belongs to\. This identifier is the same one used for access control purposes\.  
**Example Entry**  

```
79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be
```

**Request ID**  
A string generated by Amazon S3 to uniquely identify each request\.  
**Example Entry**  

```
3E57427F33A59F07
```

**Operation**  
The operation listed here is declared as `SOAP.operation`, `REST.HTTP_method.resource_type`, `WEBSITE.HTTP_method.resource_type`, or `BATCH.DELETE.OBJECT`\.  
**Example Entry**  

```
REST.COPY.OBJECT_GET
```

**Key**  
The "key" of the object being copied or "\-" if the operation does not take a key parameter\.   
**Example Entry**  

```
/photos/2019/08/puppy.jpg
```

**Request\-URI**  
The Request\-URI part of the HTTP request message\.  
**Example Entry**  

```
"GET /awsexamplebucket1/photos/2019/08/puppy.jpg?x-foo=bar"
```

**HTTP status**  
The numeric HTTP status code of the `GET` portion of the copy operation\.  
**Example Entry**  

```
200
```

**Error Code**  
The Amazon S3 [Error code](ErrorCode.md), of the `GET` portion of the copy operation or "\-" if no error occurred\.  
**Example Entry**  

```
NoSuchBucket
```

**Bytes Sent**  
The number of response bytes sent, excluding HTTP protocol overhead, or "\-" if zero\.  
**Example Entry**  

```
2662992
```

**Object Size**  
The total size of the object in question\.  
**Example Entry**  

```
3462992
```

**Total Time**  
The number of milliseconds the request was in flight from the server's perspective\. This value is measured from the time your request is received to the time that the last byte of the response is sent\. Measurements made from the client's perspective might be longer due to network latency\.  
**Example Entry**  

```
70
```

**Turn\-Around Time**  
The number of milliseconds that Amazon S3 spent processing your request\. This value is measured from the time the last byte of your request was received until the time the first byte of the response was sent\.  
**Example Entry**  

```
10
```

**Referer**  
The value of the HTTP Referer header, if present\. HTTP user\-agents \(for example, browsers\) typically set this header to the URL of the linking or embedding page when making a request\.  
**Example Entry**  

```
"http://www.amazon.com/webservices"
```

**User\-Agent**  
The value of the HTTP User\-Agent header\.  
**Example Entry**  

```
"curl/7.15.1"
```

**Version Id**  
The version ID of the object being copied or "\-" if the `x-amz-copy-source` header didn’t specify a `versionId` parameter as part of the copy source\.  
**Example Entry**  

```
3HL4kqtJvjVBH40Nrjfkd
```

**Host Id**  
The x\-amz\-id\-2 or Amazon S3 extended request ID\.  
**Example Entry**  

```
s9lzHYrFp76ZVxRcpX9+5cjAnEH2ROuNkd2BHfIa6UkFVdtjf5mKR3/eTPFvsiP/XV/VLi31234=
```

**Signature Version**  
The signature version, `SigV2` or `SigV4`, that was used to authenticate the request or a `-` for unauthenticated requests\.  
**Example Entry**  

```
SigV2
```

**Cipher Suite**  
The Secure Sockets Layer \(SSL\) cipher that was negotiated for HTTPS request or a `-` for HTTP\.  
**Example Entry**  

```
ECDHE-RSA-AES128-GCM-SHA256
```

**Authentication Type**  
The type of request authentication used, `AuthHeader` for authentication headers, `QueryString` for query string \(pre\-signed URL\) or a `-` for unauthenticated requests\.  
**Example Entry**  

```
AuthHeader
```

**Host Header**  
The endpoint used to connect to Amazon S3\.  
**Example Entry**  

```
s3.us-west-2.amazonaws.com
```
Some older Regions support legacy endpoints\. You may see these endpoints in your server access logs or CloudTrail logs\. For more information, see [Legacy Endpoints](VirtualHosting.md#s3-legacy-endpoints)\. For a complete list of Amazon S3 Regions and endpoints, see [Amazon S3 Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\.

**TLS version**  
The Transport Layer Security \(TLS\) version negotiated by the client\. The value is one of following: `TLSv1`, `TLSv1.1`, `TLSv1.2`; or `-` if TLS wasn't used\.  
**Example Entry**  

```
TLSv1.2
```

## Custom Access Log Information<a name="LogFormatCustom"></a>

You can include custom information to be stored in the access log record for a request by adding a custom query\-string parameter to the URL for the request\. Amazon S3 ignores query\-string parameters that begin with "x\-", but includes those parameters in the access log record for the request, as part of the `Request-URI` field of the log record\. For example, a `GET` request for "s3\.amazonaws\.com/awsexamplebucket1/photos/2019/08/puppy\.jpg?x\-user=johndoe" works the same as the same request for "s3\.amazonaws\.com/awsexamplebucket1/photos/2019/08/puppy\.jpg", except that the "x\-user=johndoe" string is included in the `Request-URI` field for the associated log record\. This functionality is available in the REST interface only\.

## Programming Considerations for Extensible Server Access Log Format<a name="LogFormatExtensible"></a>

From time to time, we might extend the access log record format by adding new fields to the end of each line\. Code that parses server access logs must be written to handle trailing fields that it does not understand\. 


# Transitioning objects using Amazon S3 Lifecycle<a name="lifecycle-transition-general-considerations"></a>

You can add rules in an S3 Lifecycle configuration to tell Amazon S3 to transition objects to another Amazon S3 [storage class](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)\. For example:
+ When you know that objects are infrequently accessed, you might transition them to the S3 Standard\-IA storage class\.
+ You might want to archive objects that you don't need to access in real time to the S3 Glacier storage class\.

 The following sections describe supported transitions, related constraints, and transitioning to the S3 Glacier storage class\.

## Supported transitions and related constraints<a name="lifecycle-general-considerations-transition-sc"></a>

In an S3 Lifecycle configuration, you can define rules to transition objects from one storage class to another to save on storage costs\. When you don't know the access patterns of your objects, or your access patterns are changing over time, you can transition the objects to the S3 Intelligent\-Tiering storage class for automatic cost savings\. For information about storage classes, see [Amazon S3 storage classes](storage-class-intro.md)\. 

Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the following diagram\. 

![\[Amazon S3 storage class waterfall graphic.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/lifecycle-transitions-v2.png)

### Supported lifecycle transitions<a name="supported-lifecycle-transitions"></a>

Amazon S3 supports the following lifecycle transitions between storage classes using an S3 Lifecycle configuration\. 

You *can transition* from the following:
+ The S3 Standard storage class to any other storage class\.
+ Any storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes\. 
+ The S3 Standard\-IA storage class to the S3 Intelligent\-Tiering or S3 One Zone\-IA storage classes\.
+ The S3 Intelligent\-Tiering storage class to the S3 One Zone\-IA storage class\.
+ The S3 Glacier storage class to the S3 Glacier Deep Archive storage class\.

### Unsupported lifecycle transitions<a name="unsupported-lifecycle-transitions"></a>

Amazon S3 does not support any of the following lifecycle transitions\. 

You *can't transition* from the following:
+ Any storage class to the S3 Standard storage class\.
+ Any storage class to the Reduced Redundancy storage class\.
+ The S3 Intelligent\-Tiering storage class to the S3 Standard\-IA storage class\.
+ The S3 One Zone\-IA storage class to the S3 Standard\-IA or S3 Intelligent\-Tiering storage classes\.

### Constraints<a name="lifecycle-configuration-constraints"></a>

Lifecycle storage class transitions have the following constraints:

**Object size and transitions from S3 Standard or S3 Standard\-IA to S3 Intelligent\-Tiering, S3 Standard\-IA, or S3 One Zone\-IA**  
When you transition objects from the S3 Standard or S3 Standard\-IA storage classes to S3 Intelligent\-Tiering, S3 Standard\-IA, or S3 One Zone\-IA, the following object size constraints apply:
+ **Larger objects** ‐ For the following transitions, there is a cost benefit to transitioning larger objects:
  + From the S3 Standard or S3 Standard\-IA storage classes to S3 Intelligent\-Tiering\.
  + From the S3 Standard storage class to S3 Standard\-IA or S3 One Zone\-IA\.
+  **Objects smaller than 128 KB ** ‐ For the following transitions, Amazon S3 does not transition objects that are smaller than 128 KB because it's not cost effective:
  + From the S3 Standard or S3 Standard\-IA storage classes to S3 Intelligent\-Tiering\.
  + From the S3 Standard storage class to S3 Standard\-IA or S3 One Zone\-IA\.

**Minimum days for transition from S3 Standard or S3 Standard\-IA to S3 Standard\-IA or S3 One Zone\-IA**  
Before you transition objects from the S3 Standard or S3 Standard\-IA storage classes to S3 Standard\-IA or S3 One Zone\-IA, you must store them at least 30 days in the S3 Standard storage class\. For example, you cannot create a Lifecycle rule to transition objects to the S3 Standard\-IA storage class one day after you create them\. Amazon S3 doesn't transition objects within the first 30 days because newer objects are often accessed more frequently or deleted sooner than is suitable for S3 Standard\-IA or S3 One Zone\-IA storage\.

Similarly, if you are transitioning noncurrent objects \(in versioned buckets\), you can transition only objects that are at least 30 days noncurrent to S3 Standard\-IA or S3 One Zone\-IA storage\. 

**Minimum 30\-Day storage charge for S3 Intelligent\-Tiering, S3 Standard\-IA, and S3 One Zone\-IA**  
The S3 Intelligent\-Tiering, S3 Standard\-IA, and S3 One Zone\-IA storage classes have a minimum 30\-day storage charge\. Therefore, you can't specify a single Lifecycle rule for both an S3 Intelligent\-Tiering, S3 Standard\-IA, or S3 One Zone\-IA transition and a S3 Glacier or S3 Glacier Deep Archive transition when the S3 Glacier or S3 Glacier Deep Archive transition occurs less than 30 days after the S3 Intelligent\-Tiering, S3 Standard\-IA, or S3 One Zone\-IA transition\.

The same 30\-day minimum applies when you specify a transition from S3 Standard\-IA storage to S3 One Zone\-IA or S3 Intelligent\-Tiering storage\. You can specify two rules to accomplish this, but you pay minimum storage charges\. For more information about cost considerations, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

### Manage an object's complete lifecycle<a name="manage-complete-object-lifecycle"></a>

You can combine these S3 Lifecycle actions to manage an object's complete lifecycle\.  For example, suppose that the objects you create have a well\-defined lifecycle\. Initially, the objects are frequently accessed for a period of 30 days\. Then, objects are infrequently accessed for up to 90 days\. After that, the objects are no longer needed, so you might choose to archive or delete them\. 

In this scenario, you can create an S3 Lifecycle rule in which you specify the initial transition action to S3 Intelligent\-Tiering, S3 Standard\-IA, or S3 One Zone\-IA storage, another transition action to S3 Glacier storage for archiving, and an expiration action\. As you move the objects from one storage class to another, you save on storage cost\. For more information about cost considerations, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

## Transitioning to the S3 Glacier and S3 Glacier Deep Archive storage classes \(object archival\)<a name="before-deciding-to-archive-objects"></a>

Using S3 Lifecycle configuration, you can transition objects to the S3 Glacier or S3 Glacier Deep Archive storage classes for archiving\. When you choose the S3 Glacier or S3 Glacier Deep Archive storage class, your objects remain in Amazon S3\. You cannot access them directly through the separate Amazon S3 Glacier service\. 

Before you archive objects, review the following sections for relevant considerations\.

### General considerations<a name="transition-glacier-general-considerations"></a>

The following are the general considerations for you to consider before you archive objects:
+ Encrypted objects remain encrypted throughout the storage class transition process\.
+ Objects that are stored in the S3 Glacier or S3 Glacier Deep Archive storage classes are not available in real time\.

  Archived objects are Amazon S3 objects, but before you can access an archived object, you must first restore a temporary copy of it\. The restored object copy is available only for the duration you specify in the restore request\. After that, Amazon S3 deletes the temporary copy, and the object remains archived in Amazon S3 Glacier\. 

  You can restore an object by using the Amazon S3 console or programmatically by using the AWS SDK wrapper libraries or the Amazon S3 REST API in your code\. For more information, see [Restoring archived objects](restoring-objects.md)\.
+ Objects that are stored in the S3 Glacier storage class can only be transitioned to the S3 Glacier Deep Archive storage class\.

  You can use an S3 Lifecycle configuration rule to convert the storage class of an object from S3 Glacier to the S3 Glacier Deep Archive storage class only\. If you want to change the storage class of an object that is stored in S3 Glacier to a storage class other than S3 Glacier Deep Archive, you must use the restore operation to make a temporary copy of the object first\. Then use the copy operation to overwrite the object specifying S3 Standard, S3 Intelligent\-Tiering, S3 Standard\-IA, S3 One Zone\-IA, or Reduced Redundancy as the storage class\.
+ The transition of objects to the S3 Glacier Deep Archive storage class can go only one way\.

  You cannot use an S3 Lifecycle configuration rule to convert the storage class of an object from S3 Glacier Deep Archive to any other storage class\. If you want to change the storage class of an archived object to another storage class, you must use the restore operation to make a temporary copy of the object first\. Then use the copy operation to overwrite the object specifying S3 Standard, S3 Intelligent\-Tiering, S3 Standard\-IA, S3 One Zone\-IA, S3 Glacier, or Reduced Redundancy as the storage class\.
+ The objects that are stored in the S3 Glacier and S3 Glacier Deep Archive storage classes are visible and available only through Amazon S3\. They are not available through the separate Amazon S3 Glacier service\.

  These are Amazon S3 objects, and you can access them only by using the Amazon S3 console or the Amazon S3 API\. You cannot access the archived objects through the separate Amazon S3 Glacier console or the Amazon S3 Glacier API\.

### Cost considerations<a name="glacier-pricing-considerations"></a>

If you are planning to archive infrequently accessed data for a period of months or years, the S3 Glacier and S3 Glacier Deep Archive storage classes can reduce your storage costs\. However, to ensure that the S3 Glacier or S3 Glacier Deep Archive storage class is appropriate for you, consider the following:
+ **Storage overhead charges** – When you transition objects to the S3 Glacier or S3 Glacier Deep Archive storage class, a fixed amount of storage is added to each object to accommodate metadata for managing the object\.
  + For each object archived to S3 Glacier or S3 Glacier Deep Archive, Amazon S3 uses 8 KB of storage for the name of the object and other metadata\. Amazon S3 stores this metadata so that you can get a real\-time list of your archived objects by using the Amazon S3 API\. For more information, see [Get Bucket \(List Objects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html)\. You are charged Amazon S3 Standard rates for this additional storage\.
  +  For each object that is archived to S3 Glacier or S3 Glacier Deep Archive, Amazon S3 adds 32 KB of storage for index and related metadata\. This extra data is necessary to identify and restore your object\. You are charged S3 Glacier or S3 Glacier Deep Archive rates for this additional storage\.

  If you are archiving small objects, consider these storage charges\. Also consider aggregating many small objects into a smaller number of large objects to reduce overhead costs\.
+ **Number of days you plan to keep objects archived**—S3 Glacier and S3 Glacier Deep Archive are long\-term archival solutions\. The minimal storage duration period is 90 days for the S3 Glacier storage class and 180 days for S3 Glacier Deep Archive\. Deleting data that is archived to Amazon S3 Glacier is free if the objects you delete are archived for more than the minimal storage duration period\. If you delete or overwrite an archived object within the minimal duration period, Amazon S3 charges a prorated early deletion fee\. For information about the early deletion fee, see [How am I charged for deleting objects from Amazon S3 Glacier that are less than 90 days old?](https://aws.amazon.com/s3/faqs/#How_am_I_charged_for_deleting_objects_from_Amazon_Glacier_that_are_less_than_90_days_old)
+ ** S3 Glacier and S3 Glacier Deep Archive transition request charges**— Each object that you transition to the S3 Glacier or S3 Glacier Deep Archive storage class constitutes one transition request\. There is a cost for each such request\. If you plan to transition a large number of objects, consider the request costs\. If you are archiving small objects, consider aggregating many small objects into a smaller number of large objects to reduce transition request costs\.
+ ** S3 Glacier and S3 Glacier Deep Archive data restore charges**—S3 Glacier and S3 Glacier Deep Archive are designed for long\-term archival of data that you access infrequently\. For information about data restoration charges, see [How much does it cost to retrieve data from Amazon S3 Glacier?](https://aws.amazon.com/s3/faqs/#How_much_does_it_cost_to_retrieve_data_from_Amazon_S3_Glacier) in the Amazon S3 FAQ\. For information about how to restore data from Amazon S3 Glacier, see [Restoring archived objects](restoring-objects.md)\. 

When you archive objects to Amazon S3 Glacier by using S3 Lifecycle management, Amazon S3 transitions these objects asynchronously\. There might be a delay between the transition date in the Lifecycle configuration rule and the date of the physical transition\. You are charged Amazon S3 Glacier prices based on the transition date specified in the rule\. For more information, see the Amazon S3 Glacier section of the [Amazon S3 FAQ](https://aws.amazon.com/s3/faqs/)\.

The Amazon S3 product detail page provides pricing information and example calculations for archiving Amazon S3 objects\. For more information, see the following topics:
+  [How is my storage charge calculated for Amazon S3 objects archived to Amazon S3 Glacier?](https://aws.amazon.com/s3/faqs/#How_is_my_storage_charge_calculated_for_Amazon_S3_objects_archived_to_Amazon_Glacier) 
+  [How am I charged for deleting objects from Amazon S3 Glacier that are less than 90 days old?](https://aws.amazon.com/s3/faqs/#How_am_I_charged_for_deleting_objects_from_Amazon_Glacier_that_are_less_than_90_days_old) 
+  [ How much does it cost to retrieve data from Amazon S3 Glacier?](https://aws.amazon.com/s3/faqs/#How_much_does_it_cost_to_retrieve_data_from_Amazon_S3_Glacier) 
+  [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/) for storage costs for the different storage classes\. 



### Restoring archived objects<a name="restore-glacier-objects-concepts"></a>

Archived objects are not accessible in real time\. You must first initiate a restore request and then wait until a temporary copy of the object is available for the duration that you specify in the request\. After you receive a temporary copy of the restored object, the object's storage class remains S3 Glacier or S3 Glacier Deep Archive\. \(A [HEAD Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html) or [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) API operation request will return S3 Glacier or S3 Glacier Deep Archive as the storage class\.\) 

**Note**  
When you restore an archive, you are paying for both the archive \(S3 Glacier or S3 Glacier Deep Archive rate\) and a copy that you restored temporarily \(Reduced Redundancy storage rate\)\. For information about pricing, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\. 

You can restore an object copy programmatically or by using the Amazon S3 console\. Amazon S3 processes only one restore request at a time per object\. For more information, see [Restoring archived objects](restoring-objects.md)\.


# Example: Requesting S3 Batch Operations completion reports<a name="batch-ops-examples-reports"></a>

When you create an S3 Batch Operations job, you can request a completion report for all tasks or just for failed tasks\. As long as at least one task has been invoked successfully, S3 Batch Operations generates a report for jobs that have completed, failed, or been canceled\.

The completion report contains additional information for each task, including the object key name and version, status, error codes, and descriptions of any errors\. The description of errors for each failed task can be used to diagnose issues that occur during job creation, such as permissions\.

**Example — top\-level manifest result file**  
The top\-level `manifest.json` file contains the locations of each succeeded report and \(if the job had any failures\) the location of failed reports, as shown in the following example\.  

```
{
    "Format": "Report_CSV_20180820",
    "ReportCreationDate": "2019-04-05T17:48:39.725Z",
    "Results": [
        {
            "TaskExecutionStatus": "succeeded",
            "Bucket": "my-job-reports",
            "MD5Checksum": "83b1c4cbe93fc893f54053697e10fd6e",
            "Key": "job-f8fb9d89-a3aa-461d-bddc-ea6a1b131955/results/6217b0fab0de85c408b4be96aeaca9b195a7daa5.csv"
        },
        {
            "TaskExecutionStatus": "failed",
            "Bucket": "my-job-reports",
            "MD5Checksum": "22ee037f3515975f7719699e5c416eaa",
            "Key": "job-f8fb9d89-a3aa-461d-bddc-ea6a1b131955/results/b2ddad417e94331e9f37b44f1faf8c7ed5873f2e.csv"
        }
    ],
    "ReportSchema": "Bucket, Key, VersionId, TaskStatus, ErrorCode, HTTPStatusCode, ResultMessage"
}
```


**Example — failed tasks reports**  
Failed tasks reports contain the following information for all *failed* tasks:  
+ `Bucket`
+ `Key`
+ `VersionId`
+ `TaskStatus`
+ `ErrorCode`
+ `HTTPStatusCode`
+ `ResultMessage`
The following example report shows a case in which the AWS Lambda function timed out, causing failures to exceed the failure threshold\. It was then marked as a `PermanentFailure`\.  

```
awsexamplebucket1,image_14975,,failed,200,PermanentFailure,"Lambda returned function error: {""errorMessage"":""2019-04-05T17:35:21.155Z 2845ca0d-38d9-4c4b-abcf-379dc749c452 Task timed out after 3.00 seconds""}"
awsexamplebucket1,image_15897,,failed,200,PermanentFailure,"Lambda returned function error: {""errorMessage"":""2019-04-05T17:35:29.610Z 2d0a330b-de9b-425f-b511-29232fde5fe4 Task timed out after 3.00 seconds""}"
awsexamplebucket1,image_14819,,failed,200,PermanentFailure,"Lambda returned function error: {""errorMessage"":""2019-04-05T17:35:22.362Z fcf5efde-74d4-4e6d-b37a-c7f18827f551 Task timed out after 3.00 seconds""}"
awsexamplebucket1,image_15930,,failed,200,PermanentFailure,"Lambda returned function error: {""errorMessage"":""2019-04-05T17:35:29.809Z 3dd5b57c-4a4a-48aa-8a35-cbf027b7957e Task timed out after 3.00 seconds""}"
awsexamplebucket1,image_17644,,failed,200,PermanentFailure,"Lambda returned function error: {""errorMessage"":""2019-04-05T17:35:46.025Z 10a764e4-2b26-4d8c-9056-1e1072b4723f Task timed out after 3.00 seconds""}"
awsexamplebucket1,image_17398,,failed,200,PermanentFailure,"Lambda returned function error: {""errorMessage"":""2019-04-05T17:35:44.661Z 1e306352-4c54-4eba-aee8-4d02f8c0235c Task timed out after 3.00 seconds""}"
```


**Example — succeeded tasks report**  
Succeeded tasks reports contain the following for the *completed* tasks:  
+ `Bucket`
+ `Key`
+ `VersionId`
+ `TaskStatus`
+ `ErrorCode`
+ `HTTPStatusCode`
+ `ResultMessage`
In the following example, the Lambda function successfully copied the Amazon S3 object to another bucket\. The returned Amazon S3 response is passed back to S3 Batch Operations and is then written into the final completion report\.  

```
awsexamplebucket1,image_17775,,succeeded,200,,"{u'CopySourceVersionId': 'xVR78haVKlRnurYofbTfYr3ufYbktF8h', u'CopyObjectResult': {u'LastModified': datetime.datetime(2019, 4, 5, 17, 35, 39, tzinfo=tzlocal()), u'ETag': '""fe66f4390c50f29798f040d7aae72784""'}, 'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HostId': 'nXNaClIMxEJzWNmeMNQV2KpjbaCJLn0OGoXWZpuVOFS/iQYWxb3QtTvzX9SVfx2lA3oTKLwImKw=', 'RequestId': '3ED5852152014362', 'HTTPHeaders': {'content-length': '234', 'x-amz-id-2': 'nXNaClIMxEJzWNmeMNQV2KpjbaCJLn0OGoXWZpuVOFS/iQYWxb3QtTvzX9SVfx2lA3oTKLwImKw=', 'x-amz-copy-source-version-id': 'xVR78haVKlRnurYofbTfYr3ufYbktF8h', 'server': 'AmazonS3', 'x-amz-request-id': '3ED5852152014362', 'date': 'Fri, 05 Apr 2019 17:35:39 GMT', 'content-type': 'application/xml'}}}"
awsexamplebucket1,image_17763,,succeeded,200,,"{u'CopySourceVersionId': '6HjOUSim4Wj6BTcbxToXW44pSZ.40pwq', u'CopyObjectResult': {u'LastModified': datetime.datetime(2019, 4, 5, 17, 35, 39, tzinfo=tzlocal()), u'ETag': '""fe66f4390c50f29798f040d7aae72784""'}, 'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HostId': 'GiCZNYr8LHd/Thyk6beTRP96IGZk2sYxujLe13TuuLpq6U2RD3we0YoluuIdm1PRvkMwnEW1aFc=', 'RequestId': '1BC9F5B1B95D7000', 'HTTPHeaders': {'content-length': '234', 'x-amz-id-2': 'GiCZNYr8LHd/Thyk6beTRP96IGZk2sYxujLe13TuuLpq6U2RD3we0YoluuIdm1PRvkMwnEW1aFc=', 'x-amz-copy-source-version-id': '6HjOUSim4Wj6BTcbxToXW44pSZ.40pwq', 'server': 'AmazonS3', 'x-amz-request-id': '1BC9F5B1B95D7000', 'date': 'Fri, 05 Apr 2019 17:35:39 GMT', 'content-type': 'application/xml'}}}"
awsexamplebucket1,image_17860,,succeeded,200,,"{u'CopySourceVersionId': 'm.MDD0g_QsUnYZ8TBzVFrp.TmjN8PJyX', u'CopyObjectResult': {u'LastModified': datetime.datetime(2019, 4, 5, 17, 35, 40, tzinfo=tzlocal()), u'ETag': '""fe66f4390c50f29798f040d7aae72784""'}, 'ResponseMetadata': {'HTTPStatusCode': 200, 'RetryAttempts': 0, 'HostId': 'F9ooZOgpE5g9sNgBZxjdiPHqB4+0DNWgj3qbsir+sKai4fv7rQEcF2fBN1VeeFc2WH45a9ygb2g=', 'RequestId': '8D9CA56A56813DF3', 'HTTPHeaders': {'content-length': '234', 'x-amz-id-2': 'F9ooZOgpE5g9sNgBZxjdiPHqB4+0DNWgj3qbsir+sKai4fv7rQEcF2fBN1VeeFc2WH45a9ygb2g=', 'x-amz-copy-source-version-id': 'm.MDD0g_QsUnYZ8TBzVFrp.TmjN8PJyX', 'server': 'AmazonS3', 'x-amz-request-id': '8D9CA56A56813DF3', 'date': 'Fri, 05 Apr 2019 17:35:40 GMT', 'content-type': 'application/xml'}}}"
```


**Analyzing the results can be easily done using Athena**   
Use the following SQL to create the table, while replacing the location of the results file to the directory that contains the different csv result files of your latest batch operation
```
CREATE EXTERNAL TABLE IF NOT EXISTS default.batch_operation_results (
  `bucket` string,
  `key` string,
  `versionid` string,
  `taskstatus` string,
  `errorcode` string,
  `httpstatuscode` string,
  `resultmessage` string 
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
  'serialization.format' = ',',
  'field.delim' = ','
) LOCATION 's3://YOUR-BATCH-OPERATIONS-RESULT-DIRECTORY'
TBLPROPERTIES ('has_encrypted_data'='false');
```



# Deleting multiple objects using the AWS SDK for \.NET<a name="DeletingMultipleObjectsUsingNetSDK"></a>

The AWS SDK for \.NET provides a convenient method for deleting multiple objects: `DeleteObjects`\. For each object that you want to delete, you specify the key name and the version of the object\. If the bucket is not versioning\-enabled, you specify `null` for the version ID\. If an exception occurs, review the `DeleteObjectsException` response to determine which objects were not deleted and why\. 

**Example Deleting multiple objects from a non\-versioning bucket**  
The following C\# example uses the multi\-object delete API to delete objects from a bucket that is not version\-enabled\. The example uploads the sample objects to the bucket, and then uses the `DeleteObjects` method to delete the objects in a single request\. In the `DeleteObjectsRequest`, the example specifies only the object key names because the version IDs are null\.  
For information about creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DeleteMultipleObjectsNonVersionedBucketTest
    {
        private const string bucketName = "*** versioning-enabled bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            MultiObjectDeleteAsync().Wait();
        }

        static async Task MultiObjectDeleteAsync()
        {
            // Create sample objects (for subsequent deletion).
            var keysAndVersions = await PutObjectsAsync(3);

            // a. multi-object delete by specifying the key names and version IDs.
            DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest
            {
                BucketName = bucketName,
                Objects = keysAndVersions // This includes the object keys and null version IDs.
            };
            // You can add specific object key to the delete request using the .AddKey.
            // multiObjectDeleteRequest.AddKey("TickerReference.csv", null);
            try
            {
                DeleteObjectsResponse response = await s3Client.DeleteObjectsAsync(multiObjectDeleteRequest);
                Console.WriteLine("Successfully deleted all the {0} items", response.DeletedObjects.Count);
            }
            catch (DeleteObjectsException e)
            {
                PrintDeletionErrorStatus(e);
            }
        }

        private static void PrintDeletionErrorStatus(DeleteObjectsException e)
        {
            // var errorResponse = e.ErrorResponse;
            DeleteObjectsResponse errorResponse = e.Response;
            Console.WriteLine("x {0}", errorResponse.DeletedObjects.Count);

            Console.WriteLine("No. of objects successfully deleted = {0}", errorResponse.DeletedObjects.Count);
            Console.WriteLine("No. of objects failed to delete = {0}", errorResponse.DeleteErrors.Count);

            Console.WriteLine("Printing error data...");
            foreach (DeleteError deleteError in errorResponse.DeleteErrors)
            {
                Console.WriteLine("Object Key: {0}\t{1}\t{2}", deleteError.Key, deleteError.Code, deleteError.Message);
            }
        }

        static async Task<List<KeyVersion>> PutObjectsAsync(int number)
        {
            List<KeyVersion> keys = new List<KeyVersion>();
            for (int i = 0; i < number; i++)
            {
                string key = "ExampleObject-" + new System.Random().Next();
                PutObjectRequest request = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = key,
                    ContentBody = "This is the content body!",
                };

                PutObjectResponse response = await s3Client.PutObjectAsync(request);
                KeyVersion keyVersion = new KeyVersion
                {
                    Key = key,
                    // For non-versioned bucket operations, we only need object key.
                    // VersionId = response.VersionId
                };
                keys.Add(keyVersion);
            }
            return keys;
        }
    }
}
```

**Example Multi\-object deletion for a version\-enabled bucket**  
The following C\# example uses the multi\-object delete API to delete objects from a version\-enabled bucket\. The example performs the following actions:  

1. Creates sample objects and deletes them by specifying the key name and version ID for each object\. The operation deletes specific versions of the objects\.

1. Creates sample objects and deletes them by specifying only the key names\. Because the example doesn't specify version IDs, the operation only adds delete markers\. It doesn't delete any specific versions of the objects\. After deletion, these objects don't appear in the Amazon S3 console\.

1. Deletes the delete markers by specifying the object keys and version IDs of the delete markers\. When the operation deletes the delete markers, the objects reappear in the console\.
For information about creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DeleteMultipleObjVersionedBucketTest
    {
        private const string bucketName = "*** versioning-enabled bucket name ***"; 
       // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            DeleteMultipleObjectsFromVersionedBucketAsync().Wait();
        }

        private static async Task DeleteMultipleObjectsFromVersionedBucketAsync()
        {

            // Delete objects (specifying object version in the request).
            await DeleteObjectVersionsAsync();

            // Delete objects (without specifying object version in the request). 
            var deletedObjects = await DeleteObjectsAsync();

            // Additional exercise - remove the delete markers S3 returned in the preceding response. 
            // This results in the objects reappearing in the bucket (you can 
            // verify the appearance/disappearance of objects in the console).
            await RemoveDeleteMarkersAsync(deletedObjects);
        }

        private static async Task<List<DeletedObject>> DeleteObjectsAsync()
        {
            // Upload the sample objects.
            var keysAndVersions2 = await PutObjectsAsync(3);

            // Delete objects using only keys. Amazon S3 creates a delete marker and 
            // returns its version ID in the response.
            List<DeletedObject> deletedObjects = await NonVersionedDeleteAsync(keysAndVersions2);
            return deletedObjects;
        }

        private static async Task DeleteObjectVersionsAsync()
        {
            // Upload the sample objects.
            var keysAndVersions1 = await PutObjectsAsync(3);

            // Delete the specific object versions.
            await VersionedDeleteAsync(keysAndVersions1);
        }

        private static void PrintDeletionReport(DeleteObjectsException e)
        {
            var errorResponse = e.Response;
            Console.WriteLine("No. of objects successfully deleted = {0}", errorResponse.DeletedObjects.Count);
            Console.WriteLine("No. of objects failed to delete = {0}", errorResponse.DeleteErrors.Count);
            Console.WriteLine("Printing error data...");
            foreach (var deleteError in errorResponse.DeleteErrors)
            {
                Console.WriteLine("Object Key: {0}\t{1}\t{2}", deleteError.Key, deleteError.Code, deleteError.Message);
            }
        }

        static async Task VersionedDeleteAsync(List<KeyVersion> keys)
        {
            // a. Perform a multi-object delete by specifying the key names and version IDs.
            var multiObjectDeleteRequest = new DeleteObjectsRequest
            {
                BucketName = bucketName,
                Objects = keys // This includes the object keys and specific version IDs.
            };
            try
            {
                Console.WriteLine("Executing VersionedDelete...");
                DeleteObjectsResponse response = await s3Client.DeleteObjectsAsync(multiObjectDeleteRequest);
                Console.WriteLine("Successfully deleted all the {0} items", response.DeletedObjects.Count);
            }
            catch (DeleteObjectsException e)
            {
                PrintDeletionReport(e);
            }
        }

        static async Task<List<DeletedObject>> NonVersionedDeleteAsync(List<KeyVersion> keys)
        {
            // Create a request that includes only the object key names.
            DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest();
            multiObjectDeleteRequest.BucketName = bucketName;

            foreach (var key in keys)
            {
                multiObjectDeleteRequest.AddKey(key.Key);
            }
            // Execute DeleteObjects - Amazon S3 add delete marker for each object
            // deletion. The objects disappear from your bucket. 
            // You can verify that using the Amazon S3 console.
            DeleteObjectsResponse response;
            try
            {
                Console.WriteLine("Executing NonVersionedDelete...");
                response = await s3Client.DeleteObjectsAsync(multiObjectDeleteRequest);
                Console.WriteLine("Successfully deleted all the {0} items", response.DeletedObjects.Count);
            }
            catch (DeleteObjectsException e)
            {
                PrintDeletionReport(e);
                throw; // Some deletes failed. Investigate before continuing.
            }
            // This response contains the DeletedObjects list which we use to delete the delete markers.
            return response.DeletedObjects;
        }

        private static async Task RemoveDeleteMarkersAsync(List<DeletedObject> deletedObjects)
        {
            var keyVersionList = new List<KeyVersion>();

            foreach (var deletedObject in deletedObjects)
            {
                KeyVersion keyVersion = new KeyVersion
                {
                    Key = deletedObject.Key,
                    VersionId = deletedObject.DeleteMarkerVersionId
                };
                keyVersionList.Add(keyVersion);
            }
            // Create another request to delete the delete markers.
            var multiObjectDeleteRequest = new DeleteObjectsRequest
            {
                BucketName = bucketName,
                Objects = keyVersionList
            };

            // Now, delete the delete marker to bring your objects back to the bucket.
            try
            {
                Console.WriteLine("Removing the delete markers .....");
                var deleteObjectResponse = await s3Client.DeleteObjectsAsync(multiObjectDeleteRequest);
                Console.WriteLine("Successfully deleted all the {0} delete markers",
                                            deleteObjectResponse.DeletedObjects.Count);
            }
            catch (DeleteObjectsException e)
            {
                PrintDeletionReport(e);
            }
        }

        static async Task<List<KeyVersion>> PutObjectsAsync(int number)
        {
            var keys = new List<KeyVersion>();

            for (var i = 0; i < number; i++)
            {
                string key = "ObjectToDelete-" + new System.Random().Next();
                PutObjectRequest request = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = key,
                    ContentBody = "This is the content body!",

                };

                var response = await s3Client.PutObjectAsync(request);
                KeyVersion keyVersion = new KeyVersion
                {
                    Key = key,
                    VersionId = response.VersionId
                };

                keys.Add(keyVersion);
            }
            return keys;
        }
    }
}
```


# Configuration and vulnerability analysis in Amazon S3<a name="vulnerability-analysis-and-management"></a>

AWS handles basic security tasks like guest operating system \(OS\) and database patching, firewall configuration, and disaster recovery\. These procedures have been reviewed and certified by the appropriate third parties\. For more details, see the following resources:
+ [Compliance Validation for Amazon S3](s3-compliance.md)
+ [Shared Responsibility Model](https://aws.amazon.com/compliance/shared-responsibility-model/)
+ [Amazon Web Services: Overview of Security Processes](https://d0.awsstatic.com/whitepapers/Security/AWS_Security_Whitepaper.pdf)

The following security best practices also address configuration and vulnerability analysis in Amazon S3:
+ [Identify and audit all your Amazon S3 buckets](security-best-practices.md#audit)
+ [Enable AWS Config](security-best-practices.md#config)


# Amazon Simple Storage Service Developer Guide

-----
*****Copyright &copy; 2020 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.*****

-----
Amazon's trademarks and trade dress may not be used in 
     connection with any product or service that is not Amazon's, 
     in any manner that is likely to cause confusion among customers, 
     or in any manner that disparages or discredits Amazon. All other 
     trademarks not owned by Amazon are the property of their respective
     owners, who may or may not be affiliated with, connected to, or 
     sponsored by Amazon.

-----
## Contents
+ [What is Amazon S3?](Welcome.md)
+ [Introduction to Amazon S3](Introduction.md)
+ [Making requests](MakingRequests.md)
   + [Making requests to Amazon S3 over IPv6](ipv6-access.md)
      + [Using Amazon S3 dual-stack endpoints](dual-stack-endpoints.md)
   + [Making requests using the AWS SDKs](MakingAuthenticatedRequests.md)
      + [Making requests using AWS account or IAM user credentials](AuthUsingAcctOrUserCredentials.md)
         + [Making requests using AWS account or IAM user credentials - AWS SDK for Java](AuthUsingAcctOrUserCredJava.md)
         + [Making requests using AWS account or IAM user credentials - AWS SDK for .NET](AuthUsingAcctOrUserCredDotNet.md)
         + [Making requests using AWS account or IAM user credentials - AWS SDK for PHP](AuthUsingAcctOrUserCredPHP3.md)
         + [Making requests using AWS account or IAM user credentials - AWS SDK for Ruby](AuthUsingAcctOrUserCredRuby.md)
      + [Making requests using IAM user temporary credentials](AuthUsingTempSessionToken.md)
         + [Making requests using IAM user temporary credentials - AWS SDK for Java](AuthUsingTempSessionTokenJava.md)
         + [Making requests using IAM user temporary credentials - AWS SDK for .NET](AuthUsingTempSessionTokenDotNet.md)
         + [Making requests using AWS account or IAM user temporary credentials - AWS SDK for PHP](AuthUsingTempSessionTokenPHP.md)
         + [Making requests using IAM user temporary credentials - AWS SDK for Ruby](AuthUsingTempSessionTokenRuby.md)
      + [Making requests using federated user temporary credentials](AuthUsingTempFederationToken.md)
         + [Making requests using federated user temporary credentials - AWS SDK for Java](AuthUsingTempFederationTokenJava.md)
         + [Making requests using federated user temporary credentials - AWS SDK for .NET](AuthUsingTempFederationTokenDotNet.md)
         + [Making requests using federated user temporary credentials - AWS SDK for PHP](AuthUsingTempFederationTokenPHP.md)
         + [Making requests using federated user temporary credentials - AWS SDK for Ruby](AuthUsingTempFederationTokenRuby.md)
   + [Making requests using the REST API](RESTAPI.md)
      + [Virtual hosting of buckets](VirtualHosting.md)
      + [Request redirection and the REST API](RESTRedirect.md)
+ [Working with Amazon S3 Buckets](UsingBucket.md)
   + [Bucket restrictions and limitations](BucketRestrictions.md)
   + [Examples of creating a bucket](create-bucket-get-location-example.md)
   + [Deleting or emptying a bucket](delete-or-empty-bucket.md)
   + [Setting default server-side encryption behavior for Amazon S3 buckets](bucket-encryption.md)
   + [Amazon S3 Transfer Acceleration](transfer-acceleration.md)
      + [Amazon S3 Transfer Acceleration Examples](transfer-acceleration-examples.md)
   + [Requester Pays buckets](RequesterPaysBuckets.md)
      + [Configure Requester Pays by using the Amazon S3 console](configure-requester-pays-console.md)
      + [Configure Requester Pays with the REST API](configure-requester-pays-rest.md)
         + [Setting the requestPayment Bucket Configuration](RequesterPaysBucketConfiguration.md)
         + [Retrieving the requestPayment Configuration](BucketPayerValues.md)
         + [Downloading objects in Requester Pays buckets](ObjectsinRequesterPaysBuckets.md)
   + [Buckets and access control](BucketAccess.md)
   + [Billing and usage reporting for S3 buckets](BucketBilling.md)
      + [AWS Billing reports for Amazon S3](aws-billing-reports.md)
      + [AWS usage report for Amazon S3](aws-usage-report.md)
      + [Understanding your AWS billing and usage reports for Amazon S3](aws-usage-report-understand.md)
      + [Using cost allocation S3 bucket tags](CostAllocTagging.md)
+ [Managing data access with Amazon S3 access points](access-points.md)
   + [Creating access points](creating-access-points.md)
   + [Using access points](using-access-points.md)
   + [Access points restrictions and limitations](access-points-restrictions-limitations.md)
+ [Working with Amazon S3 objects](UsingObjects.md)
   + [Object key and metadata](UsingMetadata.md)
   + [Amazon S3 storage classes](storage-class-intro.md)
   + [Object subresources](ObjectAndSubResource.md)
   + [Object Versioning](ObjectVersioning.md)
   + [Object tagging](object-tagging.md)
      + [Managing object tags](tagging-managing.md)
         + [Managing object tags using the console](tagging-manage-console.md)
         + [Managing tags using the AWS SDK for Java](tagging-manage-javasdk.md)
         + [Managing tags using the AWS SDK for .NET](tagging-manage-dotnet.md)
   + [Object lifecycle management](object-lifecycle-mgmt.md)
      + [Additional considerations for lifecycle configuration](lifecycle-additional-considerations.md)
         + [Transitioning objects using Amazon S3 Lifecycle](lifecycle-transition-general-considerations.md)
         + [Understanding object expiration](lifecycle-expire-general-considerations.md)
         + [Lifecycle and other bucket configurations](lifecycle-and-other-bucket-config.md)
      + [Lifecycle configuration elements](intro-lifecycle-rules.md)
      + [Examples of lifecycle configuration](lifecycle-configuration-examples.md)
      + [Setting lifecycle configuration on a bucket](how-to-set-lifecycle-configuration-intro.md)
         + [Manage an object's lifecycle using the Amazon S3 console](manage-lifecycle-using-console.md)
         + [Set lifecycle configurations using the AWS CLI](set-lifecycle-cli.md)
         + [Managing object lifecycles using the AWS SDK for Java](manage-lifecycle-using-java.md)
         + [Manage an object's lifecycle using the AWS SDK for .NET](manage-lifecycle-using-dot-net.md)
         + [Manage an object's lifecycle using the AWS SDK for Ruby](manage-lifecycle-using-ruby.md)
         + [Manage an object's lifecycle using the REST API](manage-lifecycle-using-rest.md)
   + [Cross-origin resource sharing (CORS)](cors.md)
      + [Enabling cross-origin resource sharing (CORS)](ManageCorsUsing.md)
         + [Enabling cross-origin resource sharing (CORS) using the AWS Management Console](ManageCorsUsingConsole.md)
         + [Enabling cross-origin resource sharing (CORS) using the AWS SDK for Java](ManageCorsUsingJava.md)
         + [Enabling cross-origin resource sharing (CORS) using the AWS SDK for .NET](ManageCorsUsingDotNet.md)
         + [Enabling cross-origin resource sharing (CORS) using the REST API](EnableCorsUsingREST.md)
      + [Troubleshooting CORS issues](cors-troubleshooting.md)
   + [Operations on objects](ObjectOperations.md)
      + [Getting objects](GettingObjectsUsingAPIs.md)
         + [Get an object Using the AWS SDK for Java](RetrievingObjectUsingJava.md)
         + [Get an object Using the AWS SDK for .NET](RetrievingObjectUsingNetSDK.md)
         + [Get an object Using the AWS SDK for PHP](RetrieveObjSingleOpPHP.md)
         + [Get an object Using the REST API](RetrieveObjSingleOpREST.md)
         + [Share an object with others](ShareObjectPreSignedURL.md)
            + [Generate a presigned object URL using AWS explorer for Visual Studio](ShareObjectPreSignedURLVSExplorer.md)
            + [Generate a presigned object URL using the AWS SDK for Java](ShareObjectPreSignedURLJavaSDK.md)
            + [Generate a presigned object URL using AWS SDK for .NET](ShareObjectPreSignedURLDotNetSDK.md)
      + [Uploading objects](UploadingObjects.md)
         + [Uploading an object in a single operation](UploadInSingleOp.md)
            + [Upload an object Using the AWS SDK for Java](UploadObjSingleOpJava.md)
            + [Upload an object using the AWS SDK for .NET](UploadObjSingleOpNET.md)
            + [Upload an object using the AWS SDK for C++](UploadObjSingleCpp.md)
            + [Upload an object using the AWS SDK for PHP](UploadObjSingleOpPHP.md)
            + [Upload an object using the AWS SDK for Ruby](UploadObjSingleOpRuby.md)
            + [Upload an object using the REST API](UploadObjSingleOpREST.md)
            + [Upload an object using the CLI](UploadObjSingleOpCLI.md)
         + [Uploading objects using multipart upload API](uploadobjusingmpu.md)
            + [Multipart upload overview](mpuoverview.md)
               + [Amazon S3 multipart upload limits](qfacts.md)
               + [API support for multipart upload](sdksupportformpu.md)
               + [Multipart upload API and permissions](mpuAndPermissions.md)
            + [Using the AWS Java SDK for multipart upload (high-level API)](usingHLmpuJava.md)
               + [Upload a file](HLuploadFileJava.md)
               + [Stop multipart uploads](HLAbortMPUploadsJava.md)
               + [Track multipart upload progress](HLTrackProgressMPUJava.md)
            + [Using the AWS Java SDK for a multipart upload (low-level API)](mpListPartsJavaAPI.md)
               + [Upload a file](llJavaUploadFile.md)
               + [List multipart uploads](LLlistMPuploadsJava.md)
               + [Abort a multipart upload](LLAbortMPUJava.md)
            + [Using the AWS SDK for .NET for multipart upload (high-level API)](usingHLmpuDotNet.md)
               + [Upload a file to an S3 bucket using the AWS SDK for .NET (high-level API)](HLuploadFileDotNet.md)
               + [Upload a directory](HLuploadDirDotNet.md)
               + [Stop multipart uploads to an S3 Bucket using the AWS SDK for .NET (high-level API)](HLAbortDotNet.md)
               + [Track the progress of a multipart upload to an S3 Bucket using the AWS SDK for .NET (high-level API)](HLTrackProgressMPUDotNet.md)
            + [Using the AWS SDK for .NET for multipart upload (low-level API)](usingLLmpuDotNet.md)
               + [Upload a file to an S3 Bucket using the AWS SDK for .NET (low-level API)](LLuploadFileDotNet.md)
               + [List multipart uploads to an S3 Bucket using the AWS SDK for .NET (low-level)](LLlistMPuploadsDotNet.md)
               + [Track the progress of a multipart upload to an S3 Bucket using the AWS SDK for .NET (low-level)](LLTrackProgressMPUNet.md)
               + [Stop multipart uploads to an S3 Bucket using the AWS SDK for .NET (low-level)](LLAbortMPUnet.md)
            + [Using the AWS PHP SDK for multipart upload](usingHLmpuPHP.md)
            + [Using the AWS PHP SDK for multipart upload (low-level API)](usingLLmpuPHP.md)
               + [Upload a file in multiple parts using the PHP SDK low-level API](LLuploadFilePHP.md)
               + [List multipart uploads using the low-level AWS SDK for PHP API](LLlistMPuploadsPHP.md)
               + [Abort a multipart upload](LLAbortMPUphp.md)
            + [Using the AWS SDK for Ruby for Multipart Upload](uploadobjusingmpu-ruby-sdk.md)
            + [Using the REST API for multipart upload](UsingRESTAPImpUpload.md)
            + [Using the AWS Command Line Interface for multipart upload](UsingCLImpUpload.md)
         + [Uploading objects using presigned URLs](PresignedUrlUploadObject.md)
            + [Upload an object using a presigned URL (AWS SDK for Java)](PresignedUrlUploadObjectJavaSDK.md)
            + [Upload an object using a presigned URL (AWS SDK for .NET)](UploadObjectPreSignedURLDotNetSDK.md)
            + [Upload an object using a presigned URL (AWS SDK for Ruby)](UploadObjectPreSignedURLRubySDK.md)
      + [Copying objects](CopyingObjectsExamples.md)
         + [Copying Objects in a Single Operation](CopyingObjectsUsingAPIs.md)
            + [Copy an Object Using the AWS SDK for Java](CopyingObjectUsingJava.md)
            + [Copy an Amazon S3 Object in a Single Operation Using the AWS SDK for .NET](CopyingObjectUsingNetSDK.md)
            + [Copy an Object Using the AWS SDK for PHP](CopyingObjectUsingPHP.md)
            + [Copy an Object Using the AWS SDK for Ruby](CopyingObjectUsingRuby.md)
            + [Copy an Object Using the REST API](CopyingObjectUsingREST.md)
         + [Copying objects using the multipart upload API](CopyingObjctsMPUapi.md)
            + [Copy an object using the AWS SDK for Java multipart upload API](CopyingObjctsUsingLLJavaMPUapi.md)
            + [Copy an Amazon S3 object using the AWS SDK for .NET multipart upload API](CopyingObjctsUsingLLNetMPUapi.md)
            + [Copy object using the REST multipart upload API](CopyingObjctsUsingRESTMPUapi.md)
      + [Listing object keys](ListingKeysUsingAPIs.md)
         + [Listing Keys Hierarchically Using a Prefix and Delimiter](ListingKeysHierarchy.md)
         + [Listing Keys Using the AWS SDK for Java](ListingObjectKeysUsingJava.md)
         + [Listing Keys Using the AWS SDK for .NET](ListingObjectKeysUsingNetSDK.md)
         + [Listing Keys Using the AWS SDK for PHP](ListingObjectKeysUsingPHP.md)
         + [Listing Keys Using the REST API](ListingObjectKeysUsingREST.md)
      + [Deleting objects](DeletingObjects.md)
         + [Deleting one object per request](DeletingOneObject.md)
            + [Deleting an object using the AWS SDK for Java](DeletingOneObjectUsingJava.md)
            + [Deleting an object using the AWS SDK for .NET](DeletingOneObjectUsingNetSDK.md)
            + [Deleting an object using the AWS SDK for PHP](DeletingOneObjectUsingPHPSDK.md)
            + [Deleting an object using the REST API](DeletingAnObjectsUsingREST.md)
         + [Deleting multiple objects per request](DeletingMultipleObjects.md)
            + [Deleting multiple objects using the AWS SDK for Java](DeletingMultipleObjectsUsingJava.md)
            + [Deleting multiple objects using the AWS SDK for .NET](DeletingMultipleObjectsUsingNetSDK.md)
            + [Deleting multiple objects using the AWS SDK for PHP](DeletingMultipleObjectsUsingPHPSDK.md)
            + [Deleting multiple objects using the REST API](DeletingMultipleObjectsUsingREST.md)
      + [Selecting content from objects](selecting-content-from-objects.md)
         + [Selecting content from objects using the SDK for Java](SelectObjectContentUsingJava.md)
         + [Selecting content from objects using the REST API](SelectObjectContentUsingRestApi.md)
         + [Selecting content from objects using other SDKs](SelectObjectContentUsingOtherSDKs.md)
      + [Restoring archived objects](restoring-objects.md)
         + [Restore an archived object using the Amazon S3 console](restoring-objects-console.md)
         + [Restore an archived object using the AWS SDK for Java](restoring-objects-java.md)
         + [Restore an archived object using the AWS SDK for .NET](restore-object-dotnet.md)
         + [Restore an archived object using the REST API](restoring-objects-rest.md)
      + [Querying archived objects programmatically](querying-glacier-archives.md)
+ [Amazon S3 analytics – Storage Class Analysis](analytics-storage-class.md)
+ [Using Amazon S3 on Outposts](S3onOutposts.md)
   + [Getting started with Amazon S3 on Outposts](S3OutpostsGS.md)
   + [Amazon S3 on Outposts restrictions and limitations](S3OnOutpostsRestrictionsLimitations.md)
   + [Using AWS Identity and Access Management with Amazon S3 on Outposts](S3OutpostsIAM.md)
   + [Working with Amazon S3 on Outposts](WorkingWithS3Outposts.md)
   + [Amazon S3 on Outposts examples](S3OutpostsExamples.md)
      + [Amazon S3 on Outposts examples using the AWS CLI](S3OutpostsCLIExamples.md)
      + [Amazon S3 on Outposts examples using the SDK for Java](S3OutpostsJavaExamples.md)
         + [Creating and managing Amazon S3 on Outposts bucket](S3OutpostsBucketJava.md)
         + [Working with objects using Amazon S3 on Outposts](S3OutpostsObjectJava.md)
+ [Amazon S3 Security](security.md)
   + [Data protection in Amazon S3](DataDurability.md)
      + [Internetwork traffic privacy](inter-network-traffic-privacy.md)
      + [Protecting data using encryption](UsingEncryption.md)
         + [Protecting data using server-side encryption](serv-side-encryption.md)
            + [Protecting data with server-side encryption using AWS KMS CMKs (SSE-KMS)](UsingKMSEncryption.md)
               + [Specifying AWS KMS in Amazon S3 using the AWS SDKs](kms-using-sdks.md)
               + [Specifying AWS KMS in Amazon S3 using the REST API](KMSUsingRESTAPI.md)
               + [Reducing the cost of SSE-KMS with Amazon S3 Bucket Keys](bucket-key.md)
                  + [Configuring your bucket to use an S3 Bucket Key with SSE-KMS for new objects](configuring-bucket-key.md)
                  + [Configuring an S3 Bucket Key at the object level using the REST API, AWS SDKs, or AWS CLI](configuring-bucket-key-object.md)
                  + [Viewing settings for an S3 Bucket Key](viewing-bucket-key-settings.md)
            + [Protecting data using server-side encryption with Amazon S3-managed encryption keys (SSE-S3)](UsingServerSideEncryption.md)
               + [Specifying Server-Side Encryption Using the AWS SDK for Java](SSEUsingJavaSDK.md)
               + [Specifying Server-Side Encryption Using the AWS SDK for .NET](SSEUsingDotNetSDK.md)
               + [Specifying server-side encryption using the AWS SDK for PHP](SSEUsingPHPSDK.md)
               + [Specifying Server-Side Encryption Using the AWS SDK for Ruby](SSEUsingRubySDK.md)
               + [Specifying Server-Side Encryption Using the REST API](SSEUsingRESTAPI.md)
               + [Specifying Server-Side Encryption Using the AWS Management Console](SSEUsingConsole.md)
            + [Protecting data using server-side encryption with customer-provided encryption keys (SSE-C)](ServerSideEncryptionCustomerKeys.md)
               + [Specifying server-side encryption with customer-provided encryption keys using the AWS SDK for Java](sse-c-using-java-sdk.md)
               + [Specifying server-side encryption with customer-provided encryption keys using the AWS SDK for .NET](sse-c-using-dot-net-sdk.md)
               + [Specifying server-side encryption with customer-provided encryption keys using the REST API](ServerSideEncryptionCustomerKeysSSEUsingRESTAPI.md)
         + [Protecting data using client-side encryption](UsingClientSideEncryption.md)
   + [Identity and access management in Amazon S3](s3-access-control.md)
      + [Overview of managing access](access-control-overview.md)
      + [How Amazon S3 Authorizes a Request](how-s3-evaluates-access-control.md)
         + [How Amazon S3 Authorizes a Request for a Bucket Operation](access-control-auth-workflow-bucket-operation.md)
         + [How Amazon S3 Authorizes a Request for an Object Operation](access-control-auth-workflow-object-operation.md)
      + [Guidelines for using the available access policy options](access-policy-alternatives-guidelines.md)
      + [Example walkthroughs: Managing access to your Amazon S3 resources](example-walkthroughs-managing-access.md)
         + [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)
         + [Example 1: Bucket owner granting its users bucket permissions](example-walkthroughs-managing-access-example1.md)
         + [Example 2: Bucket owner granting cross-account bucket permissions](example-walkthroughs-managing-access-example2.md)
         + [Example 3: Bucket owner granting its users permissions to objects it does not own](example-walkthroughs-managing-access-example3.md)
         + [Example 4: Bucket owner granting cross-account permission to objects it does not own](example-walkthroughs-managing-access-example4.md)
      + [Using Bucket Policies and User Policies](using-iam-policies.md)
         + [Policies and Permissions in Amazon S3](access-policy-language-overview.md)
            + [Amazon S3 Resources](s3-arn-format.md)
            + [Principals](s3-bucket-user-policy-specifying-principal-intro.md)
            + [Amazon S3 Actions](using-with-s3-actions.md)
            + [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)
            + [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)
         + [Bucket Policy Examples](example-bucket-policies.md)
            + [Example Bucket Policies for VPC Endpoints for Amazon S3](example-bucket-policies-vpc-endpoint.md)
         + [User Policy Examples](example-policies-s3.md)
            + [Walkthrough: Controlling access to a bucket with user policies](walkthrough1.md)
      + [Managing Access with ACLs](S3_ACLs_UsingACLs.md)
         + [Access Control List (ACL) Overview](acl-overview.md)
         + [Managing ACLs](managing-acls.md)
            + [Managing ACLs in the AWS Management Console](manage-acls-using-console.md)
            + [Managing ACLs Using the AWS SDK for Java](acl-using-java-sdk.md)
            + [Managing ACLs Using the AWS SDK for .NET](acl-using-dot-net-sdk.md)
            + [Managing ACLs Using the REST API](acl-using-rest-api.md)
      + [Controlling ownership of uploaded objects using S3 Object Ownership](about-object-ownership.md)
      + [Using Amazon S3 block public access](access-control-block-public-access.md)
      + [Using Service-Linked Roles for Amazon S3 Storage Lens](using-service-linked-roles.md)
   + [Logging and monitoring in Amazon S3](s3-incident-response.md)
   + [Compliance Validation for Amazon S3](s3-compliance.md)
      + [Amazon S3 inventory](storage-inventory.md)
   + [Resilience in Amazon S3](disaster-recovery-resiliency.md)
      + [Using versioning](Versioning.md)
         + [Examples of enabling bucket versioning](manage-versioning-examples.md)
         + [Managing objects in a versioning-enabled bucket](manage-objects-versioned-bucket.md)
            + [Adding objects to versioning-enabled buckets](AddingObjectstoVersioningEnabledBuckets.md)
            + [Listing objects in a versioning-enabled bucket](list-obj-version-enabled-bucket.md)
            + [Retrieving object versions](RetrievingObjectVersions.md)
               + [Retrieving the metadata of an object version](RetMetaOfObjVersion.md)
            + [Deleting object versions](DeletingObjectVersions.md)
               + [Using MFA delete](UsingMFADelete.md)
               + [Working with delete markers](DeleteMarker.md)
               + [Removing delete markers](RemDelMarker.md)
            + [Transitioning object versions](transitioning-object-versions.md)
            + [Restoring previous versions](RestoringPreviousVersions.md)
            + [Versioned object permissions](VersionedObjectPermissionsandACLs.md)
         + [Managing objects in a versioning-suspended bucket](VersionSuspendedBehavior.md)
            + [Adding objects to versioning-suspended buckets](AddingObjectstoVersionSuspendedBuckets.md)
            + [Retrieving objects from versioning-suspended buckets](RetrievingObjectsfromVersioningSuspendedBuckets.md)
            + [Deleting objects from versioning-suspended buckets](DeletingObjectsfromVersioningSuspendedBuckets.md)
      + [Locking objects using S3 Object Lock](object-lock.md)
         + [S3 Object Lock overview](object-lock-overview.md)
         + [Managing Amazon S3 object locks](object-lock-managing.md)
   + [Infrastructure security in Amazon S3](network-isolation.md)
   + [Configuration and vulnerability analysis in Amazon S3](vulnerability-analysis-and-management.md)
   + [Bucket owner condition](bucket-owner-condition.md)
   + [Security Best Practices for Amazon S3](security-best-practices.md)
+ [Performing S3 Batch Operations](batch-ops.md)
   + [S3 Batch Operations basics](batch-ops-basics.md)
   + [Creating an S3 Batch Operations job](batch-ops-create-job.md)
      + [Granting permissions for Amazon S3 Batch Operations](batch-ops-iam-role-policies.md)
   + [Operations](batch-ops-operations.md)
      + [Put object copy](batch-ops-copy-object.md)
      + [Initiate restore object](batch-ops-initiate-restore-object.md)
      + [Invoking a Lambda function from Amazon S3 batch operations](batch-ops-invoke-lambda.md)
      + [Put object ACL](batch-ops-put-object-acl.md)
      + [Put object tagging](batch-ops-put-object-tagging.md)
      + [Managing S3 Object Lock retention dates](batch-ops-retention-date.md)
      + [Managing S3 Object Lock legal hold](batch-ops-legal-hold.md)
   + [Managing S3 Batch Operations jobs](batch-ops-managing-jobs.md)
   + [S3 Batch Operations examples](batch-ops-examples.md)
      + [S3 Batch Operations examples using the AWS CLI](batch-ops-examples-cli.md)
      + [S3 Batch Operations examples using the AWS SDK for Java](batch-ops-examples-java.md)
      + [Example: Using job tags to control permissions for S3 Batch Operations](batch-ops-job-tags-examples.md)
      + [Example: Requesting S3 Batch Operations completion reports](batch-ops-examples-reports.md)
      + [Example: Copying objects across AWS accounts using S3 Batch Operations](batch-ops-examples-xcopy.md)
      + [Example: Tracking an S3 Batch Operations job in Amazon EventBridge through AWS CloudTrail](batch-ops-examples-event-bridge-cloud-trail.md)
+ [Assessing your storage activity and usage with Amazon S3 Storage Lens](storage_lens.md)
   + [Understanding Amazon S3 Storage Lens](storage_lens_basics_metrics_recommendations.md)
   + [Using Amazon S3 Storage Lens with AWS Organizations](storage_lens_with_organizations.md)
   + [Setting permissions to use Amazon S3 Storage Lens](storage_lens_iam_permissions.md)
   + [Viewing storage usage and activity metrics with Amazon S3 Storage Lens](storage_lens_view_metrics.md)
      + [Viewing S3 Storage Lens metrics on the dashboards](storage_lens_view_metrics_dashboard.md)
      + [Viewing Amazon S3 Storage Lens metrics using a data export](storage_lens_view_metrics_export.md)
         + [Using an AWS KMS CMK to encrypt your metrics exports](storage_lens_encrypt_permissions.md)
         + [What is an S3 Storage Lens export manifest?](storage_lens_whatis_metrics_export_manifest.md)
         + [Understanding the Amazon S3 Storage Lens export schema](storage_lens_understanding_metrics_export_schema.md)
   + [Amazon S3 Storage Lens metrics glossary](storage_lens_metrics_glossary.md)
   + [Amazon S3 Storage Lens examples and console walk-through](S3LensExamples.md)
      + [Amazon S3 Storage Lens examples using the AWS CLI](S3LensCLIExamples.md)
      + [Amazon S3 Storage Lens examples using the SDK for Java](S3LensJavaExamples.md)
+ [Hosting a static website using Amazon S3](WebsiteHosting.md)
   + [Website endpoints](WebsiteEndpoints.md)
   + [Configuring a bucket as a static website using the AWS Management Console](HowDoIWebsiteConfiguration.md)
      + [Enabling website hosting](EnableWebsiteHosting.md)
      + [Configuring an index document](IndexDocumentSupport.md)
      + [Setting permissions for website access](WebsiteAccessPermissionsReqd.md)
      + [(Optional) Logging web traffic](LoggingWebsiteTraffic.md)
      + [(Optional) Configuring a custom error document](CustomErrorDocSupport.md)
      + [(Optional) Configuring a webpage redirect](how-to-page-redirect.md)
   + [Programmatically configuring a bucket as a static website](ManagingBucketWebsiteConfig.md)
      + [Managing websites with the AWS SDK for Java](ConfigWebSiteJava.md)
      + [Managing websites with the AWS SDK for .NET](ConfigWebSiteDotNet.md)
      + [Managing websites with the AWS SDK for PHP](ConfigWebSitePHP.md)
      + [Managing websites with the REST API](ConfigWebSiteREST.md)
   + [Example walkthroughs - Hosting websites on Amazon S3](hosting-websites-on-s3-examples.md)
      + [Configuring a static website](HostingWebsiteOnS3Setup.md)
      + [Configuring a static website using a custom domain registered with Route 53](website-hosting-custom-domain-walkthrough.md)
         + [Speeding up your website with Amazon CloudFront](website-hosting-cloudfront-walkthrough.md)
         + [Cleaning up your example resources](getting-started-cleanup.md)
+ [Configuring Amazon S3 event notifications](NotificationHowTo.md)
   + [Walkthrough: Configure a bucket for notifications (SNS topic or SQS queue)](ways-to-add-notification-config-to-bucket.md)
   + [Event message structure](notification-content-structure.md)
+ [Replication](replication.md)
   + [What does Amazon S3 replicate?](replication-what-is-isnot-replicated.md)
   + [Overview of setting up replication](replication-how-setup.md)
      + [Replication configuration overview](replication-add-config.md)
      + [Setting up permissions for replication](setting-repl-config-perm-overview.md)
   + [Additional replication configurations](replication-additional-configs.md)
      + [Monitoring progress with replication metrics and Amazon S3 event notifications](replication-metrics.md)
      + [Meeting compliance requirements using S3 Replication Time Control (S3 RTC)](replication-time-control.md)
         + [Best practices and guidelines for S3 RTC](rtc-best-practices.md)
      + [Replicating delete markers between buckets](delete-marker-replication.md)
      + [Replicating metadata changes with Amazon S3 replica modification sync](replication-for-metadata-changes.md)
      + [Changing the replica owner](replication-change-owner.md)
      + [Replicating objects created with server-side encryption (SSE) using encryption keys stored in AWS KMS](replication-config-for-kms-objects.md)
   + [Replication walkthroughs](replication-example-walkthroughs.md)
      + [Example 1: Configuring replication when the source and destination buckets are owned by the same account](replication-walkthrough1.md)
      + [Example 2: Configuring replication when the source and destination buckets are owned by different accounts](replication-walkthrough-2.md)
      + [Example 3: Changing the replica owner when the source and destination buckets are owned by different accounts](replication-walkthrough-3.md)
      + [Example 4: Replicating encrypted objects](replication-walkthrough-4.md)
      + [Example 5: S3 Replication Time Control (S3 RTC) configuration](replication-walkthrough-5.md)
   + [Replication status information](replication-status.md)
   + [Troubleshooting replication](replication-troubleshoot.md)
   + [Replication additional considerations](replication-and-other-bucket-configs.md)
+ [Request routing](UsingRouting.md)
   + [Request redirection and the REST API](Redirects.md)
   + [DNS considerations](DNSConsiderations.md)
+ [Best Practices Design Patterns: Optimizing Amazon S3 Performance](optimizing-performance.md)
   + [Performance Guidelines for Amazon S3](optimizing-performance-guidelines.md)
   + [Performance Design Patterns for Amazon S3](optimizing-performance-design-patterns.md)
+ [Monitoring Amazon S3](monitoring-overview.md)
   + [Monitoring tools](monitoring-automated-manual.md)
   + [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)
   + [Metrics configurations for buckets](metrics-configurations.md)
   + [Logging with Amazon S3](logging-with-S3.md)
   + [Logging Amazon S3 API calls using AWS CloudTrail](cloudtrail-logging.md)
   + [Using AWS CloudTrail to identify Amazon S3 requests](cloudtrail-request-identification.md)
   + [Tracing Amazon S3 requests using AWS X-Ray](tracing_requests_using_xray.md)
+ [Using BitTorrent with Amazon S3](S3Torrent.md)
   + [How you are charged for BitTorrent delivery](S3TorrentCharge.md)
   + [Using BitTorrent to retrieve objects stored in Amazon S3](S3TorrentRetrieve.md)
   + [Publishing content using Amazon S3 and BitTorrent](S3TorrentPublish.md)
+ [Handling REST and SOAP errors](HandlingErrors.md)
   + [The REST error response](UsingRESTError.md)
      + [Error response](ErrorResponse.md)
         + [Error code](ErrorCode.md)
         + [Error message](ErrorMessage.md)
         + [Further details](ErrorDetails.md)
   + [The SOAP error response](UsingSOAPError.md)
   + [Amazon S3 error best practices](ErrorBestPractices.md)
+ [Troubleshooting Amazon S3](troubleshooting.md)
   + [Troubleshooting Amazon S3 by Symptom](troubleshooting-by-symptom.md)
+ [Amazon S3 server access logging](ServerLogs.md)
   + [Enabling logging using the console](enable-logging-console.md)
   + [Enabling logging programmatically](enable-logging-programming.md)
   + [Amazon S3 Server Access Log Format](LogFormat.md)
   + [Deleting Amazon S3 log files](deleting-log-files-lifecycle.md)
   + [Using Amazon S3 access logs to identify requests](using-s3-access-logs-to-identify-requests.md)
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)
   + [Setting Up the AWS CLI](setup-aws-cli.md)
   + [Using the AWS SDK for Java](UsingTheMPJavaAPI.md)
   + [Using the AWS SDK for .NET](UsingTheMPDotNetAPI.md)
   + [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md)
   + [Using the AWS SDK for Ruby - Version 3](UsingTheMPRubyAPI.md)
   + [Using the AWS SDK for Python (Boto)](UsingTheBotoAPI.md)
   + [Using the AWS Mobile SDKs for iOS and Android](using-mobile-sdks.md)
   + [Using the AWS Amplify JavaScript Library](using-aws-amplify.md)
+ [Appendices](Appendices.md)
   + [Appendix a: Using the SOAP API](SOAPAPI3.md)
      + [Common SOAP API elements](UsingSOAPOperations.md)
      + [Authenticating SOAP requests](SOAPAuthentication.md)
      + [Setting access policy with SOAP](SOAPAccessPolicy.md)
   + [Appendix b: Authenticating requests (AWS signature version 2)](auth-request-sig-v2.md)
      + [Authenticating requests using the REST API](S3_Authentication2.md)
      + [Signing and authenticating REST requests](RESTAuthentication.md)
      + [Browser-based uploads using POST (AWS signature version 2)](UsingHTTPPOST.md)
         + [HTML forms (AWS signature version 2)](HTTPPOSTForms.md)
         + [Upload examples (AWS signature version 2)](HTTPPOSTExamples.md)
         + [POST with adobe flash](HTTPPOSTFlash.md)
+ [Amazon S3 resources](RelatedResources012.md)
+ [SQL Reference for Amazon S3 Select and S3 Glacier Select](s3-glacier-select-sql-reference.md)
   + [SELECT Command](s3-glacier-select-sql-reference-select.md)
   + [Data Types](s3-glacier-select-sql-reference-data-types.md)
   + [Operators](s3-glacier-select-sql-reference-operators.md)
   + [Reserved Keywords](s3-glacier-select-sql-reference-keyword-list.md)
   + [SQL Functions](s3-glacier-select-sql-reference-sql-functions.md)
      + [Aggregate Functions (Amazon S3 Select only)](s3-glacier-select-sql-reference-aggregate.md)
      + [Conditional Functions](s3-glacier-select-sql-reference-conditional.md)
      + [Conversion Functions](s3-glacier-select-sql-reference-conversion.md)
      + [Date Functions](s3-glacier-select-sql-reference-date.md)
      + [String Functions](s3-glacier-select-sql-reference-string.md)
+ [Document history](WhatsNew.md)
+ [AWS glossary](glossary.md)


# Changing the replica owner<a name="replication-change-owner"></a>

In replication, the owner of the source object also owns the replica by default\. When source and destination buckets are owned by different AWS accounts, you can add optional configuration settings to change replica ownership to the AWS account that owns the destination buckets\. You might do this, for example, to restrict access to object replicas\. This is referred to as the *owner override* option of the replication configuration\. This section explains only the relevant additional configuration settings\. For information about setting the replication configuration, see [Replication](replication.md)\. 

To configure the owner override, you do the following:
+ Add the owner override option to the replication configuration to tell Amazon S3 to change replica ownership\. 
+ Grant Amazon S3 permissions to change replica ownership\. 
+ Add permission in the destination buckets policy to allow changing replica ownership\. This allows the owner of the destination buckets to accept the ownership of object replicas\.

The following sections describe how to perform these tasks\. For a working example with step\-by\-step instructions, see [Example 3: Changing the replica owner when the source and destination buckets are owned by different accounts](replication-walkthrough-3.md)\.

## Adding the owner override option to the replication configuration<a name="repl-ownership-owneroverride-option"></a>

**Warning**  
Add the owner override option only when the source and destination buckets are owned by different AWS accounts\. Amazon S3 doesn't check if the buckets are owned by same or different accounts\. If you add the owner override when both buckets are owned by same AWS account, Amazon S3 applies the owner override\. It grants full permissions to the owner of the destination bucket and doesn't replicate subsequent updates to the source object access control list \(ACL\)\. The replica owner can directly change the ACL associated with a replica with a `PUT ACL` request, but not through replication\.

To specify the owner override option, add the following to each `Destination` element: 
+ The `AccessControlTranslation` element, which tells Amazon S3 to change replica ownership
+ The `Account` element, which specifies the AWS account of the destination bucket owner 

```
<ReplicationConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
    ...
    <Destination>
      ...
      <AccessControlTranslation>
           <Owner>Destination</Owner>
       </AccessControlTranslation>
      <Account>destination-bucket-owner-account-id</Account>
    </Destination>
  </Rule>
</ReplicationConfiguration>
```

The following example replication configuration tells Amazon S3 to replicate objects that have the `Tax` key prefix to the destination bucket and change ownership of the replicas\.

```
<?xml version="1.0" encoding="UTF-8"?>
<ReplicationConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
   <Role>arn:aws:iam::account-id:role/role-name</Role>
   <Rule>
      <ID>Rule-1</ID>
      <Priority>1</Priority>
      <Status>Enabled</Status>
      <Status>Enabled</Status>
      <DeleteMarkerReplication>
         <Status>Disabled</Status>
      </DeleteMarkerReplication>
      <Filter>
         <Prefix>Tax</Prefix>
      </Filter>
      <Destination>
         <Bucket>arn:aws:s3:::destination-bucket</Bucket>
         <Account>destination-bucket-owner-account-id</Account>
         <AccessControlTranslation>
            <Owner>Destination</Owner>
         </AccessControlTranslation>
      </Destination>
   </Rule>
</ReplicationConfiguration>
```

## Granting Amazon S3 permission to change replica ownership<a name="repl-ownership-add-role-permission"></a>

Grant Amazon S3 permissions to change replica ownership by adding permission for the `s3:ObjectOwnerOverrideToBucketOwner` action in the permissions policy associated with the IAM role\. This is the IAM role that you specified in the replication configuration that allows Amazon S3 to assume and replicate objects on your behalf\. 

```
...
{
    "Effect":"Allow",
         "Action":[
       "s3:ObjectOwnerOverrideToBucketOwner"
    ],
    "Resource":"arn:aws:s3:::destination-bucket/*"
}
...
```

## Adding permission in the destination bucket policy to allow changing replica ownership<a name="repl-ownership-accept-ownership-b-policy"></a>

The owner of the destination bucket must grant the owner of the source bucket permission to change replica ownership\. The owner of the destination bucket grants the owner of the source bucket permission for the `s3:ObjectOwnerOverrideToBucketOwner` action\. This allows the destination bucket owner to accept ownership of the object replicas\. The following example bucket policy statement shows how to do this\.

```
...
{
    "Sid":"1",
    "Effect":"Allow",
    "Principal":{"AWS":"source-bucket-account-id/source-account-IAM-role"},
    "Action":["s3:ObjectOwnerOverrideToBucketOwner"],
    "Resource":"arn:aws:s3:::destination-bucket/*"
}
...
```

## Additional considerations<a name="repl-ownership-considerations"></a>

When you configure the ownership override option, the following considerations apply:
+ By default, the owner of the source object also owns the replica\. Amazon S3 replicates the object version and the ACL associated with it\.

   

  If you add the owner override, Amazon S3 replicates only the object version, not the ACL\. In addition, Amazon S3 doesn't replicate subsequent changes to the source object ACL\. Amazon S3 sets the ACL on the replica that grants full control to the destination bucket owner\. 

   
+  When you update a replication configuration to enable, or disable, the owner override, the following occurs\.

   
  + If you add the owner override option to the replication configuration:

     

    When Amazon S3 replicates an object version, it discards the ACL that is associated with the source object\. Instead, it sets the ACL on the replica, giving full control to the owner of the destination bucket\. It doesn't replicate subsequent changes to the source object ACL\. However, this ACL change doesn't apply to object versions that were replicated before you set the owner override option\. ACL updates on source objects that were replicated before the owner override was set continue to be replicated \(because the object and its replicas continue to have the same owner\)\.

     
  + If you remove the owner override option from the replication configuration:

     


    Amazon S3 replicates new objects that appear in the source bucket and the associated ACLs to the destination buckets\. For objects that were replicated before you removed the owner override, Amazon S3 doesn't replicate the ACLs because the object ownership change that Amazon S3 made remains in effect\. That is, ACLs put on the object version that were replicated when the owner override was set continue to be not replicated\.




# Using AWS CloudTrail to identify Amazon S3 requests<a name="cloudtrail-request-identification"></a>

Amazon S3 lets you identify requests using an AWS CloudTrail event log\. AWS CloudTrail is the preferred way of identifying Amazon S3 requests, but if you are using Amazon S3 server access logs, see [ Using Amazon S3 access logs to identify requests](using-s3-access-logs-to-identify-requests.md)\.

**Topics**
+ [How CloudTrail captures requests made to Amazon S3](#cloudtrail-logging-s3-requests)
+ [Enabling CloudTrail event logging for S3 buckets and objects](#enable-cloudtrail-logging-for-s3)
+ [Identifying requests made to Amazon S3 in a CloudTrail log](#identify-S3-requests-using-in-CTlog)
+ [Using AWS CloudTrail to identify Amazon S3 signature version 2 requests](#cloudtrail-identification-sigv2-requests)
+ [Using AWS CloudTrail to identify access to Amazon S3 objects](#cloudtrail-identification-object-access)
+ [Related resources](#cloudtrail-logging-related-resources)

## How CloudTrail captures requests made to Amazon S3<a name="cloudtrail-logging-s3-requests"></a>

By default, CloudTrail logs S3 bucket\-level API calls that were made in the last 90 days, but not log requests made to objects\. Bucket\-level calls include events like `CreateBucket`, `DeleteBucket`, `PutBucketLifeCycle`, `PutBucketPolicy`, etc\. You can see bucket\-level events on the CloudTrail console\. However, you can't view data events \(Amazon S3 object\-level calls\) there—you must parse or query CloudTrail logs for them\. 

For information about what Amazon S3 API calls are captured by CloudTrail, see [Amazon S3 information in CloudTrail](cloudtrail-logging.md#cloudtrail-logging-s3-info)\. 

## Enabling CloudTrail event logging for S3 buckets and objects<a name="enable-cloudtrail-logging-for-s3"></a>

CloudTrail data events allow you to get information about bucket and object\-level requests\. To enable CloudTrail data events for a specific bucket, see [How Do I Enable Object\-Level Logging for an S3 Bucket with AWS CloudTrail Data Events?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html) in the *Amazon Simple Storage Service Console User Guide*\.

To enable CloudTrail data events for all your buckets or for a list of specific buckets, you must [create a trail manually in CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html)\. 

**Note**  
The default setting for CloudTrail is to find only management events\. Check to ensure that you have the data events enabled for your account\.
 With an S3 bucket that is generating a high workload, you could quickly generate thousands of logs in a short amount of time\. Be mindful of how long you choose to enable CloudTrail data events for a busy bucket\. 

 CloudTrail stores Amazon S3 data event logs in an S3 bucket of your choosing\. You should consider using a bucket in a separate AWS account to better organize events from multiple buckets you might own into a central place for easier querying and analysis\. AWS Organizations makes it easy to create an AWS account that is linked to the account owning the bucket that you are monitoring\. For more information, see [What Is AWS Organizations](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html) in the *AWS Organizations User Guide*\.

When you create a trail in CloudTrail, in the data events section, you can select the **Select all S3 buckets in your account** check box to log all object level events\. 

**Note**  
It's a best practice to [create an Amazon S3 lifecycle policy](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html) for your AWS CloudTrail data event bucket\. Configure the lifecycle policy to periodically remove log files after the period of time you believe you need to audit them\. Doing so reduces the amount of data that Athena analyzes for each query\.
For information about logging format, see [Logging Amazon S3 API Calls by Using AWS CloudTrail](https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html)\.
For examples of how to query CloudTrail logs, see [Analyze Security, Compliance, and Operational Activity Using AWS CloudTrail and Amazon Athena](http://aws.amazon.com/blogs/big-data/aws-cloudtrail-and-amazon-athena-dive-deep-to-analyze-security-compliance-and-operational-activity/)\. 

## Identifying requests made to Amazon S3 in a CloudTrail log<a name="identify-S3-requests-using-in-CTlog"></a>

Events logged by CloudTrail are stored as compressed, GZipped JSON objects in your S3 bucket\. To efficiently find requests, you should use a service like Amazon Athena to index and query the CloudTrail logs\. For more information about CloudTrail and Athena, see [Querying AWS CloudTrail Logs](https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html)\.

### Using Athena with CloudTrail logs<a name="using-athena"></a>

After you set up CloudTrail to deliver events to a bucket, you should start to see objects go to your destination bucket on the Amazon S3 console\. These are formatted as follows: `s3://<myawsexamplebucket1>/AWSLogs/<111122223333>/CloudTrail/<Region>/<yyyy>/<mm>/<dd>` 

**Example — use Athena to query CloudTrail event logs for specific requests**  

Locate your CloudTrail event logs:

`s3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/us-east-2/2019/04/14`

With CloudTrail event logs, you can now create an Athena database and table to query them as follows:

1. Open the Athena console at [https://console\.aws\.amazon\.com/athena/](https://console.aws.amazon.com/athena/home)\.

1. Change the AWS Region to be the same as your CloudTrail destination S3 bucket\.

1. In the query window, create an Athena database for your CloudTrail events\.

   ```
   CREATE DATABASE s3_cloudtrail_events_db
   ```

1. Use the following query to create a table for all of your CloudTrail events in the bucket\. Be sure to change the bucket name from *<CloudTrail\_myawsexamplebucket1>* to your bucket's name\. Also provide the *AWS\_account\_ID* CloudTrail that is used in your bucket\. 

   ```
   CREATE EXTERNAL TABLE s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table(
           eventversion STRING,
           useridentity STRUCT<
               type:STRING,
               principalid:STRING,
               arn:STRING,
               accountid:STRING,
               invokedby:STRING,
               accesskeyid:STRING,
               userName:STRING,
               sessioncontext:STRUCT<
                   attributes:STRUCT<
                           mfaauthenticated:STRING,
                           creationdate:STRING>,
                           sessionissuer:STRUCT<  
                           type:STRING,
                           principalId:STRING,
                           arn:STRING, 
                           accountId:STRING,
                           userName:STRING>
                   >
                >,
           eventtime STRING,
           eventsource STRING,
           eventname STRING,
           awsregion STRING,
           sourceipaddress STRING,
           useragent STRING,
           errorcode STRING,
           errormessage STRING,
           requestparameters STRING,
           responseelements STRING,
           additionaleventdata STRING,
           requestid STRING,
           eventid STRING,
           resources ARRAY<STRUCT<
               ARN:STRING,
               accountId:STRING,
               type:STRING>>,
           eventtype STRING,
           apiversion STRING,
           readonly STRING,
           recipientaccountid STRING,
           serviceeventdetails STRING,
           sharedeventid STRING,
           vpcendpointid STRING
       )
       ROW FORMAT SERDE 'com.amazon.emr.hive.serde.CloudTrailSerde'
       STORED AS INPUTFORMAT 'com.amazon.emr.cloudtrail.CloudTrailInputFormat'
       OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
       LOCATION 's3://<myawsexamplebucket1>/AWSLogs/<111122223333>/';
   ```

1. Test Athena to ensure that the query works\.

   ```
   SELECT * FROM s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table
   WHERE eventsource='s3.amazonaws.com'
   LIMIT 2;
   ```

## Using AWS CloudTrail to identify Amazon S3 signature version 2 requests<a name="cloudtrail-identification-sigv2-requests"></a>

Amazon S3 lets you identify what API signature version was used to sign a request using an AWS CloudTrail event log\. This capability is important because support for Signature Version 2 will be turned off \(deprecated\)\. After that, Amazon S3 will no longer accept requests that use Signature Version 2, and all requests must use *Signature Version 4* signing\. 

We *strongly* recommend that you use CloudTrail to help determine whether any of your workflows are using Signature Version 2 signing\. Remediate them by upgrading your libraries and code to use Signature Version 4 instead to prevent any impact to your business\. 

For more information, see [Announcement: AWS CloudTrail for Amazon S3 adds new fields for enhanced security auditing](https://forums.aws.amazon.com/ann.jspa?annID=6551) in the AWS Discussion Forums\.

**Note**  
CloudTrail events for Amazon S3 include the signature version in the request details under the key name of '`additionalEventData`'\. To find the signature version on requests made for objects in Amazon S3 like GETs, PUTs, and DELETEs, you must enable CloudTrail data events because it is turned off by default\. 

AWS CloudTrail is the preferred method for identifying Signature Version 2 requests, if you are using Amazon S3 server access logs, see [ Using Amazon S3 access logs to identify signature version 2 requests ](using-s3-access-logs-to-identify-requests.md#using-s3-access-logs-to-identify-sigv2-requests)

### Athena query examples for identifying Amazon S3 signature version 2 requests<a name="ct-examples-identify-sigv2-requests"></a>

**Example — select all events that are signature version 2, and print only EventTime, S3 action, Request\_Parameters, Region, SourceIP, and UserAgent**  
In the following Athena query, replace *<s3\_cloudtrail\_events\_db\.cloudtrail\_myawsexamplebucket1\_table>* with your Athena details and increase or remove the limit as needed\.   

```
SELECT EventTime, EventName as S3_Action, requestParameters as Request_Parameters, awsregion as AWS_Region, sourceipaddress as Source_IP, useragent as User_Agent
FROM s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table
WHERE eventsource='s3.amazonaws.com'
AND json_extract_scalar(additionalEventData, '$.SignatureVersion')='SigV2'
LIMIT 10;
```

**Example — select all requesters that are sending signature version 2 traffic**  
   

```
SELECT useridentity.arn, Count(requestid) as RequestCount
FROM s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table
WHERE eventsource='s3.amazonaws.com'
    and json_extract_scalar(additionalEventData, '$.SignatureVersion')='SigV2'
Group by useridentity.arn
```

### Partitioning signature version 2 data<a name="partitioning-sigv2-data"></a>

If you have a large amount of data that you need to query, you can reduce the costs and runtime of Athena by creating a partitioned table\. 

To do this, create a new table with partitions as follows\.

```
   CREATE EXTERNAL TABLE s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table_partitioned(
        eventversion STRING,
        userIdentity STRUCT<
            type:STRING,
            principalid:STRING,
            arn:STRING,
            accountid:STRING,
            invokedby:STRING,
            accesskeyid:STRING,
            userName:STRING,
         sessioncontext:STRUCT<
                    attributes:STRUCT< 
                    mfaauthenticated:STRING,
                    creationdate:STRING>,
                    sessionIssuer:STRUCT<
                    type:STRING,
                    principalId:STRING,
                    arn:STRING,
                    accountId:STRING,
                    userName:STRING>
                >
             >,
        eventTime STRING,
        eventSource STRING,
        eventName STRING,
        awsRegion STRING,
        sourceIpAddress STRING,
        userAgent STRING,
        errorCode STRING,
        errorMessage STRING,
        requestParameters STRING,
        responseElements STRING,
        additionalEventData STRING,
        requestId STRING,
        eventId STRING,
        resources ARRAY<STRUCT<ARN:STRING,accountId: STRING,type:STRING>>, 
        eventType STRING,
        apiVersion STRING,
        readOnly STRING,
        recipientAccountId STRING,
        serviceEventDetails STRING,
        sharedEventID STRING,
        vpcEndpointId STRING
    )   
    PARTITIONED BY (region string, year string, month string, day string)
    ROW FORMAT SERDE 'com.amazon.emr.hive.serde.CloudTrailSerde'
    STORED AS INPUTFORMAT 'com.amazon.emr.cloudtrail.CloudTrailInputFormat'
    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
    LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/';
```

Then, create the partitions individually\. You can't get results from dates that you have not created\. 

```
ALTER TABLE s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table_partitioned ADD
    PARTITION (region= 'us-east-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/us-east-1/2019/02/19/'
    PARTITION (region= 'us-west-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/us-west-1/2019/02/19/'
    PARTITION (region= 'us-west-2', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/us-west-2/2019/02/19/'
    PARTITION (region= 'ap-southeast-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/ap-southeast-1/2019/02/19/'
    PARTITION (region= 'ap-southeast-2', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/ap-southeast-2/2019/02/19/'
    PARTITION (region= 'ap-northeast-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/ap-northeast-1/2019/02/19/'
    PARTITION (region= 'eu-west-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/eu-west-1/2019/02/19/'
    PARTITION (region= 'sa-east-1', year= '2019', month= '02', day= '19') LOCATION 's3://myawsexamplebucket1/AWSLogs/111122223333/CloudTrail/sa-east-1/2019/02/19/';
```

You can then make the request based on these partitions, and you don't need to load the full bucket\. 

```
SELECT useridentity.arn,
Count(requestid) AS RequestCount
FROM s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket1_table_partitioned
WHERE eventsource='s3.amazonaws.com'
AND json_extract_scalar(additionalEventData, '$.SignatureVersion')='SigV2'
AND region='us-east-1'
AND year='2019'
AND month='02'
AND day='19'
Group by useridentity.arn
```

## Using AWS CloudTrail to identify access to Amazon S3 objects<a name="cloudtrail-identification-object-access"></a>

You can use your AWS CloudTrail event log to identify Amazon S3 object access requests for data events such as GetObject, DeleteObject, and PutObject, and discover further information about those requests\.

The following example shows how to get all PUT object requests for Amazon S3 from the AWS CloudTrail event log\. 

### Athena query example for identifying Amazon S3 object access requests<a name="ct-examples-identify-object-access-requests"></a>

In the following Athena query examples, replace *<s3\_cloudtrail\_events\_db\.cloudtrail\_myawsexamplebucket1\_table>* with your Athena details, and modify the date range as needed\. 

**Example — select all events that have PUT object access requests, and print only EventTime, EventSource, SourceIP, UserAgent, BucketName, object, and UserARN**  

```
SELECT
  eventTime, 
  eventName, 
  eventSource, 
  sourceIpAddress, 
  userAgent, 
  json_extract_scalar(requestParameters, '$.bucketName') as bucketName, 
  json_extract_scalar(requestParameters, '$.key') as object,
  userIdentity.arn as userArn
FROM
  s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket_table
WHERE
  eventName = 'PutObject'
  AND eventTime BETWEEN '2019-07-05T00:00:00Z' and '2019-07-06T00:00:00Z'
```

**Example — select all events that have GET object access requests, and print only EventTime, EventSource, SourceIP, UserAgent, BucketName, object, and UserARN**  

```
SELECT
  eventTime, 
  eventName, 
  eventSource, 
  sourceIpAddress, 
  userAgent, 
  json_extract_scalar(requestParameters, '$.bucketName') as bucketName, 
  json_extract_scalar(requestParameters, '$.key') as object,
  userIdentity.arn as userArn
FROM
  s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket_table
WHERE
  eventName = 'GetObject'
  AND eventTime BETWEEN '2019-07-05T00:00:00Z' and '2019-07-06T00:00:00Z'
```

**Example — select all anonymous requester events to a bucket in a certain period and print only EventTime, EventSource, SourceIP, UserAgent, BucketName, UserIdentity, and UserARN**  

```
SELECT
  eventTime, 
  eventName, 
  eventSource, 
  sourceIpAddress, 
  userAgent, 
  json_extract_scalar(requestParameters, '$.bucketName') as bucketName, 
  userIdentity.arn as userArn,
  userIdentity.principalId 
FROM
  s3_cloudtrail_events_db.cloudtrail_myawsexamplebucket_table
WHERE
  userIdentity.principalId='ANONYMOUS_PRINCIPAL'
  AND eventTime BETWEEN '2019-07-05T00:00:00Z' and '2019-07-06T00:00:00Z'
```

**Note**  
These query examples may also be useful for security monitoring\. You can review the results for `PutObject` or `GetObject` calls from unexpected or unauthorized IP addresses/requesters and for identifying any anonymous requests to your buckets\.
 This query only retrieves information from the time at which logging was enabled\. 

If you are using Amazon S3 server access logs, see [ Using Amazon S3 access logs to identify object access requests](using-s3-access-logs-to-identify-requests.md#using-s3-access-logs-to-identify-objects-access)\.

## Related resources<a name="cloudtrail-logging-related-resources"></a>
+ [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/)
+ [CloudTrail Event Reference](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference.html)


# Listing object keys<a name="ListingKeysUsingAPIs"></a>

 Keys can be listed by prefix\. By choosing a common prefix for the names of related keys and marking these keys with a special character that delimits hierarchy, you can use the list operation to select and browse keys hierarchically\. This is similar to how files are stored in directories within a file system\. 

 Amazon S3 exposes a list operation that lets you enumerate the keys contained in a bucket\. Keys are selected for listing by bucket and prefix\. For example, consider a bucket named "dictionary" that contains a key for every English word\. You might make a call to list all the keys in that bucket that start with the letter "q"\. List results are always returned in UTF\-8 binary order\. 

 Both the SOAP and REST list operations return an XML document that contains the names of matching keys and information about the object identified by each key\. 

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

 Groups of keys that share a prefix terminated by a special delimiter can be rolled up by that common prefix for the purposes of listing\. This enables applications to organize and browse their keys hierarchically, much like how you would organize your files into directories in a file system\. For example, to extend the dictionary bucket to contain more than just English words, you might form keys by prefixing each word with its language and a delimiter, such as "French/logical"\. Using this naming scheme and the hierarchical listing feature, you could retrieve a list of only French words\. You could also browse the top\-level list of available languages without having to iterate through all the lexicographically intervening keys\. 

 For more information on this aspect of listing, see [Listing Keys Hierarchically Using a Prefix and Delimiter](ListingKeysHierarchy.md)\. 

**List Implementation Efficiency**  
List performance is not substantially affected by the total number of keys in your bucket, nor by the presence or absence of the prefix, marker, maxkeys, or delimiter arguments\. 

## Iterating Through Multi\-Page Results<a name="ListingKeysPaginated"></a>

As buckets can contain a virtually unlimited number of keys, the complete results of a list query can be extremely large\. To manage large result sets, the Amazon S3 API supports pagination to split them into multiple responses\. Each list keys response returns a page of up to 1,000 keys with an indicator indicating if the response is truncated\. You send a series of list keys requests until you have received all the keys\. AWS SDK wrapper libraries provide the same pagination\. 

The following Java and \.NET SDK examples show how to use pagination when listing keys in a bucket:
+ [Listing Keys Using the AWS SDK for Java](ListingObjectKeysUsingJava.md)
+ [Listing Keys Using the AWS SDK for \.NET](ListingObjectKeysUsingNetSDK.md)
+ [Listing Keys Using the AWS SDK for PHP](ListingObjectKeysUsingPHP.md)

### Related Resources<a name="RelatedResources016"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Configuring a bucket as a static website using the AWS Management Console<a name="HowDoIWebsiteConfiguration"></a>

Using the AWS Management Console, you can configure your Amazon S3 bucket as a static website without writing any code\. Depending on your website requirements, you can also use some optional configurations, including redirects, web traffic logging, and custom error documents\. 

**Required configurations:**
+ [Enabling website hosting](EnableWebsiteHosting.md)
+ [Configuring an index document](IndexDocumentSupport.md)
+ [Setting permissions for website access](WebsiteAccessPermissionsReqd.md)

**Optional configurations:**
+ [\(Optional\) Configuring a custom error document](CustomErrorDocSupport.md)
+ [\(Optional\) Configuring a webpage redirect](how-to-page-redirect.md)
+ [\(Optional\) Logging web traffic](LoggingWebsiteTraffic.md)


# What does Amazon S3 replicate?<a name="replication-what-is-isnot-replicated"></a>

Amazon S3 replicates only specific items in buckets that are configured for replication\. 

## What is replicated?<a name="replication-what-is-replicated"></a>

By default Amazon S3 replicates the following:
+ Objects created after you add a replication configuration\.

   
+ Unencrypted objects\. 

   
+ Objects encrypted at rest under Amazon S3 managed keys \(SSE\-S3\) or customer master keys \(CMKs\) stored in AWS Key Management Service \(SSE\-KMS\)\. To replicate objects encrypted with CMKs stored in AWS KMS, you must explicitly enable the option\. The replicated copy of the object is encrypted using the same type of server\-side encryption that was used for the source object\. For more information about server\-side encryption, see [Protecting data using server\-side encryption](serv-side-encryption.md)\.

   
+ Object metadata from the source objects to the replicas\. For information about replicating metadata from the replicas to the source objects, see [Replicating metadata changes with Amazon S3 replica modification sync](replication-for-metadata-changes.md)\.

   
+ Only objects in the source bucket for which the bucket owner has permissions to read objects and access control lists \(ACLs\)\. For more information about resource ownership, see [Amazon S3 bucket and object ownership](access-control-overview.md#about-resource-owner)\.

   
+ Object ACL updates, unless you direct Amazon S3 to change the replica ownership when source and destination buckets aren't owned by the same accounts\. For more information, see [Changing the replica owner](replication-change-owner.md)\. 

  It can take a while until Amazon S3 can bring the two ACLs in sync\. This applies only to objects created after you add a replication configuration to the bucket\.

   
+  Object tags, if there are any\.

   
+ S3 Object Lock retention information, if there is any\. When Amazon S3 replicates objects that have retention information applied, it applies those same retention controls to your replicas, overriding the default retention period configured on your destination buckets\. If you don't have retention controls applied to the objects in your source bucket, and you replicate into destination buckets that have a default retention period set, the destination bucket's default retention period is applied to your object replicas\. For more information, see [Locking objects using S3 Object Lock](object-lock.md)\.

### How delete operations affect replication<a name="replication-delete-op"></a>

If you delete an object from the source bucket, the following actions occur by default:
+ If you make a DELETE request without specifying an object version ID, Amazon S3 adds a delete marker\. Amazon S3 deals with the delete marker as follows:
  + If you are using the latest version of the replication configuration \(that is, you specify the `Filter` element in a replication configuration rule\), Amazon S3 does not replicate the delete marker by default\. However you can add *delete marker replication* to non\-tag\-based rules, for more information see [Replicating delete markers between buckets](https://docs.aws.amazon.com/AmazonS3/latest/dev/delete-marker-replication.html)\.
  + If you don't specify the `Filter` element, Amazon S3 assumes that the replication configuration is version V1, and it replicates delete markers that resulted from user actions\. However, if Amazon S3 deletes an object due to a lifecycle action, the delete marker is not replicated to the destination buckets\.
+ If you specify an object version ID to delete in a DELETE request, Amazon S3 deletes that object version in the source bucket\. But it doesn't replicate the deletion in the destination buckets\. In other words, it doesn't delete the same object version from the destination buckets\. This protects data from malicious deletions\. 

## What isn't replicated?<a name="replication-what-is-not-replicated"></a>

By default Amazon S3 doesn't replicate the following:
+  Objects that existed before you added the replication configuration to the bucket\. In other words, Amazon S3 doesn't replicate objects retroactively\.

   
+ The following encrypted objects:
  + Objects created with server\-side encryption using customer\-provided \(SSE\-C\) encryption keys\.
  + Objects created with server\-side encryption using CMKs stored in AWS KMS\. By default, Amazon S3 does not replicate objects encrypted using KMS CMKs\. However, you can explicitly enable replication of these objects in the replication configuration, and provide relevant information so that Amazon S3 can replicate these objects\.

   For more information about server\-side encryption, see [Protecting data using server\-side encryption](serv-side-encryption.md)\. 

   
+ Objects that are stored in S3 Glacier or S3 Glacier Deep Archive storage class\. To learn more about the Amazon S3 Glacier service, see the [Amazon S3 Glacier Developer Guide](https://docs.aws.amazon.com/amazonglacier/latest/dev/)\.

   
+ Objects in the source bucket that the bucket owner doesn't have permissions for \(when the bucket owner is not the owner of the object\)\. For information about how an object owner can grant permissions to a bucket owner, see [Granting Cross\-Account Permissions to Upload Objects While Ensuring the Bucket Owner Has Full Control](example-bucket-policies.md#example-bucket-policies-use-case-8)\.

   
+ Updates to bucket\-level subresources\. For example, if you change the lifecycle configuration or add a notification configuration to your source bucket, these changes are not applied to the destination bucket\. This makes it possible to have different configurations on source and destination buckets\. 

   
+ Actions performed by lifecycle configuration\. 

  For example, if lifecycle configuration is enabled only on your source bucket, Amazon S3 creates delete markers for expired objects but doesn't replicate those markers by default\. If you want the same lifecycle configuration applied to both source and destination buckets, enable the same lifecycle configuration on both\.

  For more information about lifecycle configuration, see [Object lifecycle management](object-lifecycle-mgmt.md)\.
**Note**  
If using the latest version of the replication configuration \(the XML specifies `Filter` as the child of `Rule`\), delete markers created either by a user action or by Amazon S3 as part of the lifecycle action are not replicated by default\. Delete markers created from user actions can be replicated using delete marker replication, see [Replicating delete markers between buckets](https://docs.aws.amazon.com/AmazonS3/latest/dev/delete-marker-replication.html)\. If you are using an earlier version of the replication configuration \(the XML specifies `Prefix` as the child of `Rule`\), delete markers resulting from user actions are replicated by default\. For more information, see [Backward compatibility](replication-add-config.md#replication-backward-compat-considerations)\.
+ Objects in the source bucket that are replicas that were created by another replication rule\. After Amazon S3 replicates an object, the object can't be replicated again\. 
  + If you change the destination bucket in an existing replication configuration, Amazon S3 won't replicate the object again\.
  + If you configure replication where bucket A is the source and bucket B is the destination\. Now suppose that you add another replication configuration where bucket B is the source and bucket C is the destination\. In this case, objects in bucket B that are replicas of objects in bucket A are not replicated to bucket C\. 

## Replicating existing objects<a name="existing-object-replication"></a>

 To enable existing object replication for your account, you must contact [AWS Support](https://console.aws.amazon.com/support/home#/case/create?issueType=customer-service&serviceCode=general-info&getting-started&categoryCode=using-aws&services)\. To prevent your request from being delayed, title your AWS Support case "Replication for Existing Objects" and be sure to include the following information:
+ Source bucket
+ Destination buckets
+ Estimated storage volume to replicate \(in terabytes\) 
+ Estimated storage object count to replicate

## Related topics<a name="replication-whatis-isnot-related-topics"></a>
+ [Replication](replication.md)
+ [Overview of setting up replication](replication-how-setup.md)
+ [Replication status information](replication-status.md)


# Object lifecycle management<a name="object-lifecycle-mgmt"></a>

To manage your objects so that they are stored cost effectively throughout their lifecycle, configure their *Amazon S3 Lifecycle*\.  An *S3 Lifecycle configuration* is a set of rules that define actions that Amazon S3 applies to a group of objects\. There are two types of actions:
+ **Transition actions**—Define when objects transition to another [storage class](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)\. For example, you might choose to transition objects to the S3 Standard\-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them\. 

  There are costs associated with the lifecycle transition requests\. For pricing information, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

   
+ **Expiration actions**—Define when objects expire\. Amazon S3 deletes expired objects on your behalf\. 

  The lifecycle expiration costs depend on when you choose to expire objects\. For more information, see [Understanding object expiration](lifecycle-expire-general-considerations.md)\.

For more information about S3 Lifecycle rules, see [Lifecycle configuration elements](intro-lifecycle-rules.md)\. 

## When should I use lifecycle configuration?<a name="lifecycle-config-overview-what"></a>

Define S3 Lifecycle configuration rules for objects that have a well\-defined lifecycle\. For example: 
+ If you upload periodic logs to a bucket, your application might need them for a week or a month\. After that, you might want to delete them\.
+ Some documents are frequently accessed for a limited period of time\. After that, they are infrequently accessed\. At some point, you might not need real\-time access to them, but your organization or regulations might require you to archive them for a specific period\. After that, you can delete them\. 
+ You might upload some types of data to Amazon S3 primarily for archival purposes\. For example, you might archive digital media, financial and healthcare records, raw genomics sequence data, long\-term database backups, and data that must be retained for regulatory compliance\.

With S3 Lifecycle configuration rules, you can tell Amazon S3 to transition objects to less expensive storage classes, or archive or delete them\.

## How do I configure a lifecycle?<a name="lifecycle-config-overview-how"></a>

An S3 Lifecycle configuration \(an XML file\), consists of a set of rules with predefined actions that you want Amazon S3 to perform on objects during their lifetime\. 

Amazon S3 provides a set of API operations for managing lifecycle configuration on a bucket\. Amazon S3 stores the configuration as a *lifecycle subresource* that is attached to your bucket\. For details, see the following:

[PUT Bucket lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html)

[GET Bucket lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlifecycle.html)

[DELETE Bucket lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETElifecycle.html)

You can also configure the lifecycle by using the Amazon S3 console or programmatically by using the AWS SDK wrapper libraries\. If you need to, you can also make the REST API calls directly\. For more information, see [Setting lifecycle configuration on a bucket](how-to-set-lifecycle-configuration-intro.md)\.

For more information, see the following topics:
+ [Additional considerations for lifecycle configuration](lifecycle-additional-considerations.md)
+ [Lifecycle configuration elements](intro-lifecycle-rules.md)
+ [Examples of lifecycle configuration](lifecycle-configuration-examples.md)
+ [Setting lifecycle configuration on a bucket](how-to-set-lifecycle-configuration-intro.md)


# Listing Keys Using the AWS SDK for Java<a name="ListingObjectKeysUsingJava"></a>

**Example**  
The following example lists the object keys in a bucket\. The example uses pagination to retrieve a set of object keys\. If there are more keys to return after the first page, Amazon S3 includes a continuation token in the response\. The example uses the continuation token in the subsequent request to fetch the next set of object keys\.   
For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListObjectsV2Request;
import com.amazonaws.services.s3.model.ListObjectsV2Result;
import com.amazonaws.services.s3.model.S3ObjectSummary;

import java.io.IOException;

public class ListKeys {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            System.out.println("Listing objects");

            // maxKeys is set to 2 to demonstrate the use of
            // ListObjectsV2Result.getNextContinuationToken()
            ListObjectsV2Request req = new ListObjectsV2Request().withBucketName(bucketName).withMaxKeys(2);
            ListObjectsV2Result result;

            do {
                result = s3Client.listObjectsV2(req);

                for (S3ObjectSummary objectSummary : result.getObjectSummaries()) {
                    System.out.printf(" - %s (size: %d)\n", objectSummary.getKey(), objectSummary.getSize());
                }
                // If there are more than maxKeys keys in the bucket, get a continuation token
                // and list the next objects.
                String token = result.getNextContinuationToken();
                System.out.println("Next Continuation Token: " + token);
                req.setContinuationToken(token);
            } while (result.isTruncated());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Retrieving the metadata of an object version<a name="RetMetaOfObjVersion"></a>

If you only want to retrieve the metadata of an object \(and not its content\), you use the `HEAD` operation\. By default, you get the metadata of the most recent version\. To retrieve the metadata of a specific object version, you specify its version ID\.

**To retrieve the metadata of an object version**

1. Set `versionId` to the ID of the version of the object whose metadata you want to retrieve\.

1. Send a `HEAD Object versionId` request\.

**Example Retrieving the metadata of a versioned object**  
The following request retrieves the metadata of version 3HL4kqCxf3vjVBH40Nrjfkd of `my-image.jpg`\.  

```
1. HEAD /my-image.jpg?versionId=3HL4kqCxf3vjVBH40Nrjfkd HTTP/1.1
2. Host: bucket.s3.amazonaws.com
3. Date: Wed, 28 Oct 2009 22:32:00 GMT
4. Authorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=
```

The following shows a sample response\.

```
 1. HTTP/1.1 200 OK
 2. x-amz-id-2: ef8yU9AS1ed4OpIszj7UDNEHGran
 3. x-amz-request-id: 318BC8BC143432E5
 4. x-amz-version-id: 3HL4kqtJlcpXroDTDmjVBH40Nrjfkd
 5. Date: Wed, 28 Oct 2009 22:32:00 GMT
 6. Last-Modified: Sun, 1 Jan 2006 12:00:00 GMT
 7. ETag: "fba9dede5f27731c9771645a39863328"
 8. Content-Length: 434234
 9. Content-Type: text/plain
10. Connection: close
11. Server: AmazonS3
```


# Manage an object's lifecycle using the AWS SDK for \.NET<a name="manage-lifecycle-using-dot-net"></a>

You can use the AWS SDK for \.NET to manage the S3 Lifecycle configuration on a bucket\. For more information about managing Lifecycle configuration, see [Object lifecycle management](object-lifecycle-mgmt.md)\. 

**Note**  
When you add a Lifecycle configuration, Amazon S3 replaces the existing configuration on the specified bucket\. To update a configuration, you must first retrieve the Lifecycle configuration, make the changes, and then add the revised Lifecycle configuration to the bucket\.

**Example \.NET code example**  
The following example shows how to use the AWS SDK for \.NET to add, update, and delete a bucket's Lifecycle configuration\. The code example does the following:  
+ Adds a Lifecycle configuration to a bucket\. 
+ Retrieves the Lifecycle configuration and updates it by adding another rule\. 
+ Adds the modified Lifecycle configuration to the bucket\. Amazon S3 replaces the existing Lifecycle configuration\.
+ Retrieves the configuration again and verifies it by printing the number of rules in the configuration\.
+ Deletes the Lifecycle configuration\.and verifies the deletion\.
For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class LifecycleTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;
        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            AddUpdateDeleteLifecycleConfigAsync().Wait();
        }

        private static async Task AddUpdateDeleteLifecycleConfigAsync()
        {
            try
            {
                var lifeCycleConfiguration = new LifecycleConfiguration()
                {
                    Rules = new List<LifecycleRule>
                        {
                            new LifecycleRule
                            {
                                 Id = "Archive immediately rule",
                                 Filter = new LifecycleFilter()
                                 {
                                     LifecycleFilterPredicate = new LifecyclePrefixPredicate()
                                     {
                                         Prefix = "glacierobjects/"
                                     }
                                 },
                                 Status = LifecycleRuleStatus.Enabled,
                                 Transitions = new List<LifecycleTransition>
                                 {
                                      new LifecycleTransition
                                      {
                                           Days = 0,
                                           StorageClass = S3StorageClass.Glacier
                                      }
                                  },
                            },
                            new LifecycleRule
                            {
                                 Id = "Archive and then delete rule",
                                  Filter = new LifecycleFilter()
                                 {
                                     LifecycleFilterPredicate = new LifecyclePrefixPredicate()
                                     {
                                         Prefix = "projectdocs/"
                                     }
                                 },
                                 Status = LifecycleRuleStatus.Enabled,
                                 Transitions = new List<LifecycleTransition>
                                 {
                                      new LifecycleTransition
                                      {
                                           Days = 30,
                                           StorageClass = S3StorageClass.StandardInfrequentAccess
                                      },
                                      new LifecycleTransition
                                      {
                                        Days = 365,
                                        StorageClass = S3StorageClass.Glacier
                                      }
                                 },
                                 Expiration = new LifecycleRuleExpiration()
                                 {
                                       Days = 3650
                                 }
                            }
                        }
                };

                // Add the configuration to the bucket. 
                await AddExampleLifecycleConfigAsync(client, lifeCycleConfiguration);

                // Retrieve an existing configuration. 
                lifeCycleConfiguration = await RetrieveLifecycleConfigAsync(client);

                // Add a new rule.
                lifeCycleConfiguration.Rules.Add(new LifecycleRule
                {
                    Id = "NewRule",
                    Filter = new LifecycleFilter()
                    {
                        LifecycleFilterPredicate = new LifecyclePrefixPredicate()
                        {
                            Prefix = "YearlyDocuments/"
                        }
                    },
                    Expiration = new LifecycleRuleExpiration()
                    {
                        Days = 3650
                    }
                });

                // Add the configuration to the bucket. 
                await AddExampleLifecycleConfigAsync(client, lifeCycleConfiguration);

                // Verify that there are now three rules.
                lifeCycleConfiguration = await RetrieveLifecycleConfigAsync(client);
                Console.WriteLine("Expected # of rulest=3; found:{0}", lifeCycleConfiguration.Rules.Count);

                // Delete the configuration.
                await RemoveLifecycleConfigAsync(client);

                // Retrieve a nonexistent configuration.
                lifeCycleConfiguration = await RetrieveLifecycleConfigAsync(client);

            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        static async Task AddExampleLifecycleConfigAsync(IAmazonS3 client, LifecycleConfiguration configuration)
        {

            PutLifecycleConfigurationRequest request = new PutLifecycleConfigurationRequest
            {
                BucketName = bucketName,
                Configuration = configuration
            };
            var response = await client.PutLifecycleConfigurationAsync(request);
        }

        static async Task<LifecycleConfiguration> RetrieveLifecycleConfigAsync(IAmazonS3 client)
        {
            GetLifecycleConfigurationRequest request = new GetLifecycleConfigurationRequest
            {
                BucketName = bucketName
            };
            var response = await client.GetLifecycleConfigurationAsync(request);
            var configuration = response.Configuration;
            return configuration;
        }

        static async Task RemoveLifecycleConfigAsync(IAmazonS3 client)
        {
            DeleteLifecycleConfigurationRequest request = new DeleteLifecycleConfigurationRequest
            {
                BucketName = bucketName
            };
            await client.DeleteLifecycleConfigurationAsync(request);
        }
    }
}
```


# Enabling cross\-origin resource sharing \(CORS\) using the REST API<a name="EnableCorsUsingREST"></a>

To set a CORS configuration on your bucket, you can use the AWS Management Console\. If your application requires it, you can also send REST requests directly\. The following sections in the *Amazon Simple Storage Service API Reference* describe the REST API actions related to the CORS configuration: 
+ [PutBucketCors](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTcors.html)
+ [GetBucketCors](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETcors.html)
+ [DeleteBucketCors](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEcors.html)
+ [OPTIONS object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTOPTIONSobject.html)


# Viewing storage usage and activity metrics with Amazon S3 Storage Lens<a name="storage_lens_view_metrics"></a>

By default, all dashboards are configured with **free metrics**, which include [usage metrics](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_basics_metrics_recommendations.html#storage_lens_basics_metrics_types) aggregated down to the bucket level with a 14\-day data retention\. This means that you can see all the usage metrics that S3 Storage Lens aggregates, and your data will be available 14 days from the day it was aggregated\. 

**Advanced metrics and recommendations** include usage and activity metrics that can be aggregated by prefix\. [Activity metrics](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_basics_metrics_recommendations.html#storage_lens_basics_metrics_types) can be aggregated by bucket with a 15\-month data retention policy\. There are additional charges for using S3 Storage Lens with advanced metrics\. For more information, see [Amazon S3 pricing](http://aws.amazon.com/s3/pricing)\.

**Topics**
+ [Viewing S3 Storage Lens metrics on the dashboards](storage_lens_view_metrics_dashboard.md)
+ [Viewing Amazon S3 Storage Lens metrics using a data export](storage_lens_view_metrics_export.md)


# Amazon S3 on Outposts examples using the AWS CLI<a name="S3OutpostsCLIExamples"></a>

With Amazon S3 on Outposts, you can create S3 buckets on your AWS Outposts and easily store and retrieve objects on\-premises for applications that require local data access, local data processing, and data residency\. You can use S3 on Outposts through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\. For more information, see [Using Amazon S3 on Outposts](https://docs.aws.amazon.com/AmazonS3/latest/dev/S3onOutposts.html)\. 

The following examples show how you can use S3 on Outposts with the AWS Command Line Interface\.

**Topics**
+ [Creating and managing Amazon S3 on Outposts bucket](#S3OutpostsBucketCLI)
+ [Working with objects using Amazon S3 on Outposts](#S3OutpostsObjectCLI)

## Creating and managing Amazon S3 on Outposts bucket<a name="S3OutpostsBucketCLI"></a>

You can use the AWS CLI to create and manage your S3 on Outposts buckets\. From these examples, you can create and get an Outposts bucket, list buckets for an Outpost, create and manage access points, lifecycleconfiguration, and policy for the Outpost bucket\. 

**Topics**
+ [Create an S3 on Outposts bucket](#S3OutpostsCreateBucketCLI)
+ [Get the S3 on Outposts bucket](#S3OutpostsGetBucketCLI)
+ [Get a list of S3 on Outposts buckets](#S3OutpostsListRegionalBucketCLI)
+ [Create an access point for an S3 on Outposts bucket](#S3OutpostsCreateAccessPointCLI)
+ [Get an access point for an S3 on Outposts bucket](#S3OutpostsGetAccessPointCLI)
+ [List access points for an Outpost](#S3OutpostsListAccessPointCLI)
+ [Put a lifecycle configuration for an S3 on Outposts bucket](#S3OutpostsPutBucketLifecycleConfigurationCLI)
+ [Get a lifecycle configuration for an S3 on Outposts bucket](#S3OutpostsGetBucketLifecycleConfigurationCLI)
+ [Put a policy on an S3 on Outposts bucket](#S3OutpostsPutBucketPolicyCLI)
+ [Get a policy for an S3 on Outposts bucket](#S3OutpostsGetBucketPolicyCLI)
+ [Put a policy on an S3 on Outposts access point](#S3OutpostsPutAccessPointPolicyCLI)
+ [Get a policy for an S3 on Outposts access point](#S3OutpostsGetAccessPointPolicyCLI)
+ [Create an endpoint on an Outpost](#S3OutpostsCreateEndpointCLI)
+ [List endpoints for an Outpost](#S3OutpostsListEndpointsCLI)
+ [Delete an endpoint on an Outpost](#S3OutpostsDeleteEndpointCLI)

### Create an S3 on Outposts bucket<a name="S3OutpostsCreateBucketCLI"></a>

The following example creates an S3 on Outposts `s3-outposts:CreateBucket` using the AWS CLI\. 

```
aws s3control create-bucket --bucket example-outpost-bucket --outpost-id op-01ac5d28a6a232904
```

### Get the S3 on Outposts bucket<a name="S3OutpostsGetBucketCLI"></a>

The following S3 on Outposts example gets a bucket using the AWS CLI\. 

```
aws s3control get-bucket --account-id 123456789012 --bucket "arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket"
```



### Get a list of S3 on Outposts buckets<a name="S3OutpostsListRegionalBucketCLI"></a>

The following AWS CLI example gets a list of buckets in an Outpost\. 

```
aws s3control list-regional-buckets --account-id 123456789012 --outpost-id op-01ac5d28a6a232904
```



### Create an access point for an S3 on Outposts bucket<a name="S3OutpostsCreateAccessPointCLI"></a>

The following AWS CLI example creates an access point for an Outposts bucket\.

```
aws s3control create-access-point --account-id 123456789012 --name example-access-point --bucket "arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket" --vpc-configuration VpcId=example-vpc-12345
```



### Get an access point for an S3 on Outposts bucket<a name="S3OutpostsGetAccessPointCLI"></a>

The following AWS CLI example gets an access point for an Outposts bucket\.

```
aws s3control get-access-point --account-id 123456789012 --name arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-access-point
```



### List access points for an Outpost<a name="S3OutpostsListAccessPointCLI"></a>

The following AWS CLI example List access points for an Outposts bucket\.

```
aws s3control list-access-points --account-id 123456789012 --bucket arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket
```



### Put a lifecycle configuration for an S3 on Outposts bucket<a name="S3OutpostsPutBucketLifecycleConfigurationCLI"></a>

The following AWS CLI example puts an lifecycle configruations for an Outposts bucket where all objects with the flagged prefix and tags expire after 10 days\.

1. Save the lifecycle configuration policy to a JSON file\.

   ```
   {
       "Rules": [
           {
               "ID": "id-1",
               "Filter": {
                   "And": {
                       "Prefix": "myprefix", 
                       "Tags": [
                           {
                               "Value": "mytagvalue1", 
                               "Key": "mytagkey1"
                           }, 
                           {
                               "Value": "mytagvalue2", 
                               "Key": "mytagkey2"
                           }
                       ]
                   }
               }, 
               "Status": "Enabled", 
               "Expiration": {
                   "Days": 10
               }
           }
       ]
   }
   ```

1. Submit the JSON file as part of the put lifecycle configuration CLI command\.

   ```
   aws s3control put-bucket-lifecycle-configuration --account-id 123456789012 --bucket arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket --lifecycle-configuration file://lifecycle1.json
   ```



### Get a lifecycle configuration for an S3 on Outposts bucket<a name="S3OutpostsGetBucketLifecycleConfigurationCLI"></a>

The following AWS CLI example gets an access point for an Outposts bucket\.

```
aws s3control get-bucket-lifecycle-configuration --account-id 123456789012 --bucket arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket
```



### Put a policy on an S3 on Outposts bucket<a name="S3OutpostsPutBucketPolicyCLI"></a>

The following AWS CLI example puts policy for an Outposts bucket\.

1. Save the bucket policy to a JSON file\.

   ```
   {
      "Version":"2012-10-17",
      "Id":"testBucketPolicy",
      "Statement":[
         {
            "Sid":"st1",
            "Effect":"Allow",
            "Principal":{
               "AWS":"123456789012"
            },
            "Action":"s3-outposts:*",
            "Resource":"arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket"
         }
      ]
   }
   ```

1. Submit the JSON file as part of the put bucket policy CLI command\.

   ```
   aws s3control put-bucket-policy --account-id 123456789012 --bucket arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket --policy file://policy1.json
   ```



### Get a policy for an S3 on Outposts bucket<a name="S3OutpostsGetBucketPolicyCLI"></a>

The following AWS CLI example gets a policy for an Outposts bucket\.



```
aws s3control get-bucket-policy --account-id 123456789012 --bucket arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket
```

### Put a policy on an S3 on Outposts access point<a name="S3OutpostsPutAccessPointPolicyCLI"></a>

The following AWS CLI example puts policy for an Outposts bucket\.

1. Save the access point policy to a JSON file\.

   ```
   {
      "Version":"2012-10-17",
      "Id":"testBucketPolicy",
      "Statement":[
         {
            "Sid":"st1",
            "Effect":"Allow",
            "Principal":{
               "AWS":"123456789012"
            },
            "Action":"s3-outposts:*",
            "Resource":"arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket"
         }
      ]
   }
   ```

1. Submit the JSON file as part of the put bucket policy CLI command\.

   ```
   aws s3control put-access-point-policy --account-id 123456789012 --name arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-access-point --policy file://appolicy1.json
   ```



### Get a policy for an S3 on Outposts access point<a name="S3OutpostsGetAccessPointPolicyCLI"></a>

The following AWS CLI example gets a policy for an Outposts bucket\.



```
aws s3control get-access-point-policy --account-id 123456789012 --name arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-access-point
```

### Create an endpoint on an Outpost<a name="S3OutpostsCreateEndpointCLI"></a>

The following AWS CLI example creates an endpoint for an Outpost\.

```
aws s3outposts create-endpoint --outpost-id op-01ac5d28a6a232904 --subnet-id subnet-8c7a57c5 —-security-group-id sg-ab19e0d1
```

### List endpoints for an Outpost<a name="S3OutpostsListEndpointsCLI"></a>

The following AWS CLI example lists endpoints for an Outpost\.

```
aws s3outposts list-endpoints 
```



### Delete an endpoint on an Outpost<a name="S3OutpostsDeleteEndpointCLI"></a>

The following AWS CLI example creates an endpoint for an Outpost\.

```
aws s3outposts delete-endpoint --endpoint-id m3kvngonpdx2kadiusm --outpost-id op-01ac5d28a6a232904 
```

## Working with objects using Amazon S3 on Outposts<a name="S3OutpostsObjectCLI"></a>

You can use the AWS CLI to put and manage your S3 on Outposts objects\. From these examples, you can put objects and get objects from an Outpost bucket\. 

**Topics**
+ [Put an object in to an S3 on Outposts bucket](#S3OutpostsPutObjectCLI)
+ [Get the S3 on Outposts bucket](#S3OutpostsGetObjectCLI)
+ [List objects in an S3 on Outposts bucket](#S3OutpostsListObjectsCLI)

### Put an object in to an S3 on Outposts bucket<a name="S3OutpostsPutObjectCLI"></a>

The following example put an S3 on Outposts `s3-outposts:PutObeject` using the AWS CLI\. 

```
aws s3api put-object --bucket arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-access-point --key testkey body sample-object.xml
```

### Get the S3 on Outposts bucket<a name="S3OutpostsGetObjectCLI"></a>

The following S3 on Outposts example gets a bucket using the AWS CLI\. 

```
aws s3api get-object --bucket arn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-access-point --key testkey sample-object.xml
```



### List objects in an S3 on Outposts bucket<a name="S3OutpostsListObjectsCLI"></a>

The following example list S3 on Outposts `s3-outposts:ListObejectsV2` using the AWS CLI\. 

```
aws s3api list-objects-v2 --bucket arn:aws:s3-outposts:us-west-2:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-access-point
```


# Understanding Amazon S3 Storage Lens<a name="storage_lens_basics_metrics_recommendations"></a>

Amazon S3 Storage Lens provides a single view of object storage usage and activity across your entire Amazon S3 storage\. It includes drill\-down options to generate insights at the organization, account, Region, bucket, or even prefix level\. 

Amazon S3 Storage Lens aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format\. You can use the dashboard to visualize insights and trends, flag outliers, and provides recommendations for optimizing storage costs and applying data protection best practices\. You can use S3 Storage Lens through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\.

## Amazon S3 Storage Lens concepts and terminology<a name="storage_lens_basics"></a>

This section contains the terminology and concepts that are essential for understanding and using Amazon S3 Storage Lens successfully\.

**Topics**
+ [Configuration](#storage_lens_basics_configuration)
+ [Default dashboard](#storage_lens_basics_default_dashboard)
+ [Dashboards](#storage_lens_basics_dashboards)
+ [Metrics export](#storage_lens_basics_metrics_export)
+ [Home Region](#storage_lens_basics_home_region)
+ [Retention period](#storage_lens_basics_retention_period)
+ [Metrics types](#storage_lens_basics_metrics_types)
+ [Recommendations](#storage_lens_basics_recommendations)
+ [Metrics selection](#storage_lens_basics_metrics_selection)
+ [S3 Storage Lens and AWS Organizations](#storage_lens_basics_organizations)

### Configuration<a name="storage_lens_basics_configuration"></a>

Amazon S3 Storage Lens requires a *configuration* that contains the properties that are used to aggregate metrics on your behalf for a single dashboard or export\. This includes all or partial sections of your organization account’s storage, including filtering by Region, bucket, and prefix\-level \(available only with advanced metrics\) scope\. It includes information about whether you chose *free metrics* or *advanced metrics and recommendations*\. It also includes whether a metrics export is required, and information about where to place the metrics export if applicable\.

### Default dashboard<a name="storage_lens_basics_default_dashboard"></a>

The S3 Storage Lens default dashboard on the console is named **default\-account\-dashboard**\. S3 preconfigures the dashboard to visualize the summarized insights and trends of your entire account’s aggregated storage usage and activity metrics, and updates them daily in the Amazon S3 console\. You can't modify the configuration scope of the default dashboard, but you can upgrade the metrics selection from **free metrics** to the paid **advanced metrics and recommendations**\. You can also configure the optional metrics export, or even disable the dashboard\. However, you can't delete the default dashboard\.

**Note**  
If you disable your default dashboard, it is no longer updated, and you will no longer receive any new daily metrics\. You can still see historic data until the 14\-day expiration period, or 15 months if you are subscribed to *advanced metrics and recommendations* for that dashboard\. You can re\-enable the dashboard within the expiration period to access this data\.

### Dashboards<a name="storage_lens_basics_dashboards"></a>

You can also use Amazon S3 Storage Lens to configure a dashboard that visualizes summarized insights and trends of aggregated storage usage and activity metrics, updated daily on the Amazon S3 console\. You can create and modify S3 Storage Lens dashboards to express all or partial sections of your organization account’s storage\. You can filter by AWS Region, bucket, and prefix \(available only with advanced metrics and recommendations\)\. You can also disable or delete the dashboard\.

**Note**  
You can use S3 Storage Lens to create up to 50 dashboards per home Region\.
If you disable a dashboard, it is no longer updated, and you will no longer receive any new daily metrics\. You can still see historic data until the 14\-day expiration period \(or 15 months, if you subscribed to advanced metrics and recommendations for that dashboard\)\. You can re\-enable the dashboard within the expiration period to access this data\.
If you delete your dashboard, you lose all your dashboard configuration settings\. You will no longer receive any new daily metrics, and you also lose access to the historical data associated with that dashboard\. If you want to access the historic data for a deleted dashboard, you must create another dashboard with the same name in the same home Region\.
Organization\-level dashboards can only be limited to a regional scope\.

### Metrics export<a name="storage_lens_basics_metrics_export"></a>

An S3 Storage Lens *metrics export* is a file that contains all the metrics identified in your S3 Storage Lens configuration\. This information is generated daily in CSV or Parquet format in an S3 bucket of your choice for further analysis\. You can generate an S3 Storage Lens metrics export from the S3 console by editing your dashboard configuration, or by using the AWS CLI and SDKs\.

### Home Region<a name="storage_lens_basics_home_region"></a>

The home Region is the Region where all Amazon S3 Storage Lens metrics for a given dashboard or configuration's are stored\. You must choose a home Region when you create your S3 Storage Lens dashboard or configuration\. After a home Region is assigned, it can't be changed\.

**Note**  
Creating a home Region is not supported the following Regions:  
Africa \(Cape Town\) \(af\-south\-1\)
Asia Pacific \(Hong Kong\) \(ap\-east\-1\)
Europe \(Milan\) \(eu\-south\-1\)
Middle East \(Bahrain\) \(me\-south\-1\)

### Retention period<a name="storage_lens_basics_retention_period"></a>

Amazon S3 Storage Lens metrics are retained so you can see historical trends and compare differences in your storage usage and activity over time\. The retention periods depend on your [metrics selection](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_basics_metrics_selection.html) and cannot be modified\. Free metrics are retained for a 14\-day period, and advanced metrics are retained for a 15\-month period\.

### Metrics types<a name="storage_lens_basics_metrics_types"></a>

S3 Storage Lens offers two types of storage metrics: *usage* and *activity*\.
+ **Usage metrics**

  S3 Storage Lens collects *usage metrics* for all dashboards and configurations\. Usage metrics describe the size, quantity, and characteristics of your storage\. This includes the total bytes stored, object count, and average object size in addition to metrics that describe feature utilization such as encrypted bytes, or delete market object counts\. For more information about the usage metrics aggregated by S3 Storage Lens, see [Metrics glossary](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_metrics_glossary.html)\.
+ **Activity metrics**

  S3 Storage Lens aggregates *activity metrics* for all dashboards and configurations that have the *advanced metrics and recommendations metrics* type enabled\. Activity metrics describe the details of how often your storage is requested\. This includes the number of requests by type, upload and download bytes, and errors\. For more information about the activity metrics that are aggregated by S3 Storage Lens, see [Metrics glossary](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_metrics_glossary.html)\.

### Recommendations<a name="storage_lens_basics_recommendations"></a>

S3 Storage Lens provides automated *recommendations* to help you optimize your storage\. Recommendations are placed contextually alongside relevant metrics in the S3 Storage Lens dashboard\. Historical data is not eligible for recommendations because recommendations are relevant to what is happening in the most recent period\. Recommendations only appear when they are relevant\.

S3 Storage Lens recommendations come in the following forms:
+ **Suggestions**

  *Suggestions* alert you to trends within your storage usage and activity that might indicate a storage cost optimization opportunity or data protection best practice\. You can use the suggested topics in the *Amazon S3 Developer Guide* and the S3 Storage Lens dashboard to drill down for more details about the specific Regions, buckets, or prefixes to further assist you\.
+ **Call\-outs**

  Call\-outs are recommendations that alert you to interesting anomalies within your storage usage and activity over a period that might need further attention or monitoring\.
  + **Outlier call\-outs**

    S3 Storage Lens provides call\-outs for metrics that are *outliers*, based on your recent 30\-day trend\. The outlier is calculated using a standard score, also known as a *z\-score*\. In this score, the current day’s metric is subtracted from the average of the last 30 days for that metric, and then divided by the standard deviation for that metric over the last 30 days\. The resulting score is usually between \-3 and \+3\. This number represents the number of standard deviations that the current day’s metric is from the mean\. 

    S3 Storage Lens considers metrics with a score >2 or <\-2 to be outliers because they are higher or lower than 95 percent of normally distributed data\. 
  + **Significant change call\-outs**

    The *significant change call\-out* applies to metrics that are expected to change less frequently\. Therefore it is set to a higher sensitivity than the outlier calculation, which is typically in the range of \+/\- 20 percent versus the prior day, week, or month\.

    **Addressing call\-outs in your storage usage and activity** – If you receive a significant change call\-out, it’s not necessarily a problem, and could be the result of an anticipated change in your storage\. For example, you might have recently added a large number of new objects, deleted a large number of objects, or made similar planned changes\. 

    If you see a significant change call\-out on your dashboard, take note of it and determine whether it can be explained by recent circumstances\. If not, use the S3 Storage Lens dashboard to drill down for more details to understand the specific Regions, buckets, or prefixes that are driving the fluctuation\.
+ **Reminders**

  *Reminders* provide insights into how Amazon S3 works\. They can help you learn more about ways to use S3 features to reduce storage costs or apply data protection best practices\.

### Metrics selection<a name="storage_lens_basics_metrics_selection"></a>

S3 Storage Lens offers two metrics selections that you can choose for your dashboard and export: *free metrics* and *advanced metrics and recommendations*\.
+ **Free metrics**

  S3 Storage Lens offers free metrics for all dashboards and configurations\. Free metrics contain metrics that are relevant to your storage usage\. This includes the number of buckets, the objects in your account, and what state they are in\. All free metrics are collected daily and retained for a 14\-day retention period\. For more information about what usage metrics are aggregated by S3 Storage Lens, see [Metrics glossary](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_metrics_glossary.html)\.
+ **Advanced metrics and recommendations**

  S3 Storage Lens offers free metrics for all dashboards and configurations and the option to upgrade to the* advanced metrics and recommendations* option\. Advanced metrics contain all the usage metrics that are included in free metrics\. This includes the number of buckets, the objects in your account, and what state they are in\. 

  With advanced metrics, you can also collect usage metrics at the prefix level\. In addition, advanced metrics include activity metrics\. Activity metrics data is relevant to your storage activity\. This includes the number of requests, scans, and errors with respect to the configuration scope and what state they are in\. All advanced metrics are collected daily and retained for a 15\-month retention period\. For more information about the storage metrics aggregated by S3 Storage Lens, see [Metrics glossary](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_metrics_glossary.html)\.

  This metrics selection also provides recommendations to help you optimize your storage\. Recommendations are placed contextually alongside relevant metrics in the dashboard\. Additional charges apply\. For more information, see [Amazon S3 pricing](http://aws.amazon.com/s3/pricing/)\.
**Note**  
Recommendations are available only when you use the S3 Storage Lens dashboard on the Amazon S3 console, and not via the AWS CLI and SDKs\.

### S3 Storage Lens and AWS Organizations<a name="storage_lens_basics_organizations"></a>

AWS Organizations is an AWS service that helps you aggregate all your AWS accounts under one organization hierarchy\. Amazon S3 Storage Lens works with AWS Organizations to provide a single view of object storage usage and activity across your Amazon S3 storage\.

For more information, see [Using Amazon S3 Storage Lens with AWS OrganizationsEnabling trusted access for S3 Storage Lens](storage_lens_with_organizations.md)\.
+ **Trusted access**

  Using your organization’s management account, you must enable *trusted access* for S3 Storage Lens to aggregate storage metrics and usage data for all member accounts in your organization\. You can then create dashboards or exports for your organization using your management account or by giving delegated administrator access to other accounts in your organization\. 

  You can disable trusted access for S3 Storage Lens at any time, which stops S3 Storage Lens from aggregating metrics for your organization\.
+ **Delegated administrator**

  You can create dashboards and metrics for S3 Storage Lens for your organization using your AWS Organizations management account, or by giving *delegated administrator* access to other accounts in your organization\. You can deregister delegated administrators at any time, which prevents S3 Storage Lens from collecting data on an organization level\.

For more information, see [Amazon S3 Storage Lens and AWS Organizations](https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-s3lens.html) in the *AWS Organizations User Guide*\.

#### Amazon S3 Storage Lens service\-linked roles<a name="storage_lens_basics_service_linked_role"></a>

Along with AWS Organizations trusted access, Amazon S3 Storage Lens uses AWS Identity and Access Management \(IAM\) service\-linked roles\. A service\-linked role is a unique type of IAM role that is linked directly to S3 Storage Lens\. Service\-linked roles are predefined by S3 Storage Lens and include all the permissions that it requires to collect daily storage usage and activity metrics from member accounts in your organization\. 

For more information, see [Using service\-linked roles for Amazon S3 Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/dev/using-service-linked-roles.html)\.


# Additional replication configurations<a name="replication-additional-configs"></a>

This section describes additional replication configuration options that are available in Amazon S3\. For information about core replication configuration, see [Overview of setting up replication](replication-how-setup.md)\.

**Topics**
+ [Monitoring progress with replication metrics and Amazon S3 event notifications](replication-metrics.md)
+ [Meeting compliance requirements using S3 Replication Time Control \(S3 RTC\)](replication-time-control.md)
+ [Replicating delete markers between buckets](delete-marker-replication.md)
+ [Replicating metadata changes with Amazon S3 replica modification sync](replication-for-metadata-changes.md)
+ [Changing the replica owner](replication-change-owner.md)
+ [Replicating objects created with server\-side encryption \(SSE\) using encryption keys stored in AWS KMS](replication-config-for-kms-objects.md)


# Using the AWS SDK for Java<a name="UsingTheMPJavaAPI"></a>

The AWS SDK for Java provides an API for the Amazon S3 bucket and object operations\. For object operations, in addition to providing the API to upload objects in a single operation, the SDK provides an API to upload large objects in parts\. For more information, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\. 

**Topics**
+ [The Java API Organization](#JavaAPIOrganization)
+ [Testing the Amazon S3 Java Code Examples](#TestingJavaSamples)

The AWS SDK for Java gives you the option of using a high\-level or low\-level API\. 

**Low\-Level API**  
The low\-level APIs correspond to the underlying Amazon S3 REST operations, such as create, update, and delete operations that apply to buckets and objects\. When you upload large objects using the low\-level multipart upload API, it provides greater control\. For example, it lets you pause and resume multipart uploads, vary part sizes during the upload, or begin uploads when you don't know the size of the data in advance\. If you don't have these requirements, use the high\-level API to upload objects\. 

**High\-Level API**  
For uploading objects, the SDK provides a higher level of abstraction by providing the `TransferManager` class\. The high\-level API is a simpler API, where in just a few lines of code you can upload files and streams to Amazon S3\. You should use this API to upload data unless you need to control the upload as described in the preceding Low\-Level API section\.

For smaller data size, the `TransferManager` API uploads data in a single operation\. However, the `TransferManager` switches to using the multipart upload API when the data size reaches a certain threshold\. When possible, the `TransferManager` uses multiple threads to concurrently upload the parts\. If a part upload fails, the API retries the failed part upload up to three times\. However, these are configurable options using the `TransferManagerConfiguration` class\. 

**Note**  
When you're using a stream for the source of data, the `TransferManager` class does not do concurrent uploads\.

## The Java API Organization<a name="JavaAPIOrganization"></a>

The following packages in the AWS SDK for Java provide the API:
+ **com\.amazonaws\.services\.s3—**Provides the APIs for creating Amazon S3 clients and working with buckets and objects\. For example, it enables you to create buckets, upload objects, get objects, delete objects, and list keys\. 
+ **com\.amazonaws\.services\.s3\.transfer—**Provides the high\-level API data operations\.

  This high\-level API is designed to simplify transferring objects to and from Amazon S3\. It includes the `TransferManager` class, which provides asynchronous methods for working with, querying, and manipulating transfers\. It also includes the `TransferManagerConfiguration` class, which you can use to configure the minimum part size for uploading parts and the threshold in bytes of when to use multipart uploads\.
+ **com\.amazonaws\.services\.s3\.model—**Provides the low\-level API classes to create requests and process responses\. For example, it includes the `GetObjectRequest` class to describe your get object request, the `ListObjectsRequest` class to describe your list keys requests, and the `InitiateMultipartUploadRequest` class to create multipart uploads\. 

For more information about the AWS SDK for Java API, see [AWS SDK for Java API Reference](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/)\.

## Testing the Amazon S3 Java Code Examples<a name="TestingJavaSamples"></a>

The Java examples in this guide are compatible with the AWS SDK for Java version 1\.11\.321\. For instructions on setting up and running code samples, see [Getting Started](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/getting-started.html) in the AWS SDK for Java Developer Guide\. 


# Virtual hosting of buckets<a name="VirtualHosting"></a>

Virtual hosting is the practice of serving multiple websites from a single web server\. One way to differentiate sites is by using the apparent hostname of the request instead of just the path name part of the URI\. An ordinary Amazon S3 REST request specifies a bucket by using the first slash\-delimited component of the Request\-URI path\. Or, you can use Amazon S3 virtual hosting to address a bucket in a REST API call by using the HTTP `Host` header\. In practice, Amazon S3 interprets `Host` as meaning that most buckets are automatically accessible for limited types of requests at `https://bucketname.s3.Region.amazonaws.com`\. For a complete list of Amazon S3 Regions and endpoints, see [Amazon S3 Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\.

Virtual hosting also has other benefits\. By naming your bucket after your registered domain name and by making that name a DNS alias for Amazon S3, you can completely customize the URL of your Amazon S3 resources, for example, `http://my.bucketname.com/`\. You can also publish to the "root directory" of your bucket's virtual server\. This ability can be important because many existing applications search for files in this standard location\. For example, `favicon.ico`, `robots.txt`, `crossdomain.xml` are all expected to be found at the root\. 

**Important**  
When using virtual hosted–style buckets with SSL, the SSL wild\-card certificate only matches buckets that do not contain dots \("\."\)\. To work around this, use HTTP or write your own certificate verification logic\. For more information, see [Amazon S3 Path Deprecation Plan](http://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/)\.

**Topics**
+ [Path\-Style Requests](#path-style-access)
+ [Virtual Hosted\-Style Requests](#virtual-hosted-style-access)
+ [HTTP Host Header Bucket Specification](#VirtualHostingSpecifyBucket)
+ [Examples](#VirtualHostingExamples)
+ [Customizing Amazon S3 URLs with CNAMEs](#VirtualHostingCustomURLs)
+ [Limitations](#VirtualHostingLimitations)
+ [Backward Compatibility](#VirtualHostingBackwardsCompatibility)

## Path\-Style Requests<a name="path-style-access"></a>

Currently Amazon S3 supports virtual hosted\-style and path\-style access in all Regions, but this will be changing \(see the following **Important** note\.

In Amazon S3, path\-style URLs follow the format shown below\.

```
https://s3.Region.amazonaws.com/bucket-name/key name
```

For example, if you create a bucket named `mybucket` in the US West \(Oregon\) Region, and you want to access the `puppy.jpg` object in that bucket, you can use the following path\-style URL:

```
https://s3.us-west-2.amazonaws.com/mybucket/puppy.jpg
```

**Important**  
Update \(September 23, 2020\) – We have decided to delay the deprecation of path\-style URLs to ensure that customers have the time that they need to transition to virtual hosted\-style URLs\. For more information, see [Amazon S3 Path Deprecation Plan – The Rest of the Story](https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/)\.

## Virtual Hosted\-Style Requests<a name="virtual-hosted-style-access"></a>

In a virtual\-hosted–style URI, the bucket name is part of the domain name in the URL\.

Amazon S3 virtual hosted style URLs follow the format shown below\.

```
https://bucket-name.s3.Region.amazonaws.com/key name
```

In this example, `my-bucket` is the bucket name, US West \(Oregon\) is the Region, and `puppy.png` is the key name:

```
https://my-bucket.s3.us-west-2.amazonaws.com/puppy.png
```

## HTTP Host Header Bucket Specification<a name="VirtualHostingSpecifyBucket"></a>

As long as your `GET` request does not use the SSL endpoint, you can specify the bucket for the request by using the HTTP `Host` header\. The `Host` header in a REST request is interpreted as follows: 
+ If the `Host` header is omitted or its value is `s3.Region.amazonaws.com`, the bucket for the request will be the first slash\-delimited component of the Request\-URI, and the key for the request will be the rest of the Request\-URI\. This is the ordinary method, as illustrated by the first and second examples in this section\. Omitting the Host header is valid only for HTTP 1\.0 requests\. 
+ Otherwise, if the value of the `Host` header ends in `.s3.Region.amazonaws.com`, the bucket name is the leading component of the `Host` header's value up to `.s3.Region.amazonaws.com`\. The key for the request is the Request\-URI\. This interpretation exposes buckets as subdomains of `.s3.Region.amazonaws.com`, as illustrated by the third and fourth examples in this section\. 
+ Otherwise, the bucket for the request is the lowercase value of the `Host` header, and the key for the request is the Request\-URI\. This interpretation is useful when you have registered the same DNS name as your bucket name and have configured that name to be a CNAME alias for Amazon S3\. The procedure for registering domain names and configuring DNS is beyond the scope of this guide, but the result is illustrated by the final example in this section\.

## Examples<a name="VirtualHostingExamples"></a>

This section provides example URLs and requests\.

**Example Path Style**  
This example uses the following:  
+ Bucket Name ‐ `awsexamplebucket1.net`
+ Region ‐ US East \(N\. Virginia\) 
+ Key Name ‐ `homepage.html`
The URL is as follows:  

```
1. http://s3.us-east-1.amazonaws.com/awsexamplebucket1.net/homepage.html
```
The request is as follows:  

```
1. GET /awsexamplebucket1.net/homepage.html HTTP/1.1
2. Host: s3.us-east-1.amazonaws.com
```
The request with HTTP 1\.0 and omitting the `host` header is as follows:  

```
1. GET /awsexamplebucket1.net/homepage.html HTTP/1.0
```

For information about DNS\-compatible names, see [Limitations](#VirtualHostingLimitations)\. For more information about keys, see [Keys](Introduction.md#BasicsKeys)\.

**Example Virtual Hosted–Style**  
This example uses the following:  
+ Bucket Name ‐ `awsexamplebucket1.eu` 
+ Region ‐ Europe \(Ireland\) 
+ Key Name ‐ `homepage.html`
The URL is as follows:  

```
1. http://awsexamplebucket1.eu.s3.eu-west-1.amazonaws.com/homepage.html
```
The request is as follows:  

```
1. GET /homepage.html HTTP/1.1
2. Host: awsexamplebucket1.eu.s3.eu-west-1.amazonaws.com
```

**Example CNAME Method**  
To use this method, you must configure your DNS name as a CNAME alias for `bucketname.s3.us-east-1.amazonaws.com`\. For more information, see [Customizing Amazon S3 URLs with CNAMEs](#VirtualHostingCustomURLs)\. This example uses the following:  
+ Bucket Name ‐ `awsexamplebucket1.net` 
+ Key Name ‐ `homepage.html`
The URL is as follows:  

```
1. http://www.awsexamplebucket1.net/homepage.html
```
The example is as follows:  

```
1. GET /homepage.html HTTP/1.1
2. Host: www.awsexamplebucket1.net
```

## Customizing Amazon S3 URLs with CNAMEs<a name="VirtualHostingCustomURLs"></a>

Depending on your needs, you might not want `s3.Region.amazonaws.com` to appear on your website or service\. For example, if you're hosting website images on Amazon S3, you might prefer `http://images.awsexamplebucket1.net/` instead of `http://images.awsexamplebucket1.net.s3.us-east-1.amazonaws.com/`\. Any bucket with a DNS\-compatible name can be referenced as follows: ` http://BucketName.s3.Region.amazonaws.com/[Filename]`, for example, `http://images.awsexamplebucket1.net.s3.us-east-1.amazonaws.com/mydog.jpg`\. By using CNAME, you can map `images.awsexamplebucket1.net` to an Amazon S3 hostname so that the previous URL could become `http://images.awsexamplebucket1.net/mydog.jpg`\. 

Your bucket name must be the same as the CNAME\. For example, if you create a CNAME to map `images.awsexamplebucket1.net` to `images.awsexamplebucket1.net.s3.us-east-1.amazonaws.com`, both `http://images.awsexamplebucket1.net/filename` and `http://images.awsexamplebucket1.net.s3.us-east-1.amazonaws.com/filename` will be the same\.

The CNAME DNS record should alias your domain name to the appropriate virtual hosted–style hostname\. For example, if your bucket name and domain name are `images.awsexamplebucket1.net` and your bucket is in the US East \(N\. Virginia\) Region, the CNAME record should alias to `images.awsexamplebucket1.net.s3.us-east-1.amazonaws.com`\. 

```
1. images.awsexamplebucket1.net CNAME 			images.awsexamplebucket1.net.s3.us-east-1.amazonaws.com.
```

Amazon S3 uses the hostname to determine the bucket name\. So the CNAME and the bucket name must be the same\. For example, suppose that you have configured `www.example.com` as a CNAME for `www.example.com.s3.us-east-1.amazonaws.com`\. When you access `http://www.example.com`, Amazon S3 receives a request similar to the following:

**Example**  

```
1. GET / HTTP/1.1
2. Host: www.example.com
3. Date: date
4. Authorization: signatureValue
```

Amazon S3 sees only the original hostname `www.example.com` and is unaware of the CNAME mapping used to resolve the request\. 

Any Amazon S3 endpoint can be used in a CNAME\. For example, `s3.ap-southeast-1.amazonaws.com` can be used in CNAMEs\. For more information about endpoints, see [Request Endpoints](MakingRequests.md#RequestEndpoints)\.

**To associate a hostname with an Amazon S3 bucket using CNAMEs**

1. Select a hostname that belongs to a domain you control\. 

   This example uses the `images` subdomain of the `awsexamplebucket1.net` domain\.

1. Create a bucket that matches the hostname\. 

   In this example, the host and bucket names are `images.awsexamplebucket1.net`\. The bucket name must *exactly* match the hostname\. 

1. Create a CNAME record that defines the hostname as an alias for the Amazon S3 bucket\. 

   For example:

   `images.awsexamplebucket1.net CNAME images.awsexamplebucket1.net.s3.us-west-2.amazonaws.com`
**Important**  
For request routing reasons, the CNAME record must be defined exactly as shown in the preceding example\. Otherwise, it might appear to operate correctly but eventually result in unpredictable behavior\.

   The procedure for configuring DNS depends on your DNS server or DNS provider\. For specific information, see your server documentation or contact your provider\.

## Limitations<a name="VirtualHostingLimitations"></a>

 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

## Backward Compatibility<a name="VirtualHostingBackwardsCompatibility"></a>

### Legacy Endpoints<a name="s3-legacy-endpoints"></a>

Some Regions support legacy endpoints\. You might see these endpoints in your server access logs or CloudTrail logs\. For more information, review the information below\. For a complete list of Amazon S3 Regions and endpoints, see [Amazon S3 Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\.

**Important**  
Although you might see legacy endpoints in your logs, we recommend that you always use the standard endpoint syntax to access your buckets\.   
Amazon S3 virtual hosted style URLs follow the format shown below\.  

```
https://bucket-name.s3.Region.amazonaws.com/key name
```
In Amazon S3, path\-style URLs follow the format shown below\.  

```
https://s3.Region.amazonaws.com/bucket-name/key name
```

#### s3‐Region<a name="s3-dash-region"></a>

Some older Amazon S3 Regions support endpoints that contains a dash between S3 and the Region \(for example, `S3‐us-west-2`\), instead of a dot \(for example, `S3.us-west-2`\)\. If your bucket is in one of these Regions, you might see the following endpoint format in your server access logs or CloudTrail logs:

```
https://bucket-name.s3-Region.amazonaws.com
```

In this example, the bucket name is `my-bucket` and the Region is US West \(Oregon\):

```
https://my-bucket.s3-us-west-2.amazonaws.com
```

#### Legacy Global Endpoint<a name="deprecated-global-endpoint"></a>

For some Regions, the legacy global endpoint can be used to construct requests that do not specify a Region\-specific endpoint\. The legacy global endpoint point is as follows:

```
bucket-name.s3.amazonaws.com
```

In your server access logs or CloudTrail logs, you might see requests that use the legacy global endpoint\. In this example, the bucket name is `my-bucket` and the legacy global endpoint is shown: 

```
https://my-bucket.s3.amazonaws.com
```

**Virtual Hosted\-Style Requests for US East \(N\. Virginia\)**  
Requests made with the legacy global endpoint go to US East \(N\. Virginia\) by default\. Therefore, the legacy global endpoint is sometimes used in place of the Regional endpoint for US East \(N\. Virginia\)\. If you create a bucket in US East \(N\. Virginia\) and use the global endpoint, Amazon S3 routes your request to this Region by default\. 

**Virtual Hosted\-Style Requests for Other Regions**  
The legacy global endpoint is also used for virtual hosted\-style requests in other supported Regions\. If you create a bucket in a Region that was launched before March 20, 2019 and use the legacy global endpoint, Amazon S3 updates the DNS to reroute the request to the correct location, which might take time\. In the meantime, the default rule applies, and your virtual hosted–style request goes to the US East \(N\. Virginia\) Region\. Amazon S3 then redirects it with an HTTP 307 redirect to the correct Region\. For S3 buckets in Regions launched after March 20, 2019, the DNS doesn't route your request directly to the AWS Region where your bucket resides\. It returns an HTTP 400 Bad Request error instead\. For more information, see [Request redirection and the REST API](Redirects.md)\.

**Path Style Requests**  
For the US East \(N\. Virginia\) Region, the legacy global endpoint can be used for path\-style requests\. 

For all other Regions, the path\-style syntax requires that you use the Region\-specific endpoint when attempting to access a bucket\. If you try to access a bucket with the legacy global endpoint or another endpoint that is different than the one for the Region where the bucket resides, you will receive an HTTP response code 307 Temporary Redirect error and a message indicating the correct URI for your resource\. For example, if you use `https://s3.amazonaws.com/bucket-name` for a bucket that was created in the US West \(Oregon\) Region, you will receive an HTTP 307 Temporary Redirect error\.


# Granting permissions for Amazon S3 Batch Operations<a name="batch-ops-iam-role-policies"></a>

This section describes how to grant the necessary permissions required for creating and performing S3 Batch Operations jobs\.

**Topics**
+ [Required permissions for creating an S3 Batch Operations job](#batch-ops-job-required-permissions)
+ [Creating an S3 Batch Operations IAM role](#batch-ops-iam-role-policies-create)

## Required permissions for creating an S3 Batch Operations job<a name="batch-ops-job-required-permissions"></a>

To create an Amazon S3 Batch Operations job, the `s3:CreateJob` permission is required\. The same entity creating the job must also have the `iam:PassRole` permission to pass the AWS Identity and Access Management \(IAM\) role specified for the job to Amazon S3 Batch Operations\. For information about creating this IAM role, see the next topic [Creating an S3 Batch Operations IAM role](#batch-ops-iam-role-policies-create)\.

## Creating an S3 Batch Operations IAM role<a name="batch-ops-iam-role-policies-create"></a>

Amazon S3 must have your permissions to perform S3 Batch Operations on your behalf\. You grant these permissions through an AWS Identity and Access Management \(IAM\) role\. This section provides examples of the trust and permissions policies you use when creating an IAM role\. For more information, see [IAM Roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html)\. For examples, see [Example: Using job tags to control permissions for S3 Batch Operations](batch-ops-job-tags-examples.md) and [Example: Copying objects across AWS accounts using S3 Batch Operations](batch-ops-examples-xcopy.md)\.

In your IAM policies, you can also use condition keys to filter access permissions for S3 Batch Operations jobs\. For more information and a complete list of Amazon S3‐specific condition keys, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\.

The following video shows how to set up IAM permissions for Batch Operations jobs using the AWS Management Console\.

[![AWS Videos](http://img.youtube.com/vi/https://www.youtube.com/embed/GrxlP39ye20//0.jpg)](http://www.youtube.com/watch?v=https://www.youtube.com/embed/GrxlP39ye20/)

### Trust policy<a name="batch-ops-iam-role-policies-trust"></a>

To allow the S3 Batch Operations service principal to assume the IAM role, attach the following trust policy to the role\.

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Effect":"Allow",
         "Principal":{
            "Service":"batchoperations.s3.amazonaws.com"
         },
         "Action":"sts:AssumeRole"
      }
   ]
}
```

### Permissions policies<a name="batch-ops-iam-role-policies-perm"></a>

Depending on the type of operations, you can attach one of the following policies\.

**Note**  
Regardless of the operation, Amazon S3 needs permissions to read your manifest object from your S3 bucket and optionally write a report to your bucket\. Therefore, all of the following policies include these permissions\.
For Amazon S3 inventory report manifests, S3 Batch Operations require permission to read the manifest\.json object and all associated CSV data files\.
Version\-specific permissions such as `s3:GetObjectVersion` are only required when you are specifying the version ID of the objects\.
If you are running S3 Batch Operations on encrypted objects, the IAM role must also have access to the AWS KMS keys used to encrypt them\.
+ **PUT copy object**

  ```
  {
      "Version": "2012-10-17",
      "Statement": [
          {
              "Action": [
                  "s3:PutObject",
                  "s3:PutObjectAcl",
                  "s3:PutObjectTagging"
              ],
              "Effect": "Allow",
              "Resource": "arn:aws:s3:::{{DestinationBucket}}/*"
          },
          {
              "Action": [
                  "s3:GetObject",
                  "s3:GetObjectAcl",
                  "s3:GetObjectTagging"
              ],
              "Effect": "Allow",
              "Resource": "arn:aws:s3:::{{SourceBucket}}/*"
          },
          {
              "Effect": "Allow",
              "Action": [
                  "s3:GetObject",
                  "s3:GetObjectVersion",
                  "s3:GetBucketLocation"
              ],
              "Resource": [
                  "arn:aws:s3:::{{ManifestBucket}}/*"
              ]
          },
          {
              "Effect": "Allow",
              "Action": [
                  "s3:PutObject",
                  "s3:GetBucketLocation"
              ],
              "Resource": [
                  "arn:aws:s3:::{{ReportBucket}}/*"
              ]
          }
      ]
  }
  ```
+ **PUT object tagging**

  ```
  {
    "Version":"2012-10-17",
    "Statement":[
      {
        "Effect":"Allow",
        "Action":[
          "s3:PutObjectTagging",
          "s3:PutObjectVersionTagging"
        ],
        "Resource": "arn:aws:s3:::{{TargetResource}}/*"
      },
      {
        "Effect": "Allow",
        "Action": [
          "s3:GetObject",
          "s3:GetObjectVersion",
          "s3:GetBucketLocation"
        ],
        "Resource": [
          "arn:aws:s3:::{{ManifestBucket}}/*"
        ]
      },
      {
        "Effect":"Allow",
        "Action":[
          "s3:PutObject",
          "s3:GetBucketLocation"
        ],
        "Resource":[
          "arn:aws:s3:::{{ReportBucket}}/*"
        ]
      }
    ]
  }
  ```
+ **PUT object ACL**

  ```
  {
    "Version":"2012-10-17",
    "Statement":[
      {
        "Effect":"Allow",
        "Action":[
          "s3:PutObjectAcl",
          "s3:PutObjectVersionAcl"
        ],
        "Resource": "arn:aws:s3:::{{TargetResource}}/*"
      },
      {
        "Effect": "Allow",
        "Action": [
          "s3:GetObject",
          "s3:GetObjectVersion",
          "s3:GetBucketLocation"
        ],
        "Resource": [
          "arn:aws:s3:::{{ManifestBucket}}/*"
        ]
      },
      {
        "Effect":"Allow",
        "Action":[
          "s3:PutObject",
          "s3:GetBucketLocation"
        ],
        "Resource":[
          "arn:aws:s3:::{{ReportBucket}}/*"
        ]
      }
    ]
  }
  ```
+ **Initiate S3 Glacier restore**

  ```
  {
    "Version":"2012-10-17",
    "Statement":[
      {
        "Effect":"Allow",
        "Action":[
            "s3:RestoreObject"
        ],
        "Resource": "arn:aws:s3:::{{TargetResource}}/*"
      },
      {
        "Effect": "Allow",
        "Action": [
          "s3:GetObject",
          "s3:GetObjectVersion",
          "s3:GetBucketLocation"
        ],
        "Resource": [
          "arn:aws:s3:::{{ManifestBucket}}/*"
        ]
      },
      {
        "Effect":"Allow",
        "Action":[
          "s3:PutObject",
          "s3:GetBucketLocation"
        ],
        "Resource":[
          "arn:aws:s3:::{{ReportBucket}}/*"
        ]
      }
    ]
  }
  ```
+ **PUT S3 Object Lock retention**

  ```
  {
      "Version": "2012-10-17",
      "Statement": [
          {
              "Effect": "Allow",
              "Action": "s3:GetBucketObjectLockConfiguration",
              "Resource": [
                  "arn:aws:s3:::{{TargetResource}}"
              ]
          },
          {
              "Effect": "Allow",
              "Action": [
                  "s3:PutObjectRetention",
                  "s3:BypassGovernanceRetention"
              ],
              "Resource": [
                  "arn:aws:s3:::{{TargetResource}}/*"
              ]
          },
          {
              "Effect": "Allow",
              "Action": [
                  "s3:GetObject",
                  "s3:GetObjectVersion",
                  "s3:GetBucketLocation"
              ],
              "Resource": [
                  "arn:aws:s3:::{{ManifestBucket}}/*"
              ]
          },
          {
              "Effect": "Allow",
              "Action": [
                  "s3:PutObject",
                  "s3:GetBucketLocation"
              ],
              "Resource": [
                  "arn:aws:s3:::{{ReportBucket}}/*"
              ]
          }
      ]
  }
  ```
+ **PUT S3 Object Lock legal hold**

  ```
  {
      "Version": "2012-10-17",
      "Statement": [
          {
              "Effect": "Allow",
              "Action": "s3:GetBucketObjectLockConfiguration",
              "Resource": [
                  "arn:aws:s3:::{{TargetResource}}"
              ]
          },
          {
              "Effect": "Allow",
              "Action": "s3:PutObjectLegalHold",
              "Resource": [
                  "arn:aws:s3:::{{TargetResource}}/*"
              ]
          },
          {
              "Effect": "Allow",
              "Action": [
                  "s3:GetObject",
                  "s3:GetObjectVersion",
                  "s3:GetBucketLocation"
              ],
              "Resource": [
                  "arn:aws:s3:::{{ManifestBucket}}/*"
              ]
          },
          {
              "Effect": "Allow",
              "Action": [
                  "s3:PutObject",
                  "s3:GetBucketLocation"
              ],
              "Resource": [
                  "arn:aws:s3:::{{ReportBucket}}/*"
              ]
          }
      ]
  }
  ```

### Related resources<a name="batch-ops-create-job-related-resources"></a>
+ [S3 Batch Operations basics](batch-ops-basics.md)
+ [Example: Copying objects across AWS accounts](batch-ops-examples-xcopy.md)
+ [Managing S3 Batch Operations jobs](batch-ops-managing-jobs.md)


# Bucket restrictions and limitations<a name="BucketRestrictions"></a>

A bucket is owned by the AWS account that created it\. Bucket ownership is not transferable\.

When you create a bucket, you choose its name and the Region to create it in\. After you create a bucket, you can't change its name or Region\.

When naming a bucket you should choose a name that is relevant to you or your business\. Avoid using names associated with others\. For example, you should avoid using AWS or Amazon in your bucket name\.

By default, you can create up to 100 buckets in each of your AWS accounts\. If you need additional buckets, you can increase your account bucket limit to a maximum of 1,000 buckets by submitting a service limit increase\. There is no difference in performance whether you use many buckets or just a few\. For information about how to increase your bucket limit, see [AWS service quotas](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) in the *AWS General Reference*\. 

**Reusing bucket names**  
If a bucket is empty, you can delete it\. After a bucket is deleted, the name becomes available for reuse\. However, after you delete the bucket, you might not be able to reuse the name for various reasons\. For example, when you delete the bucket and the name becomes available for reuse, another account might create a bucket with that name\. Additionally, some time might pass before you can reuse the name of a deleted bucket\. If you want to use the same bucket name, we recommend that you don't delete the bucket\. 

**Objects and buckets**  
There is no limit to the number of objects that you can store in a bucket\. You can store all of your objects in a single bucket, or you can organize them across several buckets\. However, you can't create a bucket from within another bucket\.

**Bucket operations**  
The high\-availability engineering of Amazon S3 is focused on get, put, list, and delete operations\. Because bucket operations work against a centralized, global resource space, it is not appropriate to create or delete buckets on the high\-availability code path of your application\. It is better to create or delete buckets in a separate initialization or setup routine that you run less often\. 

**Bucket naming and automatically created buckets**  
If your application automatically creates buckets, choose a bucket naming scheme that is unlikely to cause naming conflicts\. Ensure that your application logic will choose a different bucket name if a bucket name is already taken\.

## Rules for bucket naming<a name="bucketnamingrules"></a>

The following rules apply for naming S3 buckets:
+ Bucket names must be between 3 and 63 characters long\.
+ Bucket names can consist only of lowercase letters, numbers, dots \(\.\), and hyphens \(\-\)\.
+ Bucket names must begin and end with a letter or number\.
+ Bucket names must not be formatted as an IP address \(for example, 192\.168\.5\.4\)\.
+ Bucket names can't begin with `xn--` \(for buckets created after February 2020\)\.
+ Bucket names must be unique within a partition\. A partition is a grouping of Regions\. AWS currently has three partitions: `aws` \(Standard Regions\), `aws-cn` \(China Regions\), and `aws-us-gov` \(AWS GovCloud \[US\] Regions\)\.
+ Buckets used with Amazon S3 Transfer Acceleration can't have dots \(\.\) in their names\. For more information about transfer acceleration, see [Amazon S3 Transfer Acceleration](transfer-acceleration.md)\.

For best compatibility, we recommend that you avoid using dots \(\.\) in bucket names, except for buckets that are used only for static website hosting\. If you include dots in a bucket's name, you can't use virtual\-host\-style addressing over HTTPS, unless you perform your own certificate validation\. This is because the security certificates used for virtual hosting of buckets don't work for buckets with dots in their names\. 

This limitation doesn't affect buckets used for static website hosting, because static website hosting is only available over HTTP\. For more information about virtual\-host\-style addressing, see [Virtual hosting of buckets](VirtualHosting.md)\. For more information about static website hosting, see [Hosting a static website using Amazon S3](WebsiteHosting.md)\.

**Note**  
Before March 1, 2018, buckets created in the US East \(N\. Virginia\) Region could have names that were up to 255 characters long and included uppercase letters and underscores\. Beginning March 1, 2018, new buckets in US East \(N\. Virginia\) must conform to the same rules applied in all other Regions\.

**Example Bucket names**  
The following example bucket names are valid and follow the recommended naming guidelines:  
+ `docexamplebucket`
+ `log-delivery-march-2020`
+ `my-hosted-content`
The following example bucket names are valid but not recommended for uses other than static website hosting:  
+ `docexamplewebsite.com`
+ `www.docexamplewebsite.com`
+ `my.example.s3.bucket`
The following example bucket names are *not* valid:  
+ `doc_example_bucket` \(contains underscores\)
+ `DocExampleBucket` \(contains uppercase letters\)
+ `doc-example-bucket-` \(ends with a hyphen\)


# Understanding object expiration<a name="lifecycle-expire-general-considerations"></a>

 When an object reaches the end of its lifetime, Amazon S3 queues it for removal and removes it asynchronously\. There might be a delay between the expiration date and the date at which Amazon S3 removes an object\. You are not charged for storage time associated with an object that has expired\. 

 To find when an object is scheduled to expire, use the [HEAD Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html) or the [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) API operations\. These API operations return response headers that provide this information\. 

If you create an S3 Lifecycle expiration rule that causes objects that have been in S3 Intelligent\-Tiering, S3 Standard\-IA, or S3 One Zone\-IA storage for less than 30 days to expire, you are charged for 30 days\. If you create a Lifecycle expiration rule that causes objects that have been in S3 Glacier storage for less than 90 days to expire, you are charged for 90 days\. If you create a Lifecycle expiration rule that causes objects that have been in S3 Glacier Deep Archive storage for less than 180 days to expire, you are charged for 180 days\. For more information, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)and [How do I create a lifecycle policy for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html) in the *Amazon Simple Storage Service Console User Guide*\.


# Managing ACLs<a name="managing-acls"></a>

 There are several ways you can add grants to your resource ACL\. You can use the AWS Management Console, which provides a UI to manage permissions without writing any code\. You can also use the AWS Command Line Interface \(CLI\), REST API, or one of the AWS SDKs\. These libraries further simplify your programming tasks\. 

**Topics**
+ [Managing ACLs in the AWS Management Console](manage-acls-using-console.md)
+ [Managing ACLs Using the AWS SDK for Java](acl-using-java-sdk.md)
+ [Managing ACLs Using the AWS SDK for \.NET](acl-using-dot-net-sdk.md)
+ [Managing ACLs Using the REST API](acl-using-rest-api.md)

For more information about managing ACLs using the AWS CLI, see [put\-bucket\-acl](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/put-bucket-acl.html) in the *AWS CLI Command Reference*\.


# Restore an archived object using the AWS SDK for \.NET<a name="restore-object-dotnet"></a>

**Example**  
The following C\# example initiates a request to restore an archived object for 2 days\. Amazon S3 maintains the restoration status in the object metadata\. After initiating the request, the example retrieves the object metadata and checks the value of the `RestoreInProgress` property\. For instructions on creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class RestoreArchivedObjectTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string objectKey = "** archived object key name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            RestoreObjectAsync(client, bucketName, objectKey).Wait();
        }

        static async Task RestoreObjectAsync(IAmazonS3 client, string bucketName, string objectKey)
        {
            try
            {
                var restoreRequest = new RestoreObjectRequest
                {
                    BucketName = bucketName,
                    Key = objectKey,
                    Days = 2
                };
                RestoreObjectResponse response = await client.RestoreObjectAsync(restoreRequest);

                // Check the status of the restoration.
                await CheckRestorationStatusAsync(client, bucketName, objectKey);
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("An AmazonS3Exception was thrown. Exception: " + amazonS3Exception.ToString());
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
            }
        }

        static async Task CheckRestorationStatusAsync(IAmazonS3 client, string bucketName, string objectKey)
        {
            GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest
            {
                BucketName = bucketName,
                Key = objectKey
            };
            GetObjectMetadataResponse response = await client.GetObjectMetadataAsync(metadataRequest);
            Console.WriteLine("restoration status: {0}", response.RestoreInProgress ? "in-progress" : "finished or failed");
        }
    }
}
```


# Listing Keys Hierarchically Using a Prefix and Delimiter<a name="ListingKeysHierarchy"></a>

The [prefix](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix) and delimiter parameters limit the kind of results returned by a list operation\. The prefix limits the results to only those keys that begin with the specified prefix\. The delimiter causes a list operation to roll up all the keys that share a common prefix into a single summary list result\. 

 The purpose of the prefix and delimiter parameters is to help you organize and then browse your keys hierarchically\. To do this, first pick a delimiter for your bucket, such as slash \(/\), that doesn't occur in any of your anticipated key names\. Next, construct your key names by concatenating all containing levels of the hierarchy, separating each level with the delimiter\. 

 For example, if you were storing information about cities, you might naturally organize them by continent, then by country, then by province or state\. Because these names don't usually contain punctuation, you might select slash \(/\) as the delimiter\. The following examples use a slash \(/\) delimiter\.
+ Europe/France/Nouvelle\-Aquitaine/Bordeaux
+ North America/Canada/Quebec/Montreal
+ North America/USA/Washington/Bellevue
+ North America/USA/Washington/Seattle

 If you stored data for every city in the world in this manner, it would become awkward to manage a flat key namespace\. By using `Prefix` and `Delimiter` with the list operation, you can use the hierarchy you've created to list your data\. For example, to list all the states in USA, set `Delimiter`='/' and `Prefix`='North America/USA/'\. To list all the provinces in Canada for which you have data, set `Delimiter`='/' and `Prefix`='North America/Canada/'\.

 A list request with a delimiter lets you browse your hierarchy at just one level, skipping over and summarizing the \(possibly millions of\) keys nested at deeper levels\. For example, assume you have a bucket \(`ExampleBucket`\) the following keys\.

`sample.jpg` 

`photos/2006/January/sample.jpg` 

`photos/2006/February/sample2.jpg` 

`photos/2006/February/sample3.jpg` 

`photos/2006/February/sample4.jpg` 

The sample bucket has only the `sample.jpg` object at the root level\. To list only the root level objects in the bucket you send a GET request on the bucket with "/" delimiter character\. In response, Amazon S3 returns the `sample.jpg` object key because it does not contain the "/" delimiter character\. All other keys contain the delimiter character\. Amazon S3 groups these keys and return a single `CommonPrefixes` element with prefix value `photos/` that is a substring from the beginning of these keys to the first occurrence of the specified delimiter\.

**Example**  

```
 1. <ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
 2.   <Name>ExampleBucket</Name>
 3.   <Prefix></Prefix>
 4.   <Marker></Marker>
 5.   <MaxKeys>1000</MaxKeys>
 6.   <Delimiter>/</Delimiter>
 7.   <IsTruncated>false</IsTruncated>
 8.   <Contents>
 9.     <Key>sample.jpg</Key>
10.     <LastModified>2011-07-24T19:39:30.000Z</LastModified>
11.     <ETag>&quot;d1a7fb5eab1c16cb4f7cf341cf188c3d&quot;</ETag>
12.     <Size>6</Size>
13.     <Owner>
14.       <ID>75cc57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a</ID>
15.       <DisplayName>displayname</DisplayName>
16.     </Owner>
17.     <StorageClass>STANDARD</StorageClass>
18.   </Contents>
19.   <CommonPrefixes>
20.     <Prefix>photos/</Prefix>
21.   </CommonPrefixes>
22. </ListBucketResult>
```


# Using the REST API for multipart upload<a name="UsingRESTAPImpUpload"></a>

The following sections in the Amazon Simple Storage Service API Reference describe the REST API for multipart upload\. 
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)
+ [Upload Part](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html)
+ [Complete Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html)
+ [Stop Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadAbort.html)
+ [List Parts](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html)
+ [List Multipart Uploads](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html)

You can use these APIs to make your own REST requests, or you can use the AWS Command Line Interface, or you can use one the SDKs we provide\. For more information about using Multipart Upload with the AWS CLI, see [Using the AWS Command Line Interface for multipart upload](UsingCLImpUpload.md)\. For more information about the SDKs, see [API support for multipart upload](sdksupportformpu.md)\.


# S3 Batch Operations basics<a name="batch-ops-basics"></a>

You can use S3 Batch Operations to perform large\-scale batch operations on Amazon S3 objects\. S3 Batch Operations can run a single operation or action on lists of Amazon S3 objects that you specify\. 

**Topics**
+ [How an S3 Batch Operations job works](#batch-ops-basics-how-it-works)
+ [Specifying a manifest](#specify-batchjob-manifest)

## How an S3 Batch Operations job works<a name="batch-ops-basics-how-it-works"></a>

A job is the basic unit of work for S3 Batch Operations\. A job contains all of the information necessary to run the specified operation on a list of objects\. To create a job, you give S3 Batch Operations a list of objects and specify the action to perform on those objects\. 

S3 Batch Operations supports the following operations:
+ [Put object copy](batch-ops-copy-object.md)
+ [Initiate restore object](batch-ops-initiate-restore-object.md)
+ [Invoke Lambda function](batch-ops-invoke-lambda.md)
+ [Put object ACL](batch-ops-put-object-acl.md)
+ [Put object tagging](batch-ops-put-object-tagging.md)
+ [Manage Object Lock retention dates](batch-ops-retention-date.md)
+ [Manage Object Lock legal hold](batch-ops-legal-hold.md)

A batch job performs a specified operation on each object that is included in its *manifest*\. A manifest lists the objects that you want a batch job to process and it is stored as an object in a bucket\. You can use a CSV\-formatted [ Amazon S3 inventory](storage-inventory.md) report as a manifest, which makes it easy to create large lists of objects located in a bucket\. You can also specify a manifest in a simple CSV format that enables you to perform batch operations on a customized list of objects contained within a single bucket\. 

After you create a job, Amazon S3 processes the list of objects in the manifest and runs the specified operation against each object\. While a job is executing, you can monitor its progress programmatically or through the Amazon S3 console\. You can also configure a job to generate a completion report when it finishes\. The completion report describes the results of each task that was performed by the job\. For more information about monitoring jobs, see [Managing S3 Batch Operations jobs](batch-ops-managing-jobs.md)\.

## Specifying a manifest<a name="specify-batchjob-manifest"></a>

 A manifest is an Amazon S3 object that lists object keys that you want Amazon S3 to act upon\. To create a manifest for a job, you specify the manifest object key, ETag, and optional version ID\. The contents of the manifest must be URL encoded\. Manifests that use server\-side encryption with customer\-provided keys \(SSE\-C\) and server\-side encryption with AWS Key Management Service \(SSE\-KMS\) customer master keys \(CMKs\) are not supported\. Your manifest must contain the bucket name, object key, and optionally, the object version for each object\. Any other fields in the manifest are not used by S3 Batch Operations\. 

You can specify a manifest in a create job request using one of the following two formats\.
+ Amazon S3 inventory report — Must be a CSV\-formatted Amazon S3 inventory report\. You must specify the `manifest.json` file that is associated with the inventory report\. For more information about inventory reports, see [ Amazon S3 inventory](storage-inventory.md)\. If the inventory report includes version IDs, S3 Batch Operations operates on the specific object versions\.
**Note**  
S3 Batch Operations supports CSV *inventory reports *that are AWS KMS\-encrypted\.
+ CSV file — Each row in the file must include the bucket name, object key, and optionally, the object version\. Object keys must be URL\-encoded, as shown in the following examples\. The manifest must either include version IDs for all objects or omit version IDs for all objects\. For more information about the CSV manifest format, see [JobManifestSpec](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_JobManifestSpec.html) in the *Amazon Simple Storage Service API Reference*\.
**Note**  
S3 Batch Operations does not support CSV *manifest files* that are AWS KMS\-encrypted\.

  The following is an example manifest in CSV format without version IDs\.

  ```
  Examplebucket,objectkey1
  Examplebucket,objectkey2
  Examplebucket,objectkey3
  Examplebucket,photos/jpgs/objectkey4
  Examplebucket,photos/jpgs/newjersey/objectkey5
  Examplebucket,object%20key%20with%20spaces
  ```

  The following is an example manifest in CSV format including version IDs\.

  ```
  Examplebucket,objectkey1,PZ9ibn9D5lP6p298B7S9_ceqx1n5EJ0p
  Examplebucket,objectkey2,YY_ouuAJByNW1LRBfFMfxMge7XQWxMBF
  Examplebucket,objectkey3,jbo9_jhdPEyB4RrmOxWS0kU0EoNrU_oI
  Examplebucket,photos/jpgs/objectkey4,6EqlikJJxLTsHsnbZbSRffn24_eh5Ny4
  Examplebucket,photos/jpgs/newjersey/objectkey5,imHf3FAiRsvBW_EHB8GOu.NHunHO1gVs
  Examplebucket,object%20key%20with%20spaces,9HkPvDaZY5MVbMhn6TMn1YTb5ArQAo3w
  ```

**Important**  
If the objects in your manifest are in a versioned bucket, you should specify the version IDs for the objects\. When you create a job, S3 Batch Operations parses the entire manifest before running the job\. However, it doesn't take a "snapshot" of the state of the bucket\.   
Because manifests can contain billions of objects, jobs might take a long time to run\. If you overwrite an object with a new version while a job is running, and you didn't specify a version ID for that object, Amazon S3 performs the operation on the latest version of the object, and not the version that existed when you created the job\. The only way to avoid this behavior is to specify version IDs for the objects that are listed in the manifest\. 


# Using the AWS Java SDK for a multipart upload \(low\-level API\)<a name="mpListPartsJavaAPI"></a>

**Topics**
+ [Upload a file](llJavaUploadFile.md)
+ [List multipart uploads](LLlistMPuploadsJava.md)
+ [Abort a multipart upload](LLAbortMPUJava.md)

The AWS SDK for Java exposes a low\-level API that closely resembles the Amazon S3 REST API for multipart uploads \(see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\. Use the low\-level API when you need to pause and resume multipart uploads, vary part sizes during the upload, or do not know the size of the upload data in advance\. When you don't have these requirements, use the high\-level API \(see [Using the AWS Java SDK for multipart upload \(high\-level API\)](usingHLmpuJava.md)\)\.


# Amazon S3 on Outposts examples using the SDK for Java<a name="S3OutpostsJavaExamples"></a>

With Amazon S3 on Outposts, you can create S3 buckets on your AWS Outposts and easily store and retrieve objects on\-premises for applications that require local data access, local data processing, and data residency\. You can use S3 on Outposts through the AWS Management Console, SDK for Java, AWS SDKs, or REST API\. For more information, see [Using Amazon S3 on Outposts](https://docs.aws.amazon.com/AmazonS3/latest/dev/S3onOutposts.html)\. 

The following examples show how you can use S3 on Outposts with the AWS SDK for Java\.

**Topics**
+ [Creating and managing Amazon S3 on Outposts bucket](S3OutpostsBucketJava.md)
+ [Working with objects using Amazon S3 on Outposts](S3OutpostsObjectJava.md)


# Track the progress of a multipart upload to an S3 Bucket using the AWS SDK for \.NET \(low\-level\)<a name="LLTrackProgressMPUNet"></a>

To track the progress of a multipart upload, use the `UploadPartRequest.StreamTransferProgress` event provided by the AWS SDK for \.NET low\-level multipart upload API\. The event occurs periodically\. It returns information such as the total number of bytes to transfer and the number of bytes transferred\. 

The following C\# example shows how to track the progress of multipart uploads\. For a complete C\# sample that includes the following code, see [Upload a file to an S3 Bucket using the AWS SDK for \.NET \(low\-level API\)](LLuploadFileDotNet.md)\.

```
UploadPartRequest uploadRequest = new UploadPartRequest
{
// Provide the request data.
};

uploadRequest.StreamTransferProgress += 
     new EventHandler<StreamTransferProgressArgs>(UploadPartProgressEventCallback);

...
public static void UploadPartProgressEventCallback(object sender, StreamTransferProgressArgs e)
{
    // Process the event. 
    Console.WriteLine("{0}/{1}", e.TransferredBytes, e.TotalBytes);
}
```

## More info<a name="LLTrackProgressMPUNet-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# S3 Batch Operations examples using the AWS SDK for Java<a name="batch-ops-examples-java"></a>

This section provides examples of how to create and manage S3 Batch Operations jobs using the AWS SDK for Java\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.

**Topics**
+ [Creating a Batch Operations job](#batch-ops-examples-java-create-job)
+ [Canceling a Batch Operations job](#batch-ops-examples-java-cancel-job)
+ [Updating the status of a Batch Operations job](#batch-ops-examples-java-update-job-status)
+ [Updating the priority of a Batch Operations job](#batch-ops-examples-java-update-job-priority.)
+ [Using Batch Operations with tags](#batch-ops-examples-java-job-with-tags.)
+ [Using S3 Batch Operations with S3 Object Lock](#batchops-examples-java-object-lock)

## Creating a Batch Operations job<a name="batch-ops-examples-java-create-job"></a>

The following example creates an S3 Batch Operations job using the AWS SDK for Java\.

For more information about creating a job, see [Creating an S3 Batch Operations job](batch-ops-create-job.md)\.

For information about setting up the permissions that you need to create a job, see [Granting permissions for Amazon S3 Batch Operations](batch-ops-iam-role-policies.md)\. 

**Example**  

```
package aws.example.s3control;



import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.*;

import java.util.UUID;
import java.util.ArrayList;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class CreateJob {
    public static void main(String[] args) {
        String accountId = "Account ID";
        String iamRoleArn = "IAM Role ARN";
        String reportBucketName = "arn:aws:s3:::bucket-where-completion-report-goes";
        String uuid = UUID.randomUUID().toString();

        ArrayList tagSet = new ArrayList<S3Tag>();
        tagSet.add(new S3Tag().withKey("keyOne").withValue("ValueOne"));


        try {
            JobOperation jobOperation = new JobOperation()
                    .withS3PutObjectTagging(new S3SetObjectTaggingOperation()
                            .withTagSet(tagSet)
                    );

            JobManifest manifest = new JobManifest()
                    .withSpec(new JobManifestSpec()
                            .withFormat("S3BatchOperations_CSV_20180820")
                            .withFields(new String[]{
                                    "Bucket", "Key"
                            }))
                    .withLocation(new JobManifestLocation()
                            .withObjectArn("arn:aws:s3:::my_manifests/manifest.csv")
                            .withETag("60e460c9d1046e73f7dde5043ac3ae85"));
            JobReport jobReport = new JobReport()
                    .withBucket(reportBucketName)
                    .withPrefix("reports")
                    .withFormat("Report_CSV_20180820")
                    .withEnabled(true)
                    .withReportScope("AllTasks");

            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            s3ControlClient.createJob(new CreateJobRequest()
                    .withAccountId(accountId)
                    .withOperation(jobOperation)
                    .withManifest(manifest)
                    .withReport(jobReport)
                    .withPriority(42)
                    .withRoleArn(iamRoleArn)
                    .withClientRequestToken(uuid)
                    .withDescription("job description")
                    .withConfirmationRequired(false)
            );

        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Canceling a Batch Operations job<a name="batch-ops-examples-java-cancel-job"></a>

The following example cancels an S3 Batch Operations job using the AWS SDK for Java\.

**Example**  

```
package aws.example.s3control;


import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.UpdateJobStatusRequest;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class CancelJob {
    public static void main(String[] args) {
        String accountId = "Account ID";
        String jobId = "00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c";

        try {
            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            s3ControlClient.updateJobStatus(new UpdateJobStatusRequest()
                    .withAccountId(accountId)
                    .withJobId(jobId)
                    .withStatusUpdateReason("No longer needed")
                    .withRequestedJobStatus("Cancelled"));

        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Updating the status of a Batch Operations job<a name="batch-ops-examples-java-update-job-status"></a>

The following example updates the status of an S3 Batch Operations job using the AWS SDK for Java\.

For more information about job status, see [Job status](batch-ops-managing-jobs.md#batch-ops-job-status)\.

**Example**  

```
package aws.example.s3control;


import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.UpdateJobStatusRequest;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class UpdateJobStatus {
    public static void main(String[] args) {
        String accountId = "Account ID";
        String jobId = "00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c";

        try {
            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            s3ControlClient.updateJobStatus(new UpdateJobStatusRequest()
                    .withAccountId(accountId)
                    .withJobId(jobId)
                    .withRequestedJobStatus("Ready"));

        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Updating the priority of a Batch Operations job<a name="batch-ops-examples-java-update-job-priority."></a>

The following example updates the priority of an S3 Batch Operations job using the AWS SDK for Java\.

For more information about job priority, see [Assigning job priority](batch-ops-managing-jobs.md#batch-ops-job-priority)\.

**Example**  

```
package aws.example.s3control;



import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.UpdateJobPriorityRequest;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class UpdateJobPriority {
    public static void main(String[] args) {
        String accountId = "Account ID";
        String jobId = "00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c";

        try {
            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            s3ControlClient.updateJobPriority(new UpdateJobPriorityRequest()
                    .withAccountId(accountId)
                    .withJobId(jobId)
                    .withPriority(98));

        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Using Batch Operations with tags<a name="batch-ops-examples-java-job-with-tags."></a>

You can label and control access to your S3 Batch Operations jobs by adding *tags*\. Tags can be used to identify who is responsible for a Batch Operations job\. You can create jobs with tags attached to them, and you can add tags to jobs after they are created\. For more information, see [Controlling access and labeling jobs using tags](batch-ops-managing-jobs.md#batch-ops-job-tags)\.

**Topics**
+ [Create a Batch Operations job with job tags used for labeling](#batch-ops-examples-java-job-with-tags-create)
+ [Delete the job tags of a Batch Operations job](#batch-ops-examples-java-job-with-tags-delete)
+ [Get the job tags of a Batch Operations job](#batch-ops-examples-java-job-with-tags-get)
+ [Put job tags in a Batch Operations job](#batch-ops-examples-java-job-with-tags-put)

### Create a Batch Operations job with job tags used for labeling<a name="batch-ops-examples-java-job-with-tags-create"></a>

**Example**  
The following example creates an S3 Batch Operations job with tags using the AWS SDK for Java\.  

```
public String createJob(final AWSS3ControlClient awss3ControlClient) {
    final String manifestObjectArn = "arn:aws:s3:::example-manifest-bucket/manifests/10_manifest.csv";
    final String manifestObjectVersionId = "example-5dc7a8bfb90808fc5d546218";

    final JobManifestLocation manifestLocation = new JobManifestLocation()
            .withObjectArn(manifestObjectArn)
            .withETag(manifestObjectVersionId);

    final JobManifestSpec manifestSpec =
            new JobManifestSpec().withFormat(JobManifestFormat.S3InventoryReport_CSV_20161130);

    final JobManifest manifestToPublicApi = new JobManifest()
            .withLocation(manifestLocation)
            .withSpec(manifestSpec);

    final String jobReportBucketArn = "arn:aws:s3:::example-report-bucket";
    final String jobReportPrefix = "example-job-reports";

    final JobReport jobReport = new JobReport()
            .withEnabled(true)
            .withReportScope(JobReportScope.AllTasks)
            .withBucket(jobReportBucketArn)
            .withPrefix(jobReportPrefix)
            .withFormat(JobReportFormat.Report_CSV_20180820);

    final String lambdaFunctionArn = "arn:aws:lambda:us-west-2:123456789012:function:example-function";

    final JobOperation jobOperation = new JobOperation()
            .withLambdaInvoke(new LambdaInvokeOperation().withFunctionArn(lambdaFunctionArn));

    final S3Tag departmentTag = new S3Tag().withKey("department").withValue("Marketing");
    final S3Tag fiscalYearTag = new S3Tag().withKey("FiscalYear").withValue("2020");

    final String roleArn = "arn:aws:iam::123456789012:role/example-batch-operations-role";
    final Boolean requiresConfirmation = true;
    final int priority = 10;

    final CreateJobRequest request = new CreateJobRequest()
            .withAccountId("123456789012")
            .withDescription("Test lambda job")
            .withManifest(manifestToPublicApi)
            .withOperation(jobOperation)
            .withPriority(priority)
            .withRoleArn(roleArn)
            .withReport(jobReport)
            .withTags(departmentTag, fiscalYearTag)
            .withConfirmationRequired(requiresConfirmation);

    final CreateJobResult result = awss3ControlClient.createJob(request);

    return result.getJobId();
}
```

### Delete the job tags of a Batch Operations job<a name="batch-ops-examples-java-job-with-tags-delete"></a>

**Example**  
The following example deletes the tags of an S3 Batch Operations job using the AWS SDK for Java\.  

```
public void deleteJobTagging(final AWSS3ControlClient awss3ControlClient,
                             final String jobId) {
    final DeleteJobTaggingRequest deleteJobTaggingRequest = new DeleteJobTaggingRequest()
            .withJobId(jobId);

    final DeleteJobTaggingResult deleteJobTaggingResult =
                awss3ControlClient.deleteJobTagging(deleteJobTaggingRequest);
}
```

### Get the job tags of a Batch Operations job<a name="batch-ops-examples-java-job-with-tags-get"></a>

**Example**  
The following example gets the tags of an S3 Batch Operations job using the AWS SDK for Java\.  

```
public List<S3Tag> getJobTagging(final AWSS3ControlClient awss3ControlClient,
                                 final String jobId) {
    final GetJobTaggingRequest getJobTaggingRequest = new GetJobTaggingRequest()
            .withJobId(jobId);

    final GetJobTaggingResult getJobTaggingResult =
            awss3ControlClient.getJobTagging(getJobTaggingRequest);

    final List<S3Tag> tags = getJobTaggingResult.getTags();

    return tags;
}
```

### Put job tags in a Batch Operations job<a name="batch-ops-examples-java-job-with-tags-put"></a>

**Example**  
The following example puts the tags of an S3 Batch Operations job using the AWS SDK for Java\.  

```
public void putJobTagging(final AWSS3ControlClient awss3ControlClient,
                          final String jobId) {
    final S3Tag departmentTag = new S3Tag().withKey("department").withValue("Marketing");
    final S3Tag fiscalYearTag = new S3Tag().withKey("FiscalYear").withValue("2020");

    final PutJobTaggingRequest putJobTaggingRequest = new PutJobTaggingRequest()
            .withJobId(jobId)
            .withTags(departmentTag, fiscalYearTag);

    final PutJobTaggingResult putJobTaggingResult = awss3ControlClient.putJobTagging(putJobTaggingRequest);
}
```

## Using S3 Batch Operations with S3 Object Lock<a name="batchops-examples-java-object-lock"></a>

You can use S3 Batch Operations with S3 Object Lock to manage retention or to enable a legal hold for many Amazon S3 objects at once\. You specify the list of target objects in your manifest and submit it to Batch Operations for completion\. For more information, see [Managing S3 Object Lock retention dates](batch-ops-retention-date.md) and [Managing S3 Object Lock legal hold](batch-ops-legal-hold.md)\. 

The following examples show how to create an IAM role with S3 Batch Operations permissions, and update the role permissions to create jobs that enable object lock using the AWS SDK for Java\. In the code, replace any variable values with those that suit your needs\. You must also have a `CSV` manifest identifying the objects for your S3 Batch Operations job\. For more information, see [Specifying a manifest](batch-ops-basics.md#specify-batchjob-manifest)\.

You perform the following steps:

1. Create an IAM role and assign S3 Batch Operations permissions to run\. This step is required for all S3 Batch Operations jobs\.

1. Set up S3 Batch Operations with S3 Object Lock to run\.

   You allow the role to do the following:

   1. Run Object Lock on the S3 bucket that contains the target objects that you want Batch Operations to run on\.

   1. Read the S3 bucket where the manifest CSV file and the objects are located\.

   1. Write the results of the S3 Batch Operations job to the reporting bucket\.

```
public void createObjectLockRole() {
    final String roleName = "bops-object-lock";

    final String trustPolicy = "{" +
            "  \"Version\": \"2012-10-17\", " +
            "  \"Statement\": [ " +
            "    { " +
            "      \"Effect\": \"Allow\", " +
            "      \"Principal\": { " +
            "        \"Service\": [" +
            "          \"batchoperations.s3.amazonaws.com\"" +
            "        ]" +
            "      }, " +
            "      \"Action\": \"sts:AssumeRole\" " +
            "    } " +
            "  ]" +
            "}";

    final String bopsPermissions = "{" +
            "    \"Version\": \"2012-10-17\"," +
            "    \"Statement\": [" +
            "        {" +
            "            \"Effect\": \"Allow\"," +
            "            \"Action\": \"s3:GetBucketObjectLockConfiguration\"," +
            "            \"Resource\": [" +
            "                \"arn:aws:s3:::ManifestBucket\"" +
            "            ]" +
            "        }," +
            "        {" +
            "            \"Effect\": \"Allow\"," +
            "            \"Action\": [" +
            "                \"s3:GetObject\"," +
            "                \"s3:GetObjectVersion\"," +
            "                \"s3:GetBucketLocation\"" +
            "            ]," +
            "            \"Resource\": [" +
            "                \"arn:aws:s3:::ManifestBucket/*\"" +
            "            ]" +
            "        }," +
            "        {" +
            "            \"Effect\": \"Allow\"," +
            "            \"Action\": [" +
            "                \"s3:PutObject\"," +
            "                \"s3:GetBucketLocation\"" +
            "            ]," +
            "            \"Resource\": [" +
            "                \"arn:aws:s3:::ReportBucket/*\"" +
            "            ]" +
            "        }" +
            "    ]" +
            "}";

    final AmazonIdentityManagement iam =
            AmazonIdentityManagementClientBuilder.defaultClient();

    final CreateRoleRequest createRoleRequest = new CreateRoleRequest()
            .withAssumeRolePolicyDocument(bopsPermissions)
            .withRoleName(roleName);

    final CreateRoleResult createRoleResult = iam.createRole(createRoleRequest);

    final PutRolePolicyRequest putRolePolicyRequest = new PutRolePolicyRequest()
            .withPolicyDocument(bopsPermissions)
            .withPolicyName("bops-permissions")
            .withRoleName(roleName);

    final PutRolePolicyResult putRolePolicyResult = iam.putRolePolicy(putRolePolicyRequest);
}
```

**Topics**
+ [Use S3 Batch Operations with S3 Object Lock retention](#batch-ops-examples-java-object-lock-retention)
+ [Use S3 Batch Operations with S3 Object Lock legal hold](#batch-ops-examples-java-object-lock-legalhold)

### Use S3 Batch Operations with S3 Object Lock retention<a name="batch-ops-examples-java-object-lock-retention"></a>

The following example allows the rule to set S3 Object Lock retention for your objects in the manifest bucket\.

 You update the role to include `s3:PutObjectRetention` permissions so you can run Object Lock retention on the objects in your bucket\.

```
public void allowPutObjectRetention() {
    final String roleName = "bops-object-lock";

    final String retentionPermissions = "{" +
            "    \"Version\": \"2012-10-17\"," +
            "    \"Statement\": [" +
            "        {" +
            "            \"Effect\": \"Allow\"," +
            "            \"Action\": [" +
            "                \"s3:PutObjectRetention\"" +
            "            ]," +
            "            \"Resource\": [" +
            "                \"arn:aws:s3:::ManifestBucket*\"" +
            "            ]" +
            "        }" +
            "    ]" +
            "}";
            
    final AmazonIdentityManagement iam =
            AmazonIdentityManagementClientBuilder.defaultClient();

    final PutRolePolicyRequest putRolePolicyRequest = new PutRolePolicyRequest()
            .withPolicyDocument(retentionPermissions)
            .withPolicyName("retention-permissions")
            .withRoleName(roleName);

    final PutRolePolicyResult putRolePolicyResult = iam.putRolePolicy(putRolePolicyRequest);
}
```

#### Use S3 Batch Operations with S3 Object Lock retention compliance mode<a name="batch-ops-examples-java-object-lock-compliance"></a>

The following example builds on the previous examples of creating a trust policy, and setting S3 Batch Operations and S3 Object Lock configuration permissions on your objects\. This example sets the retention mode to `COMPLIANCE` and the `retain until date` to January 1, 2020, and creates a job to target objects in the manifest bucket and report the results in the reports bucket that you identified\.

```
public String createComplianceRetentionJob(final AWSS3ControlClient awss3ControlClient) throws ParseException {
    final String manifestObjectArn = "arn:aws:s3:::ManifestBucket/compliance-objects-manifest.csv";
    final String manifestObjectVersionId = "your-object-version-Id";

    final JobManifestLocation manifestLocation = new JobManifestLocation()
            .withObjectArn(manifestObjectArn)
            .withETag(manifestObjectVersionId);

    final JobManifestSpec manifestSpec =
            new JobManifestSpec()
                    .withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)
                    .withFields("Bucket", "Key");

    final JobManifest manifestToPublicApi = new JobManifest()
            .withLocation(manifestLocation)
            .withSpec(manifestSpec);

    final String jobReportBucketArn = "arn:aws:s3:::ReportBucket";
    final String jobReportPrefix = "reports/compliance-objects-bops";

    final JobReport jobReport = new JobReport()
            .withEnabled(true)
            .withReportScope(JobReportScope.AllTasks)
            .withBucket(jobReportBucketArn)
            .withPrefix(jobReportPrefix)
            .withFormat(JobReportFormat.Report_CSV_20180820);

    final SimpleDateFormat format = new SimpleDateFormat("dd/MM/yyyy");
    final Date janFirst = format.parse("01/01/2020");

    final JobOperation jobOperation = new JobOperation()
            .withS3PutObjectRetention(new S3SetObjectRetentionOperation()
                    .withRetention(new S3Retention()
                            .withMode(S3ObjectLockRetentionMode.COMPLIANCE)
                            .withRetainUntilDate(janFirst)));

    final String roleArn = "arn:aws:iam::123456789012:role/bops-object-lock";
    final Boolean requiresConfirmation = true;
    final int priority = 10;

    final CreateJobRequest request = new CreateJobRequest()
            .withAccountId("123456789012")
            .withDescription("Set compliance retain-until to 1 Jan 2020")
            .withManifest(manifestToPublicApi)
            .withOperation(jobOperation)
            .withPriority(priority)
            .withRoleArn(roleArn)
            .withReport(jobReport)
            .withConfirmationRequired(requiresConfirmation);

    final CreateJobResult result = awss3ControlClient.createJob(request);

    return result.getJobId();
}
```



The following example extends the `COMPLIANCE` mode's `retain until date` to January 15, 2020\.

```
public String createExtendComplianceRetentionJob(final AWSS3ControlClient awss3ControlClient) throws ParseException {
    final String manifestObjectArn = "arn:aws:s3:::ManifestBucket/compliance-objects-manifest.csv";
    final String manifestObjectVersionId = "15ad5ba069e6bbc465c77bf83d541385";

    final JobManifestLocation manifestLocation = new JobManifestLocation()
            .withObjectArn(manifestObjectArn)
            .withETag(manifestObjectVersionId);

    final JobManifestSpec manifestSpec =
            new JobManifestSpec()
                    .withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)
                    .withFields("Bucket", "Key");

    final JobManifest manifestToPublicApi = new JobManifest()
            .withLocation(manifestLocation)
            .withSpec(manifestSpec);

    final String jobReportBucketArn = "arn:aws:s3:::ReportBucket";
    final String jobReportPrefix = "reports/compliance-objects-bops";

    final JobReport jobReport = new JobReport()
            .withEnabled(true)
            .withReportScope(JobReportScope.AllTasks)
            .withBucket(jobReportBucketArn)
            .withPrefix(jobReportPrefix)
            .withFormat(JobReportFormat.Report_CSV_20180820);

    final SimpleDateFormat format = new SimpleDateFormat("dd/MM/yyyy");
    final Date jan15th = format.parse("15/01/2020");

    final JobOperation jobOperation = new JobOperation()
            .withS3PutObjectRetention(new S3SetObjectRetentionOperation()
                    .withRetention(new S3Retention()
                            .withMode(S3ObjectLockRetentionMode.COMPLIANCE)
                            .withRetainUntilDate(jan15th)));

    final String roleArn = "arn:aws:iam::123456789012:role/bops-object-lock";
    final Boolean requiresConfirmation = true;
    final int priority = 10;

    final CreateJobRequest request = new CreateJobRequest()
            .withAccountId("123456789012")
            .withDescription("Extend compliance retention to 15 Jan 2020")
            .withManifest(manifestToPublicApi)
            .withOperation(jobOperation)
            .withPriority(priority)
            .withRoleArn(roleArn)
            .withReport(jobReport)
            .withConfirmationRequired(requiresConfirmation);

    final CreateJobResult result = awss3ControlClient.createJob(request);

    return result.getJobId();
}
```

#### Use S3 Batch Operations with S3 Object Lock retention governance mode<a name="batch-ops-examples-java-object-lock-governance"></a>

The following example builds on the previous example of creating a trust policy, and setting S3 Batch Operations and S3 Object Lock configuration permissions\. It shows how to apply S3 Object Lock retention governance with the `retain until date` set to January 30, 2020 across multiple objects\. It creates a Batch Operations job that uses the manifest bucket and reports the results in the reports bucket\.



```
public String createGovernanceRetentionJob(final AWSS3ControlClient awss3ControlClient) throws ParseException {
    final String manifestObjectArn = "arn:aws:s3:::ManifestBucket/governance-objects-manifest.csv";
    final String manifestObjectVersionId = "15ad5ba069e6bbc465c77bf83d541385";

    final JobManifestLocation manifestLocation = new JobManifestLocation()
            .withObjectArn(manifestObjectArn)
            .withETag(manifestObjectVersionId);

    final JobManifestSpec manifestSpec =
            new JobManifestSpec()
                    .withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)
                    .withFields("Bucket", "Key");

    final JobManifest manifestToPublicApi = new JobManifest()
            .withLocation(manifestLocation)
            .withSpec(manifestSpec);

    final String jobReportBucketArn = "arn:aws:s3:::ReportBucket";
    final String jobReportPrefix = "reports/governance-objects";

    final JobReport jobReport = new JobReport()
            .withEnabled(true)
            .withReportScope(JobReportScope.AllTasks)
            .withBucket(jobReportBucketArn)
            .withPrefix(jobReportPrefix)
            .withFormat(JobReportFormat.Report_CSV_20180820);

    final SimpleDateFormat format = new SimpleDateFormat("dd/MM/yyyy");
    final Date jan30th = format.parse("30/01/2020");

    final JobOperation jobOperation = new JobOperation()
            .withS3PutObjectRetention(new S3SetObjectRetentionOperation()
                    .withRetention(new S3Retention()
                            .withMode(S3ObjectLockRetentionMode.GOVERNANCE)
                            .withRetainUntilDate(jan30th)));

    final String roleArn = "arn:aws:iam::123456789012:role/bops-object-lock";
    final Boolean requiresConfirmation = true;
    final int priority = 10;

    final CreateJobRequest request = new CreateJobRequest()
            .withAccountId("123456789012")
            .withDescription("Put governance retention")
            .withManifest(manifestToPublicApi)
            .withOperation(jobOperation)
            .withPriority(priority)
            .withRoleArn(roleArn)
            .withReport(jobReport)
            .withConfirmationRequired(requiresConfirmation);

    final CreateJobResult result = awss3ControlClient.createJob(request);

    return result.getJobId();
}
```



The following example builds on the previous example of creating a trust policy, and setting S3 Batch Operations and S3 Object Lock configuration permissions\. It shows how to bypass retention governance across multiple objects and creates a Batch Operations job that uses the manifest bucket and reports the results in the reports bucket\.

```
public void allowBypassGovernance() {
    final String roleName = "bops-object-lock";

    final String bypassGovernancePermissions = "{" +
            "    \"Version\": \"2012-10-17\"," +
            "    \"Statement\": [" +
            "        {" +
            "            \"Effect\": \"Allow\"," +
            "            \"Action\": [" +
            "                \"s3:BypassGovernanceRetention\"" +
            "            ]," +
            "            \"Resource\": [" +
            "                \"arn:aws:s3:::ManifestBucket/*\"" +
            "            ]" +
            "        }" +
            "    ]" +
            "}";

    final AmazonIdentityManagement iam =
            AmazonIdentityManagementClientBuilder.defaultClient();

    final PutRolePolicyRequest putRolePolicyRequest = new PutRolePolicyRequest()
            .withPolicyDocument(bypassGovernancePermissions)
            .withPolicyName("bypass-governance-permissions")
            .withRoleName(roleName);

    final PutRolePolicyResult putRolePolicyResult = iam.putRolePolicy(putRolePolicyRequest);
} 
public String createRemoveGovernanceRetentionJob(final AWSS3ControlClient awss3ControlClient) {
    final String manifestObjectArn = "arn:aws:s3:::ManifestBucket/governance-objects-manifest.csv";
    final String manifestObjectVersionId = "15ad5ba069e6bbc465c77bf83d541385";

    final JobManifestLocation manifestLocation = new JobManifestLocation()
            .withObjectArn(manifestObjectArn)
            .withETag(manifestObjectVersionId);

    final JobManifestSpec manifestSpec =
            new JobManifestSpec()
                    .withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)
                    .withFields("Bucket", "Key");

    final JobManifest manifestToPublicApi = new JobManifest()
            .withLocation(manifestLocation)
            .withSpec(manifestSpec);

    final String jobReportBucketArn = "arn:aws:s3:::ReportBucket";
    final String jobReportPrefix = "reports/bops-governance";

    final JobReport jobReport = new JobReport()
            .withEnabled(true)
            .withReportScope(JobReportScope.AllTasks)
            .withBucket(jobReportBucketArn)
            .withPrefix(jobReportPrefix)
            .withFormat(JobReportFormat.Report_CSV_20180820);

    final JobOperation jobOperation = new JobOperation()
            .withS3PutObjectRetention(new S3SetObjectRetentionOperation()
                    .withRetention(new S3Retention()));

    final String roleArn = "arn:aws:iam::123456789012:role/bops-object-lock";
    final Boolean requiresConfirmation = true;
    final int priority = 10;

    final CreateJobRequest request = new CreateJobRequest()
            .withAccountId("123456789012")
            .withDescription("Remove governance retention")
            .withManifest(manifestToPublicApi)
            .withOperation(jobOperation)
            .withPriority(priority)
            .withRoleArn(roleArn)
            .withReport(jobReport)
            .withConfirmationRequired(requiresConfirmation);

    final CreateJobResult result = awss3ControlClient.createJob(request);

    return result.getJobId();
}
```

### Use S3 Batch Operations with S3 Object Lock legal hold<a name="batch-ops-examples-java-object-lock-legalhold"></a>

The following example builds on the previous examples of creating a trust policy, and setting S3 Batch Operations and S3 Object Lock configuration permissions\. It shows how to disable Object Lock legal hold on objects using Batch Operations\. 

This example first updates the role allow `s3:PutObjectLegalHold` permissions and then creates a Batch Operations job that turns off \(removes\) legal hold from the objects identified in the manifest and reports on it\.

```
public void allowPutObjectLegalHold() {
    final String roleName = "bops-object-lock";

    final String legalHoldPermissions = "{" +
            "    \"Version\": \"2012-10-17\"," +
            "    \"Statement\": [" +
            "        {" +
            "            \"Effect\": \"Allow\"," +
            "            \"Action\": [" +
            "                \"s3:PutObjectLegalHold\"" +
            "            ]," +
            "            \"Resource\": [" +
            "                \"arn:aws:s3:::ManifestBucket/*\"" +
            "            ]" +
            "        }" +
            "    ]" +
            "}";

    final AmazonIdentityManagement iam =
            AmazonIdentityManagementClientBuilder.defaultClient();

    final PutRolePolicyRequest putRolePolicyRequest = new PutRolePolicyRequest()
            .withPolicyDocument(legalHoldPermissions)
            .withPolicyName("legal-hold-permissions")
            .withRoleName(roleName);

    final PutRolePolicyResult putRolePolicyResult = iam.putRolePolicy(putRolePolicyRequest);
}
```

Use the example below if you want to turn off legal hold\.

```
public String createLegalHoldOffJob(final AWSS3ControlClient awss3ControlClient) {
    final String manifestObjectArn = "arn:aws:s3:::ManifestBucket/legalhold-object-manifest.csv";
    final String manifestObjectVersionId = "15ad5ba069e6bbc465c77bf83d541385";

    final JobManifestLocation manifestLocation = new JobManifestLocation()
            .withObjectArn(manifestObjectArn)
            .withETag(manifestObjectVersionId);

    final JobManifestSpec manifestSpec =
            new JobManifestSpec()
                    .withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)
                    .withFields("Bucket", "Key");

    final JobManifest manifestToPublicApi = new JobManifest()
            .withLocation(manifestLocation)
            .withSpec(manifestSpec);

    final String jobReportBucketArn = "arn:aws:s3:::ReportBucket";
    final String jobReportPrefix = "reports/legalhold-objects-bops";

    final JobReport jobReport = new JobReport()
            .withEnabled(true)
            .withReportScope(JobReportScope.AllTasks)
            .withBucket(jobReportBucketArn)
            .withPrefix(jobReportPrefix)
            .withFormat(JobReportFormat.Report_CSV_20180820);

    final JobOperation jobOperation = new JobOperation()
            .withS3PutObjectLegalHold(new S3SetObjectLegalHoldOperation()
                    .withLegalHold(new S3ObjectLockLegalHold()
                            .withStatus(S3ObjectLockLegalHoldStatus.OFF)));

    final String roleArn = "arn:aws:iam::123456789012:role/bops-object-lock";
    final Boolean requiresConfirmation = true;
    final int priority = 10;

    final CreateJobRequest request = new CreateJobRequest()
            .withAccountId("123456789012")
            .withDescription("Turn off legal hold")
            .withManifest(manifestToPublicApi)
            .withOperation(jobOperation)
            .withPriority(priority)
            .withRoleArn(roleArn)
            .withReport(jobReport)
            .withConfirmationRequired(requiresConfirmation);

    final CreateJobResult result = awss3ControlClient.createJob(request);

    return result.getJobId();
}
```


# Amazon S3 inventory<a name="storage-inventory"></a>

Amazon S3 inventory is one of the tools Amazon S3 provides to help manage your storage\. You can use it to audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs\. You can also simplify and speed up business workflows and big data jobs using Amazon S3 inventory, which provides a scheduled alternative to the Amazon S3 synchronous `List` API operation\.

Amazon S3 inventory provides comma\-separated values \(CSV\), [Apache optimized row columnar \(ORC\)](https://orc.apache.org/) or [Apache Parquet \(Parquet\)](https://parquet.apache.org/) output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or a shared prefix \(that is, objects that have names that begin with a common string\)\. If weekly, a report is generated every Sunday \(UTC timezone\) after the initial report\. For information about Amazon S3 inventory pricing, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\.

You can configure multiple inventory lists for a bucket\. You can configure what object metadata to include in the inventory, whether to list all object versions or only current versions, where to store the inventory list file output, and whether to generate the inventory on a daily or weekly basis\. You can also specify that the inventory list file be encrypted\.

You can query Amazon S3 inventory using standard SQL by using [Amazon Athena](https://docs.aws.amazon.com/athena/latest/ug/what-is.html), Amazon Redshift Spectrum, and other tools such as [Presto](https://prestodb.io/), [Apache Hive](https://hive.apache.org/), and [Apache Spark](https://databricks.com/spark/about/)\. It's easy to use Athena to run queries on your inventory files\. You can use Athena for Amazon S3 inventory queries in all Regions where Athena is available\. 

**Topics**
+ [How do I set up Amazon S3 inventory?](#storage-inventory-how-to-set-up)
+ [What's included in an Amazon S3 inventory?](#storage-inventory-contents)
+ [Where are inventory lists located?](#storage-inventory-location)
+ [How do I know when an inventory is complete?](#storage-inventory-notification)
+ [Querying inventory with Amazon Athena](#storage-inventory-athena-query)
+ [Amazon S3 inventory REST APIs](#storage-inventory-related-resources)

## How do I set up Amazon S3 inventory?<a name="storage-inventory-how-to-set-up"></a>

This section describes how to set up an inventory, including details about the inventory source and destination buckets\.

### Amazon S3 inventory source and destination buckets<a name="storage-inventory-buckets"></a>

The bucket that the inventory lists the objects for is called the *source bucket*\. The bucket where the inventory list file is stored is called the *destination bucket*\. 

**Source Bucket**

The inventory lists the objects that are stored in the source bucket\. You can get inventory lists for an entire bucket or filtered by \(object key name\) prefix\.

The source bucket:
+ Contains the objects that are listed in the inventory\.
+ Contains the configuration for the inventory\.

**Destination Bucket**

Amazon S3 inventory list files are written to the destination bucket\. To group all the inventory list files in a common location in the destination bucket, you can specify a destination \(object key name\) prefix in the inventory configuration\.

The destination bucket:
+ Contains the inventory file lists\. 
+ Contains the manifest files that list all the file inventory lists that are stored in the destination bucket\. For more information, see [What is an inventory manifest?](#storage-inventory-location-manifest)
+ Must have a bucket policy to give Amazon S3 permission to verify ownership of the bucket and permission to write files to the bucket\. 
+ Must be in the same AWS Region as the source bucket\.
+ Can be the same as the source bucket\.
+ Can be owned by a different AWS account than the account that owns the source bucket\.

### Setting up Amazon S3 inventory<a name="storage-inventory-setting-up"></a>

Amazon S3 inventory helps you manage your storage by creating lists of the objects in an S3 bucket on a defined schedule\. You can configure multiple inventory lists for a bucket\. The inventory lists are published to CSV, ORC, or Parquet files in a destination bucket\. 

The easiest way to set up an inventory is by using the AWS Management Console, but you can also use the REST API, AWS CLI, or AWS SDKs\. The console performs the first step of the following procedure for you: adding a bucket policy to the destination bucket\.

**To set up Amazon S3 inventory for an S3 bucket**

1. **Add a bucket policy for the destination bucket\.**

   You must create a bucket policy on the destination bucket to grant permissions to Amazon S3 to write objects to the bucket in the defined location\. For an example policy, see [Granting Permissions for Amazon S3 Inventory and Amazon S3 Analytics](example-bucket-policies.md#example-bucket-policies-use-case-9)\. 

1. **Configure an inventory to list the objects in a source bucket and publish the list to a destination bucket\.**

   When you configure an inventory list for a source bucket, you specify the destination bucket where you want the list to be stored, and whether you want to generate the list daily or weekly\. You can also configure what object metadata to include and whether to list all object versions or only current versions\. 

   You can specify that the inventory list file be encrypted by using an Amazon S3 managed key \(SSE\-S3\) or an AWS Key Management Service \(AWS KMS\) customer managed customer master key \(CMK\)\. For more information about SSE\-S3 and SSE\-KMS, see [Protecting data using server\-side encryption](serv-side-encryption.md)\. If you plan to use SSE\-KMS encryption, see Step 3\.
   + For information about how to use the console to configure an inventory list, see [How Do I Configure Amazon S3 Inventory?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-inventory.html) in the *Amazon Simple Storage Service Console User Guide*\.
   + To use the Amazon S3 API to configure an inventory list, use the [PUT Bucket inventory configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTInventoryConfig.html) REST API or the equivalent from the AWS CLI or AWS SDKs\. 

1. **To encrypt the inventory list file with SSE\-KMS, grant Amazon S3 permission to use the CMK stored in AWS KMS\.**

   You can configure encryption for the inventory list file by using the AWS Management Console, REST API, AWS CLI, or AWS SDKs\. Whichever way you choose, you must grant Amazon S3 permission to use the AWS KMS customer managed CMK to encrypt the inventory file\. You grant Amazon S3 permission by modifying the key policy for the customer managed CMK that you want to use to encrypt the inventory file\. For more information, see the next section, [Granting Amazon S3 permission to use your AWS KMS CMK for encryption for your inventory file](#storage-inventory-kms-key-policy)\.

#### Granting Amazon S3 permission to use your AWS KMS CMK for encryption for your inventory file<a name="storage-inventory-kms-key-policy"></a>

To grant Amazon S3 permission to encrypt using a customer managed AWS Key Management Service \(AWS KMS\) customer master key \(CMK\), you must use a key policy\. To update your key policy so that you can use an AWS KMS customer managed CMK to encrypt the inventory file, follow these steps\.

**To grant permissions to encrypt using your AWS KMS CMK**

1. Using the AWS account that owns the customer managed CMK, sign into the AWS Management Console\.

1. Open the AWS KMS console at [https://console\.aws\.amazon\.com/kms](https://console.aws.amazon.com/kms)\.

1. To change the AWS Region, use the Region selector in the upper\-right corner of the page\.

1. In the left navigation pane, choose **Customer managed keys**\.

1. Under **Customer managed keys**, choose the key that you want to use to encrypt the inventory file\. CMKs are Region specific and must be in the same Region as the source bucket\.

1. Under **Key policy**, choose **Switch to policy view**\.

1. To update the key policy, choose **Edit**\.

1. Under **Edit key policy**, add the following key policy to the existing key policy\.

   ```
   {
       "Sid": "Allow Amazon S3 use of the CMK",
       "Effect": "Allow",
       "Principal": {
           "Service": "s3.amazonaws.com"
       },
       "Action": [
           "kms:GenerateDataKey"
       ],
       "Resource": "*"
   }
   ```

1. Choose **Save changes**\.

   For more information about creating AWS KMS customer managed CMKs and using key policies, see the following topics in the *AWS Key Management Service Developer Guide*:
   + [Getting Started](https://docs.aws.amazon.com/kms/latest/developerguide/getting-started.html)
   + [Using Key Policies in AWS KMS](https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html)

   You can also use the AWS KMS PUT key policy API [PutKeyPolicy](https://docs.aws.amazon.com/kms/latest/APIReference/API_PutKeyPolicy.html) to copy the key policy to the customer managed CMK that you want to use to encrypt the inventory file\. 

## What's included in an Amazon S3 inventory?<a name="storage-inventory-contents"></a>

An inventory list file contains a list of the objects in the source bucket and metadata for each object\. The inventory lists are stored in the destination bucket as a CSV file compressed with GZIP, as an Apache optimized row columnar \(ORC\) file compressed with ZLIB, or as an Apache Parquet \(Parquet\) file compressed with Snappy\. 

The inventory list contains a list of the objects in an S3 bucket and the following metadata for each listed object: 
+ **Bucket name** – The name of the bucket that the inventory is for\.
+ **Key name** – Object key name \(or key\) that uniquely identifies the object in the bucket\. When using the CSV file format, the key name is URL\-encoded and must be decoded before you can use it\.
+ **Version ID** – Object version ID\. When you enable versioning on a bucket, Amazon S3 assigns a version number to objects that are added to the bucket\. For more information, see [Object Versioning](ObjectVersioning.md)\. \(This field is not included if the list is only for the current version of objects\.\)
+ **IsLatest** – Set to `True` if the object is the current version of the object\. \(This field is not included if the list is only for the current version of objects\.\)
+ **Size** – Object size in bytes\.
+ **Last modified date** – Object creation date or the last modified date, whichever is the latest\.
+ **ETag** – The entity tag is a hash of the object\. The ETag reflects changes only to the contents of an object, not its metadata\. The ETag may or may not be an MD5 digest of the object data\. Whether it is depends on how the object was created and how it is encrypted\.
+ **Storage class** – Storage class used for storing the object\. For more information, see [Amazon S3 storage classes](storage-class-intro.md)\.
+ **Intelligent\-Tiering access tier** – Access tier \(frequent or infrequent\) of the object if stored in Intelligent\-Tiering\. For more information, see [Amazon S3 Intelligent\-Tiering](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-dynamic-data-access)\.
+ **Multipart upload flag** – Set to `True` if the object was uploaded as a multipart upload\. For more information, see [Multipart upload overview](mpuoverview.md)\.
+ **Delete marker** – Set to `True`, if the object is a delete marker\. For more information, see [Object Versioning](ObjectVersioning.md)\. \(This field is automatically added to your report if you've configured the report to include all versions of your objects\)\.
+ **Replication status** – Set to `PENDING`, `COMPLETED`, `FAILED`, or `REPLICA.` For more information, see [Replication status information](replication-status.md)\.
+ **Encryption status** – Set to `SSE-S3`, `SSE-C`, `SSE-KMS`, or `NOT-SSE`\. The server\-side encryption status for SSE\-S3, SSE\-KMS, and SSE with customer\-provided keys \(SSE\-C\)\. A status of `NOT-SSE` means that the object is not encrypted with server\-side encryption\. For more information, see [Protecting data using encryption](UsingEncryption.md)\.
+ **S3 Object Lock Retain until date** – The date until which the locked object cannot be deleted\. For more information, see [Locking objects using S3 Object Lock](object-lock.md)\.
+ **S3 Object Lock Mode** – Set to `Governance` or `Compliance` for objects that are locked\. For more information, see [Locking objects using S3 Object Lock](object-lock.md)\.
+ **S3 Object Lock Legal hold status ** – Set to `On` if a legal hold has been applied to an object; otherwise it is set to `Off`\. For more information, see [Locking objects using S3 Object Lock](object-lock.md)\.

We recommend that you create a lifecycle policy that deletes old inventory lists\. For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

### Inventory consistency<a name="storage-inventory-contents-consistency"></a>

All of your objects might not appear in each inventory list\. The inventory list provides eventual consistency for PUTs of both new objects and overwrites, and DELETEs\. Inventory lists are a rolling snapshot of bucket items, which are eventually consistent \(that is, the list might not include recently added or deleted objects\)\. 

To validate the state of the object before you take action on the object, we recommend that you perform a `HEAD Object` REST API request to retrieve metadata for the object, or check the object's properties in the Amazon S3 console\. You can also check object metadata with the AWS CLI or the AWS SDKS\. For more information, see [HEAD Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html) in the *Amazon Simple Storage Service API Reference*\.

## Where are inventory lists located?<a name="storage-inventory-location"></a>

When an inventory list is published, the manifest files are published to the following location in the destination bucket\.

```
 destination-prefix/source-bucket/config-ID/YYYY-MM-DDTHH-MMZ/manifest.json
 destination-prefix/source-bucket/config-ID/YYYY-MM-DDTHH-MMZ/manifest.checksum
 destination-prefix/source-bucket/config-ID/hive/dt=YYYY-MM-DD-HH-MM/symlink.txt
```
+ *destination\-prefix* is the \(object key name\) prefix set in the inventory configuration, which can be used to group all the inventory list files in a common location within the destination bucket\.
+ *source\-bucket* is the source bucket that the inventory list is for\. It is added to prevent collisions when multiple inventory reports from different source buckets are sent to the same destination bucket\.
+ *config\-ID* is added to prevent collisions with multiple inventory reports from the same source bucket that are sent to the same destination bucket\. The *config\-ID* comes from the inventory report configuration, and is the name for the report that is defined on setup\.
+ *YYYY\-MM\-DDTHH\-MMZ* is the timestamp that consists of the start time and the date when the inventory report generation begins scanning the bucket; for example, `2016-11-06T21-32Z`\.
+ `manifest.json` is the manifest file\. 
+ `manifest.checksum` is the MD5 of the content of the `manifest.json` file\. 
+ `symlink.txt` is the Apache Hive\-compatible manifest file\. 

The inventory lists are published daily or weekly to the following location in the destination bucket\.

```
      destination-prefix/source-bucket/config-ID/example-file-name.csv.gz
      ...
      destination-prefix/source-bucket/config-ID/example-file-name-1.csv.gz
```
+ *destination\-prefix* is the \(object key name\) prefix set in the inventory configuration\. It can be used to group all the inventory list files in a common location in the destination bucket\.
+ *source\-bucket* is the source bucket that the inventory list is for\. It is added to prevent collisions when multiple inventory reports from different source buckets are sent to the same destination bucket\.
+ *example\-file\-name*`.csv.gz` is one of the CSV inventory files\. ORC inventory names end with the file name extension `.orc`, and Parquet inventory names end with the file name extension `.parquet`\.

### What is an inventory manifest?<a name="storage-inventory-location-manifest"></a>

The manifest files `manifest.json` and `symlink.txt` describe where the inventory files are located\. Whenever a new inventory list is delivered, it is accompanied by a new set of manifest files\. These files may overwrite each other and in versioning enabled buckets will create a new versions of the manifest files\. 

Each manifest contained in the `manifest.json` file provides metadata and other basic information about an inventory\. This information includes the following:
+ Source bucket name
+ Destination bucket name
+ Version of the inventory
+ Creation timestamp in the epoch date format that consists of the start time and the date when the inventory report generation begins scanning the bucket
+ Format and schema of the inventory files
+ Actual list of the inventory files that are in the destination bucket

Whenever a `manifest.json` file is written, it is accompanied by a `manifest.checksum` file that is the MD5 of the content of `manifest.json` file\.

The following is an example of a manifest in a `manifest.json` file for a CSV\-formatted inventory\.

```
{
    "sourceBucket": "example-source-bucket",
    "destinationBucket": "arn:aws:s3:::example-inventory-destination-bucket",
    "version": "2016-11-30",
    "creationTimestamp" : "1514944800000",
    "fileFormat": "CSV",
    "fileSchema": "Bucket, Key, VersionId, IsLatest, IsDeleteMarker, Size, LastModifiedDate, ETag, StorageClass, IsMultipartUploaded, ReplicationStatus, EncryptionStatus, ObjectLockRetainUntilDate, ObjectLockMode, ObjectLockLegalHoldStatus",
    "files": [
        {
            "key": "Inventory/example-source-bucket/2016-11-06T21-32Z/files/939c6d46-85a9-4ba8-87bd-9db705a579ce.csv.gz",
            "size": 2147483647,
            "MD5checksum": "f11166069f1990abeb9c97ace9cdfabc"
        }
    ]
}
```

The following is an example of a manifest in a `manifest.json` file for an ORC\-formatted inventory\.

```
{
    "sourceBucket": "example-source-bucket",
    "destinationBucket": "arn:aws:s3:::example-destination-bucket",
    "version": "2016-11-30",
    "creationTimestamp" : "1514944800000",
    "fileFormat": "ORC",
    "fileSchema": "struct<bucket:string,key:string,version_id:string,is_latest:boolean,is_delete_marker:boolean,size:bigint,last_modified_date:timestamp,e_tag:string,storage_class:string,is_multipart_uploaded:boolean,replication_status:string,encryption_status:string,object_lock_retain_until_date:timestamp,object_lock_mode:string,object_lock_legal_hold_status:string>",
    "files": [
        {
            "key": "inventory/example-source-bucket/data/d794c570-95bb-4271-9128-26023c8b4900.orc",
            "size": 56291,
            "MD5checksum": "5925f4e78e1695c2d020b9f6eexample"
        }
    ]
}
```

The following is an example of a manifest in a `manifest.json` file for a Parquet\-formatted inventory\.

```
{
    "sourceBucket": "example-source-bucket",
    "destinationBucket": "arn:aws:s3:::example-destination-bucket",
    "version": "2016-11-30",
    "creationTimestamp" : "1514944800000",
    "fileFormat": "Parquet",
    "fileSchema": "message s3.inventory { required binary bucket (UTF8); required binary key (UTF8); optional binary version_id (UTF8); optional boolean is_latest; optional boolean is_delete_marker;  optional int64 size;  optional int64 last_modified_date (TIMESTAMP_MILLIS);  optional binary e_tag (UTF8);  optional binary storage_class (UTF8);  optional boolean is_multipart_uploaded;  optional binary replication_status (UTF8);  optional binary encryption_status (UTF8);}"
  "files": [
        {
           "key": "inventory/example-source-bucket/data/d754c470-85bb-4255-9218-47023c8b4910.parquet",
            "size": 56291,
            "MD5checksum": "5825f2e18e1695c2d030b9f6eexample" 
        }
    ]
}
```

The `symlink.txt` file is an Apache Hive\-compatible manifest file that allows Hive to automatically discover inventory files and their associated data files\. The Hive\-compatible manifest works with the Hive\-compatible services Athena and Amazon Redshift Spectrum\. It also works with Hive\-compatible applications, including [Presto](https://prestodb.io/), [Apache Hive](https://hive.apache.org/), [Apache Spark](https://databricks.com/spark/about/), and many others\.

**Important**  
The `symlink.txt` Apache Hive\-compatible manifest file does not currently work with AWS Glue\.  
Reading `symlink.txt` with [Apache Hive](https://hive.apache.org/) and [Apache Spark](https://databricks.com/spark/about/) is not supported for ORC and Parquet\-formatted inventory files\. 

## How do I know when an inventory is complete?<a name="storage-inventory-notification"></a>

You can set up an Amazon S3 event notification to receive notice when the manifest checksum file is created, which indicates that an inventory list has been added to the destination bucket\. The manifest is an up\-to\-date list of all the inventory lists at the destination location\.

Amazon S3 can publish events to an Amazon Simple Notification Service \(Amazon SNS\) topic, an Amazon Simple Queue Service \(Amazon SQS\) queue, or an AWS Lambda function\. For more information, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.

The following notification configuration defines that all `manifest.checksum` files newly added to the destination bucket are processed by the AWS Lambda `cloud-function-list-write`\.

```
<NotificationConfiguration>
  <QueueConfiguration>
      <Id>1</Id>
      <Filter>
          <S3Key>
              <FilterRule>
                  <Name>prefix</Name>
                  <Value>destination-prefix/source-bucket</Value>
              </FilterRule>
              <FilterRule>
                  <Name>suffix</Name>
                  <Value>checksum</Value>
              </FilterRule>
          </S3Key>
     </Filter>
     <Cloudcode>arn:aws:lambda:us-west-2:222233334444:cloud-function-list-write</Cloudcode>
     <Event>s3:ObjectCreated:*</Event>
  </QueueConfiguration>
  </NotificationConfiguration>
```

For more information, see [Using AWS Lambda with Amazon S3](https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html) in the *AWS Lambda Developer Guide*\.

## Querying inventory with Amazon Athena<a name="storage-inventory-athena-query"></a>

You can query Amazon S3 inventory using standard SQL by using Amazon Athena in all Regions where Athena is available\. To check for AWS Region availability, see the [AWS Region Table](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/)\. 

Athena can query Amazon S3 inventory files in ORC, Parquet, or CSV format\. When you use Athena to query inventory, we recommend that you use ORC\-formatted or Parquet\-formatted inventory files\. ORC and Parquet formats provide faster query performance and lower query costs\. ORC and Parquet are self\-describing type\-aware columnar file formats designed for [Apache Hadoop](http://hadoop.apache.org/)\. The columnar format lets the reader read, decompress, and process only the columns that are required for the current query\. The ORC and Parquet formats for Amazon S3 inventory are available in all AWS Regions\. To decide whether to use ORC or Parquet format really depends on your data and requirements\. For more information, see [ Updates and Data Formats in Athena](https://docs.aws.amazon.com/athena/latest/ug/handling-schema-updates-chapter.html#index-access)\.

**To get started using Athena to query Amazon S3 inventory**

1. Create an Athena table\. For information about creating a table, see [Creating Tables in Amazon Athena](https://docs.aws.amazon.com/athena/latest/ug/creating-tables.html) in the *Amazon Athena User Guide*\.

   The following sample query includes all optional fields in an ORC\-formatted inventory report\. Drop any optional field that you did not choose for your inventory so that the query corresponds to the fields chosen for your inventory\. Also, you must use your bucket name and the location\. The location points to your inventory destination path; for example, `s3://destination-prefix/source-bucket/config-ID/hive/`\.

   ```
   CREATE EXTERNAL TABLE your_table_name(
     `bucket` string,
     key string,
     version_id string,
     is_latest boolean,
     is_delete_marker boolean,
     size bigint,
     last_modified_date timestamp,
     e_tag string,
     storage_class string,
     is_multipart_uploaded boolean,
     replication_status string,
     encryption_status string,
     object_lock_retain_until_date timestamp,
     object_lock_mode string,
     object_lock_legal_hold_status string
     )
     PARTITIONED BY (dt string)
     ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
     STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'
     OUTPUTFORMAT  'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
     LOCATION 's3://destination-prefix/source-bucket/config-ID/hive/';
   ```

    When using Athena to query a Parquet\-formatted inventory report, use the following Parquet SerDe in place of the ORC SerDe in the `ROW FORMAT SERDE` statement\.

   ```
   ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
   ```

    When using Athena to query a CSV\-formatted inventory report, use the following Parquet SerDe in place of the ORC SerDe in the `ROW FORMAT SERDE` statement\.

   ```
   ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
   ```

1. To add new inventory lists to your table, use the following `MSCK REPAIR TABLE` command\.

   ```
   MSCK REPAIR TABLE your-table-name;
   ```

1. After performing the first two steps, you can run ad hoc queries on your inventory, as shown in the following examples\. 

   ```
   # Get list of latest inventory report dates available
   SELECT DISTINCT dt FROM your-table-name ORDER BY 1 DESC limit 10;
             
   # Get encryption status for a provided report date.
   SELECT encryption_status, count(*) FROM your-table-name WHERE dt = 'YYYY-MM-DD-HH-MM' GROUP BY encryption_status;
             
   # Get encryption status for report dates in the provided range.
   SELECT dt, encryption_status, count(*) FROM your-table-name 
   WHERE dt > 'YYYY-MM-DD-HH-MM' AND dt < 'YYYY-MM-DD-HH-MM' GROUP BY dt, encryption_status;
   ```

For more information about using Athena, see [Amazon Athena User Guide](https://docs.aws.amazon.com/athena/latest/ug/)\.

## Amazon S3 inventory REST APIs<a name="storage-inventory-related-resources"></a>

The following are the REST operations used for Amazon S3 inventory\.
+  [ DELETE Bucket Inventory ](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEInventoryConfiguration.html) 
+  [ GET Bucket Inventory](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETInventoryConfig.html) 
+  [ List Bucket Inventory](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketListInventoryConfigs.html) 
+  [ PUT Bucket Inventory](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTInventoryConfig.html) 


# Invoking a Lambda function from Amazon S3 batch operations<a name="batch-ops-invoke-lambda"></a>

S3 Batch Operations can invoke AWS Lambda functions to perform custom actions on objects that are listed in a manifest\. This section describes how to create a Lambda function to use with S3 Batch Operations and how to create a job to invoke the function\. The S3 Batch Operations job uses the `LambdaInvoke` operation to run a Lambda function on each object listed in a manifest\. 

You can work with S3 Batch Operations for Lambda using the AWS Management Console, AWS Command Line Interface \(AWS CLI\), AWS SDKs, or REST APIs\. For more information about using Lambda, see [ Getting Started with AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html) in the *AWS Lambda Developer Guide*\. 

The following sections explain how you can get started using S3 Batch Operations with Lambda\. 

**Topics**
+ [Using Lambda with Amazon S3 batch operations](#batch-ops-invoke-lambda-using)
+ [Creating a Lambda function to use with S3 Batch Operations](#batch-ops-invoke-lambda-custom-functions)
+ [Creating an S3 Batch Operations job that invokes a Lambda function](#batch-ops-invoke-lambda-create-job)
+ [Providing task\-level information in Lambda manifests](#storing-task-level-information-in-lambda)

## Using Lambda with Amazon S3 batch operations<a name="batch-ops-invoke-lambda-using"></a>

When using S3 Batch Operations with AWS Lambda, you must create new Lambda functions specifically for use with S3 Batch Operations\. You can't reuse existing Amazon S3 event\-based functions with S3 Batch Operations\. Event functions can only receive messages; they don't return messages\. The Lambda functions that are used with S3 Batch Operations must accept and return messages\. For more information about using Lambda with Amazon S3 events, see [Using AWS Lambda with Amazon S3](https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html) in the *AWS Lambda Developer Guide*\.

You create an S3 Batch Operations job that invokes your Lambda function\. The job runs the same Lambda function on all of the objects listed in your manifest\. You can control what versions of your Lambda function to use while processing the objects in your manifest\. S3 Batch Operations support unqualified Amazon Resource Names \(ARNs\), aliases, and specific versions\. For more information, see [ Introduction to AWS Lambda Versioning](https://docs.aws.amazon.com/lambda/latest/dg/versioning-intro.html) in the *AWS Lambda Developer Guide*\.

If you provide the S3 Batch Operations job with a function ARN that uses an alias or the `$LATEST` qualifier, and you update the version that either of those points to, S3 Batch Operations starts calling the new version of your Lambda function\. This can be useful when you want to update functionality part of the way through a large job\. If you don't want S3 Batch Operations to change the version that is used, provide the specific version in the `FunctionARN` parameter when you create your job\.

### Response and result codes<a name="batch-ops-invoke-lambda-response-codes"></a>

There are two levels of codes that S3 Batch Operations expect from Lambda functions\. The first is the response code for the entire request, and the second is a per\-task result code\. The following table contains the response codes\.


****  

| Response code | Description | 
| --- | --- | 
| Succeeded | The task completed normally\. If you requested a job completion report, the task's result string is included in the report\. | 
| TemporaryFailure | The task suffered a temporary failure and will be redriven before the job completes\. The result string is ignored\. If this is the final redrive, the error message is included in the final report\. | 
| PermanentFailure | The task suffered a permanent failure\. If you requested a job\-completion report, the task is marked as Failed and includes the error message string\. Result strings from failed tasks are ignored\. | 

## Creating a Lambda function to use with S3 Batch Operations<a name="batch-ops-invoke-lambda-custom-functions"></a>

This section provides example AWS Identity and Access Management \(IAM\) permissions that you must use with your Lambda function\. It also contains an example Lambda function to use with S3 Batch Operations\. If you have never created a Lambda function before, see [Tutorial: Using AWS Lambda with Amazon S3](https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html) in the *AWS Lambda Developer Guide*\.

You must create Lambda functions specifically for use with S3 Batch Operations\. You can't reuse existing Amazon S3 event\-based Lambda functions\. This is because Lambda functions that are used for S3 Batch Operations must accept and return special data fields\. 

**Important**  
AWS Lambda functions written in Java accept either [RequestHandler](https://github.com/aws/aws-lambda-java-libs/blob/master/aws-lambda-java-core/src/main/java/com/amazonaws/services/lambda/runtime/RequestHandler.java) or [RequestStreamHandler](https://github.com/aws/aws-lambda-java-libs/blob/master/aws-lambda-java-core/src/main/java/com/amazonaws/services/lambda/runtime/RequestStreamHandler.java) handler interfaces\. However, to support S3 Batch Operations request and response format, AWS Lambda requires the `RequestStreamHandler` interface for custom serialization and deserialization of a request and response\. This interface allows Lambda to pass an InputStream and OutputStream to the Java `handleRequest` method\.   
Be sure to use the `RequestStreamHandler` interface when using Lambda functions with S3 Batch Operations\. If you use a `RequestHandler` interface, the batch job will fail with "Invalid JSON returned in Lambda payload" in the completion report\.   
For more information, see [Handler interfaces](https://docs.aws.amazon.com/lambda/latest/dg/java-handler.html#java-handler-interfaces) in the *AWS Lambda User Guide*\. 

### Example IAM permissions<a name="batch-ops-invoke-lambda-custom-functions-iam"></a>

The following are examples of the IAM permissions that are necessary to use a Lambda function with S3 Batch Operations\. 

**Example — S3 Batch Operations trust policy**  
The following is an example of the trust policy that you can use for the Batch Operations IAM role\. This IAM role is specified when you create the job and gives Batch Operations permission to assume the IAM role\.  

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "batchoperations.s3.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

**Example — Lambda IAM policy**  
The following is an example of an IAM policy that gives S3 Batch Operations permission to invoke the Lambda function and read the input manifest\.  

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "BatchOperationsLambdaPolicy",
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:GetObjectVersion",
                "s3:PutObject",
                "lambda:InvokeFunction"
            ],
            "Resource": "*"
        }
    ]
}
```

### Example request and response<a name="batch-ops-invoke-lambda-custom-functions-request"></a>

This section provides request and response examples for the Lambda function\.

**Example Request**  
The following is a JSON example of a request for the Lambda function\.  

```
{
    "invocationSchemaVersion": "1.0",
    "invocationId": "YXNkbGZqYWRmaiBhc2RmdW9hZHNmZGpmaGFzbGtkaGZza2RmaAo",
    "job": {
        "id": "f3cc4f60-61f6-4a2b-8a21-d07600c373ce"
    },
    "tasks": [
        {
            "taskId": "dGFza2lkZ29lc2hlcmUK",
            "s3Key": "customerImage1.jpg",
            "s3VersionId": "1",
            "s3BucketArn": "arn:aws:s3:us-east-1:0123456788:awsexamplebucket1"
        }
    ]
}
```

**Example Response**  
The following is a JSON example of a response for the Lambda function\.  

```
{
  "invocationSchemaVersion": "1.0",
  "treatMissingKeysAs" : "PermanentFailure",
  "invocationId" : "YXNkbGZqYWRmaiBhc2RmdW9hZHNmZGpmaGFzbGtkaGZza2RmaAo",
  "results": [
    {
      "taskId": "dGFza2lkZ29lc2hlcmUK",
      "resultCode": "Succeeded",
      "resultString": "[\"Mary Major", \"John Stiles\"]"
    }
  ]
}
```

### Example Lambda function for S3 Batch Operations<a name="batch-ops-invoke-lambda-custom-functions-example"></a>

The following example Python Lambda function iterates through the manifest, copying and renaming each object\.

As the example shows, keys from S3 Batch Operations are URL encoded\. To use Amazon S3 with other AWS services, it's important that you URL decode the key that is passed from S3 Batch Operations\.

```
import boto3
import urllib
from botocore.exceptions import ClientError

def lambda_handler(event, context):
    # Instantiate boto client
    s3Client = boto3.client('s3')
    
    # Parse job parameters from S3 Batch Operations
    jobId = event['job']['id']
    invocationId = event['invocationId']
    invocationSchemaVersion = event['invocationSchemaVersion']
    
    # Prepare results
    results = []
    
    # Parse Amazon S3 Key, Key Version, and Bucket ARN
    taskId = event['tasks'][0]['taskId']
    s3Key = urllib.unquote(event['tasks'][0]['s3Key']).decode('utf8')
    s3VersionId = event['tasks'][0]['s3VersionId']
    s3BucketArn = event['tasks'][0]['s3BucketArn']
    s3Bucket = s3BucketArn.split(':::')[-1]
    
    # Construct CopySource with VersionId
    copySrc = {'Bucket': s3Bucket, 'Key': s3Key}
    if s3VersionId is not None:
        copySrc['VersionId'] = s3VersionId
        
    # Copy object to new bucket with new key name
    try:
        # Prepare result code and string
        resultCode = None
        resultString = None
        
        # Construct New Key
        newKey = rename_key(s3Key)
        newBucket = 'destination-bucket-name'
        
        # Copy Object to New Bucket
        response = s3Client.copy_object(
            CopySource = copySrc,
            Bucket = newBucket,
            Key = newKey
        )
        
        # Mark as succeeded
        resultCode = 'Succeeded'
        resultString = str(response)
    except ClientError as e:
        # If request timed out, mark as a temp failure
        # and S3 Batch Operations will make the task for retry. If
        # any other exceptions are received, mark as permanent failure.
        errorCode = e.response['Error']['Code']
        errorMessage = e.response['Error']['Message']
        if errorCode == 'RequestTimeout':
            resultCode = 'TemporaryFailure'
            resultString = 'Retry request to Amazon S3 due to timeout.'
        else:
            resultCode = 'PermanentFailure'
            resultString = '{}: {}'.format(errorCode, errorMessage)
    except Exception as e:
        # Catch all exceptions to permanently fail the task
        resultCode = 'PermanentFailure'
        resultString = 'Exception: {}'.format(e.message)
    finally:
        results.append({
            'taskId': taskId,
            'resultCode': resultCode,
            'resultString': resultString
        })
    
    return {
        'invocationSchemaVersion': invocationSchemaVersion,
        'treatMissingKeysAs': 'PermanentFailure',
        'invocationId': invocationId,
        'results': results
    }

def rename_key(s3Key):
    # Rename the key by adding additional suffix
    return s3Key + '_new_suffix'
```

## Creating an S3 Batch Operations job that invokes a Lambda function<a name="batch-ops-invoke-lambda-create-job"></a>

When creating an S3 Batch Operations job to invoke a Lambda function, you must provide the following:
+ The ARN of your Lambda function \(which might include the function alias or a specific version number\)
+ An IAM role with permission to invoke the function
+ The action parameter `LambdaInvokeFunction`

For more information about creating an S3 Batch Operations job, see [Creating an S3 Batch Operations job](batch-ops-create-job.md) and [Operations](batch-ops-operations.md)\.

The following example creates an S3 Batch Operations job that invokes a Lambda function using the AWS CLI\.

```
aws s3control create-job
    --account-id <AccountID>
    --operation  '{"LambdaInvoke": { "FunctionArn": "arn:aws:lambda:Region:AccountID:function:LambdaFunctionName" } }'
    --manifest '{"Spec":{"Format":"S3BatchOperations_CSV_20180820","Fields":["Bucket","Key"]},"Location":{"ObjectArn":"arn:aws:s3:::ManifestLocation","ETag":"ManifestETag"}}'
    --report '{"Bucket":"arn:aws:s3:::awsexamplebucket1","Format":"Report_CSV_20180820","Enabled":true,"Prefix":"ReportPrefix","ReportScope":"AllTasks"}'
    --priority 2
    --role-arn arn:aws:iam::AccountID:role/BatchOperationsRole
    --region Region
    --description “Lambda Function"
```

## Providing task\-level information in Lambda manifests<a name="storing-task-level-information-in-lambda"></a>

When you use AWS Lambda functions with S3 Batch Operations, you might want additional data to accompany each task/key that is operated on\. For example, you might want to have both a source object key and new object key provided\. Your Lambda function could then copy the source key to a new S3 bucket under a new name\. By default, Amazon S3 batch operations let you specify only the destination bucket and a list of source keys in the input manifest to your job\. The following describes how you can include additional data in your manifest so that you can run more complex Lambda functions\.

To specify per\-key parameters in your S3 Batch Operations manifest to use in your Lambda function's code, use the following URL\-encoded JSON format\. The `key` field is passed to your Lambda function as if it were an Amazon S3 object key\. But it can be interpreted by the Lambda function to contain other values or multiple keys, as shown following\. 

**Note**  
The maximum number of characters for the `key` field in the manifest is 1,024\.

**Example — manifest substituting the "Amazon S3 keys" with JSON strings**  
The URL\-encoded version must be provided to S3 Batch Operations\.  

```
my-bucket,{"origKey": "object1key", "newKey": "newObject1Key"}
my-bucket,{"origKey": "object2key", "newKey": "newObject2Key"}
my-bucket,{"origKey": "object3key", "newKey": "newObject3Key"}
```

**Example — manifest URL\-encoded**  
This URL\-encoded version must be provided to S3 Batch Operations\. The non\-URL\-encoded version does not work\.  

```
my-bucket,%7B%22origKey%22%3A%20%22object1key%22%2C%20%22newKey%22%3A%20%22newObject1Key%22%7D
my-bucket,%7B%22origKey%22%3A%20%22object2key%22%2C%20%22newKey%22%3A%20%22newObject2Key%22%7D
my-bucket,%7B%22origKey%22%3A%20%22object3key%22%2C%20%22newKey%22%3A%20%22newObject3Key%22%7D
```

**Example — Lambda function with manifest format writing results to the job report**  
 This Lambda function shows how to parse JSON that is encoded into the S3 Batch Operations manifest\.  

```
import json
from urllib.parse import unquote_plus


# This example Lambda function shows how to parse JSON that is encoded into the Amazon S3 batch
# operations manifest containing lines like this:
#
# bucket,encoded-json
# bucket,encoded-json
# bucket,encoded-json
#
# For example, if we wanted to send the following JSON to this Lambda function:
#
# bucket,{"origKey": "object1key", "newKey": "newObject1Key"}
# bucket,{"origKey": "object2key", "newKey": "newObject2Key"}
# bucket,{"origKey": "object3key", "newKey": "newObject3Key"}
#
# We would simply URL-encode the JSON like this to create the real manifest to create a batch
# operations job with:
#
# my-bucket,%7B%22origKey%22%3A%20%22object1key%22%2C%20%22newKey%22%3A%20%22newObject1Key%22%7D
# my-bucket,%7B%22origKey%22%3A%20%22object2key%22%2C%20%22newKey%22%3A%20%22newObject2Key%22%7D
# my-bucket,%7B%22origKey%22%3A%20%22object3key%22%2C%20%22newKey%22%3A%20%22newObject3Key%22%7D
#
def lambda_handler(event, context):
    # Parse job parameters from S3 batch operations
    jobId = event['job']['id']
    invocationId = event['invocationId']
    invocationSchemaVersion = event['invocationSchemaVersion']

    # Prepare results
    results = []

    # S3 batch operations currently only passes a single task at a time in the array of tasks.
    task = event['tasks'][0]

    # Extract the task values we might want to use
    taskId = task['taskId']
    s3Key = task['s3Key']
    s3VersionId = task['s3VersionId']
    s3BucketArn = task['s3BucketArn']
    s3BucketName = s3BucketArn.split(':::')[-1]

    try:
        # Assume it will succeed for now
        resultCode = 'Succeeded'
        resultString = ''

        # Decode the JSON string that was encoded into the S3 Key value and convert the
        # resulting string into a JSON structure.
        s3Key_decoded = unquote_plus(s3Key)
        keyJson = json.loads(s3Key_decoded)

        # Extract some values from the JSON that we might want to operate on.  In this example
        # we won't do anything except return the concatenated string as a fake result.
        newKey = keyJson['newKey']
        origKey = keyJson['origKey']
        resultString = origKey + " --> " + newKey

    except Exception as e:
        # If we run into any exceptions, fail this task so batch operations does not retry it and
        # return the exception string so we can see the failure message in the final report
        # created by batch operations.
        resultCode = 'PermanentFailure'
        resultString = 'Exception: {}'.format(e)
    finally:
        # Send back the results for this task.
        results.append({
            'taskId': taskId,
            'resultCode': resultCode,
            'resultString': resultString
        })

    return {
        'invocationSchemaVersion': invocationSchemaVersion,
        'treatMissingKeysAs': 'PermanentFailure',
        'invocationId': invocationId,
        'results': results
    }
```


# Example 1: Configuring replication when the source and destination buckets are owned by the same account<a name="replication-walkthrough1"></a>

In this example, you set up replication for source and destination buckets that are owned by the same AWS account\. Examples are provided for using the Amazon S3 console, the AWS Command Line Interface \(AWS CLI\), and the AWS SDK for Java and AWS SDK for \.NET\.

**Topics**

## Configure replication when buckets are owned by the same account \(console\)<a name="replication-ex1-console"></a>

For step\-by\-step instructions, see [How Do I Add a Replication Rule to an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html) in the *Amazon Simple Storage Service Console User Guide*\. This topic provides instructions for setting replication configuration when buckets are owned by same and different AWS accounts\.

## Configure replication when buckets are owned by the same account \(AWS CLI\)<a name="replication-ex1-cli"></a>

To use the AWS CLI to set up replication when the source and destination buckets are owned by the same AWS account, you create source and destination buckets, enable versioning on the buckets, create an IAM role that gives Amazon S3 permission to replicate objects, and add the replication configuration to the source bucket\. To verify your setup, you test it\.

**To set up replication when source and destination buckets are owned by the same AWS account**

1. Set a credentials profile for the AWS CLI\. In this example, we use the profile name `acctA`\. For information about setting credential profiles, see [Named Profiles](https://docs.aws.amazon.com/cli/latest/userguide/cli-multiple-profiles.html) in the *AWS Command Line Interface User Guide*\. 
**Important**  
The profile you use for this exercise must have the necessary permissions\. For example, in the replication configuration, you specify the IAM role that Amazon S3 can assume\. You can do this only if the profile you use has the `iam:PassRole` permission\. For more information, see [Granting a User Permissions to Pass a Role to an AWS Service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html) in the *IAM User Guide*\. If you use administrator user credentials to create a named profile, you can perform all the tasks\. 

1. Create a *source* bucket and enable versioning on it\. The following code creates a *source* bucket in the US East \(N\. Virginia\) \(us\-east\-1\) Region\.

   

   ```
   aws s3api create-bucket \
   --bucket source \
   --region us-east-1 \
   --profile acctA
   ```

   ```
   aws s3api put-bucket-versioning \
   --bucket source \
   --versioning-configuration Status=Enabled \
   --profile acctA
   ```

1. Create a *destination* bucket and enable versioning on it\. The following code creates a *destination* bucket in the US West \(Oregon\) \(us\-west\-2\) Region\. 
**Note**  
To set up replication configuration when both source and destination buckets are in the same AWS account, you use the same profile\. This example uses `acctA`\. To test replication configuration when the buckets are owned by different AWS accounts, you specify different profiles for each\. This example uses the `acctB` profile for the destination bucket\.

   

   ```
   aws s3api create-bucket \
   --bucket destination \
   --region us-west-2 \
   --create-bucket-configuration LocationConstraint=us-west-2 \
   --profile acctA
   ```

   ```
   aws s3api put-bucket-versioning \
   --bucket destination \
   --versioning-configuration Status=Enabled \
   --profile acctA
   ```

1. Create an IAM role\. You specify this role in the replication configuration that you add to the *source* bucket later\. Amazon S3 assumes this role to replicate objects on your behalf\. You create an IAM role in two steps:
   + Create a role\.
   + Attach a permissions policy to the role\.

   1. Create the IAM role\.

      1. Copy the following trust policy and save it to a file named `S3-role-trust-policy.json` in the current directory on your local computer\. This policy grants Amazon S3 service principal permissions to assume the role\.

         ```
         {
            "Version":"2012-10-17",
            "Statement":[
               {
                  "Effect":"Allow",
                  "Principal":{
                     "Service":"s3.amazonaws.com"
                  },
                  "Action":"sts:AssumeRole"
               }
            ]
         }
         ```

      1. Run the following command to create a role\.

         ```
         $ aws iam create-role \
         --role-name replicationRole \
         --assume-role-policy-document file://s3-role-trust-policy.json  \
         --profile acctA
         ```

   1. Attach a permissions policy to the role\.

      1. Copy the following permissions policy and save it to a file named `S3-role-permissions-policy.json` in the current directory on your local computer\. This policy grants permissions for various Amazon S3 bucket and object actions\. 

         ```
         {
            "Version":"2012-10-17",
            "Statement":[
               {
                  "Effect":"Allow",
                  "Action":[
                     "s3:GetObjectVersionForReplication",
                     "s3:GetObjectVersionAcl"
                  ],
                  "Resource":[
                     "arn:aws:s3:::source-bucket/*"
                  ]
               },
               {
                  "Effect":"Allow",
                  "Action":[
                     "s3:ListBucket",
                     "s3:GetReplicationConfiguration"
                  ],
                  "Resource":[
                     "arn:aws:s3:::source-bucket"
                  ]
               },
               {
                  "Effect":"Allow",
                  "Action":[
                     "s3:ReplicateObject",
                     "s3:ReplicateDelete",
                     "s3:ReplicateTags",
                     "s3:GetObjectVersionTagging"
         
                  ],
                  "Resource":"arn:aws:s3:::destination-bucket/*"
               }
            ]
         }
         ```

      1. Run the following command to create a policy and attach it to the role\.

         ```
         $ aws iam put-role-policy \
         --role-name replicationRole \
         --policy-document file://s3-role-permissions-policy.json \
         --policy-name replicationRolePolicy \
         --profile acctA
         ```

1. Add replication configuration to the *source* bucket\. 

   1. Although the Amazon S3 API requires replication configuration as XML, the AWS CLI requires that you specify the replication configuration as JSON\. Save the following JSON in a file called `replication.json` to the local directory on your computer\.

      ```
      {
        "Role": "IAM-role-ARN",
        "Rules": [
          {
            "Status": "Enabled",
            "Priority": 1,
            "DeleteMarkerReplication": { "Status": "Disabled" },
            "Filter" : { "Prefix": "Tax"},
            "Destination": {
              "Bucket": "arn:aws:s3:::destination-bucket"
            }
          }
        ]
      }
      ```

   1. Update the JSON by providing values for the *destination\-bucket* and *IAM\-role\-ARN*\. Save the changes\.

   1. Run the following command to add the replication configuration to your source bucket\. Be sure to provide the *source* bucket name\.

      ```
      $ aws s3api put-bucket-replication \
      --replication-configuration file://replication.json \
      --bucket source \
      --profile acctA
      ```

   To retrieve the replication configuration, use the `get-bucket-replication` command\.

   ```
   $ aws s3api get-bucket-replication \
   --bucket source \
   --profile acctA
   ```

1. Test the setup in the Amazon S3 console: 

   1.  Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\. 

   1. In the *source* bucket, create a folder named `Tax`\. 

   1. Add sample objects to the `Tax` folder in the *source* bucket\. 
**Note**  
The amount of time it takes for Amazon S3 to replicate an object depends on the size of the object\. For information about how to see the status of replication, see [Replication status information](replication-status.md)\.

      In the *destination* bucket, verify the following:
      + That Amazon S3 replicated the objects\.
      + In object **properties**, that the **Replication Status** is set to `Replica` \(identifying this as a replica object\)\.
      + In object **properties**, that the permission section shows no permissions\. This means that the replica is still owned by the *source* bucket owner, and the *destination* bucket owner has no permission on the object replica\. You can add optional configuration to tell Amazon S3 to change the replica ownership\. For an example, see [Example 3: Changing the replica owner when the source and destination buckets are owned by different accounts](replication-walkthrough-3.md)\.   
![\[Screen shot of object properties showing the replication status and permissions.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/crr-wt2-10.png)

       

   1. Update an object's ACL in the *source* bucket and verify that changes appear in the *destination* bucket\.

      For instructions, see [How Do I Set Permissions on an Object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-object-permissions.html) in the *Amazon Simple Storage Service Console User Guide*\.

## Configure replication when buckets are owned by the same account \(AWS SDK\)<a name="replication-ex1-sdk"></a>

Use the following code examples to add a replication configuration to a bucket with the AWS SDK for Java and AWS SDK for \.NET, respectively\.

------
#### [ Java ]

The following example adds a replication configuration to a bucket and then retrieves and verifies the configuration\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.identitymanagement.AmazonIdentityManagement;
import com.amazonaws.services.identitymanagement.AmazonIdentityManagementClientBuilder;
import com.amazonaws.services.identitymanagement.model.CreateRoleRequest;
import com.amazonaws.services.identitymanagement.model.PutRolePolicyRequest;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3Client;
import com.amazonaws.services.s3.model.BucketReplicationConfiguration;
import com.amazonaws.services.s3.model.BucketVersioningConfiguration;
import com.amazonaws.services.s3.model.CreateBucketRequest;
import com.amazonaws.services.s3.model.DeleteMarkerReplication;
import com.amazonaws.services.s3.model.DeleteMarkerReplicationStatus;
import com.amazonaws.services.s3.model.ReplicationDestinationConfig;
import com.amazonaws.services.s3.model.ReplicationRule;
import com.amazonaws.services.s3.model.ReplicationRuleStatus;
import com.amazonaws.services.s3.model.SetBucketVersioningConfigurationRequest;
import com.amazonaws.services.s3.model.StorageClass;
import com.amazonaws.services.s3.model.replication.ReplicationFilter;
import com.amazonaws.services.s3.model.replication.ReplicationFilterPredicate;
import com.amazonaws.services.s3.model.replication.ReplicationPrefixPredicate;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class CrossRegionReplication {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String accountId = "*** Account ID ***";
        String roleName = "*** Role name ***";
        String sourceBucketName = "*** Source bucket name ***";
        String destBucketName = "*** Destination bucket name ***";
        String prefix = "Tax/";

        String roleARN = String.format("arn:aws:iam::%s:role/%s", accountId, roleName);
        String destinationBucketARN = "arn:aws:s3:::" + destBucketName;

        AmazonS3 s3Client = AmazonS3Client.builder()
            .withCredentials(new ProfileCredentialsProvider())
            .withRegion(clientRegion)
            .build();

        createBucket(s3Client, clientRegion, sourceBucketName);
        createBucket(s3Client, clientRegion, destBucketName);
        assignRole(roleName, clientRegion, sourceBucketName, destBucketName);


        try {


            // Create the replication rule.
            List<ReplicationFilterPredicate> andOperands = new ArrayList<ReplicationFilterPredicate>();
            andOperands.add(new ReplicationPrefixPredicate(prefix));


            Map<String, ReplicationRule> replicationRules = new HashMap<String, ReplicationRule>();
            replicationRules.put("ReplicationRule1",
                new ReplicationRule()
                    .withPriority(0)
                    .withStatus(ReplicationRuleStatus.Enabled)
                    .withDeleteMarkerReplication(new DeleteMarkerReplication().withStatus(DeleteMarkerReplicationStatus.DISABLED))
                    .withFilter(new ReplicationFilter().withPredicate(new ReplicationPrefixPredicate(prefix)))
                    .withDestinationConfig(new ReplicationDestinationConfig()
                        .withBucketARN(destinationBucketARN)
                        .withStorageClass(StorageClass.Standard)));

            // Save the replication rule to the source bucket.
            s3Client.setBucketReplicationConfiguration(sourceBucketName,
                new BucketReplicationConfiguration()
                    .withRoleARN(roleARN)
                    .withRules(replicationRules));

            // Retrieve the replication configuration and verify that the configuration
            // matches the rule we just set.
            BucketReplicationConfiguration replicationConfig = s3Client.getBucketReplicationConfiguration(sourceBucketName);
            ReplicationRule rule = replicationConfig.getRule("ReplicationRule1");
            System.out.println("Retrieved destination bucket ARN: " + rule.getDestinationConfig().getBucketARN());
            System.out.println("Retrieved priority: " + rule.getPriority());
            System.out.println("Retrieved source-bucket replication rule status: " + rule.getStatus());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void createBucket(AmazonS3 s3Client, Regions region, String bucketName) {
        CreateBucketRequest request = new CreateBucketRequest(bucketName, region.getName());
        s3Client.createBucket(request);
        BucketVersioningConfiguration configuration = new BucketVersioningConfiguration().withStatus(BucketVersioningConfiguration.ENABLED);

        SetBucketVersioningConfigurationRequest enableVersioningRequest = new SetBucketVersioningConfigurationRequest(bucketName, configuration);
        s3Client.setBucketVersioningConfiguration(enableVersioningRequest);


    }

    private static void assignRole(String roleName, Regions region, String sourceBucket, String destinationBucket) {
        AmazonIdentityManagement iamClient = AmazonIdentityManagementClientBuilder.standard()
            .withRegion(region)
            .withCredentials(new ProfileCredentialsProvider())
            .build();
        StringBuilder trustPolicy = new StringBuilder();
        trustPolicy.append("{\\r\\n   ");
        trustPolicy.append("\\\"Version\\\":\\\"2012-10-17\\\",\\r\\n   ");
        trustPolicy.append("\\\"Statement\\\":[\\r\\n      {\\r\\n         ");
        trustPolicy.append("\\\"Effect\\\":\\\"Allow\\\",\\r\\n         \\\"Principal\\\":{\\r\\n            ");
        trustPolicy.append("\\\"Service\\\":\\\"s3.amazonaws.com\\\"\\r\\n         },\\r\\n         ");
        trustPolicy.append("\\\"Action\\\":\\\"sts:AssumeRole\\\"\\r\\n      }\\r\\n   ]\\r\\n}");

        CreateRoleRequest createRoleRequest = new CreateRoleRequest()
            .withRoleName(roleName)
            .withAssumeRolePolicyDocument(trustPolicy.toString());

        iamClient.createRole(createRoleRequest);

        StringBuilder permissionPolicy = new StringBuilder();
        permissionPolicy.append("{\\r\\n   \\\"Version\\\":\\\"2012-10-17\\\",\\r\\n   \\\"Statement\\\":[\\r\\n      {\\r\\n         ");
        permissionPolicy.append("\\\"Effect\\\":\\\"Allow\\\",\\r\\n         \\\"Action\\\":[\\r\\n             ");
        permissionPolicy.append("\\\"s3:GetObjectVersionForReplication\\\",\\r\\n            ");
        permissionPolicy.append("\\\"s3:GetObjectVersionAcl\\\"\\r\\n         ],\\r\\n         \\\"Resource\\\":[\\r\\n            ");
        permissionPolicy.append("\\\"arn:aws:s3:::");
        permissionPolicy.append(sourceBucket);
        permissionPolicy.append("/*\\\"\\r\\n         ]\\r\\n      },\\r\\n      {\\r\\n         ");
        permissionPolicy.append("\\\"Effect\\\":\\\"Allow\\\",\\r\\n         \\\"Action\\\":[\\r\\n            ");
        permissionPolicy.append("\\\"s3:ListBucket\\\",\\r\\n            \\\"s3:GetReplicationConfiguration\\\"\\r\\n         ");
        permissionPolicy.append("],\\r\\n         \\\"Resource\\\":[\\r\\n            \\\"arn:aws:s3:::");
        permissionPolicy.append(sourceBucket);
        permissionPolicy.append("\\r\\n         ");
        permissionPolicy.append("]\\r\\n      },\\r\\n      {\\r\\n         \\\"Effect\\\":\\\"Allow\\\",\\r\\n         ");
        permissionPolicy.append("\\\"Action\\\":[\\r\\n            \\\"s3:ReplicateObject\\\",\\r\\n            ");
        permissionPolicy.append("\\\"s3:ReplicateDelete\\\",\\r\\n            \\\"s3:ReplicateTags\\\",\\r\\n            ");
        permissionPolicy.append("\\\"s3:GetObjectVersionTagging\\\"\\r\\n\\r\\n         ],\\r\\n         ");
        permissionPolicy.append("\\\"Resource\\\":\\\"arn:aws:s3:::");
        permissionPolicy.append(destinationBucket);
        permissionPolicy.append("/*\\\"\\r\\n      }\\r\\n   ]\\r\\n}");

        PutRolePolicyRequest putRolePolicyRequest = new PutRolePolicyRequest()
            .withRoleName(roleName)
            .withPolicyDocument(permissionPolicy.toString())
            .withPolicyName("crrRolePolicy");

        iamClient.putRolePolicy(putRolePolicyRequest);


    }
}
```

------
#### [ C\# ]

The following AWS SDK for \.NET code example adds a replication configuration to a bucket and then retrieves it\. To use this code, provide the names for your buckets and the Amazon Resource Name \(ARN\) for your IAM role\. For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CrossRegionReplicationTest
    {
        private const string sourceBucket = "*** source bucket ***";
        // Bucket ARN example - arn:aws:s3:::destinationbucket
        private const string destinationBucketArn = "*** destination bucket ARN ***";
        private const string roleArn = "*** IAM Role ARN ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint sourceBucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        public static void Main()
        {
            s3Client = new AmazonS3Client(sourceBucketRegion);
            EnableReplicationAsync().Wait();
        }
        static async Task EnableReplicationAsync()
        {
            try
            {
                ReplicationConfiguration replConfig = new ReplicationConfiguration
                {
                    Role = roleArn,
                    Rules =
                        {
                            new ReplicationRule
                            {
                                Prefix = "Tax",
                                Status = ReplicationRuleStatus.Enabled,
                                Destination = new ReplicationDestination
                                {
                                    BucketArn = destinationBucketArn
                                }
                            }
                        }
                };

                PutBucketReplicationRequest putRequest = new PutBucketReplicationRequest
                {
                    BucketName = sourceBucket,
                    Configuration = replConfig
                };

                PutBucketReplicationResponse putResponse = await s3Client.PutBucketReplicationAsync(putRequest);

                // Verify configuration by retrieving it.
                await RetrieveReplicationConfigurationAsync(s3Client);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
        private static async Task RetrieveReplicationConfigurationAsync(IAmazonS3 client)
        {
            // Retrieve the configuration.
            GetBucketReplicationRequest getRequest = new GetBucketReplicationRequest
            {
                BucketName = sourceBucket
            };
            GetBucketReplicationResponse getResponse = await client.GetBucketReplicationAsync(getRequest);
            // Print.
            Console.WriteLine("Printing replication configuration information...");
            Console.WriteLine("Role ARN: {0}", getResponse.Configuration.Role);
            foreach (var rule in getResponse.Configuration.Rules)
            {
                Console.WriteLine("ID: {0}", rule.Id);
                Console.WriteLine("Prefix: {0}", rule.Prefix);
                Console.WriteLine("Status: {0}", rule.Status);
            }
        }
    }
}
```

------


# Example 2: Configuring replication when the source and destination buckets are owned by different accounts<a name="replication-walkthrough-2"></a>

Setting up replication when *source* and *destination* buckets are owned by different AWS accounts is similar to setting replication when both buckets are owned by the same account\. The only difference is that the *destination* bucket owner must grant the *source* bucket owner permission to replicate objects by adding a bucket policy\. 

**To configure replication when the source and destination buckets are owned by different AWS accounts**

1. In this example, you create *source* and *destination* buckets in two different AWS accounts\. You need to have two credential profiles set for the AWS CLI \(in this example, we use `acctA` and `acctB` for profile names\)\. For more information about setting credential profiles, see [Named Profiles](https://docs.aws.amazon.com/cli/latest/userguide/cli-multiple-profiles.html) in the *AWS Command Line Interface User Guide*\. 

1. Follow the step\-by\-step instructions in [Example 1: Configuring for buckets in the same account](replication-walkthrough1.md) with the following changes:
   + For all AWS CLI commands related to *source* bucket activities \(for creating the *source* bucket, enabling versioning, and creating the IAM role\), use the `acctA` profile\. Use the `acctB` profile to create the *destination* bucket\. 
   + Make sure that the permissions policy specifies the *source* and *destination* buckets that you created for this example\.

1. In the console, add the following bucket policy on the *destination* bucket to allow the owner of the *source* bucket to replicate objects\. Be sure to edit the policy by providing the AWS account ID of the *source* bucket owner and the *destination* bucket name\.

   ```
   {
      "Version":"2008-10-17",
      "Id":"",
      "Statement":[
         {
            "Sid":"1",
            "Effect":"Allow",
            "Principal":{
               "AWS":"arn:aws:iam::source-bucket-acct-ID:source-acct-IAM-role"
            },
            "Action":["s3:ReplicateObject", "s3:ReplicateDelete"],
            "Resource":"arn:aws:s3:::destination/*"
         },
         {
            "Sid":"2",
            "Effect":"Allow",
            "Principal":{
               "AWS":"arn:aws:iam::source-bucket-acct-ID:source-acct-IAM-role"
            },
            "Action":["s3:GetBucketVersioning", "s3:PutBucketVersioning"],
            "Resource":"arn:aws:s3:::destination"
         }
      ]
   }
   ```

Choose the bucket and add the bucket policy\. For instructions, see [How Do I Add an S3 Bucket Policy?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html) in the *Amazon Simple Storage Service Console User Guide*\.


# Retrieving objects from versioning\-suspended buckets<a name="RetrievingObjectsfromVersioningSuspendedBuckets"></a>

A `GET Object` request returns the current version of an object whether you've enabled versioning on a bucket or not\. The following figure shows how a simple `GET` returns the current version of an object\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_GET_suspended.png)


# Making requests to Amazon S3 over IPv6<a name="ipv6-access"></a>

Amazon Simple Storage Service \(Amazon S3\) supports the ability to access S3 buckets using the Internet Protocol version 6 \(IPv6\), in addition to the IPv4 protocol\. Amazon S3 dual\-stack endpoints support requests to S3 buckets over IPv6 and IPv4\. There are no additional charges for accessing Amazon S3 over IPv6\. For more information about pricing, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\.

**Topics**
+ [Getting started making requests over IPv6](#ipv6-access-getting-started)
+ [Using IPv6 addresses in IAM policies](#ipv6-access-iam)
+ [Testing IP address compatibility](#ipv6-access-test-compatabilty)
+ [Using Amazon S3 dual\-stack endpoints](dual-stack-endpoints.md)

## Getting started making requests over IPv6<a name="ipv6-access-getting-started"></a>

To make a request to an S3 bucket over IPv6, you need to use a dual\-stack endpoint\. The next section describes how to make requests over IPv6 by using dual\-stack endpoints\. 

The following are some things you should know before trying to access a bucket over IPv6: 
+ The client and the network accessing the bucket must be enabled to use IPv6\. 
+ Both virtual hosted\-style and path style requests are supported for IPv6 access\. For more information, see [Amazon S3 dual\-stack endpoints](dual-stack-endpoints.md#dual-stack-endpoints-description)\.
+ If you use source IP address filtering in your AWS Identity and Access Management \(IAM\) user or bucket policies, you need to update the policies to include IPv6 address ranges\. For more information, see [Using IPv6 addresses in IAM policies](#ipv6-access-iam)\.
+ When using IPv6, server access log files output IP addresses in an IPv6 format\. You need to update existing tools, scripts, and software that you use to parse Amazon S3 log files so that they can parse the IPv6 formatted `Remote IP` addresses\. For more information, see [Amazon S3 Server Access Log Format](LogFormat.md) and [Amazon S3 server access logging](ServerLogs.md)\. 
**Note**  
If you experience issues related to the presence of IPv6 addresses in log files, contact [AWS Support](https://aws.amazon.com/premiumsupport/)\.



### Making requests over IPv6 by using dual\-stack endpoints<a name="ipv6-access-api"></a>

You make requests with Amazon S3 API calls over IPv6 by using dual\-stack endpoints\. The Amazon S3 API operations work the same way whether you're accessing Amazon S3 over IPv6 or over IPv4\. Performance should be the same too\.

When using the REST API, you access a dual\-stack endpoint directly\. For more information, see [Dual\-stack endpoints](dual-stack-endpoints.md#dual-stack-endpoints-description)\.

When using the AWS Command Line Interface \(AWS CLI\) and AWS SDKs, you can use a parameter or flag to change to a dual\-stack endpoint\. You can also specify the dual\-stack endpoint directly as an override of the Amazon S3 endpoint in the config file\.

You can use a dual\-stack endpoint to access a bucket over IPv6 from any of the following:
+ The AWS CLI, see [Using dual\-stack endpoints from the AWS CLI](dual-stack-endpoints.md#dual-stack-endpoints-cli)\.
+ The AWS SDKs, see [Using dual\-stack endpoints from the AWS SDKs](dual-stack-endpoints.md#dual-stack-endpoints-sdks)\.
+ The REST API, see [Making requests to dual\-stack endpoints by using the REST API](RESTAPI.md#rest-api-dual-stack)\.

### Features not available over IPv6<a name="ipv6-not-supported"></a>

The following features are not currently supported when accessing an S3 bucket over IPv6:
+ Static website hosting from an S3 bucket
+ BitTorrent

## Using IPv6 addresses in IAM policies<a name="ipv6-access-iam"></a>

Before trying to access a bucket using IPv6, you must ensure that any IAM user or S3 bucket polices that are used for IP address filtering are updated to include IPv6 address ranges\. IP address filtering policies that are not updated to handle IPv6 addresses may result in clients incorrectly losing or gaining access to the bucket when they start using IPv6\. For more information about managing access permissions with IAM, see [Identity and access management in Amazon S3](s3-access-control.md)\.

IAM policies that filter IP addresses use [IP Address Condition Operators](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Conditions_IPAddress)\. The following bucket policy identifies the 54\.240\.143\.\* range of allowed IPv4 addresses by using IP address condition operators\. Any IP addresses outside of this range will be denied access to the bucket \(`examplebucket`\)\. Since all IPv6 addresses are outside of the allowed range, this policy prevents IPv6 addresses from being able to access `examplebucket`\. 

```
 1. {
 2.   "Version": "2012-10-17",
 3.   "Statement": [
 4.     {
 5.       "Sid": "IPAllow",
 6.       "Effect": "Allow",
 7.       "Principal": "*",
 8.       "Action": "s3:*",
 9.       "Resource": "arn:aws:s3:::examplebucket/*",
10.       "Condition": {
11.          "IpAddress": {"aws:SourceIp": "54.240.143.0/24"}
12.       } 
13.     } 
14.   ]
15. }
```

You can modify the bucket policy's `Condition` element to allow both IPv4 \(`54.240.143.0/24`\) and IPv6 \(`2001:DB8:1234:5678::/64`\) address ranges as shown in the following example\. You can use the same type of `Condition` block shown in the example to update both your IAM user and bucket policies\.

```
1.        "Condition": {
2.          "IpAddress": {
3.             "aws:SourceIp": [
4.               "54.240.143.0/24",
5.                "2001:DB8:1234:5678::/64"
6.              ]
7.           }
8.         }
```

Before using IPv6 you must update all relevant IAM user and bucket policies that use IP address filtering to allow IPv6 address ranges\. We recommend that you update your IAM policies with your organization's IPv6 address ranges in addition to your existing IPv4 address ranges\. For an example of a bucket policy that allows access over both IPv6 and IPv4, see [Limiting Access to Specific IP Addresses](example-bucket-policies.md#example-bucket-policies-use-case-3)\.

You can review your IAM user policies using the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\. For more information about IAM, see the [IAM User Guide](https://docs.aws.amazon.com/IAM/latest/UserGuide/)\. For information about editing S3 bucket policies, see [How Do I Add an S3 Bucket Policy?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html) in the *Amazon Simple Storage Service Console User Guide*\. 

## Testing IP address compatibility<a name="ipv6-access-test-compatabilty"></a>

If you are using use Linux/Unix or Mac OS X, you can test whether you can access a dual\-stack endpoint over IPv6 by using the `curl` command as shown in the following example:

**Example**  

```
curl -v  http://s3.dualstack.us-west-2.amazonaws.com/
```
You get back information similar to the following example\. If you are connected over IPv6 the connected IP address will be an IPv6 address\.   

```
* About to connect() to s3-us-west-2.amazonaws.com port 80 (#0)
*   Trying IPv6 address... connected
* Connected to s3.dualstack.us-west-2.amazonaws.com (IPv6 address) port 80 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.18.1 (x86_64-unknown-linux-gnu) libcurl/7.18.1 OpenSSL/1.0.1t zlib/1.2.3
> Host: s3.dualstack.us-west-2.amazonaws.com
```

If you are using Microsoft Windows 7 or Windows 10, you can test whether you can access a dual\-stack endpoint over IPv6 or IPv4 by using the `ping` command as shown in the following example\.

```
ping ipv6.s3.dualstack.us-west-2.amazonaws.com 
```


# Cleaning up your example resources<a name="getting-started-cleanup"></a>

If you created your static website as a learning exercise, you should delete the AWS resources that you allocated so that you no longer accrue charges\. After you delete your AWS resources, your website is no longer available\.

**Topics**
+ [Step 1: Delete the Amazon CloudFront distribution](#getting-started-cleanup-cloudfront)
+ [Step 2: Delete the Route 53 hosted zone](#getting-started-cleanup-route53)
+ [Step 3: Disable logging and delete your S3 bucket](#getting-started-cleanup-s3)

## Step 1: Delete the Amazon CloudFront distribution<a name="getting-started-cleanup-cloudfront"></a>

Before you delete an Amazon CloudFront distribution, you must disable it\. A disabled distribution is no longer functional and does not accrue charges\. You can enable a disabled distribution at any time\. After you delete a disabled distribution, it is no longer available\.

**To disable and delete a CloudFront distribution**

1. Open the CloudFront console at [ https://console\.aws\.amazon\.com/cloudfront/](https://console.aws.amazon.com/cloudfront/)\.

1. Select the distribution that you want to disable, and then choose **Disable**\.

1. When prompted for confirmation, choose **Yes, Disable**\.

1. Select the disabled distribution, and then choose **Delete**\.

1. When prompted for confirmation, choose **Yes, Delete**\.

## Step 2: Delete the Route 53 hosted zone<a name="getting-started-cleanup-route53"></a>

Before you delete the hosted zone, you must delete the record sets that you created\. You don't need to delete the NS and SOA records; these are automatically deleted when you delete the hosted zone\.

**To delete the record sets**

1. Open the Route 53 console at [https://console\.aws\.amazon\.com/route53/](https://console.aws.amazon.com/route53/)\.

1.  In the list of domain names, select your domain name, and then choose **Go to Record Sets**\. 

1. In the list of record sets, select the A records that you created\. 

   The type of each record set is listed in the **Type** column\. 

1. Choose **Delete Record Set**\. 

1. When prompted for confirmation, choose **Confirm**\. 

**To delete a Route 53 hosted zone**

1.  Continuing from the previous procedure, choose **Back to Hosted Zones**\. 

1.  Select your domain name, and then choose **Delete Hosted Zone**\. 

1.  When prompted for confirmation, choose **Confirm**\. 

## Step 3: Disable logging and delete your S3 bucket<a name="getting-started-cleanup-s3"></a>

Before you delete your S3 bucket, make sure that logging is disabled for the bucket\. Otherwise, AWS continues to write logs to your bucket as you delete it\.

**To disable logging for a bucket**

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Under **Buckets**, choose your bucket name, and then choose **Properties**\.

1. From **Properties**, choose **Logging**\.

1. Clear the **Enabled** check box\.

1. Choose **Save**\.

Now, you can delete your bucket\. For more information, see [How Do I Delete an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/delete-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\.


# Configure Requester Pays with the REST API<a name="configure-requester-pays-rest"></a>

**Topics**
+ [Setting the requestPayment Bucket Configuration](RequesterPaysBucketConfiguration.md)
+ [Retrieving the requestPayment Configuration](BucketPayerValues.md)
+ [Downloading objects in Requester Pays buckets](ObjectsinRequesterPaysBuckets.md)


# Uploading objects using multipart upload API<a name="uploadobjusingmpu"></a>

Multipart upload allows you to upload a single object as a set of parts\. Each part is a contiguous portion of the object's data\. You can upload these object parts independently and in any order\. If transmission of any part fails, you can retransmit that part without affecting other parts\. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object\. In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation\.

Using multipart upload provides the following advantages:
+  Improved throughput \- You can upload parts in parallel to improve throughput\. 
+ Quick recovery from any network issues \- Smaller part size minimizes the impact of restarting a failed upload due to a network error\.
+ Pause and resume object uploads \- You can upload object parts over time\. Once you initiate a multipart upload there is no expiry; you must explicitly complete or stop the multipart upload\.
+ Begin an upload before you know the final object size \- You can upload an object as you are creating it\. 

**Topics**
+ [Multipart upload overview](mpuoverview.md)
+ [Using the AWS Java SDK for multipart upload \(high\-level API\)](usingHLmpuJava.md)
+ [Using the AWS Java SDK for a multipart upload \(low\-level API\)](mpListPartsJavaAPI.md)
+ [Using the AWS SDK for \.NET for multipart upload \(high\-level API\)](usingHLmpuDotNet.md)
+ [Using the AWS SDK for \.NET for multipart upload \(low\-level API\)](usingLLmpuDotNet.md)
+ [Using the AWS PHP SDK for multipart upload](usingHLmpuPHP.md)
+ [Using the AWS PHP SDK for multipart upload \(low\-level API\)](usingLLmpuPHP.md)
+ [Using the AWS SDK for Ruby for Multipart Upload](uploadobjusingmpu-ruby-sdk.md)
+ [Using the REST API for multipart upload](UsingRESTAPImpUpload.md)
+ [Using the AWS Command Line Interface for multipart upload](UsingCLImpUpload.md)
+ [ Using the AWS SDK for JavaScript for Multipart Upload](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#createMultipartUpload-property)


# Overview of managing access<a name="access-control-overview"></a>

When granting permissions, you decide who is getting them, which Amazon S3 resources they are getting permissions for, and specific actions you want to allow on those resources\. 

**Topics**
+ [Amazon S3 resources: Buckets and objects](#access-control-resources-basics)
+ [Amazon S3 bucket and object ownership](#about-resource-owner)
+ [Resource operations](#access-control-resource-operations-basics)
+ [Managing access to resources](#access-control-resources-manage-permissions-basics)
+ [Which access control method should I use?](#so-which-one-should-i-use)
+ [More info](#access-control-overview-related-topics)

## Amazon S3 resources: Buckets and objects<a name="access-control-resources-basics"></a>

In Amazon Web Services \(AWS\), a resource is an entity that you can work with\. In Amazon S3, buckets and objects are the resources, and both have associated subresources\. For example, bucket subresources include the following:
+ `lifecycle` – Stores lifecycle configuration information \(see [Object lifecycle management](object-lifecycle-mgmt.md)\)\.
+ `website` – Stores website configuration information if you configure your bucket for website hosting \(see [Hosting a static website using Amazon S3](WebsiteHosting.md)\)\. 
+ `versioning` – Stores versioning configuration \(see [PUT Bucket versioning](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTVersioningStatus.html)\)\. 
+ `policy` and `acl` \(access control list\) – Store access permission information for the bucket\. 
+ `cors` \(cross\-origin resource sharing\) – Supports configuring your bucket to allow cross\-origin requests \(see [Cross\-origin resource sharing \(CORS\)](cors.md)\)\.
+ `object ownership` – Enables the bucket owner to take ownership of new objects in the bucket, regardless of who uploads them \(see [Controlling ownership of uploaded objects using S3 Object Ownership](about-object-ownership.md)\)\.
+ `logging` – Enables you to request Amazon S3 to save bucket access logs\.

Object subresources include the following:
+ `acl` – Stores a list of access permissions on the object\. This topic discusses how to use this subresource to manage object permissions \(see [Managing Access with ACLs](S3_ACLs_UsingACLs.md)\)\.
+ `restore` – Supports temporarily restoring an archived object \(see [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html)\)\. An object in the S3 Glacier storage class is an archived object\. To access the object, you must first initiate a restore request, which restores a copy of the archived object\. In the request, you specify the number of days that you want the restored copy to exist\. For more information about archiving objects, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

## Amazon S3 bucket and object ownership<a name="about-resource-owner"></a>

Buckets and objects are Amazon S3 resources\. By default, only the resource owner can access these resources\. The resource owner refers to the AWS account that creates the resource\. For example: 
+ The AWS account that you use to create buckets and upload objects owns those resources\. 
+  If you upload an object using AWS Identity and Access Management \(IAM\) user or role credentials, the AWS account that the user or role belongs to owns the object\. 
+ A bucket owner can grant cross\-account permissions to another AWS account \(or users in another account\) to upload objects\. In this case, the AWS account that uploads objects owns those objects\. The bucket owner does not have permissions on the objects that other accounts own, with the following exceptions:
  + The bucket owner pays the bills\. The bucket owner can deny access to any objects, or delete any objects in the bucket, regardless of who owns them\. 
  + The bucket owner can archive any objects or restore archived objects regardless of who owns them\. Archival refers to the storage class used to store the objects\. For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

### Ownership and request authentication<a name="about-resource-owner-requests"></a>

All requests to a bucket are either authenticated or unauthenticated\. Authenticated requests must include a signature value that authenticates the request sender, unauthenticated requests do not\. For more information on request authentication, see [Making requests](MakingRequests.md)\.

A bucket owner can allow unauthenticated requests\. For example, unauthenticated [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) requests are allowed when a bucket has a public bucket policy, or when a bucket ACL grants `WRITE` or `FULL_CONTROL` access to the All Users group or the anonymous user specifically\. For more information about public bucket policies and public ACLs, see [The meaning of "public"](access-control-block-public-access.md#access-control-block-public-access-policy-status)\.

All unauthenticated requests are made by the anonymous user\. This user is represented in access control lists \(ACLs\) by the specific canonical user ID `65a011a29cdf8ec533ec3d1ccaae921c`\. If an object is uploaded to a bucket through an unauthenticated request, the anonymous user owns the object\. The default object ACL grants `FULL_CONTROL` to the anonymous user as the object's owner\. Therefore, Amazon S3 allows unauthenticated requests to retrieve the object or modify its ACL\. 

To prevent objects from being modified by the anonymous user, we recommend that you do not implement bucket policies that allow anonymous public writes to your bucket or use ACLs that allow the anonymous user write access to your bucket\. You can enforce this recommended behavior by using Amazon S3 Block Public Access\. 

For more information about blocking public access, see [Using Amazon S3 block public access](access-control-block-public-access.md)\. For more information about ACLs, see [Access Control List \(ACL\) Overview](acl-overview.md)\.

**Important**  
AWS recommends that you don't use the AWS account root user credentials to make authenticated requests\. Instead, create an IAM user and grant that user full access\. We refer to these users as administrator users\. You can use the administrator user credentials, instead of AWS account root user credentials, to interact with AWS and perform tasks, such as create a bucket, create users, and grant them permissions\. For more information, see [AWS Account Root User Credentials vs\. IAM User Credentials](https://docs.aws.amazon.com/general/latest/gr/root-vs-iam.html) in the *AWS General Reference* and [IAM Best Practices](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html) in the *IAM User Guide*\.

## Resource operations<a name="access-control-resource-operations-basics"></a>

Amazon S3 provides a set of operations to work with the Amazon S3 resources\. For a list of available operations, see [Actions defined by Amazon S3](list_amazons3.md#amazons3-actions-as-permissions)\.

## Managing access to resources<a name="access-control-resources-manage-permissions-basics"></a>

Managing access refers to granting others \(AWS accounts and users\) permission to perform the resource operations by writing an access policy\. For example, you can grant `PUT Object` permission to a user in an AWS account so the user can upload objects to your bucket\. In addition to granting permissions to individual users and accounts, you can grant permissions to everyone \(also referred as anonymous access\) or to all authenticated users \(users with AWS credentials\)\. For example, if you configure your bucket as a website, you may want to make objects public by granting the `GET Object` permission to everyone\. 

### Access policy options<a name="access-policies-alternatives-intro"></a>

Access policy describes who has access to what\. You can associate an access policy with a resource \(bucket and object\) or a user\. Accordingly, you can categorize the available Amazon S3 access policies as follows:
+ **Resource\-based policies ** – Bucket policies and access control lists \(ACLs\) are resource\-based because you attach them to your Amazon S3 resources\.   
![\[Diagram depicting AWS account resources, including an S3 bucket with a bucket ACL and bucket policy, and S3 objects with object ACLs.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/resource-based-policy.png)
  + ACL – Each bucket and object has an ACL associated with it\. An ACL is a list of grants identifying grantee and permission granted\. You use ACLs to grant basic read/write permissions to other AWS accounts\. ACLs use an Amazon S3–specific XML schema\. 

    The following is an example bucket ACL\. The grant in the ACL shows a bucket owner as having full control permission\. 

    ```
    <?xml version="1.0" encoding="UTF-8"?>
    <AccessControlPolicy xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
      <Owner>
        <ID>*** Owner-Canonical-User-ID ***</ID>
        <DisplayName>owner-display-name</DisplayName>
      </Owner>
      <AccessControlList>
        <Grant>
          <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
                   xsi:type="Canonical User">
            <ID>*** Owner-Canonical-User-ID ***</ID>
            <DisplayName>display-name</DisplayName>
          </Grantee>
          <Permission>FULL_CONTROL</Permission>
        </Grant>
      </AccessControlList>
    </AccessControlPolicy>
    ```

    Both bucket and object ACLs use the same XML schema\.
  + Bucket policy – For your bucket, you can add a bucket policy to grant other AWS accounts or IAM users permissions for the bucket and the objects in it\. Any object permissions apply only to the objects that the bucket owner creates\. Bucket policies supplement, and in many cases, replace ACL\-based access policies\.

    The following is an example bucket policy\. You express bucket policy \(and user policy\) using a JSON file\. The policy grants anonymous read permission on all objects in a bucket\. The bucket policy has one statement, which allows the `s3:GetObject` action \(read permission\) on objects in a bucket named `examplebucket`\.  By specifying the `principal` with a wild card \(\*\), the policy grants anonymous access, and should be used carefully\. For example, the following bucket policy would make objects publicly accessible\. 

    ```
    {
        "Version":"2012-10-17",
        "Statement": [
            {
                "Sid":"GrantAnonymousReadPermissions",
                "Effect":"Allow",
                "Principal": "*",
                "Action":["s3:GetObject"],
                "Resource":["arn:aws:s3:::awsexamplebucket1/*"]
            }
        ]
    }
    ```
+ **User policies** – You can use IAM to manage access to your Amazon S3 resources\. You can create IAM users, groups, and roles in your account and attach access policies to them granting them access to AWS resources, including Amazon S3\.   
![\[Diagram depicting the AWS account admin and other users with attached user policies.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/user-policy.png)

  For more information about IAM, see the [AWS Identity and Access Management \(IAM\)](https://aws.amazon.com/iam/) product detail page\. 

  The following is an example of a user policy\. You cannot grant anonymous permissions in an IAM user policy, because the policy is attached to a user\. The example policy allows the associated user that it's attached to perform six different Amazon S3 actions on a bucket and the objects in it\. You can attach this policy to a specific IAM user, group, or role\.

  ```
  {
      "Version": "2012-10-17",
      "Statement": [
          {
              "Sid": "AssignUserActions",
              "Effect": "Allow",
              "Action": [
                  "s3:PutObject",
                  "s3:GetObject",
                  "s3:ListBucket",
                  "s3:DeleteObject",
                  "s3:GetBucketLocation"
              ],
              "Resource": [
                   "arn:aws:s3:::awsexamplebucket1/*",
                   "arn:aws:s3:::awsexamplebucket1"
              ]
          },
          {
              "Sid": "ExampleStatement2",
              "Effect": "Allow",
              "Action": "s3:ListAllMyBuckets",
              "Resource": "*"
          }
      ]
  }
  ```

When Amazon S3 receives a request, it must evaluate all the access policies to determine whether to authorize or deny the request\. For more information about how Amazon S3 evaluates these policies, see [How Amazon S3 Authorizes a Request](how-s3-evaluates-access-control.md)\.

### Access Analyzer for S3<a name="access-analyzer-s3-info"></a>

On the Amazon S3 console, you can use Access Analyzer for S3 to review all buckets that have bucket access control lists \(ACLs\), bucket policies, or access point policies that grant public or shared access\. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization\. For each public or shared bucket, you receive findings that report the source and level of public or shared access\. 

In Access Analyzer for S3, you can block all public access to a bucket with a single click\. We recommend that you block all access to your buckets unless you require public access to support a specific use case\. Before you block all public access, ensure that your applications will continue to work correctly without public access\. For more information, see [Using Amazon S3 block public access](access-control-block-public-access.md)\.

You can also drill down into bucket\-level permission settings to configure granular levels of access\. For specific and verified use cases that require public or shared access, you can acknowledge and record your intent for the bucket to remain public or shared by archiving the findings for the bucket\. You can revisit and modify these bucket configurations at any time\. You can also download your findings as a CSV report for auditing purposes\.

Access Analyzer for S3 is available at no extra cost on the Amazon S3 console\. Access Analyzer for S3 is powered by AWS Identity and Access Management \(IAM\) Access Analyzer\. To use Access Analyzer for S3 on the Amazon S3 console, you must visit the IAM console and create an account\-level analyzer in IAM Access Analyzer on a per\-Region basis\. 

For more information about Access Analyzer for S3, see [Using Access Analyzer for S3](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html) in the *Amazon Simple Storage Service Console User Guide*\.

## Which access control method should I use?<a name="so-which-one-should-i-use"></a>

 With the options available to write an access policy, the following questions arise:
+ When should I use which access control method? For example, to grant bucket permissions, should I use a bucket policy or bucket ACL? I own a bucket and the objects in the bucket\. Should I use a resource\-based access policy or an IAM user policy? If I use a resource\-based access policy, should I use a bucket policy or an object ACL to manage object permissions?
+ I own a bucket, but I don't own all of the objects in it\. How are access permissions managed for the objects that somebody else owns? 
+ If I grant access by using a combination of these access policy options, how does Amazon S3 determine if a user has permission to perform a requested operation? 

 The following sections explain these access control alternatives, how Amazon S3 evaluates access control mechanisms, and when to use which access control method\. They also provide example walkthroughs\.
+  [How Amazon S3 Authorizes a Request](how-s3-evaluates-access-control.md) 
+  [Guidelines for using the available access policy options](access-policy-alternatives-guidelines.md) 
+  [Example walkthroughs: Managing access to your Amazon S3 resources ](example-walkthroughs-managing-access.md) 
+  [ Access control best practices](https://docs.aws.amazon.com/AmazonS3/latest/gsg/access-control-best-practices.html) in the *Amazon Simple Storage Service Getting Started Guide* 

## More info<a name="access-control-overview-related-topics"></a>

We recommend that you first review the introductory topics that explain the options available for you to manage access to your Amazon S3 resources\. For more information, see [Identity and access management in Amazon S3](s3-access-control.md)\. You can then use the following topics for more information about specific access policy options\. 
+  [Using Bucket Policies and User Policies](using-iam-policies.md) 
+  [Managing Access with ACLs](S3_ACLs_UsingACLs.md) 


# Selecting content from objects using the SDK for Java<a name="SelectObjectContentUsingJava"></a>

You use Amazon S3 Select to select contents of an object with Java using the `selectObjectContent` method, which on success returns the results of the SQL expression\. The specified bucket and object key must exist, or an error results\.

**Example**  
The following Java code returns the value of the first column for each record that is stored in an object that contains data stored in CSV format\. It also requests `Progress` and `Stats` messages to be returned\. You must provide a valid bucket name and an object that contains data in CSV format\.  
For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
package com.amazonaws;

import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CSVInput;
import com.amazonaws.services.s3.model.CSVOutput;
import com.amazonaws.services.s3.model.CompressionType;
import com.amazonaws.services.s3.model.ExpressionType;
import com.amazonaws.services.s3.model.InputSerialization;
import com.amazonaws.services.s3.model.OutputSerialization;
import com.amazonaws.services.s3.model.SelectObjectContentEvent;
import com.amazonaws.services.s3.model.SelectObjectContentEventVisitor;
import com.amazonaws.services.s3.model.SelectObjectContentRequest;
import com.amazonaws.services.s3.model.SelectObjectContentResult;

import java.io.File;
import java.io.FileOutputStream;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.concurrent.atomic.AtomicBoolean;

import static com.amazonaws.util.IOUtils.copy;

/**
 * This example shows how to query data from S3Select and consume the response in the form of an
 * InputStream of records and write it to a file.
 */

public class RecordInputStreamExample {

    private static final String BUCKET_NAME = "${my-s3-bucket}";
    private static final String CSV_OBJECT_KEY = "${my-csv-object-key}";
    private static final String S3_SELECT_RESULTS_PATH = "${my-s3-select-results-path}";
    private static final String QUERY = "select s._1 from S3Object s";

    public static void main(String[] args) throws Exception {
        final AmazonS3 s3Client = AmazonS3ClientBuilder.defaultClient();

        SelectObjectContentRequest request = generateBaseCSVRequest(BUCKET_NAME, CSV_OBJECT_KEY, QUERY);
        final AtomicBoolean isResultComplete = new AtomicBoolean(false);

        try (OutputStream fileOutputStream = new FileOutputStream(new File (S3_SELECT_RESULTS_PATH));
             SelectObjectContentResult result = s3Client.selectObjectContent(request)) {
            InputStream resultInputStream = result.getPayload().getRecordsInputStream(
                    new SelectObjectContentEventVisitor() {
                        @Override
                        public void visit(SelectObjectContentEvent.StatsEvent event)
                        {
                            System.out.println(
                                    "Received Stats, Bytes Scanned: " + event.getDetails().getBytesScanned()
                                            +  " Bytes Processed: " + event.getDetails().getBytesProcessed());
                        }

                        /*
                         * An End Event informs that the request has finished successfully.
                         */
                        @Override
                        public void visit(SelectObjectContentEvent.EndEvent event)
                        {
                            isResultComplete.set(true);
                            System.out.println("Received End Event. Result is complete.");
                        }
                    }
            );

            copy(resultInputStream, fileOutputStream);
        }

        /*
         * The End Event indicates all matching records have been transmitted.
         * If the End Event is not received, the results may be incomplete.
         */
        if (!isResultComplete.get()) {
            throw new Exception("S3 Select request was incomplete as End Event was not received.");
        }
    }

    private static SelectObjectContentRequest generateBaseCSVRequest(String bucket, String key, String query) {
        SelectObjectContentRequest request = new SelectObjectContentRequest();
        request.setBucketName(bucket);
        request.setKey(key);
        request.setExpression(query);
        request.setExpressionType(ExpressionType.SQL);

        InputSerialization inputSerialization = new InputSerialization();
        inputSerialization.setCsv(new CSVInput());
        inputSerialization.setCompressionType(CompressionType.NONE);
        request.setInputSerialization(inputSerialization);

        OutputSerialization outputSerialization = new OutputSerialization();
        outputSerialization.setCsv(new CSVOutput());
        request.setOutputSerialization(outputSerialization);

        return request;
    }
}
```


# Viewing settings for an S3 Bucket Key<a name="viewing-bucket-key-settings"></a>

You can view settings for an S3 Bucket Key at the bucket or object level using the Amazon S3 console, REST API, AWS CLI, or AWS SDKs\.

S3 Bucket Keys decrease request traffic from Amazon S3 to AWS KMS and reduce the cost of server\-side encryption using AWS Key Management Service \(SSE\-KMS\)\. For more information, see [Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys](bucket-key.md)\. 

## Using the Amazon S3 console<a name="bucket-key-settings-console"></a>

You can use the S3 console to view settings for an S3 Bucket Key at the bucket or object level\. For more information, see [Configuring an S3 Bucket Key in the S3 console](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/s3-bucket-key.html)\.

## Using the REST API<a name="bucket-key-settings-rest"></a>

**To return bucket\-level S3 Bucket Key settings**  
To return encryption information for a bucket, including settings for an S3 Bucket Key, use the `GetBucketEncryption` operation\. S3 Bucket Key settings are returned in the response body in the `ServerSideEncryptionConfiguration` with the `BucketKeyEnabled` setting\. For more information, see [GetBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetBucketEncryption.html) in the *Amazon S3 API Reference*\. 

**To return object\-level settings for an S3 Bucket Key**  
To return the S3 Bucket Key status for an object, use the `HeadObject` operation\. `HeadObject` returns the `x-amz-server-side-encryption-bucket-key-enabled` response header to show whether an S3 Bucket Key is enabled or disabled for the object\. For more information, see [HeadObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadObject.html) in the *Amazon S3 API Reference*\. 

The following API operations also return the `x-amz-server-side-encryption-bucket-key-enabled` response header if an S3 Bucket Key is configured for an object: 
+ [PutObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html) 
+ [PostObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html) 
+ [CopyObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CopyObject.html) 
+ [CreateMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateMultipartUpload.html) 
+ [UploadPartCopy](https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPartCopy.html) 
+ [UploadPart](https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPart.html) 
+ [CompleteMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CompleteMultipartUpload.html) 
+ [GetObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html) 


# Copy an Object Using the AWS SDK for Java<a name="CopyingObjectUsingJava"></a>

**Example**  
The following example shows how to copy an object in Amazon S3 using the AWS SDK for Java\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CopyObjectRequest;

import java.io.IOException;

public class CopyObjectSingleOperation {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String sourceKey = "*** Source object key *** ";
        String destinationKey = "*** Destination object key ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Copy the object into a new object in the same bucket.
            CopyObjectRequest copyObjRequest = new CopyObjectRequest(bucketName, sourceKey, bucketName, destinationKey);
            s3Client.copyObject(copyObjRequest);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Using an AWS KMS CMK to encrypt your metrics exports<a name="storage_lens_encrypt_permissions"></a>

To grant Amazon S3 Storage Lens permission to encrypt using a customer managed AWS Key Management Service \(AWS KMS\) customer master key \(CMK\), you must use a key policy\. To update your key policy so that you can use an AWS KMS CMK to encrypt your S3 Storage Lens metrics exports, follow these steps\. 

**To grant permissions to encrypt using your AWS KMS CMK**

1. Sign into the AWS Management Console using the AWS account that owns the customer managed CMK\.

1. Open the AWS KMS console at [https://console\.aws\.amazon\.com/kms](https://console.aws.amazon.com/kms)\.

1. To change the AWS Region, use the **Region selector** in the upper\-right corner of the page\.

1. In the navigation pane, choose **Customer managed keys**\.

1. Under **Customer managed keys**, choose the key that you want to use to encrypt the metrics exports\. CMKs are Region\-specific and must be in the same Region as the metrics export destination S3 bucket\.

1. Under **Key policy**, choose **Switch to policy view**\.

1. To update the key policy, choose **Edit**\.

1. Under **Edit key policy**, add the following key policy to the existing key policy\.

   ```
   {
       "Sid": "Allow Amazon S3 Storage Lens use of the CMK",
       "Effect": "Allow",
       "Principal": {
           "Service": "storage-lens.s3.amazonaws.com"
       },
       "Action": [
           "kms:GenerateDataKey"
       ],
       "Resource": "*"
   }
   ```

1. Choose **Save changes**\.

For more information about creating AWS KMS customer managed CMKs and using key policies, see the following topics in the *AWS Key Management Service Developer Guide*:
+ [Getting started](https://docs.aws.amazon.com/kms/latest/developerguide/getting-started.html)
+ [Using key policies in AWS KMS](https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html)

You can also use the AWS KMS PUT key policy \([ PutKeyPolicy](http://amazonaws.com/kms/latest/APIReference/API_PutKeyPolicy.html)\) to copy the key policy to the customer managed CMK that you want to use to encrypt the metrics exports using the REST API, AWS CLI, and SDKs\.


# HTML forms \(AWS signature version 2\)<a name="HTTPPOSTForms"></a>

**Topics**
+ [HTML form encoding](#HTTPPOSTFormEncoding)
+ [HTML form declaration](#HTTPPOSTFormDeclaration)
+ [HTML form fields](#HTTPPOSTFormFields)
+ [Policy construction](#HTTPPOSTConstructPolicy)
+ [Constructing a signature](#HTTPPOSTConstructingPolicySignature)
+ [Redirection](#HTTPPOSTConstructingPolicyRedirection)

When you communicate with Amazon S3, you normally use the REST or SOAP API to perform put, get, delete, and other operations\. With POST, users upload data directly to Amazon S3 through their browsers, which cannot process the SOAP API or create a REST `PUT` request\.

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

To allow users to upload content to Amazon S3 by using their browsers, you use HTML forms\. HTML forms consist of a form declaration and form fields\. The form declaration contains high\-level information about the request\. The form fields contain detailed information about the request, as well as the policy that is used to authenticate it and ensure that it meets the conditions that you specify\.

**Note**  
The form data and boundaries \(excluding the contents of the file\) cannot exceed 20 KB\.

This section explains how to use HTML forms\.

## HTML form encoding<a name="HTTPPOSTFormEncoding"></a>

The form and policy must be UTF\-8 encoded\. You can apply UTF\-8 encoding to the form by specifying it in the HTML heading or as a request header\.

**Note**  
 The HTML form declaration does not accept query string authentication parameters\. 

The following is an example of UTF\-8 encoding in the HTML heading:

```
1. <html>
2.   <head>
3.     ...
4.     <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
5.     ...
6.   </head>
7.   <body>
```

The following is an example of UTF\-8 encoding in a request header:

```
1. Content-Type: text/html; charset=UTF-8
```

## HTML form declaration<a name="HTTPPOSTFormDeclaration"></a>

The form declaration has three components: the action, the method, and the enclosure type\. If any of these values is improperly set, the request fails\.

The action specifies the URL that processes the request, which must be set to the URL of the bucket\. For example, if the name of your bucket is `awsexamplebucket1` and the Region is US West \(N\. California\), the URL is `https://awsexamplebucket1.s3.us-west-1.amazonaws.com/`\.

**Note**  
The key name is specified in a form field\.

The method must be POST\.

The enclosure type \(enctype\) must be specified and must be set to multipart/form\-data for both file uploads and text area uploads\. For more information, go to [RFC 1867](http://www.ietf.org/rfc/rfc1867.txt)\.

**Example**  
The following example is a form declaration for the bucket "awsexamplebucket1"\.  

```
1. <form action="https://awsexamplebucket1.s3.us-west-1.amazonaws.com/" method="post"
2. 
3. enctype="multipart/form-data">
```

## HTML form fields<a name="HTTPPOSTFormFields"></a>

The following table describes fields that can be used within an HTML form\.

**Note**  
The variable `${filename}` is automatically replaced with the name of the file provided by the user and is recognized by all form fields\. If the browser or client provides a full or partial path to the file, only the text following the last slash \(/\) or backslash \(\\\) will be used\. For example, "C:\\Program Files\\directory1\\file\.txt" will be interpreted as "file\.txt"\. If no file or file name is provided, the variable is replaced with an empty string\.


| Field name | Description | Required | 
| --- | --- | --- | 
| AWSAccessKeyId |  The AWS Access Key ID of the owner of the bucket who grants an anonymous user access for a request that satisfies the set of constraints in the policy\. This field is required if the request includes a policy document\.  |  Conditional  | 
| acl |  An Amazon S3 access control list \(ACL\)\. If an invalid access control list is specified, an error is generated\. For more information on ACLs, see [Access control lists](Introduction.md#S3_ACLs)\. Type: String Default: private  Valid Values: `private \| public-read \| public-read-write \| aws-exec-read \| authenticated-read \| bucket-owner-read \| bucket-owner-full-control `   |  No  | 
| Cache\-Control, Content\-Type, Content\-Disposition, Content\-Encoding, Expires |  REST\-specific headers\. For more information, see [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)\.  |  No  | 
| key |  The name of the uploaded key\. To use the filename provided by the user, use the $\{filename\} variable\. For example, if user Betty uploads the file lolcatz\.jpg and you specify /user/betty/$\{filename\}, the file is stored as /user/betty/lolcatz\.jpg\. For more information, see [Object key and metadata](UsingMetadata.md)\.  |  Yes  | 
| policy |  Security policy describing what is permitted in the request\. Requests without a security policy are considered anonymous and will succeed only on publicly writable buckets\.   |  No  | 
| success\_action\_redirect, redirect |  The URL to which the client is redirected upon successful upload\. Amazon S3 appends the bucket, key, and etag values as query string parameters to the URL\. If success\_action\_redirect is not specified, Amazon S3 returns the empty document type specified in the success\_action\_status field\. If Amazon S3 cannot interpret the URL, it ignores the field\. If the upload fails, Amazon S3 displays an error and does not redirect the user to a URL\. For more information, see [Redirection](#HTTPPOSTConstructingPolicyRedirection)\.    The redirect field name is deprecated and support for the redirect field name will be removed in the future\.    |  No  | 
| success\_action\_status |  The status code returned to the client upon successful upload if success\_action\_redirect is not specified\. Valid values are 200, 201, or 204 \(default\)\. If the value is set to 200 or 204, Amazon S3 returns an empty document with a 200 or 204 status code\. If the value is set to 201, Amazon S3 returns an XML document with a 201 status code\. For information about the content of the XML document, see [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)\. If the value is not set or if it is set to an invalid value, Amazon S3 returns an empty document with a 204 status code\.   Some versions of the Adobe Flash player do not properly handle HTTP responses with an empty body\. To support uploads through Adobe Flash, we recommend setting `success_action_status` to 201\.   |  No  | 
| signature |   The HMAC signature constructed by using the secret access key that corresponds to the provided AWSAccessKeyId\. This field is required if a policy document is included with the request\.  For more information, see [Using Auth Access](https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html) \.  |  Conditional  | 
| x\-amz\-security\-token |  A security token used by  session credentials  If the request is using Amazon DevPay then it requires two `x-amz-security-token` form fields: one for the product token and one for the user token\.  If the request is using session credentials, then it requires one `x-amz-security-token` form\. For more information, see [Temporary Security Credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html) in the *IAM User Guide*\.   |  No  | 
| Other field names prefixed with x\-amz\-meta\- |  User\-specified metadata\.  Amazon S3 does not validate or use this data\. For more information, see [ PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)\.  |  No  | 
| file |  File or text content\.  The file or content must be the last field in the form\. Any fields below it are ignored\. You cannot upload more than one file at a time\.  |  Yes  | 

## Policy construction<a name="HTTPPOSTConstructPolicy"></a>

**Topics**
+ [Expiration](#HTTPPOSTExpiration)
+ [Conditions](#PolicyConditions)
+ [Condition matching](#ConditionMatching)
+ [Character escaping](#HTTPPOSTEscaping)

The policy is a UTF\-8 and Base64\-encoded JSON document that specifies conditions that the request must meet and is used to authenticate the content\. Depending on how you design your policy documents, you can use them per upload, per user, for all uploads, or according to other designs that meet your needs\.

**Note**  
 Although the policy document is optional, we highly recommend it over making a bucket publicly writable\. 

The following is an example of a policy document:

```
 1. { "expiration": "2007-12-01T12:00:00.000Z",
 2. 
 3.   "conditions": [
 4. 
 5.     {"acl": "public-read" },
 6. 
 7.     {"bucket": "awsexamplebucket1" },
 8. 
 9.     ["starts-with", "$key", "user/eric/"],
10. 
11.   ]
12. 
13. }
```

The policy document contains the expiration and conditions\.

### Expiration<a name="HTTPPOSTExpiration"></a>

The expiration element specifies the expiration date of the policy in ISO 8601 UTC date format\. For example, "2007\-12\-01T12:00:00\.000Z" specifies that the policy is not valid after midnight UTC on 2007\-12\-01\. Expiration is required in a policy\.

### Conditions<a name="PolicyConditions"></a>

The conditions in the policy document validate the contents of the uploaded object\. Each form field that you specify in the form \(except AWSAccessKeyId, signature, file, policy, and field names that have an x\-ignore\- prefix\) must be included in the list of conditions\. 

**Note**  
If you have multiple fields with the same name, the values must be separated by commas\. For example, if you have two fields named "x\-amz\-meta\-tag" and the first one has a value of "Ninja" and second has a value of "Stallman", you would set the policy document to `Ninja,Stallman`\.  
 All variables within the form are expanded before the policy is validated\. Therefore, all condition matching should be performed against the expanded fields\. For example, if you set the key field to `user/betty/${filename}`, your policy might be `[ "starts-with", "$key", "user/betty/" ]`\. Do not enter `[ "starts-with", "$key", "user/betty/${filename}" ]`\. For more information, see [Condition matching](#ConditionMatching)\. 

The following table describes policy document conditions\.


| Element name | Description | 
| --- | --- | 
| acl |  Specifies conditions that the ACL must meet\.  Supports exact matching and `starts-with`\.   | 
| content\-length\-range |  Specifies the minimum and maximum allowable size for the uploaded content\.  Supports range matching\.  | 
| Cache\-Control, Content\-Type, Content\-Disposition, Content\-Encoding, Expires |  REST\-specific headers\.  Supports exact matching and `starts-with`\.  | 
| key |  The name of the uploaded key\. Supports exact matching and `starts-with`\.   | 
| success\_action\_redirect, redirect  |  The URL to which the client is redirected upon successful upload\. Supports exact matching and `starts-with`\.   | 
| success\_action\_status |  The status code returned to the client upon successful upload if success\_action\_redirect is not specified\. Supports exact matching\.   | 
| x\-amz\-security\-token |  Amazon DevPay security token\.  Each request that uses Amazon DevPay requires two `x-amz-security-token` form fields: one for the product token and one for the user token\. As a result, the values must be separated by commas\. For example, if the user token is `eW91dHViZQ==` and the product token is `b0hnNVNKWVJIQTA=`, you set the policy entry to: `{ "x-amz-security-token": "eW91dHViZQ==,b0hnNVNKWVJIQTA=" }`\.   | 
| Other field names prefixed with x\-amz\-meta\- |  User\-specified metadata\.  Supports exact matching and `starts-with`\.   | 

**Note**  
 If your toolkit adds additional fields \(e\.g\., Flash adds filename\), you must add them to the policy document\. If you can control this functionality, prefix `x-ignore-` to the field so Amazon S3 ignores the feature and it won't affect future versions of this feature\. 

### Condition matching<a name="ConditionMatching"></a>

The following table describes condition matching types\. Although you must specify one condition for each form field that you specify in the form, you can create more complex matching criteria by specifying multiple conditions for a form field\.


|  Condition  |  Description  | 
| --- | --- | 
|  Exact Matches  |  Exact matches verify that fields match specific values\. This example indicates that the ACL must be set to public\-read: <pre>{"acl": "public-read" }</pre> This example is an alternate way to indicate that the ACL must be set to public\-read: <pre>[ "eq", "$acl", "public-read" ]</pre>  | 
|  Starts With  |  If the value must start with a certain value, use starts\-with\. This example indicates that the key must start with user/betty: <pre>["starts-with", "$key", "user/betty/"]</pre>  | 
|  Matching Any Content  |  To configure the policy to allow any content within a field, use starts\-with with an empty value\. This example allows any success\_action\_redirect: <pre>["starts-with", "$success_action_redirect", ""]</pre>  | 
|  Specifying Ranges  |  For fields that accept ranges, separate the upper and lower ranges with a comma\. This example allows a file size from 1 to 10 megabytes: <pre>["content-length-range", 1048579, 10485760]</pre>  | 

### Character escaping<a name="HTTPPOSTEscaping"></a>

The following table describes characters that must be escaped within a policy document\.


|  Escape sequence  |  Description  | 
| --- | --- | 
|  \\\\  |  Backslash  | 
|  \\$  |  Dollar sign  | 
|  \\b  |  Backspace  | 
|  \\f  |  Form feed  | 
|  \\n  |  New line  | 
|  \\r  |  Carriage return  | 
|  \\t  |  Horizontal tab  | 
|  \\v  |  Vertical tab  | 
|  \\u*xxxx*  |  All Unicode characters  | 

## Constructing a signature<a name="HTTPPOSTConstructingPolicySignature"></a>


| Step | Description | 
| --- | --- | 
| 1 |  Encode the policy by using UTF\-8\.  | 
| 2 |  Encode those UTF\-8 bytes by using Base64\.  | 
| 3 |  Sign the policy with your secret access key by using HMAC SHA\-1\.  | 
| 4 |  Encode the SHA\-1 signature by using Base64\.  | 

For general information about authentication, see [Using Auth Access](https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html) \.

## Redirection<a name="HTTPPOSTConstructingPolicyRedirection"></a>

This section describes how to handle redirects\.

### General redirection<a name="HTTPPOSTGeneralRedirection"></a>

On completion of the POST request, the user is redirected to the location that you specified in the `success_action_redirect` field\. If Amazon S3 cannot interpret the URL, it ignores the `success_action_redirect` field\.

If `success_action_redirect` is not specified, Amazon S3 returns the empty document type specified in the `success_action_status` field\.

If the POST request fails, Amazon S3 displays an error and does not provide a redirect\.

### Pre\-upload redirection<a name="HTTPPOSTPreUpload"></a>

If your bucket was created using <CreateBucketConfiguration>, your end users might require a redirect\. If this occurs, some browsers might handle the redirect incorrectly\. This is relatively rare but is most likely to occur right after a bucket is created\.


# Making requests using federated user temporary credentials \- AWS SDK for Java<a name="AuthUsingTempFederationTokenJava"></a>

You can provide temporary security credentials for your federated users and applications so that they can send authenticated requests to access your AWS resources\. When requesting these temporary credentials, you must provide a user name and an IAM policy that describes the resource permissions that you want to grant\. By default, the session duration is one hour\. You can explicitly set a different duration value when requesting the temporary security credentials for federated users and applications\.

**Note**  
For added security when requesting temporary security credentials for federated users and applications, we recommend that you use a dedicated IAM user with only the necessary access permissions\. The temporary user you create can never get more permissions than the IAM user who requested the temporary security credentials\. For more information, see [ AWS Identity and Access Management FAQs ](https://aws.amazon.com/iam/faqs/#What_are_the_best_practices_for_using_temporary_security_credentials)\.

To provide security credentials and send authenticated request to access resources, do the following:
+ Create an instance of the `AWSSecurityTokenServiceClient` class\. For information about providing credentials, see [Using the AWS SDK for Java](UsingTheMPJavaAPI.md)\.
+ Start a session by calling the `getFederationToken()` method of the Security Token Service \(STS\) client\. Provide session information, including the user name and an IAM policy, that you want to attach to the temporary credentials\. You can provide an optional session duration\. This method returns your temporary security credentials\.
+ Package the temporary security credentials in an instance of the `BasicSessionCredentials` object\. You use this object to provide the temporary security credentials to your Amazon S3 client\.
+ Create an instance of the `AmazonS3Client` class using the temporary security credentials\. You send requests to Amazon S3 using this client\. If you send requests using expired credentials, Amazon S3 returns an error\. 

**Example**  
The example lists keys in the specified S3 bucket\. In the example, you obtain temporary security credentials for a two\-hour session for your federated user and use the credentials to send authenticated requests to Amazon S3\. To run the example, you need to create an IAM user with an attached policy that allows the user to request temporary security credentials and list your AWS resources\. The following policy accomplishes this:  

```
 1. {
 2.   "Statement":[{
 3.       "Action":["s3:ListBucket",
 4.         "sts:GetFederationToken*"
 5.       ],
 6.       "Effect":"Allow",
 7.       "Resource":"*"
 8.     }
 9.   ]
10. }
```
For more information about how to create an IAM user, see [Creating Your First IAM User and Administrators Group](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html) in the *IAM User Guide*\.   
After creating an IAM user and attaching the preceding policy, you can run the following example\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicSessionCredentials;
import com.amazonaws.auth.policy.Policy;
import com.amazonaws.auth.policy.Resource;
import com.amazonaws.auth.policy.Statement;
import com.amazonaws.auth.policy.Statement.Effect;
import com.amazonaws.auth.policy.actions.S3Actions;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ObjectListing;
import com.amazonaws.services.securitytoken.AWSSecurityTokenService;
import com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder;
import com.amazonaws.services.securitytoken.model.Credentials;
import com.amazonaws.services.securitytoken.model.GetFederationTokenRequest;
import com.amazonaws.services.securitytoken.model.GetFederationTokenResult;

import java.io.IOException;

public class MakingRequestsWithFederatedTempCredentials {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Specify bucket name ***";
        String federatedUser = "*** Federated user name ***";
        String resourceARN = "arn:aws:s3:::" + bucketName;

        try {
            AWSSecurityTokenService stsClient = AWSSecurityTokenServiceClientBuilder
                    .standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            GetFederationTokenRequest getFederationTokenRequest = new GetFederationTokenRequest();
            getFederationTokenRequest.setDurationSeconds(7200);
            getFederationTokenRequest.setName(federatedUser);

            // Define the policy and add it to the request.
            Policy policy = new Policy();
            policy.withStatements(new Statement(Effect.Allow)
                    .withActions(S3Actions.ListObjects)
                    .withResources(new Resource(resourceARN)));
            getFederationTokenRequest.setPolicy(policy.toJson());

            // Get the temporary security credentials.
            GetFederationTokenResult federationTokenResult = stsClient.getFederationToken(getFederationTokenRequest);
            Credentials sessionCredentials = federationTokenResult.getCredentials();

            // Package the session credentials as a BasicSessionCredentials
            // object for an Amazon S3 client object to use.
            BasicSessionCredentials basicSessionCredentials = new BasicSessionCredentials(
                    sessionCredentials.getAccessKeyId(),
                    sessionCredentials.getSecretAccessKey(),
                    sessionCredentials.getSessionToken());
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new AWSStaticCredentialsProvider(basicSessionCredentials))
                    .withRegion(clientRegion)
                    .build();

            // To verify that the client works, send a listObjects request using 
            // the temporary security credentials.
            ObjectListing objects = s3Client.listObjects(bucketName);
            System.out.println("No. of Objects = " + objects.getObjectSummaries().size());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Related resources<a name="RelatedResources005"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Bucket owner condition<a name="bucket-owner-condition"></a>

Amazon S3 bucket owner condition ensures that the buckets you use in your S3 operations belong to the AWS accounts that you expect\.

Most S3 operations read from or write to specific S3 buckets\. These operations include uploading, copying, and downloading objects, retrieving or modifying bucket configurations, and retrieving or modifying object configurations\. When you perform these operations, you specify the bucket that you want to use by including its name with the request\. For example, to retrieve an object from S3, you make a request that specifies the name of a bucket and the object key to retrieve from that bucket\.

Because Amazon S3 identifies buckets based on their names, an application that uses an incorrect bucket name in a request could inadvertently perform operations against a different bucket than expected\. To help avoid unintentional bucket interactions in situations like this, you can use *bucket owner condition*\. Bucket owner condition enables you to verify that the target bucket is owned by the expected AWS account, providing an additional layer of assurance that your S3 operations are having the effects you intend\.

**Topics**
+ [When to use bucket owner condition](#bucket-owner-condition-when-to-use)
+ [Verifying a bucket owner](#bucket-owner-condition-use)
+ [Examples](#bucket-owner-condition-examples)
+ [Restrictions and limitations](#bucket-owner-condition-restrictions-limitations)

## When to use bucket owner condition<a name="bucket-owner-condition-when-to-use"></a>

We recommend using bucket owner condition whenever you perform a supported S3 operation and know the account ID of the expected bucket owner\. Bucket owner condition is available for all S3 object operations and most S3 bucket operations\. For a list of S3 operations that don't support bucket owner condition, see [Restrictions and limitations](#bucket-owner-condition-restrictions-limitations)\.

To see the benefit of using bucket owner condition, consider the following scenario involving AWS customer Bea:

1. Bea develops an application that uses Amazon S3\. During development, Bea uses her testing\-only AWS account to create a bucket named `bea-data-test`, and configures her application to make requests to `bea-data-test`\.

1. Bea deploys her application, but forgets to reconfigure the application to use a bucket in her production AWS account\.

1. In production, Bea's application makes requests to `bea-data-test`, which succeed\. This results in production data being written to the bucket in Bea's test account\.

Bea can help protect against situations like this by using bucket owner condition\. With bucket owner condition, Bea can include the AWS account ID of the expected bucket owner in her requests\. Amazon S3 then checks the account ID of the bucket owner before processing each request\. If the actual bucket owner doesn't match the expected bucket owner, the request fails\.

If Bea uses bucket owner condition, the scenario described earlier won't result in Bea's application inadvertently writing to a test bucket\. Instead, the requests that her application makes at step 3 will fail with a descriptive error message\. By using bucket owner condition, Bea helps eliminate the risk of accidentally interacting with buckets in the wrong AWS account\.

## Verifying a bucket owner<a name="bucket-owner-condition-use"></a>

To use bucket owner condition, you include a parameter with your request that specifies the expected bucket owner\. Most S3 operations involve only a single bucket, and require only this single parameter to use bucket owner condition\. For `CopyObject` operations, this first parameter specifies the expected owner of the destination bucket, and you include a second parameter to specify the expected owner of the source bucket\.

When you make a request that includes a bucket owner condition parameter, S3 checks the account ID of the bucket owner against the specified parameter before processing the request\. If the parameter matches the bucket owner's account ID, S3 processes the request\. If the parameter doesn't match the bucket owner's account ID, the request fails with a descriptive error message\.

You can use bucket owner condition with the AWS Command Line Interface \(AWS CLI\), AWS SDKs, and Amazon S3 REST APIs\. When using bucket owner condition with the AWS CLI and Amazon S3 REST APIs, use the following parameter names\.


****  

| Access method | Parameter for non\-copy operations | Copy operation source parameter | Copy operation destination parameter | 
| --- | --- | --- | --- | 
| AWS CLI | \-\-expected\-bucket\-owner | \-\-expected\-source\-bucket\-owner | \-\-expected\-bucket\-owner | 
| Amazon S3 REST APIs | x\-amz\-expected\-bucket\-owner header | x\-amz\-source\-expected\-bucket\-owner header | x\-amz\-expected\-bucket\-owner header | 

The parameter names that are required to use bucket owner condition with the AWS SDKs vary depending on the language\. To determine the required parameters, see the SDK documentation for your desired language\. You can find the SDK documentation at [Tools to Build on AWS](https://aws.amazon.com/tools/)\.

## Examples<a name="bucket-owner-condition-examples"></a>

The following examples show how you can implement bucket owner condition in Amazon S3 using the AWS CLI or the AWS SDK for Java 2\.x\.

**Example**  
***Example: Upload an object***  
The following example uploads an object to S3 bucket `DOC-EXAMPLE-BUCKET1`, using bucket owner condition to ensure that `DOC-EXAMPLE-BUCKET1` is owned by AWS account `111122223333`\.  

```
aws s3api put-object \
                 --bucket DOC-EXAMPLE-BUCKET1 --key exampleobject --body example_file.txt \
                 --expected-bucket-owner 111122223333
```

```
public void putObjectExample() {
    S3Client s3Client = S3Client.create();;
    PutObjectRequest request = PutObjectRequest.builder()
            .bucket("DOC-EXAMPLE-BUCKET1")
            .key("exampleobject")
            .expectedBucketOwner("111122223333")
            .build();
    Path path = Paths.get("example_file.txt");
    s3Client.putObject(request, path);
}
```

**Example**  
***Example: Copy an object***  
The following example copies the object `object1` from S3 bucket `DOC-EXAMPLE-BUCKET1` to S3 bucket `DOC-EXAMPLE-BUCKET2`\. It uses bucket owner condition to ensure that the buckets are owned by the expected accounts according to the following table\.   


****  

| Bucket | Expected owner | 
| --- | --- | 
| DOC\-EXAMPLE\-BUCKET1 | 111122223333 | 
| DOC\-EXAMPLE\-BUCKET2 | 444455556666 | 

```
aws s3api copy-object --copy-source DOC-EXAMPLE-BUCKET1/object1 \
                            --bucket DOC-EXAMPLE-BUCKET2 --key object1copy \
                            --expected-source-bucket-owner 111122223333 --expected-bucket-owner 444455556666
```

```
public void copyObjectExample() {
        S3Client s3Client = S3Client.create();
        CopyObjectRequest request = CopyObjectRequest.builder()
                .copySource("DOC-EXAMPLE-BUCKET1/object1")
                .destinationBucket("DOC-EXAMPLE-BUCKET2")
                .destinationKey("object1copy")
                .expectedSourceBucketOwner("111122223333")
                .expectedBucketOwner("444455556666")
                .build();
        s3Client.copyObject(request);
    }
```

**Example**  
***Example: Retrieve a bucket policy***  
The following example retrieves the access policy for S3 bucket `DOC-EXAMPLE-BUCKET1`, using bucket owner condition to ensure that `DOC-EXAMPLE-BUCKET1` is owned by AWS account `111122223333`\.  

```
aws s3api get-bucket-policy --bucket DOC-EXAMPLE-BUCKET1 --expected-bucket-owner 111122223333
```

```
public void getBucketPolicyExample() {
    S3Client s3Client = S3Client.create();
    GetBucketPolicyRequest request = GetBucketPolicyRequest.builder()
            .bucket("DOC-EXAMPLE-BUCKET1")
            .expectedBucketOwner("111122223333")
            .build();
    try {
        GetBucketPolicyResponse response = s3Client.getBucketPolicy(request);
    }
    catch (S3Exception e) {
        // The call was transmitted successfully, but Amazon S3 couldn't process 
        // it, so it returned an error response.
        e.printStackTrace();
    }
}
```

## Restrictions and limitations<a name="bucket-owner-condition-restrictions-limitations"></a>

Amazon S3 bucket owner condition has the following restrictions and limitations:
+ The value of the bucket owner condition parameter must be an AWS account ID \(12\-digit alphanumeric string\)\. Service principals aren't supported\. 
+ Bucket owner condition isn't available for [CreateBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateBucket.html), [ListBuckets](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListBuckets.html), or any of the operations included in [AWS S3 Control](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations_AWS_S3_Control.html)\. Amazon S3 ignores any bucket owner condition parameters included with requests to these operations\. 
+ Bucket owner condition only verifies that the account specified in the verification parameter owns the bucket\. Bucket owner condition doesn't check the configuration of the bucket\. It also doesn't guarantee that the bucket's configuration meets any specific conditions or matches any past state\. 


# Deleting an object using the AWS SDK for \.NET<a name="DeletingOneObjectUsingNetSDK"></a>

When you delete an object from a non\-versioned bucket, the object is removed\. If you have S3 Versioning enabled on the bucket, you have the following options:
+ Delete a specific version of an object by specifying a version ID\.
+ Delete an object without specifying a version ID\. Amazon S3 adds a delete marker\. For more information about delete markers, see [Object Versioning](ObjectVersioning.md)\.

The following examples show how to delete an object from both versioned and non\-versioned buckets\. For more information about S3 Versioning, see [Object Versioning](ObjectVersioning.md)\. 

**Example Deleting an object from a non\-versioned bucket**  
The following C\# example deletes an object from a non\-versioned bucket\. The example assumes that the objects don't have version IDs, so you don't specify version IDs\. You specify only the object key\. For information about how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.   

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DeleteObjectNonVersionedBucketTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string keyName = "*** object key ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            DeleteObjectNonVersionedBucketAsync().Wait();
        }
        private static async Task DeleteObjectNonVersionedBucketAsync()
        {
            try
            {
                var deleteObjectRequest = new DeleteObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName
                };

                Console.WriteLine("Deleting an object");
                await client.DeleteObjectAsync(deleteObjectRequest);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when deleting an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when deleting an object", e.Message);
            }
        }
    }
}
```

**Example Deleting an object from a versioned bucket**  
The following C\# example deletes an object from a versioned bucket\. It deletes a specific version of the object by specifying the object key name and version ID\.   
The code performs the following tasks:  

1. Enables S3 Versioning on a bucket that you specify \(if Versioning is already enabled, this has no effect\)\.

1. Adds a sample object to the bucket\. In response, Amazon S3 returns the version ID of the newly added object\. The example uses this version ID in the delete request\.

1. Deletes the sample object by specifying both the object key name and a version ID\.
**Note**  
You can also get the version ID of an object by sending a `ListVersions` request:  

   ```
   var listResponse = client.ListVersions(new ListVersionsRequest { BucketName = bucketName, Prefix = keyName }); 
   ```
For information about how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.   

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DeleteObjectVersion
    {
        private const string bucketName = "*** versioning-enabled bucket name ***";
        private const string keyName = "*** Object Key Name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            CreateAndDeleteObjectVersionAsync().Wait();
        }

        private static async Task CreateAndDeleteObjectVersionAsync()
        {
            try
            {
                // Add a sample object. 
                string versionID = await PutAnObject(keyName);

                // Delete the object by specifying an object key and a version ID.
                DeleteObjectRequest request = new DeleteObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName,
                    VersionId = versionID
                };
                Console.WriteLine("Deleting an object");
                await client.DeleteObjectAsync(request);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when deleting an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when deleting an object", e.Message);
            }
        }

        static async Task<string> PutAnObject(string objectKey)
        {
            PutObjectRequest request = new PutObjectRequest
            {
                BucketName = bucketName,
                Key = objectKey,
                ContentBody = "This is the content body!"
            };
            PutObjectResponse response = await client.PutObjectAsync(request);
            return response.VersionId;
        }
    }
}
```


# Monitoring progress with replication metrics and Amazon S3 event notifications<a name="replication-metrics"></a>

S3 replication metrics provide detailed metrics for the replication rules in your replication configuration\. With replication metrics, you can monitor minute\-by\-minute progress of replication by tracking bytes pending, operations pending, and replication latency\. Additionally, you can set up Amazon S3 Event Notifications to receive replication failure events to assist in troubleshooting any configuration issues\.

When enabled, S3 replication metrics publish the following metrics to Amazon CloudWatch:

**Bytes Pending Replication**—The total number of bytes of objects pending replication for a given replication rule\.

**Replication Latency**—The maximum number of seconds by which the replication destination buckets are behind the source bucket for a given replication rule\.

**Operations Pending Replication**—The number of operations pending replication for a given replication rule\. Operations include objects, delete markers, tags, ACLs, and Object Lock operations\.

**Note**  
S3 replication metrics are billed at the same rate as Amazon CloudWatch custom metrics\. For information, see [Amazon CloudWatch pricing](https://aws.amazon.com/cloudwatch/pricing/)\.

S3 replication metrics are turned on automatically when you enable S3 Replication Time Control \(S3 RTC\)\. S3 RTC includes other features such as a service level agreement \(SLA\) and notifications for missed thresholds\. For more information, see [Meet compliance requirements using S3 RTC](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-time-control.html)\.

**Topics**
+ [Enabling S3 replication metrics](#enabling-replication-metrics)
+ [Viewing replication metrics in the AWS Management Console](#viewing-replication-metrics)
+ [Receiving replication failure events with Amazon S3 event notifications](#replication-metrics-events)

## Enabling S3 replication metrics<a name="enabling-replication-metrics"></a>

You can start using S3 replication metrics with a new or existing replication rule\. You can choose to apply your replication rule to an entire S3 bucket, or to Amazon S3 objects with a specific prefix or tag\.

To enable replication metrics using the Amazon S3 console, see [How do I add a replication rule to an S3 bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html) in the *Amazon Simple Storage Service Console User Guide*\. This topic provides instructions for enabling S3 replication metrics in your replication configuration when buckets are owned by the same or different AWS accounts\.

To enable replication metrics using the AWS Command Line Interface \(AWS CLI\), you must add a replication configuration to the source bucket with `Metrics` enabled\. In this example configuration, objects under the prefix *Tax* are replicated to the destination bucket *DOC\-EXAMPLE\-BUCKET*, and metrics are generated for those objects\.

```
{
    "Rules": [
        {
            "Status": "Enabled",
            "Filter": {
                "Prefix": "Tax"
            },
            "Destination": {
                "Bucket": "arn:aws:s3:::DOC-EXAMPLE-BUCKET",
                "Metrics": {
                    "Status": "Enabled"
                }
            },
            "Priority": 1
        }
    ],
    "Role": "IAM-Role-ARN"
}
```

For full instructions on creating replication rules through the AWS CLI, see [Example 1: Configuring replication when the source and destination buckets are owned by the same account](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-walkthrough1.html) in the [Replication walkthroughs](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-example-walkthroughs.html) section\.

## Viewing replication metrics in the AWS Management Console<a name="viewing-replication-metrics"></a>

For instructions on viewing metrics, see [How do I view replication metrics?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/replication-metrics.html) in the *Amazon Simple Storage Service Console User Guide*\.

## Receiving replication failure events with Amazon S3 event notifications<a name="replication-metrics-events"></a>

Amazon S3 event notifications can notify you in the rare instance when objects do not replicate to their destination Region\. Amazon S3 events are available through Amazon Simple Queue Service \(Amazon SQS\), Amazon Simple Notification Service \(Amazon SNS\), or AWS Lambda\. For more information, see [Configuring Amazon S3 event notifications](https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html)\.


# Deleting objects<a name="DeletingObjects"></a>

**Topics**
+ [Deleting objects from a version\-enabled bucket](#DeletingObjectsfromaVersion-EnabledBucket)
+ [Deleting objects from an MFA\-enabled bucket](#DeletingObjectsfromanMFA-EnabledBucket)
+ [Related resources](#RelatedResources001)
+ [Deleting one object per request](DeletingOneObject.md)
+ [Deleting multiple objects per request](DeletingMultipleObjects.md)

 You can delete one or more objects directly from Amazon S3\. You have the following options when deleting an object: 
+ **Delete a single object—**Amazon S3 provides the DELETE API that you can use to delete one object in a single HTTP request\. 
+ **Delete multiple objects—**Amazon S3 also provides the Multi\-Object Delete API that you can use to delete up to 1000 objects in a single HTTP request\. 

When deleting objects from a bucket that is not version\-enabled, you provide only the object key name, however, when deleting objects from a version\-enabled bucket, you can optionally provide version ID of the object to delete a specific version of the object\. 

## Deleting objects from a version\-enabled bucket<a name="DeletingObjectsfromaVersion-EnabledBucket"></a>

If your bucket is version\-enabled, then multiple versions of the same object can exist in the bucket\. When working with version\-enabled buckets, the delete API enables the following options:
+ **Specify a non\-versioned delete request—**That is, you specify only the object's key, and not the version ID\. In this case, Amazon S3 creates a delete marker and returns its version ID in the response\. This makes your object disappear from the bucket\. For information about object versioning and the delete marker concept, see [Object Versioning](ObjectVersioning.md)\.
+ **Specify a versioned delete request—**That is, you specify both the key and also a version ID\. In this case the following two outcomes are possible:
  + If the version ID maps to a specific object version, then Amazon S3 deletes the specific version of the object\.
  + If the version ID maps to the delete marker of that object, Amazon S3 deletes the delete marker\. This makes the object reappear in your bucket\. 

## Deleting objects from an MFA\-enabled bucket<a name="DeletingObjectsfromanMFA-EnabledBucket"></a>

When deleting objects from a Multi Factor Authentication \(MFA\) enabled bucket, note the following:
+ If you provide an invalid MFA token, the request always fails\.
+ If you have an MFA\-enabled bucket, and you make a versioned delete request \(you provide an object key and version ID\), the request will fail if you don't provide a valid MFA token\. In addition, when using the Multi\-Object Delete API on an MFA\-enabled bucket, if any of the deletes is a versioned delete request \(that is, you specify object key and version ID\), the entire request will fail if you don't provide an MFA token\. 

On the other hand, in the following cases the request succeeds:
+ If you have an MFA\-enabled bucket, and you make a non\-versioned delete request \(you are not deleting a versioned object\),  and you don't provide an MFA token, the delete succeeds\. 
+ If you have a Multi\-Object Delete request specifying only non\-versioned objects to delete from an MFA\-enabled bucket,  and you don't provide an MFA token, the deletions succeed\.

For information on MFA delete, see [MFA delete](Versioning.md#MultiFactorAuthenticationDelete)\.

## Related resources<a name="RelatedResources001"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Amazon S3 Condition Keys<a name="amazon-s3-policy-keys"></a>

The access policy language enables you to specify conditions when granting permissions\. To specify conditions for when a policy is in effect, you can use the optional `Condition` element, or `Condition` block, to specify conditions for when a policy is in effect\. You can use predefined AWS‐wide keys and Amazon S3‐specific keys to specify conditions in an Amazon S3 access policy\.

In the `Condition` element, you build expressions in which you use Boolean operators \(equal, less than, etc\.\) to match your condition against values in the request\. For example, when granting a user permission to upload an object, the bucket owner can require that the object be publicly readable by adding the `StringEquals` condition, as shown here\.

```
{ 
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "statement1",
      "Effect": "Allow",
      "Action": "s3:PutObject",
      "Resource": [
        "arn:aws:s3:::awsexamplebucket1/*"
      ],
      "Condition": {
        "StringEquals": {
          "s3:x-amz-acl": "public-read"
        }
      }
    }
  ]
}
```

In the example, the `Condition` block specifies the `StringEquals` condition that is applied to the specified key\-value pair, `"s3:x-amz-acl":["public-read"]`\. There is a set of predefined keys that you can use in expressing a condition\. The example uses the `s3:x-amz-acl` condition key\. This condition requires the user to include the `x-amz-acl` header with value `public-read` in every PUT object request\.

**Topics**
+ [AWS‐Wide Condition Keys](#AvailableKeys-iamV2)
+ [Amazon S3‐Specific Condition Keys](#s3-specific-keys)
+ [Examples — Amazon S3 Condition Keys for Object Operations](#object-keys-in-amazon-s3-policies)
+ [Examples — Amazon S3 Condition Keys for Bucket Operations](#bucket-keys-in-amazon-s3-policies)

## AWS‐Wide Condition Keys<a name="AvailableKeys-iamV2"></a>

AWS provides a set of common keys that are supported by all AWS services that support policies\. These keys are called *AWS‐wide keys* and use the prefix `aws:`\. For a complete list of AWS‐wide condition keys, see [Available AWS Keys for Conditions](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html) in the *IAM User Guide*\.

You can use AWS‐wide condition keys in Amazon S3\. The following example bucket policy allows authenticated users permission to use the `s3:GetObject` action if the request originates from a specific range of IP addresses \(192\.0\.2\.0\.\*\), unless the IP address is 192\.0\.2\.188\. In the condition block, the `IpAddress` and the `NotIpAddress` are conditions, and each condition is provided a key\-value pair for evaluation\. Both the key\-value pairs in this example use the `aws:SourceIp` AWS‐wide key\.

**Note**  
The `IPAddress` and `NotIpAddress` key values specified in the condition uses CIDR notation as described in RFC 4632\. For more information, see [http://www\.rfc\-editor\.org/rfc/rfc4632\.txt](http://www.rfc-editor.org/rfc/rfc4632.txt)\.

```
{
    "Version": "2012-10-17",
    "Id": "S3PolicyId1",
    "Statement": [
        {
            "Sid": "statement1",
            "Effect": "Allow",
            "Principal": "*",
            "Action":"s3:GetObject",
            "Resource": "arn:aws:s3:::awsexamplebucket1/*",
            "Condition" : {
                "IpAddress" : {
                    "aws:SourceIp": "192.0.2.0/24" 
                },
                "NotIpAddress" : {
                    "aws:SourceIp": "192.0.2.188/32" 
                } 
            } 
        } 
    ]
}
```

You can also use other AWS‐wide condition keys in Amazon S3 policies\. For example, you can specify the `aws:SourceVpce` and `aws:SourceVpc` condition keys in bucket policies for VPC endpoints\. For examples, see [Example Bucket Policies for VPC Endpoints for Amazon S3](example-bucket-policies-vpc-endpoint.md)\.

## Amazon S3‐Specific Condition Keys<a name="s3-specific-keys"></a>

You can use Amazon S3 condition keys with specific Amazon S3 actions\. Each condition key maps to the same name request header allowed by the API on which the condition can be set\. Amazon S3‐specific condition keys dictate the behavior of the same name request headers\. For a complete list of Amazon S3‐specific condition keys, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\.

For example, the condition key `s3:x-amz-acl` that you can use to grant condition permission for the `s3:PutObject` permission defines behavior of the `x-amz-acl` request header that the PUT Object API supports\. The condition key `s3:VersionId` that you can use to grant conditional permission for the `s3:GetObjectVersion` permission defines behavior of the `versionId` query parameter that you set in a GET Object request\.

The following bucket policy grants the `s3:PutObject` permission for two AWS accounts if the request includes the `x-amz-acl` header making the object publicly readable\. The `Condition` block uses the `StringEquals` condition, and it is provided a key\-value pair, `"s3:x-amz-acl":["public-read"`, for evaluation\. In the key\-value pair, the `s3:x-amz-acl` is an Amazon S3–specific key, as indicated by the prefix `s3:`\. 

```
{
    "Version":"2012-10-17",
    "Statement": [ 
        {
	        "Sid":"AddCannedAcl",
            "Effect":"Allow",
	        "Principal": {
                "AWS": [
                "arn:aws:iam::Account1-ID:root",
				"arn:aws:iam::Account2-ID:root"
				]
            },
	        "Action":"s3:PutObject",
            "Resource": ["arn:aws:s3:::awsexamplebucket1/*"],
            "Condition": {
                "StringEquals": {
                    "s3:x-amz-acl":["public-read"]
                }
            }
        }
    ]
}
```

**Important**  
Not all conditions make sense for all actions\. For example, it makes sense to include an `s3:LocationConstraint` condition on a policy that grants the `s3:CreateBucket` Amazon S3 permission\. However, it does not make sense to include this condition on a policy that grants the `s3:GetObject` permission\. Amazon S3 can test for semantic errors of this type that involve Amazon S3–specific conditions\. However, if you are creating a policy for an IAM user and you include a semantically invalid Amazon S3 condition, no error is reported because IAM cannot validate Amazon S3 conditions\. 

## Examples — Amazon S3 Condition Keys for Object Operations<a name="object-keys-in-amazon-s3-policies"></a>

This section provides examples that show you how you can use Amazon S3‐specific condition keys for object operations\. For a complete list of Amazon S3 actions, condition keys, and resources that you can specify in policies, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\.

Several of the example policies show how you can use conditions keys with [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) operations\. PUT Object operations allow access control list \(ACL\)–specific headers that you can use to grant ACL\-based permissions\. Using these keys, the bucket owner can set a condition to require specific access permissions when the user uploads an object\. You can also grant ACL–based permissions with the PutObjectAcl operation\. For more information, see [PutObjectAcl](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObjectAcl.html) in the *Amazon S3 Amazon Simple Storage Service API Reference*\. For more information about ACLs, see [Access Control List \(ACL\) Overview](acl-overview.md)\.

### Example 1: Granting s3:PutObject Permission with a Condition Requiring the Bucket Owner to Get Full Control<a name="grant-putobject-conditionally-1"></a>

The [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) operation allows access control list \(ACL\)–specific headers that you can use to grant ACL\-based permissions\. Using these keys, the bucket owner can set a condition to require specific access permissions when the user uploads an object\. 

Suppose that Account A owns a bucket, and the account administrator wants to grant Dave, a user in Account B, permissions to upload objects\. By default, objects that Dave uploads are owned by Account B, and Account A has no permissions on these objects\. Because the bucket owner is paying the bills, it wants full permissions on the objects that Dave uploads\. The Account A administrator can do this by granting the `s3:PutObject` permission to Dave, with a condition that the request include ACL\-specific headers that either grant full permission explicitly or use a canned ACL\. For more information, see [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)\.

#### Require the `x-amz-full-control` Header<a name="require-x-amz-full-control"></a>

You can require the `x-amz-full-control` header in the request with full control permission to the bucket owner\. The following bucket policy grants the `s3:PutObject` permission to user Dave with a condition using the `s3:x-amz-grant-full-control` condition key, which requires the request to include the `x-amz-full-control` header\.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "statement1",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::AccountB-ID:user/Dave"
      },
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::awsexamplebucket1/*",
      "Condition": {
        "StringEquals": {
          "s3:x-amz-grant-full-control": "id=AccountA-CanonicalUserID"
        }
      }
    }
  ]
}
```

**Note**  
This example is about cross\-account permission\. However, if Dave \(who is getting the permission\) belongs to the AWS account that owns the bucket, this conditional permission is not necessary\. This is because the parent account to which Dave belongs owns objects that the user uploads\.

**Add Explicit Deny**  
The preceding bucket policy grants conditional permission to user Dave in Account B\. While this policy is in effect, it is possible for Dave to get the same permission without any condition via some other policy\. For example, Dave can belong to a group, and you grant the group `s3:PutObject` permission without any condition\. To avoid such permission loopholes, you can write a stricter access policy by adding explicit deny\. In this example, you explicitly deny the user Dave upload permission if he does not include the necessary headers in the request granting full permissions to the bucket owner\. Explicit deny always supersedes any other permission granted\. The following is the revised access policy example with explicit deny added\.

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "statement1",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::AccountB-ID:user/AccountBadmin"
            },
            "Action": "s3:PutObject",
            "Resource": "arn:aws:s3:::awsexamplebucket1/*",
            "Condition": {
                "StringEquals": {
                    "s3:x-amz-grant-full-control": "id=AccountA-CanonicalUserID"
                }
            }
        },
        {
            "Sid": "statement2",
            "Effect": "Deny",
            "Principal": {
                "AWS": "arn:aws:iam::AccountB-ID:user/AccountBadmin"
            },
            "Action": "s3:PutObject",
            "Resource": "arn:aws:s3:::awsexamplebucket1/*",
            "Condition": {
                "StringNotEquals": {
                    "s3:x-amz-grant-full-control": "id=AccountA-CanonicalUserID"
                }
            }
        }
    ]
}
```

**Test the Policy with the AWS CLI**  
If you have two AWS accounts, you can test the policy using the AWS Command Line Interface \(AWS CLI\)\. You attach the policy and use Dave's credentials to test the permission using the following AWS CLI `put-object` command\. You provide Dave's credentials by adding the `--profile` parameter\. You grant full control permission to the bucket owner by adding the `--grant-full-control` parameter\. For more information about setting up and using the AWS CLI, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\. 

```
aws s3api put-object --bucket examplebucket --key HappyFace.jpg --body c:\HappyFace.jpg --grant-full-control id="AccountA-CanonicalUserID" --profile AccountBUserProfile
```

#### Require the `x-amz-acl` Header<a name="require-x-amz-acl-header"></a>

You can require the `x-amz-acl` header with a canned ACL granting full control permission to the bucket owner\. To require the `x-amz-acl` header in the request, you can replace the key\-value pair in the `Condition` block and specify the `s3:x-amz-acl` condition key, as shown in the following example\.

```
"Condition": {
	"StringNotEquals": {
		"s3:x-amz-acl": "bucket-owner-full-control"
	}
```

To test the permission using the AWS CLI, you specify the `--acl` parameter\. The AWS CLI then adds the `x-amz-acl` header when it sends the request\.

```
aws s3api put-object --bucket examplebucket --key HappyFace.jpg --body c:\HappyFace.jpg --acl "bucket-owner-full-control" --profile AccountBadmin
```

### Example 2: Granting s3:PutObject Permission Requiring Objects Stored Using Server\-Side Encryption<a name="putobject-require-sse-2"></a>

Suppose that Account A owns a bucket\. The account administrator wants to grant Jane, a user in Account A, permission to upload objects with a condition that Jane always request server\-side encryption so that Amazon S3 saves objects encrypted\. The Account A administrator can accomplish using the `s3:x-amz-server-side-encryption` condition key as shown\. The key\-value pair in the `Condition` block specifies the `s3:x-amz-server-side-encryption` key\.

```
"Condition": {
     "StringNotEquals": {
         "s3:x-amz-server-side-encryption": "AES256"
     }
```

When testing the permission using the AWS CLI, you must add the required parameter using the `--server-side-encryption` parameter\.

```
aws s3api put-object --bucket example1bucket --key HappyFace.jpg --body c:\HappyFace.jpg --server-side-encryption "AES256" --profile AccountBadmin
```

### Example 3: Granting s3:PutObject Permission to Copy Objects with a Restriction on the Copy Source<a name="putobject-limit-copy-source-3"></a>

In the PUT Object request, when you specify a source object, it is a copy operation \(see [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)\)\. Accordingly, the bucket owner can grant a user permission to copy objects with restrictions on the source, for example:
+ Allow copying objects only from the `sourcebucket` bucket\.
+ Allow copying objects from the source bucket and only the objects whose key name prefix starts with public/ f \(for example, sourcebucket/public/\*\)\.
+ Allow copying only a specific object from the sourcebucket \(for example, sourcebucket/example\.jpg\)\.

The following bucket policy grants user \(Dave\) `s3:PutObject` permission\. It allows him to copy objects only with a condition that the request include the `s3:x-amz-copy-source` header and the header value specify the `/awsexamplebucket1/public/*` key name prefix\. 

```
{
    "Version": "2012-10-17",
    "Statement": [
       {
            "Sid": "cross-account permission to user in your own account",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:user/Dave"
            },
            "Action": "s3:PutObject",
            "Resource": "arn:aws:s3:::awsexamplebucket1/*"
        },
        {
            "Sid": "Deny your user permission to upload object if copy source is not /bucket/folder",
            "Effect": "Deny",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:user/Dave"
            },
            "Action": "s3:PutObject",
            "Resource": "arn:aws:s3:::awsexamplebucket1/*",
            "Condition": {
                "StringNotLike": {
                    "s3:x-amz-copy-source": "awsexamplebucket1/public/*"
                }
            }
        }
    ]
}
```

**Test the Policy with the AWS CLI**  
You can test the permission using the AWS CLI `copy-object` command\. You specify the source by adding the `--copy-source` parameter; the key name prefix must match the prefix allowed in the policy\. You need to provide the user Dave credentials using the `--profile` parameter\. For more information about setting up the AWS CLI, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\.

```
aws s3api copy-object --bucket awsexamplebucket1 --key HappyFace.jpg 
--copy-source examplebucket/public/PublicHappyFace1.jpg --profile AccountADave
```

**Give Permission to Copy Only a Specific Object**  
The preceding policy uses the `StringNotLike` condition\. To grant permission to copy only a specific object, you must change the condition from `StringNotLike` to `StringNotEquals` and then specify the exact object key as shown\. 

```
"Condition": {
       "StringNotEquals": {
           "s3:x-amz-copy-source": "awsexamplebucket1/public/PublicHappyFace1.jpg"
       }
}
```

### Example 4: Granting Access to a Specific Version of an Object<a name="getobjectversion-limit-access-to-specific-version-3"></a>

Suppose that Account A owns a version\-enabled bucket\. The bucket has several versions of the `HappyFace.jpg` object\. The account administrator now wants to grant its user Dave permission to get only a specific version of the object\. The account administrator can accomplish this by granting Dave `s3:GetObjectVersion` permission conditionally as shown below\. The key\-value pair in the `Condition` block specifies the `s3:VersionId` condition key\. In this case, Dave needs to know the exact object version ID to retrieve the object\. 

For more information, see [GetObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html) in the *Amazon Simple Storage Service API Reference*\.

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "statement1",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:user/Dave"
            },
            "Action": "s3:GetObjectVersion",
            "Resource": "arn:aws:s3:::examplebucketversionenabled/HappyFace.jpg"
        },
        {
            "Sid": "statement2",
            "Effect": "Deny",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:user/Dave"
            },
            "Action": "s3:GetObjectVersion",
            "Resource": "arn:aws:s3:::examplebucketversionenabled/HappyFace.jpg",
            "Condition": {
                "StringNotEquals": {
                    "s3:VersionId": "AaaHbAQitwiL_h47_44lRO2DDfLlBO5e"
                }
            }
        }
    ]
}
```

**Test the Policy with the AWS CLI**  
You can test the permissions using the AWS CLI `get-object` command with the `--version-id` parameter identifying the specific object version\. The command retrieves the object and saves it to the `OutputFile.jpg` file\.

```
aws s3api get-object --bucket examplebucketversionenabled --key HappyFace.jpg OutputFile.jpg --version-id AaaHbAQitwiL_h47_44lRO2DDfLlBO5e --profile AccountADave
```

### Example 5: Restricting Object Uploads to Objects with a Specific Storage Class<a name="example-storage-class-condition-key"></a>

Suppose that Account A, represented by account ID 123456789012, owns a bucket\. The account administrator wants to restrict Dave, a user in Account A, to be able to only upload objects to the bucket that are stored with the `STANDARD_IA` storage class\. To restrict object uploads to a specific storage class, the Account A administrator can use the `s3:x-amz-storage-class` condition key,as shown in the following example bucket policy\. 

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "statement1",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/Dave"
      },
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::awsexamplebucket1/*",
      "Condition": {
        "StringEquals": {
          "s3:x-amz-storage-class": [
            "STANDARD_IA"
          ]
        }
      }
    }
  ]
}
```

### Example 6: Granting Permissions Based on Object Tags<a name="example-object-tagging-access-control"></a>

For examples on how to use object tagging condition keys with Amazon S3 operations, see [Object tagging and access control policies](object-tagging.md#tagging-and-policies)\.

## Examples — Amazon S3 Condition Keys for Bucket Operations<a name="bucket-keys-in-amazon-s3-policies"></a>

This section provides example policies that show you how you can use Amazon S3‐specific condition keys for bucket operations\.

### Example 1: Granting a User Permission to Create a Bucket Only in a Specific Region<a name="condition-key-bucket-ops-1"></a>

Suppose that an AWS account administrator wants to grant its user \(Dave\) permission to create a bucket in the South America \(São Paulo\) Region only\. The account administrator can attach the following user policy granting the `s3:CreateBucket` permission with a condition as shown\. The key\-value pair in the `Condition` block specifies the `s3:LocationConstraint` key and the `sa-east-1` Region as its value\.

**Note**  
In this example, the bucket owner is granting permission to one of its users, so either a bucket policy or a user policy can be used\. This example shows a user policy\.

For a list of Amazon S3 Regions, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\. 

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Sid":"statement1",
         "Effect":"Allow",
         "Action": "s3:CreateBucket",
         "Resource": "arn:aws:s3:::*",
         "Condition": {
             "StringLike": {
                 "s3:LocationConstraint": "sa-east-1"
             }
         }
       }
    ]
}
```

**Add Explicit Deny**  
The preceding policy restricts the user from creating a bucket in any other Region except `sa-east-1`\. However, some other policy might grant this user permission to create buckets in another Region\. For example, if the user belongs to a group, the group might have a policy attached to it that allows all users in the group permission to create buckets in another Region\. To ensure that the user does not get permission to create buckets in any other Region, you can add an explicit deny statement in the above policy\. 

The `Deny` statement uses the `StringNotLike` condition\. That is, a create bucket request is denied if the location constraint is not `sa-east-1`\. The explicit deny does not allow the user to create a bucket in any other Region, no matter what other permission the user gets\. The below policy includes an explicit deny statement\.

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Sid":"statement1",
         "Effect":"Allow",
         "Action": "s3:CreateBucket",
         "Resource": "arn:aws:s3:::*",
         "Condition": {
             "StringLike": {
                 "s3:LocationConstraint": "sa-east-1"
             }
         }
       },
      {
         "Sid":"statement2",
         "Effect":"Deny",
         "Action": "s3:CreateBucket",
         "Resource": "arn:aws:s3:::*",
         "Condition": {
             "StringNotLike": {
                 "s3:LocationConstraint": "sa-east-1"
             }
         }
       }
    ]
}
```

**Test the Policy with the AWS CLI**  
You can test the policy using the following `create-bucket` AWS CLI command\. This example uses the `bucketconfig.txt` file to specify the location constraint\. Note the Windows file path\. You need to update the bucket name and path as appropriate\. You must provide user credentials using the `--profile` parameter\. For more information about setting up and using the AWS CLI, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\.

```
aws s3api create-bucket --bucket examplebucket --profile AccountADave --create-bucket-configuration file://c:/Users/someUser/bucketconfig.txt
```

The `bucketconfig.txt` file specifies the configuration as follows\.

```
{"LocationConstraint": "sa-east-1"}
```

### Example 2: Getting a List of Objects in a Bucket with a Specific Prefix<a name="condition-key-bucket-ops-2"></a>

You can use the `s3:prefix` condition key to limit the response of the [GET Bucket \(ListObjects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjects.html) API to key names with a specific prefix\. If you are the bucket owner, you can restrict a user to list the contents of a specific prefix in the bucket\. This condition key is useful if objects in the bucket are organized by key name prefixes\. The Amazon S3 console uses key name prefixes to show a folder concept\. Only the console supports the concept of folders; the Amazon S3 API supports only buckets and objects\. For more information about using prefixes and delimiters to filter access permissions, see [Walkthrough: Controlling access to a bucket with user policies](walkthrough1.md)\.

For example, if you have two objects with key names `public/object1.jpg` and `public/object2.jpg`, the console shows the objects under the `public` folder\. In the Amazon S3 API, these are objects with prefixes, not objects in folders\. However, in the Amazon S3 API, if you organize your object keys using such prefixes, you can grant `s3:ListBucket` permission with the `s3:prefix` condition that will allow the user to get a list of key names with those specific prefixes\. 

In this example, the bucket owner and the parent account to which the user belongs are the same\. So the bucket owner can use either a bucket policy or a user policy\. For more information about other condition keys that you can use with the GET Bucket \(ListObjects\) API, see [ ListObjects](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjects.html)\.

**User Policy**  
The following user policy grants the `s3:ListBucket` permission \(see [GET Bucket \(List Objects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html)\) with a condition that requires the user to specify the `prefix` in the request with the value `projects`\. 

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Sid":"statement1",
         "Effect":"Allow",
         "Action": "s3:ListBucket",
         "Resource":"arn:aws:s3:::awsexamplebucket1",
         "Condition" : {
             "StringEquals" : {
                 "s3:prefix": "projects" 
             }
          } 
       },
      {
         "Sid":"statement2",
         "Effect":"Deny",
         "Action": "s3:ListBucket",
         "Resource": "arn:aws:s3:::awsexamplebucket1",
         "Condition" : {
             "StringNotEquals" : {
                 "s3:prefix": "projects" 
             }
          } 
       }         
    ]
}
```

The condition restricts the user to listing object keys with the `projects` prefix\. The added explicit deny denies the user request for listing keys with any other prefix no matter what other permissions the user might have\. For example, it is possible that the user gets permission to list object keys without any restriction, either by updates to the preceding user policy or via a bucket policy\. Because explicit deny always supersedes, the user request to list keys other than the `projects` prefix is denied\. 

**Bucket Policy**  
If you add the `Principal` element to the above user policy, identifying the user, you now have a bucket policy as shown\.

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Sid":"statement1",
         "Effect":"Allow",
         "Principal": {
            "AWS": "arn:aws:iam::123456789012:user/bucket-owner"
         },  
         "Action":  "s3:ListBucket",
         "Resource": "arn:aws:s3:::awsexamplebucket1",
         "Condition" : {
             "StringEquals" : {
                 "s3:prefix": "projects" 
             }
          } 
       },
      {
         "Sid":"statement2",
         "Effect":"Deny",
         "Principal": {
            "AWS": "arn:aws:iam::123456789012:user/bucket-owner"
         },  
         "Action": "s3:ListBucket",
         "Resource": "arn:aws:s3:::awsexamplebucket1",
         "Condition" : {
             "StringNotEquals" : {
                 "s3:prefix": "projects" 
             }
          } 
       }         
    ]
}
```

**Test the Policy with the AWS CLI**  
You can test the policy using the following `list-object` AWS CLI command\. In the command, you provide user credentials using the `--profile` parameter\. For more information about setting up and using the AWS CLI, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\.

```
aws s3api list-objects --bucket awsexamplebucket1 --prefix examplefolder --profile AccountADave
```

If the bucket is version\-enabled, to list the objects in the bucket, you must grant the `s3:ListBucketVersions` permission in the preceding policy, instead of `s3:ListBucket` permission\. This permission also supports the `s3:prefix` condition key\. 

### Example 3: Setting the Maximum Number of Keys<a name="example-numeric-condition-operators"></a>

You can use the `s3:max-keys` condition key to set the maximum number of keys that requester can return in a [GET Bucket \(ListObjects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjects.html) or [ListObjectVersions](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectVersions.html) request\. By default, the API returns up to 1,000 keys\. For a list of numeric condition operators that you can use with `s3:max-keys` and accompanying examples, see [Numeric Condition Operators](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition_operators.html#Conditions_Numeric) in the *IAM User Guide*\.


# Using the AWS PHP SDK for multipart upload<a name="usingHLmpuPHP"></a>

You can upload large files to Amazon S3 in multiple parts\. You must use a multipart upload for files larger than 5 GB\. The AWS SDK for PHP exposes the [MultipartUploader](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.MultipartUploader.html) class that simplifies multipart uploads\. 

The `upload` method of the `MultipartUploader` class is best used for a simple multipart upload\. If you need to pause and resume multipart uploads, vary part sizes during the upload, or do not know the size of the data in advance, use the low\-level PHP API\. For more information, see [Using the AWS PHP SDK for multipart upload \(low\-level API\)](usingLLmpuPHP.md)\. 

For more information about multipart uploads, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\. For information on uploading files that are less than 5GB in size, see [Upload an object using the AWS SDK for PHP](UploadObjSingleOpPHP.md)\. 

## Upload a file using the high\-level multipart upload<a name="HLuploadFilePHP"></a>

This topic explains how to use the high\-level `Aws\S3\Model\MultipartUpload\UploadBuilder` class from the AWS SDK for PHP for multipart file uploads\. It assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\.

The following PHP example uploads a file to an Amazon S3 bucket\. The example demonstrates how to set parameters for the `MultipartUploader` object\. 

For information about running the PHP examples in this guide, see [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\.

```
 require 'vendor/autoload.php';

use Aws\Common\Exception\MultipartUploadException;
use Aws\S3\MultipartUploader;
use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
                        
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);
 
// Prepare the upload parameters.
$uploader = new MultipartUploader($s3, '/path/to/large/file.zip', [
    'bucket' => $bucket,
    'key'    => $keyname
]);

// Perform the upload.
try {
    $result = $uploader->upload();
    echo "Upload complete: {$result['ObjectURL']}" . PHP_EOL;
} catch (MultipartUploadException $e) {
    echo $e->getMessage() . PHP_EOL;
}
```

### Related resources<a name="RelatedResources-HLuploadFilePHP"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [ Amazon S3 Multipart Uploads](https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-multipart-upload.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Making requests using IAM user temporary credentials \- AWS SDK for Java<a name="AuthUsingTempSessionTokenJava"></a>

An IAM user or an AWS Account can request temporary security credentials \(see [Making requests](MakingRequests.md)\) using the AWS SDK for Java and use them to access Amazon S3\. These credentials expire after the specified session duration\. 

By default, the session duration is one hour\. If you use IAM user credentials, you can specify the duration when requesting the temporary security credentials from 15 minutes to the maximum session duration for the role\. For more information about temporary security credentials, see [Temporary Security Credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html) in the *IAM User Guide*\. For more information about making requests, see [Making requests](MakingRequests.md)\.

**To get temporary security credentials and access Amazon S3**

1. Create an instance of the `AWSSecurityTokenService` class\. For information about providing credentials, see [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)\.

1. Retrieve the temporary security credentials for the desired role by calling the `assumeRole()` method of the Security Token Service \(STS\) client\.

1. Package the temporary security credentials into a `BasicSessionCredentials` object\. You use this object to provide the temporary security credentials to your Amazon S3 client\.

1. Create an instance of the `AmazonS3Client` class using the temporary security credentials\. You send requests to Amazon S3 using this client\. If you send requests using expired credentials, Amazon S3 will return an error\.

**Note**  
If you obtain temporary security credentials using your AWS account security credentials, the temporary credentials are valid for only one hour\. You can specify the session duration only if you use IAM user credentials to request a session\.

The following example lists a set of object keys in the specified bucket\. The example obtains temporary security credentials for a session and uses them to send an authenticated request to Amazon S3\.

If you want to test the sample using IAM user credentials, you will need to create an IAM user under your AWS Account\. For more information about how to create an IAM user, see [Creating Your First IAM User and Administrators Group](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html) in the *IAM User Guide*\.

For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicSessionCredentials;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ObjectListing;
import com.amazonaws.services.securitytoken.AWSSecurityTokenService;
import com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder;
import com.amazonaws.services.securitytoken.model.AssumeRoleRequest;
import com.amazonaws.services.securitytoken.model.AssumeRoleResult;
import com.amazonaws.services.securitytoken.model.Credentials;

public class MakingRequestsWithIAMTempCredentials {
    public static void main(String[] args) {
        String clientRegion = "*** Client region ***";
        String roleARN = "*** ARN for role to be assumed ***";
        String roleSessionName = "*** Role session name ***";
        String bucketName = "*** Bucket name ***";

        try {
            // Creating the STS client is part of your trusted code. It has
            // the security credentials you use to obtain temporary security credentials.
            AWSSecurityTokenService stsClient = AWSSecurityTokenServiceClientBuilder.standard()
                                                    .withCredentials(new ProfileCredentialsProvider())
                                                    .withRegion(clientRegion)
                                                    .build();

            // Obtain credentials for the IAM role. Note that you cannot assume the role of an AWS root account;
            // Amazon S3 will deny access. You must use credentials for an IAM user or an IAM role.
            AssumeRoleRequest roleRequest = new AssumeRoleRequest()
                                                    .withRoleArn(roleARN)
                                                    .withRoleSessionName(roleSessionName);
            AssumeRoleResult roleResponse = stsClient.assumeRole(roleRequest);
            Credentials sessionCredentials = roleResponse.getCredentials();
            
            // Create a BasicSessionCredentials object that contains the credentials you just retrieved.
            BasicSessionCredentials awsCredentials = new BasicSessionCredentials(
                    sessionCredentials.getAccessKeyId(),
                    sessionCredentials.getSecretAccessKey(),
                    sessionCredentials.getSessionToken());

            // Provide temporary security credentials so that the Amazon S3 client 
	    // can send authenticated requests to Amazon S3. You create the client 
	    // using the sessionCredentials object.
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                                    .withCredentials(new AWSStaticCredentialsProvider(awsCredentials))
                                    .withRegion(clientRegion)
                                    .build();

            // Verify that assuming the role worked and the permissions are set correctly
            // by getting a set of object keys from the bucket.
            ObjectListing objects = s3Client.listObjects(bucketName);
            System.out.println("No. of Objects: " + objects.getObjectSummaries().size());
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Related resources<a name="RelatedResources008"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Querying archived objects programmatically<a name="querying-glacier-archives"></a>

With the select type of [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html), you can perform filtering operations using simple Structured Query Language \(SQL\) statements directly on your data that is archived by Amazon S3 to S3 Glacier\. When you provide an SQL query for an archived object, select runs the query in place and writes the output results to an S3 bucket\. You can run queries and custom analytics on your data that is stored in S3 Glacier, without having to restore your entire object to Amazon S3\.

When you perform select queries, S3 Glacier provides three data access tiers—*expedited*, *standard*, and *bulk*\. All of these tiers provide different data access times and costs, and you can choose any one of them depending on how quickly you want your data to be available\. For more information, see [Data Access Tiers](#querying-glacier-archives-access-tiers)\.

You can use the select type of restore with the AWS SDKs, the S3 Glacier REST API, and the AWS Command Line Interface \(AWS CLI\)\.

**Topics**
+ [Select Requirements and Limits](#glacier-select-requirements-and-limits)
+ [How Do I Query Data Using Select?](#glacier-select-restrictions)
+ [Error Handling](#glacier-select-access-control)
+ [Data Access Tiers](#querying-glacier-archives-access-tiers)
+ [More Info](#querying-glacier-archives-more-info)

## Select Requirements and Limits<a name="glacier-select-requirements-and-limits"></a>

The following are requirements for using select:
+ Archive objects that are queried by select must be formatted as uncompressed comma\-separated values \(CSV\)\. 
+ A S3 bucket for output\. The AWS account that you use to initiate a S3 Glacier select job must have write permissions for the S3 bucket\. The Amazon S3 bucket must be in the same AWS Region as the bucket that contains the archived object that is being queried\.
+ The requesting AWS account must have permissions to perform the `s3:RestoreObject` and `s3:GetObject` actions\. For more information about these permissions, see [Example — Bucket Subresource Operations](using-with-s3-actions.md#using-with-s3-actions-related-to-bucket-subresources)\. 
+ The archive must not be encrypted with SSE\-C or client\-side encryption\. 

The following limits apply when using select:
+ There are no limits on the number of records that select can process\. An input or output record must not exceed 1 MB; otherwise, the query fails\. There is a limit of 1,048,576 columns per record\.
+ There is no limit on the size of your final result\. However, your results are broken into multiple parts\. 
+ An SQL expression is limited to 128 KB\.

## How Do I Query Data Using Select?<a name="glacier-select-restrictions"></a>

Using select, you can use SQL commands to query S3 Glacier archive objects that are in encrypted uncompressed CSV format\. With this restriction, you can perform simple query operations on your text\-based data in S3 Glacier\. For example, you might look for a specific name or ID among a set of archived text files\. 

To query your S3 Glacier data, create a select request using the [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html) operation\. When performing a select request, you provide the SQL expression, the archive to query, and the location to store the results\. 

The following example expression returns all records from the archived object specified in [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html)\.

```
SELECT * FROM object
```

S3 Glacier Select supports a subset of the ANSI SQL language\. It supports common filtering SQL clauses like `SELECT`, `FROM`, and `WHERE`\. It does not support `SUM`, `COUNT`, `GROUP BY`, `JOINS`, `DISTINCT`, `UNION`, `ORDER BY`, and `LIMIT`\. For more information about support for SQL, see [SQL Reference for Amazon S3 Select and S3 Glacier Select](https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-glacier-select-sql-reference.html) in the *Amazon Simple Storage Service Developer Guide*\.



### Select Output<a name="glacier-select-output"></a>

When you initiate a select request, you define an output location for the results of your select query\. This location must be an Amazon S3 bucket in the same AWS Region as the bucket that contains the archived object that is being queried\. The AWS account that initiates the job must have permissions to write to the S3 bucket\. 

You can specify the Amazon S3 storage class and encryption for the output objects stored in Amazon S3\. Select supports AWS Key Management Service \(SSE\-KMS\) and Amazon S3 \(SSE\-S3\) encryption\. Select doesn't support SSE\-C and client\-side encryption\. For more information about Amazon S3 storage classes and encryption, see [Amazon S3 storage classes](storage-class-intro.md) and [Protecting data using server\-side encryption](serv-side-encryption.md)\.

S3 Glacier Select results are stored in the S3 bucket using the prefix provided in the output location specified in [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html)\. From this information, select creates a unique prefix referring to the job ID\. \(Prefixes are used to group Amazon S3 objects together by beginning object names with a common string\.\) Under this unique prefix, there are two new prefixes created, `results` for results and `errors` for logs and errors\. Upon completion of the job, a result manifest is written which contains the location of all results\.

There is also a placeholder file named `job.txt` that is written to the output location\. After it is written, it is never updated\. The placeholder file is used for the following:
+ Validation of the write permission and majority of SQL syntax errors synchronously\. 
+ Providing a static output about your select request that you can easily reference whenever you want\. 

For example, suppose that you make a select request with the output location for the results specified as `s3://example-bucket/my-prefix`, and the job response returns the job ID as `examplekne1209ualkdjh812elkassdu9012e`\. After the select job finishes, you can see the following Amazon S3 objects in your bucket:

```
s3://example-bucket/my-prefix/examplekne1209ualkdjh812elkassdu9012e/job.txt
s3://example-bucket/my-prefix/examplekne1209ualkdjh812elkassdu9012e/results/abc
s3://example-bucket/my-prefix/examplekne1209ualkdjh812elkassdu9012e/results/def
s3://example-bucket/my-prefix/examplekne1209ualkdjh812elkassdu9012e/results/ghi
s3://example-bucket/my-prefix/examplekne1209ualkdjh812elkassdu9012e/result_manifest.txt
```

The select query results are broken into multiple parts\. In the example, select uses the prefix that you specified when setting the output location and appends the job ID and the `results` prefix\. It then writes the results in three parts, with the object names ending in `abc`, `def`, and `ghi`\. The result manifest contains all three files to allow programmatic retrieval\. If the job fails with any error, then a file is visible under the error prefix and an `error_manifest.txt` is produced\.

Presence of a `result_manifest.txt` file along with the absence of `error_manifest.txt` guarantees that the job finished successfully\. There is no guarantee provided on how results are ordered\.

**Note**  
The length of an Amazon S3 object name, also referred to as the *key*, can be no more than 1,024 bytes\. S3 Glacier select reserves 128 bytes for prefixes\. And, the length of your Amazon S3 location path cannot be more than 512 bytes\. A request with a length greater than 512 bytes returns an exception, and the request is not accepted\.

## Error Handling<a name="glacier-select-access-control"></a>

Select notifies you of two kinds of errors\. The first set of errors is sent to you synchronously when you submit the query in [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html)\. These errors are sent to you as part of the HTTP response\. Another set of errors can occur after the query has been accepted successfully, but they happen during query execution\. In this case, the errors are written to the specified output location under the `errors` prefix\.

Select stops executing the query after encountering an error\. To run the query successfully, you must resolve all errors\. You can check the logs to identify which records caused a failure\. 

Because queries run in parallel across multiple compute nodes, the errors that you get are not in sequential order\. For example, if your query fails with an error in row 6,234, it does not mean that all rows before row 6,234 were successfully processed\. The next run of the query might show an error in a different row\. 

## Data Access Tiers<a name="querying-glacier-archives-access-tiers"></a>

You can specify one of the following data access tiers when querying an archived object: 
+ **`Expedited`** – Allows you to quickly access your data when occasional urgent requests for a subset of archives are required\. For all but the largest archived object \(250 MB\+\), data accessed using `Expedited` retrievals are typically made available within 1–5 minutes\. There are two types of `Expedited` data access: On\-Demand and Provisioned\. On\-Demand requests are similar to EC2 On\-Demand instances and are available most of the time\. Provisioned requests are guaranteed to be available when you need them\. For more information, see [Provisioned Capacity](#querying-glacier-archives-expedited-capacity)\. 
+ **`Standard`** – Allows you to access any of your archived objects within several hours\. Standard retrievals typically finish within 3–5 hours\. This is the default tier\.
+ **`Bulk`** – The lowest\-cost data access option in S3 Glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively in a day\. `Bulk` access typically finishes within 5–12 hours\. 

To make an `Expedited`, `Standard`, or `Bulk` request, set the `Tier` request element in the [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html) REST API request to the option you want, or the equivalent in the AWS CLI or AWS SDKs\. For `Expedited` access, there is no need to designate whether an expedited retrieval is On\-Demand or Provisioned\. If you purchased provisioned capacity, all `Expedited` retrievals are automatically served through your provisioned capacity\. For information about tier pricing, see [S3 Glacier Pricing](http://aws.amazon.com/glacier/pricing/)\.



### Provisioned Capacity<a name="querying-glacier-archives-expedited-capacity"></a>

Provisioned capacity guarantees that your retrieval capacity for expedited retrievals is available when you need it\. Each unit of capacity ensures that at least three expedited retrievals can be performed every five minutes and provides up to 150 MB/s of retrieval throughput\.

You should purchase provisioned retrieval capacity if your workload requires highly reliable and predictable access to a subset of your data in minutes\. Without provisioned capacity, `Expedited` retrievals are accepted, except for rare situations of unusually high demand\. However, if you require access to `Expedited` retrievals under all circumstances, you must purchase provisioned retrieval capacity\. You can purchase provisioned capacity using the Amazon S3 console, the S3 Glacier console, the [Purchase Provisioned Capacity](https://docs.aws.amazon.com/amazonglacier/latest/dev/api-PurchaseProvisionedCapacity.html) REST API, the AWS SDKs, or the AWS CLI\. For provisioned capacity pricing information, see the [S3 Glacier Pricing](https://aws.amazon.com/glacier/pricing/)\. 

## More Info<a name="querying-glacier-archives-more-info"></a>
+ [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html) in the *Amazon Simple Storage Service API Reference*
+ [SQL Reference for Amazon S3 Select and S3 Glacier Select](https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-glacier-select-sql-reference.html) in the *Amazon Simple Storage Service Developer Guide*


# List multipart uploads<a name="LLlistMPuploadsJava"></a>

**Example**  
The following example shows how to retrieve a list of in\-progress multipart uploads using the low\-level Java API:  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListMultipartUploadsRequest;
import com.amazonaws.services.s3.model.MultipartUpload;
import com.amazonaws.services.s3.model.MultipartUploadListing;

import java.util.List;

public class ListMultipartUploads {

    public static void main(String[] args) {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Retrieve a list of all in-progress multipart uploads.
            ListMultipartUploadsRequest allMultipartUploadsRequest = new ListMultipartUploadsRequest(bucketName);
            MultipartUploadListing multipartUploadListing = s3Client.listMultipartUploads(allMultipartUploadsRequest);
            List<MultipartUpload> uploads = multipartUploadListing.getMultipartUploads();

            // Display information about all in-progress multipart uploads.
            System.out.println(uploads.size() + " multipart upload(s) in progress.");
            for (MultipartUpload u : uploads) {
                System.out.println("Upload in progress: Key = \"" + u.getKey() + "\", id = " + u.getUploadId());
            }
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Tracing Amazon S3 requests using AWS X\-Ray<a name="tracing_requests_using_xray"></a>

AWS X\-Ray collects data about requests that your application serves\. You can then view and filter the data to identify and troubleshoot performance issues and errors in your distributed applications and micro\-services architecture\. For any traced request to your application, it shows you detailed information about the request, the response, and the calls that your application makes to downstream AWS resources, micro\-services, databases, and HTTP web APIs\. 

For more information, see [What is AWS X\-Ray?](https://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html) in the *AWS X\-Ray Developer Guide*\.

**Topics**
+ [How X\-Ray works with Amazon S3](#tracing_requests_using_xray-s3)
+ [Available Regions](#tracing_requests_using_xray-regions)

## How X\-Ray works with Amazon S3<a name="tracing_requests_using_xray-s3"></a>

AWS X\-Ray supports trace context propagation for Amazon S3, so you can view end\-to\-end requests as they travel through your entire application\. X\-Ray aggregates the data that is generated by the individual services such as Amazon S3, AWS Lambda, and Amazon EC2, and the many resources that make up your application\. It provides you with an overall view of how your application is performing\. 

Amazon S3 integrates with X\-Ray to get one request chain integrates with X\-Ray to propagate [ trace context](https://www.w3.org/TR/trace-context/#:~:text=Trace%20context%20is%20split%20into,design%20focuses%20on%20fast%20parsing) and give you one request chain with [upstream and downstream](https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html) nodes\. If an upstream service includes a valid\-formatted trace header with its S3 request, Amazon S3 passes the trace header when delivering event notifications to downstream services such as Lambda, Amazon SQS, and Amazon SNS\. If you have all these services actively integrated with X\-Ray, they are linked in one request chain to give you the complete details of your Amazon S3 requests\.

To send X\-Ray trace headers through Amazon S3, you must include a [formatted X\-Amzn\-Trace\-Id](https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-tracingheader) in your requests\. You can also instrument the Amazon S3 client using the AWS X\-Ray SDKs\. For a list of the supported SDKs, see the [AWS X\-Ray documentation](https://docs.aws.amazon.com/xray/index.html)\.

### Service maps<a name="tracing_requests_using_xray-s3-maps"></a>

X\-Ray *service maps* show you the relationships between Amazon S3 and other AWS services and resources in your application in near\-real time\. To see the end\-to\-end requests using the X\-Ray service maps, you can use the X\-Ray console to view a map of the connections between Amazon S3 and other services that your application uses\. You can easily detect where high latency is occurring, visualize node distribution for these services, and then drill down into the specific services and paths impacting application performance\. 

### X\-Ray Analytics<a name="tracing_requests_using_xray-s3-analytics"></a>

You can also use the [ X\-Ray Analytics](https://docs.aws.amazon.com/xray/latest/devguide/xray-console-analytics.html) console to analyze traces, view metrics such as latency and failure rates, and [generate insights](https://docs.aws.amazon.com/xray/latest/devguide/xray-console-insights.html) to help you identify and troubleshoot issues\. This console also shows you metrics such as average latency and failure rates\. For more information, see [AWS X\-Ray console](https://docs.aws.amazon.com/xray/latest/devguide/xray-console.html) in the *AWS X\-Ray Developer Guide*\.

## Available Regions<a name="tracing_requests_using_xray-regions"></a>

AWS X\-Ray support for Amazon S3 is available in all [ AWS X\-Ray Regions](http://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/)\. For more information, see [Amazon S3 and AWS X\-Ray](https://docs.aws.amazon.com/xray/latest/devguide/xray-services-s3.html) in the *AWS X\-Ray Developer Guide*\.


# Specifying Server\-Side Encryption Using the AWS Management Console<a name="SSEUsingConsole"></a>

When uploading an object using the AWS Management Console, you can specify server\-side encryption\. For an example of how to upload an object, see [Uploading S3 Objects](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html)\. 

When you copy an object using the AWS Management Console, the console copies the object as is\. That is, if the copy source is encrypted, the target object is encrypted\.The console also allows you to add encryption to an object\. For more information, see [How Do I Add Encryption to an S3 Object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-object-encryption.html)\. 


# Uploading objects<a name="UploadingObjects"></a>

 Depending on the size of the data you are uploading, Amazon S3 offers the following options: 
+ **Upload an object in a single operation using the AWS SDKs, REST API, or AWS CLI—**With a single PUT operation, you can upload a single object up to 5 GB in size\.

  For more information, see [Uploading an object in a single operation](UploadInSingleOp.md)\.
+ **Upload a single object using the Amazon S3 Console—**With the Amazon S3 Console, you can upload a single object up to 160 GB in size\. 

  For more information, see [How do I upload files and folders to an S3 bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html) in the *Amazon Simple Storage Service Console User Guide*\.
+ **Upload an object in parts using the AWS SDKs, REST API, or AWS CLI—**Using the multipart upload API, you can upload a single large object, up to 5 TB in size\.

  The multipart upload API is designed to improve the upload experience for larger objects\. You can upload an object in parts\. These object parts can be uploaded independently, in any order, and in parallel\. You can use a multipart upload for objects from 5 MB to 5 TB in size\. For more information, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\.

We recommend that you use multipart upload in the following ways:
+ If you're uploading large objects over a stable high\-bandwidth network, use multipart upload to maximize the use of your available bandwidth by uploading object parts in parallel for multi\-threaded performance\.
+ If you're uploading over a spotty network, use multipart upload to increase resiliency to network errors by avoiding upload restarts\. When using multipart upload, you need to retry uploading only parts that are interrupted during the upload\. You don't need to restart uploading your object from the beginning\.

For more information about multipart uploads, see [Multipart upload overview](mpuoverview.md)\.

**Topics**
+ [Uploading an object in a single operation](UploadInSingleOp.md)
+ [Uploading objects using multipart upload API](uploadobjusingmpu.md)
+ [Uploading objects using presigned URLs](PresignedUrlUploadObject.md)

When uploading an object, you can optionally request that Amazon S3 encrypt it before saving it to disk, and decrypt it when you download it\. For more information, see [Protecting data using encryption](UsingEncryption.md)\. 

**Related Topics**  
[Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Example: Using job tags to control permissions for S3 Batch Operations<a name="batch-ops-job-tags-examples"></a>

To help you manage your S3 Batch Operations jobs, you can add *job tags*\. With job tags, you can control access to your Batch Operations jobs and enforce that tags be applied when any job is created\. 

You can apply up to 50 job tags to each Batch Operations job\. This allows you to set very granular policies restricting the set of users that can edit the job\. Job tags can grant or limit a user’s ability to cancel a job, activate a job in the confirmation state, or change a job’s priority level\. In addition, you can enforce that tags be applied to all new jobs, and specify the allowed key\-value pairs for the tags\. You can express all of these conditions using the same [IAM policy language](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_iam-tags.html)\. For more information, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\.



The following example shows how you can use S3 Batch Operations job tags to grant users permission to create and edit only the jobs that are run within a specific *department* \(for example, the *Finance* or *Compliance* department\)\. You can also assign jobs based on the stage of *development* that they are related to, such as *QA* or *Production*\.

In this example, you use S3 Batch Operations job tags in AWS Identity and Access Management \(IAM\) policies to grant users permission to create and edit only the jobs being run within their department\. You assign jobs based on the stage of development that they are related to, such as *QA* or *Production*\. 

This example uses the following departments, with each using Batch Operations in different ways:
+ Finance
+ Compliance
+ Business Intelligence
+ Engineering

## Controlling access by assigning tags to users and resources<a name="job-tags-examples-attaching-tags"></a>

In this scenario, the administrators are using [attribute\-based access control \(ABAC\)](https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_attribute-based-access-control.html)\. ABAC is an IAM authorization strategy that defines permissions by attaching tags to both IAM users and AWS resources\.

Users and jobs are assigned one of the following department tags:

**Key : Value**
+ `department : Finance`
+ `department : Compliance`
+ `department : BusinessIntelligence`
+ `department : Engineering`
**Note**  
Job tag keys and values are case sensitive\.

Using the ABAC access control strategy, you grant a user in the Finance department permission to create and manage S3 Batch Operations jobs within their department by associating the tag `department=Finance` with their IAM user\.

Furthermore, you can attach a managed policy to the IAM user that allows any user in their company to create or modify S3 Batch Operations jobs within their respective departments\. 

The policy in this example includes three policy statements:
+ The first statement in the policy allows the user to create a Batch Operations job provided that the job creation request includes a job tag that matches their respective department\. This is expressed using the `"${aws:PrincipalTag/department}"` syntax, which is replaced by the IAM user’s department tag at policy evaluation time\. The condition is satisfied when the value provided for the department tag in the request `("aws:RequestTag/department")` matches the user’s department\. 
+ The second statement in the policy allows users to change the priority of jobs or update a job’s status provided that the job the user is updating matches the user’s department\. 
+ The third statement allows a user to update a Batch Operations job’s tags at any time via a `PutJobTagging` request as long as \(1\) their department tag is preserved and \(2\) the job they’re updating is within their department\. 

```
{
      "Version": "2012-10-17",
      "Statement": [
            {
                  "Effect": "Allow",
                  "Action": "s3:CreateJob",
                  "Resource": "*",
                  "Condition": {
                        "StringEquals": {
                              "aws:RequestTag/department": "${aws:PrincipalTag/department}"        
                }      
            }    
        },
            {
                  "Effect": "Allow",
                  "Action": [
                        "s3:UpdateJobPriority",
                        "s3:UpdateJobStatus"      
            ],
                  "Resource": "*",
                  "Condition": {
                        "StringEquals": {
                              "aws:ResourceTag/department": "${aws:PrincipalTag/department}"        
                }      
            }    
        },
            {
                  "Effect": "Allow",
                  "Action": "s3:PutJobTagging",
                  "Resource": "*",
                  "Condition": {
                        "StringEquals": {
                              "aws:RequestTag/department": "${aws:PrincipalTag/department}",
                              "aws:ResourceTag/department": "${aws:PrincipalTag/department}"        
                }      
            }    
        }  
    ]
}
```

## Tagging Batch Operations jobs by stage and enforcing limits on job priority<a name="tagging-jobs-by-stage-and-enforcing-limits-on-job-priority"></a>



All S3 Batch Operations jobs have a numeric priority, which Amazon S3 uses to decide in what order to run the jobs\. For this example, you restrict the maximum priority that most users can assign to jobs, with higher priority ranges reserved for a limited set of privileged users, as follows:
+ QA stage priority range \(low\): 1\-100
+ Production stage priority range \(high\): 1\-300

To do this, introduce a new tag set representing the *stage* of the job:

**Key : Value**
+ `stage : QA`
+ `stage : Production`

### Creating and updating low\-priority jobs within a department<a name="creating-and-updating-low-priority-jobs"></a>



This policy introduces two new restrictions on S3 Batch Operations job creation and update, in addition to the department\-based restriction:
+ It allows users to create or update jobs in their department with a new condition that requires the job to include the tag `stage=QA`\.
+ It allows users to create or update a job’s priority up to a new maximum priority of 100\.

```
{
        "Version": "2012-10-17",
        "Statement": [
        {
        "Effect": "Allow",
        "Action": "s3:CreateJob",
        "Resource": "*",
        "Condition": {
            "StringEquals": {
                "aws:RequestTag/department": "${aws:PrincipalTag/department}",
                "aws:RequestTag/stage": "QA"
            },
            "NumericLessThanEquals": {
                "s3:RequestJobPriority": 100
            }
        }
    },
    {
        "Effect": "Allow",
        "Action": [
            "s3:UpdateJobStatus"
        ],
        "Resource": "*",
        "Condition": {
            "StringEquals": {
                "aws:ResourceTag/department": "${aws:PrincipalTag/department}"
            }
        }
    },
    {
        "Effect": "Allow",
        "Action": "s3:UpdateJobPriority",
        "Resource": "*",
        "Condition": {
            "StringEquals": {
                "aws:ResourceTag/department": "${aws:PrincipalTag/department}",
                "aws:ResourceTag/stage": "QA"
            },
            "NumericLessThanEquals": {
                "s3:RequestJobPriority": 100
            }
        }
    },
    {
        "Effect": "Allow",
        "Action": "s3:PutJobTagging",
        "Resource": "*",
        "Condition": {
            "StringEquals": {
                "aws:RequestTag/department" : "${aws:PrincipalTag/department}",
                "aws:ResourceTag/department": "${aws:PrincipalTag/department}",
                "aws:RequestTag/stage": "QA",
                "aws:ResourceTag/stage": "QA"
            }
        }
    },
    {
        "Effect": "Allow",
        "Action": "s3:GetJobTagging",
        "Resource": "*"
    }
    ]
}
```

### Creating and updating high\-priority jobs within a department<a name="creating-and-updating-high-priority-jobs"></a>



A small number of users might require the ability to create high priority jobs in either *QA* or *Production*\. To support this need, you create a managed policy that’s adapted from the low\-priority policy in the previous section\. 

This policy does the following: 
+ Allows users to create or update jobs in their department with either the tag `stage=QA` or `stage=Production`\.
+ Allows users to create or update a job’s priority up to a maximum of 300\.

```
{
      "Version": "2012-10-17",
      "Statement": [
          {
                "Effect": "Allow",
                "Action": "s3:CreateJob",
                "Resource": "*",
                "Condition": {
                      "ForAnyValue:StringEquals": {
                            "aws:RequestTag/stage": [
                                  "QA",
                                  "Production"        
                    ]      
                },
                      "StringEquals": {
                            "aws:RequestTag/department": "${aws:PrincipalTag/department}"      
                },
                      "NumericLessThanEquals": {
                            "s3:RequestJobPriority": 300      
                }    
            }  
        },
          {
                "Effect": "Allow",
                "Action": [
                      "s3:UpdateJobStatus"    
            ],
                "Resource": "*",
                "Condition": {
                      "StringEquals": {
                            "aws:ResourceTag/department": "${aws:PrincipalTag/department}"      
                }    
            }  
        },
          {
                "Effect": "Allow",
                "Action": "s3:UpdateJobPriority",
                "Resource": "*",
                "Condition": {
                      "ForAnyValue:StringEquals": {
                            "aws:ResourceTag/stage": [
                                  "QA",
                                  "Production"        
                    ]      
                },
                      "StringEquals": {
                            "aws:ResourceTag/department": "${aws:PrincipalTag/department}"      
                },
                      "NumericLessThanEquals": {
                            "s3:RequestJobPriority": 300      
                }    
            }  
        },
          {
                "Effect": "Allow",
                "Action": "s3:PutJobTagging",
                "Resource": "*",
                "Condition": {
                      "StringEquals": {
                            "aws:RequestTag/department": "${aws:PrincipalTag/department}",
                            "aws:ResourceTag/department": "${aws:PrincipalTag/department}"      
                },
                      "ForAnyValue:StringEquals": {
                            "aws:RequestTag/stage": [
                                  "QA",
                                  "Production"        
                    ],
                            "aws:ResourceTag/stage": [
                                  "QA",
                                  "Production"        
                    ]      
                }    
            }  
        }  
    ]
}
```


# Get an object Using the REST API<a name="RetrieveObjSingleOpREST"></a>

You can use the AWS SDK to retrieve object keys from a bucket\. However, if your application requires it, you can send REST requests directly\. You can send a GET request to retrieve object keys\. For more information about the request and response format, go to [Get Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html)\.


# Enabling cross\-origin resource sharing \(CORS\) using the AWS Management Console<a name="ManageCorsUsingConsole"></a>

You can use the AWS Management Console to set a CORS configuration on your bucket\. For instructions, see [How do I add cross\-domain resource sharing with CORS? ](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-cors-configuration.html) in the *Amazon Simple Storage Service Console User Guide*\. 

**Important**  
In the new S3 console, the CORS configuration must be JSON\. For JSON examples, see [Cross\-origin resource sharing \(CORS\)](cors.md)


# Setting up permissions for replication<a name="setting-repl-config-perm-overview"></a>

When setting up replication, you must acquire necessary permissions as follows:
+ Create an IAM role—Amazon S3 needs permissions to replicate objects on your behalf\. You grant these permissions by creating an IAM role and specify the role in your replication configuration\.
+ When source and destination buckets aren't owned by the same accounts, the owner of the destination bucket must grant the source bucket owner permissions to store the replicas\.

**Topics**
+ [Creating an IAM role](#setting-repl-config-same-acctowner)
+ [Granting permissions when source and destination buckets are owned by different AWS accounts](#setting-repl-config-crossacct)

## Creating an IAM role<a name="setting-repl-config-same-acctowner"></a>



By default, all Amazon S3 resources—buckets, objects, and related subresources—are private and only the resource owner can access the resource\. Amazon S3 needs permissions read and replicate objects from the source bucket\. You grant these permissions by creating an IAM role and specifying the role in your replication configuration\. 

This section explains the trust policy and minimum required permissions policy\. The example walkthroughs provide step\-by\-step instructions to create an IAM role\. For more information, see [Replication walkthroughs](replication-example-walkthroughs.md)\.


+ The following shows a *trust policy*, where you identify Amazon S3 as the service principal who can assume the role\.

  ```
  {
  
     "Version":"2012-10-17",
     "Statement":[
        {
           "Effect":"Allow",
           "Principal":{
              "Service":"s3.amazonaws.com"
           },
           "Action":"sts:AssumeRole"
        }
     ]
  }
  ```

  For more information about IAM roles, see [IAM Roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) in the *IAM User Guide*\.
+ The following shows an *access policy*, where you grant the role permissions to perform replication tasks on your behalf\. When Amazon S3 assumes the role, it has the permissions that you specify in this policy\.

  ```
  {
  
     "Version":"2012-10-17",
     "Statement":[
        {
           "Effect":"Allow",
           "Action":[
              "s3:GetReplicationConfiguration",
              "s3:ListBucket"
           ],
           "Resource":[
              "arn:aws:s3:::SourceBucket"
           ]
        },
        {
           "Effect":"Allow",
           "Action":[
  
              "s3:GetObjectVersion",
              "s3:GetObjectVersionAcl",
              "s3:GetObjectVersionTagging"
  
           ],
           "Resource":[
              "arn:aws:s3:::SourceBucket/*"
           ]
        },
        {
           "Effect":"Allow",
           "Action":[
              "s3:ReplicateObject",
              "s3:ReplicateDelete",
              "s3:ReplicateTags"
           ],
           "Resource":"arn:aws:s3:::DestinationBucket/*",
           "Resource":"arn:aws:s3:::DestinationBucket-2/*"
        }
     ]
  }
  ```

  The access policy grants permissions for the following actions:
  +  `s3:GetReplicationConfiguration` and `s3:ListBucket`—Permissions for these actions on the *source* bucket allow Amazon S3 to retrieve the replication configuration and list bucket content \(the current permissions model requires the `s3:ListBucket` permission for accessing delete markers\)\.
  + `s3:GetObjectVersion` and `s3:GetObjectVersionAcl`— Permissions for these actions granted on all objects allow Amazon S3 to get a specific object version and access control list \(ACL\) associated with objects\. 
  + `s3:ReplicateObject` and `s3:ReplicateDelete`—Permissions for these actions on objects in all *destination* buckets allow Amazon S3 to replicate objects or delete markers to the destination buckets\. For information about delete markers, see [How delete operations affect replication](replication-what-is-isnot-replicated.md#replication-delete-op)\. 
**Note**  
Permissions for the `s3:ReplicateObject` action on the *destination* buckets also allow replication of object tags, so you don't need to explicitly grant permission for the `s3:ReplicateTags` action\.
  + `s3:GetObjectVersionTagging`—Permissions for this action on objects in the *source* bucket allow Amazon S3 to read object tags for replication \(see [Object tagging](object-tagging.md)\)\. If Amazon S3 doesn't have these permissions, it replicates the objects, but not the object tags\.

  For a list of Amazon S3 actions, see [Amazon S3 Actions](using-with-s3-actions.md)\.
**Important**  
The AWS account that owns the IAM role must have permissions for the actions that it grants to the IAM role\.   
For example, suppose that the source bucket contains objects owned by another AWS account\. The owner of the objects must explicitly grant the AWS account that owns the IAM role the required permissions through the object ACL\. Otherwise, Amazon S3 can't access the objects, and replication of the objects fails\. For information about ACL permissions, see [Access Control List \(ACL\) Overview](acl-overview.md)\.  
The permissions described here are related to minimum replication configuration\. If you choose to add optional replication configurations, you must grant additional permissions to Amazon S3\. For more information, see [Additional replication configurations](replication-additional-configs.md)\. 

## Granting permissions when source and destination buckets are owned by different AWS accounts<a name="setting-repl-config-crossacct"></a>

When source and destination buckets aren't owned by the same accounts, the owner of the destination bucket must also add a bucket policy to grant the owner of the source bucket permissions to perform replication actions, as follows\. 

```
{
   "Version":"2012-10-17",
   "Id":"PolicyForDestinationBucket",
   "Statement":[
      {
         "Sid":"Permissions on objects",
         "Effect":"Allow",
         "Principal":{
            "AWS":"arn:aws:iam::SourceBucket-AcctID:root"
         },
         "Action":[
            "s3:ReplicateDelete",
            "s3:ReplicateObject"
         ],
         "Resource":"arn:aws:s3:::destinationbucket/*"
      },
      {
         "Sid":"Permissions on bucket",
         "Effect":"Allow",
         "Principal":{
            "AWS":"arn:aws:iam::SourceBucket-AcctID:root"
         },
         "Action": [
            "s3:List*",
            "s3:GetBucketVersioning",
            "s3:PutBucketVersioning"
         ],
         "Resource":"arn:aws:s3:::destinationbucket"
      }
   ]
}
```

For an example, see [Example 2: Configuring replication when the source and destination buckets are owned by different accounts](replication-walkthrough-2.md)\.

If objects in the source bucket are tagged, note the following:
+ If the source bucket owner grants Amazon S3 permission for the `s3:GetObjectVersionTagging` and `s3:ReplicateTags` actions to replicate object tags \(through the IAM role\), Amazon S3 replicates the tags along with the objects\. For information about the IAM role, see [Creating an IAM role](#setting-repl-config-same-acctowner)\. 
+ If the owner of the destination bucket doesn't want to replicate the tags, they can add the following statement to the destination bucket policy to explicitly deny permission for the `s3:ReplicateTags` action\.

  ```
  ...
     "Statement":[
        {
           "Effect":"Deny",
           "Principal":{
              "AWS":"arn:aws:iam::SourceBucket-AcctID:root"
           },
           "Action":"s3:ReplicateTags",
           "Resource":"arn:aws:s3:::DestinationBucket/*"
        }
     ]
  ...
  ```

### Changing replica ownership<a name="change-replica-ownership"></a>

When different AWS accounts own the source and destination buckets, you can tell Amazon S3 to change the ownership of the replica to the AWS account that owns the destination bucket\. This is called the *owner override* option\. For more information, see [Changing the replica owner](replication-change-owner.md)\.


# Troubleshooting Amazon S3 by Symptom<a name="troubleshooting-by-symptom"></a>

The following topics list symptoms to help you troubleshoot some of the issues that you might encounter when working with Amazon S3\.

**Topics**
+ [Significant Increases in HTTP 503 Responses to Requests to Buckets with Versioning Enabled](#troubleshooting-by-symptom-increase-503-reponses)
+ [Unexpected Behavior When Accessing Buckets Set with CORS](#troubleshooting-by-symptom-increase)

## Significant Increases in HTTP 503 Responses to Amazon S3 Requests to Buckets with Versioning Enabled<a name="troubleshooting-by-symptom-increase-503-reponses"></a>

If you notice a significant increase in the number of HTTP 503\-slow down responses received for Amazon S3 PUT or DELETE object requests to a bucket that has versioning enabled, you might have one or more objects in the bucket for which there are millions of versions\. When you have objects with millions of versions, Amazon S3 automatically throttles requests to the bucket to protect the customer from an excessive amount of request traffic, which could potentially impede other requests made to the same bucket\. 

To determine which S3 objects have millions of versions, use the Amazon S3 inventory tool\. The inventory tool generates a report that provides a flat file list of the objects in a bucket\. For more information, see [ Amazon S3 inventory](storage-inventory.md)\.

The Amazon S3 team encourages customers to investigate applications that repeatedly overwrite the same S3 object, potentially creating millions of versions for that object, to determine whether the application is working as intended\. If you have a use case that requires millions of versions for one or more S3 objects, contact the AWS Support team at [AWS Support](https://console.aws.amazon.com/support/home) to discuss your use case and to help us assist you in determining the optimal solution for your use case scenario\.

To help prevent this issue, consider the following best practices:
+ Enable a lifecycle management "NonCurrentVersion" expiration policy and an "ExpiredObjectDeleteMarker" policy to expire the previous versions of objects and delete markers without associated data objects in the bucket\. 
+ Keep your directory structure as flat as possible and make each directory name unique\.

## Unexpected Behavior When Accessing Buckets Set with CORS<a name="troubleshooting-by-symptom-increase"></a>

 If you encounter unexpected behavior when accessing buckets set with the cross\-origin resource sharing \(CORS\) configuration, see [Troubleshooting CORS issues](cors-troubleshooting.md)\.


# Upload an object using a presigned URL \(AWS SDK for \.NET\)<a name="UploadObjectPreSignedURLDotNetSDK"></a>

The following C\# example shows how to use the AWS SDK for \.NET to upload an object to an S3 bucket using a presigned URL\. For more information about presigned URLs, see [Uploading objects using presigned URLs](PresignedUrlUploadObject.md)\.

This example generates a presigned URL for a specific object and uses it to upload a file\. For information about the example's compatibility with a specific version of the AWS SDK for \.NET and instructions about how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.IO;
using System.Net;

namespace Amazon.DocSamples.S3
{
    class UploadObjectUsingPresignedURLTest
    {
        private const string bucketName = "*** provide bucket name ***";
        private const string objectKey  = "*** provide the name for the uploaded object ***";
        private const string filePath   = "*** provide the full path name of the file to upload ***";
        // Specify how long the presigned URL lasts, in hours
        private const double timeoutDuration = 12;
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2; 
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            var url = GeneratePreSignedURL(timeoutDuration);
            UploadObject(url);
        }

        private static void UploadObject(string url)
        {
            HttpWebRequest httpRequest = WebRequest.Create(url) as HttpWebRequest;
            httpRequest.Method = "PUT";
            using (Stream dataStream = httpRequest.GetRequestStream())
            {
                var buffer = new byte[8000];
                using (FileStream fileStream = new FileStream(filePath, FileMode.Open, FileAccess.Read))
                {
                    int bytesRead = 0;
                    while ((bytesRead = fileStream.Read(buffer, 0, buffer.Length)) > 0)
                    {
                        dataStream.Write(buffer, 0, bytesRead);
                    }
                }
            }
            HttpWebResponse response = httpRequest.GetResponse() as HttpWebResponse;
        }

        private static string GeneratePreSignedURL(double duration)
        {
            var request = new GetPreSignedUrlRequest
            {
                BucketName = bucketName,
                Key        = objectKey,
                Verb       = HttpVerb.PUT,
                Expires    = DateTime.UtcNow.AddHours(duration)
            };

           string url = s3Client.GetPreSignedURL(request);
           return url;
        }
    }
}
```

## More info<a name="UploadObjectPreSignedURLDotNetSDK-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# Appendix b: Authenticating requests \(AWS signature version 2\)<a name="auth-request-sig-v2"></a>

**Important**  
This section describes how to authenticate requests using AWS Signature Version 2\. Signature Version 2 is being turned off \(deprecated\),  Amazon S3 will only accept API requests that are signed using Signature Version 4\. For more information, see [AWS Signature Version 2 Turned Off \(Deprecated\) for Amazon S3](UsingAWSSDK.md#UsingAWSSDK-sig2-deprecation)   
Signature Version 4 is supported in all AWS Regions, and it is the only version that is supported for new Regions\. For more information, see [Authenticating Requests \(AWS Signature Version 4\)](https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html) in the *Amazon Simple Storage Service API Reference*\.   
Amazon S3 offers you the ability to identify what API signature version was used to sign a request\. It is important to identify if any of your workflows are utilizing Signature Version 2 signing and upgrading them to use Signature Version 4 to prevent impact to your business\.   
If you are using CloudTrail event logs\(recommended option\), please see [Using AWS CloudTrail to identify Amazon S3 signature version 2 requests ](cloudtrail-request-identification.md#cloudtrail-identification-sigv2-requests) on how to query and identify such requests\. 
If you are using the Amazon S3 Server Access logs, see [ Using Amazon S3 access logs to identify signature version 2 requests ](using-s3-access-logs-to-identify-requests.md#using-s3-access-logs-to-identify-sigv2-requests) 

**Topics**
+ [Authenticating requests using the REST API](S3_Authentication2.md)
+ [Signing and authenticating REST requests](RESTAuthentication.md)
+ [Browser\-based uploads using POST \(AWS signature version 2\)](UsingHTTPPOST.md)


# Deleting Amazon S3 log files<a name="deleting-log-files-lifecycle"></a>

An S3 bucket with server access logging enabled can accumulate many server log objects over time\. Your application might need these access logs for a specific period after creation, and after that, you might want to delete them\. You can use Amazon S3 lifecycle configuration to set rules so that Amazon S3 automatically queues these objects for deletion at the end of their life\. 

You can define a lifecycle configuration for a subset of objects in your S3 bucket by using a shared prefix \(that is, objects that have names that begin with a common string\)\. If you specified a prefix in your server access logging configuration, you can set a lifecycle configuration rule to delete log objects that have that prefix\. For example, if your log objects have the prefix `logs/`, you can set a lifecycle configuration rule to delete all objects in the bucket that have the prefix `/logs` after a specified period of time\. For more information about lifecycle configuration, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

## Related resources<a name="deleting-log-files-lifecycle-more-info"></a>

[Amazon S3 server access logging](ServerLogs.md)


# Using the AWS SDK for PHP and Running PHP Examples<a name="UsingTheMPphpAPI"></a>

The AWS SDK for PHP provides access to the API for Amazon S3 bucket and object operations\. The SDK gives you the option of using the service's low\-level API or using higher\-level abstractions\.

The SDK is available at [AWS SDK for PHP](https://aws.amazon.com/sdk-for-php/), which also has instructions for installing and getting started with the SDK\. 

The setup for using the AWS SDK for PHP depends on your environment and how you want to run your application\. To set up your environment to run the examples in this documentation, see the [AWS SDK for PHP Getting Started Guide](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/welcome.html#getting-started)\.

**Topics**
+ [AWS SDK for PHP Levels](#TheMPphpAPI)
+ [Running PHP Examples](#running-php-samples)
+ [Related Resources](#RelatedResources-UsingTheMPphpAPI)

## AWS SDK for PHP Levels<a name="TheMPphpAPI"></a>

The AWS SDK for PHP gives you the option of using a high\-level or low\-level API\. 

### Low\-Level API<a name="Lowlevel-php-api"></a>

The low\-level APIs correspond to the underlying Amazon S3 REST operations, including the create, update, and delete operations on buckets and objects\. The low\-level APIs provide greater control over these operations\. For example, you can batch your requests and run them in parallel\. Or, when using the multipart upload API, you can manage the object parts individually\. Note that these low\-level API calls return a result that includes all of the Amazon S3 response details\. For more information about the multipart upload API, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\.

### High\-Level Abstractions<a name="Highlevel-php-api"></a>

The high\-level abstractions are intended to simplify common use cases\. For example, for uploading large objects using the low\-level API, you call `Aws\S3\S3Client::createMultipartUpload()`, call the `Aws\S3\S3Client::uploadPart()` method to upload the object parts, then call the `Aws\S3\S3Client::completeMultipartUpload()` method to complete the upload\. You can use the higher\-level `Aws\S3\\MultipartUploader` object that simplifies creating a multipart upload instead\.

As another example, when enumerating objects in a bucket, you can use the iterators feature of the AWS SDK for PHP to return all of the object keys, regardless of how many objects you have stored in the bucket\. If you use the low\-level API, the response returns a maximum of 1,000 keys\. If a bucket contains more than 1,000 objects, the result is truncated and you have to manage the response and check for truncation\.

## Running PHP Examples<a name="running-php-samples"></a>

To set up and use the Amazon S3 samples for version 3 of the AWS SDK for PHP, see [ Installation](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_installation.html) in the AWS SDK for PHP Developer Guide\.

## Related Resources<a name="RelatedResources-UsingTheMPphpAPI"></a>
+ [AWS SDK for PHP for Amazon S3](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-examples.html)
+ [AWS SDK for PHP Documentation](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/welcome.html) 
+ [AWS SDK for PHP API for Amazon S3](https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html) 


# Setting Up the AWS CLI<a name="setup-aws-cli"></a>

Follow the steps to download and configure AWS Command Line Interface \(AWS CLI\)\.

**Note**  
Services in AWS, such as Amazon S3, require that you provide credentials when you access them\. The service can then determine whether you have permissions to access the resources that it owns\. The console requires your password\. You can create access keys for your AWS account to access the AWS CLI or API\. However, we don't recommend that you access AWS using the credentials for your AWS account\. Instead, we recommend that you use AWS Identity and Access Management \(IAM\)\. Create an IAM user, add the user to an IAM group with administrative permissions, and then grant administrative permissions to the IAM user that you created\. You can then access AWS using a special URL and that IAM user's credentials\. For instructions, go to [Creating Your First IAM User and Administrators Group](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html) in the *IAM User Guide*\.





**To set up the AWS CLI**

1.  Download and configure the AWS CLI\. For instructions, see the following topics in the *AWS Command Line Interface User Guide*: 
   +  [Getting Set Up with the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html) 
   +  [Configuring the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html) 

1.  Add a named profile for the administrator user in the AWS CLI config file\. You use this profile when executing the AWS CLI commands\. 

   ```
   [adminuser] 
   aws_access_key_id = adminuser access key ID 
   aws_secret_access_key = adminuser secret access key 
   region = aws-region
   ```

    For a list of available AWS Regions, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html) in the *AWS General Reference*\. 

1.  Verify the setup by typing the following commands at the command prompt\. 
   +  Try the `help` command to verify that the AWS CLI is installed on your computer: 

     ```
     aws help  
     ```
   +  Try an `S3` command to verify that the user can reach Amazon S3\. This command lists buckets in your account\. The AWS CLI uses the `adminuser` credentials to authenticate the request\. 

     ```
      aws s3 ls --profile adminuser
     ```


# String Functions<a name="s3-glacier-select-sql-reference-string"></a>

Amazon S3 Select and S3 Glacier Select support the following string functions\.

**Topics**
+ [CHAR\_LENGTH, CHARACTER\_LENGTH](#s3-glacier-select-sql-reference-char-length)
+ [LOWER](#s3-glacier-select-sql-reference-lower)
+ [SUBSTRING](#s3-glacier-select-sql-reference-substring)
+ [TRIM](#s3-glacier-select-sql-reference-trim)
+ [UPPER](#s3-glacier-select-sql-reference-upper)

## CHAR\_LENGTH, CHARACTER\_LENGTH<a name="s3-glacier-select-sql-reference-char-length"></a>

Counts the number of characters in the specified string\.

**Note**  
`CHAR_LENGTH` and `CHARACTER_LENGTH` are synonyms\.

### Syntax<a name="s3-glacier-select-sql-reference-char-length-syntax"></a>

```
CHAR_LENGTH ( string )
```

### Parameters<a name="s3-glacier-select-sql-reference-char-length-parameters"></a>

 *string*   
The target string that the function operates on\.

### Examples<a name="s3-glacier-select-sql-reference-char-length-examples"></a>

```
CHAR_LENGTH('')          -- 0
CHAR_LENGTH('abcdefg')   -- 7
```

## LOWER<a name="s3-glacier-select-sql-reference-lower"></a>

Given a string, converts all uppercase characters to lowercase characters\. Any non\-uppercased characters remain unchanged\.

### Syntax<a name="s3-glacier-select-sql-reference-lower-syntax"></a>

```
LOWER ( string )
```

### Parameters<a name="s3-glacier-select-sql-reference-lower-parameters"></a>

 *string*   
The target string that the function operates on\.

### Examples<a name="s3-glacier-select-sql-reference-lower-examples"></a>

```
LOWER('AbCdEfG!@#$') -- 'abcdefg!@#$'
```

## SUBSTRING<a name="s3-glacier-select-sql-reference-substring"></a>

Given a string, a start index, and optionally a length, returns the substring from the start index up to the end of the string, or up to the length provided\.

**Note**  
The first character of the input string has index 1\. If `start` is < 1, it is set to 1\.

### Syntax<a name="s3-glacier-select-sql-reference-substring-syntax"></a>

```
SUBSTRING( string FROM start [ FOR length ] )
```

### Parameters<a name="s3-glacier-select-sql-reference-substring-parameters"></a>

 *string*   
The target string that the function operates on\.

 *start*   
The start position of the string\.

 *length*   
The length of the substring to return\. If not present, proceed to the end of the string\.

### Examples<a name="s3-glacier-select-sql-reference-substring-examples"></a>

```
SUBSTRING("123456789", 0)      -- "123456789"
SUBSTRING("123456789", 1)      -- "123456789"
SUBSTRING("123456789", 2)      -- "23456789"
SUBSTRING("123456789", -4)     -- "123456789"
SUBSTRING("123456789", 0, 999) -- "123456789" 
SUBSTRING("123456789", 1, 5)   -- "12345"
```

## TRIM<a name="s3-glacier-select-sql-reference-trim"></a>

Trims leading or trailing characters from a string\. The default character to remove is ' '\.

### Syntax<a name="s3-glacier-select-sql-reference-trim-syntax"></a>

```
TRIM ( [[LEADING | TRAILING | BOTH remove_chars] FROM] string )
```

### Parameters<a name="s3-glacier-select-sql-reference-trim-parameters"></a>

 *string*   
The target string that the function operates on\.

 *LEADING \| TRAILING \| BOTH*   
Whether to trim leading or trailing characters, or both leading and trailing characters\.

 *remove\_chars*   
The set of characters to remove\. Note that `remove_chars` can be a string with length > 1\. This function returns the string with any character from `remove_chars` found at the beginning or end of the string that was removed\.

### Examples<a name="s3-glacier-select-sql-reference-trim-examples"></a>

```
TRIM('       foobar         ')               -- 'foobar'
TRIM('      \tfoobar\t         ')            -- '\tfoobar\t'
TRIM(LEADING FROM '       foobar         ')  -- 'foobar         '
TRIM(TRAILING FROM '       foobar         ') -- '       foobar'
TRIM(BOTH FROM '       foobar         ')     -- 'foobar'
TRIM(BOTH '12' FROM '1112211foobar22211122') -- 'foobar'
```

## UPPER<a name="s3-glacier-select-sql-reference-upper"></a>

Given a string, converts all lowercase characters to uppercase characters\. Any non\-lowercased characters remain unchanged\.

### Syntax<a name="s3-glacier-select-sql-reference-upper-syntax"></a>

```
UPPER ( string )
```

### Parameters<a name="s3-glacier-select-sql-reference-upper-parameters"></a>

 *string*   
The target string that the function operates on\.

### Examples<a name="s3-glacier-select-sql-reference-upper-examples"></a>

```
UPPER('AbCdEfG!@#$') -- 'ABCDEFG!@#$'
```


# Example 2: Bucket owner granting cross\-account bucket permissions<a name="example-walkthroughs-managing-access-example2"></a>

**Topics**
+ [Step 0: Preparing for the walkthrough](#cross-acct-access-step0)
+ [Step 1: Do the account a tasks](#access-policies-walkthrough-cross-account-permissions-acctA-tasks)
+ [Step 2: Do the account b tasks](#access-policies-walkthrough-cross-account-permissions-acctB-tasks)
+ [Step 3: Extra credit: Try explicit deny](#access-policies-walkthrough-example2-explicit-deny)
+ [Step 4: Clean up](#access-policies-walkthrough-example2-cleanup-step)

An AWS account—for example, Account A—can grant another AWS account, Account B, permission to access its resources such as buckets and objects\. Account B can then delegate those permissions to users in its account\. In this example scenario, a bucket owner grants cross\-account permission to another account to perform specific bucket operations\.

**Note**  
Account A can also directly grant a user in Account B permissions using a bucket policy\. But the user will still need permission from the parent account, Account B, to which the user belongs, even if Account B does not have permissions from Account A\. As long as the user has permission from both the resource owner and the parent account, the user will be able to access the resource\.

The following is a summary of the walkthrough steps:

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/access-policy-ex2.png)

1. Account A administrator user attaches a bucket policy granting cross\-account permissions to Account B to perform specific bucket operations\.

   Note that administrator user in Account B will automatically inherit the permissions\.

1. Account B administrator user attaches user policy to the user delegating the permissions it received from Account A\.

1. User in Account B then verifies permissions by accessing an object in the bucket owned by Account A\.

For this example, you need two accounts\. The following table shows how we refer to these accounts and the administrator users in them\. Per IAM guidelines \(see [About using an administrator user to create resources and grant permissions](example-walkthroughs-managing-access.md#about-using-root-credentials)\) we do not use the account root credentials in this walkthrough\. Instead, you create an administrator user in each account and use those credentials in creating resources and granting them permissions\.


| AWS account ID | Account referred to as | Administrator user in the account  | 
| --- | --- | --- | 
|  *1111\-1111\-1111*  |  Account A  |  AccountAadmin  | 
|  *2222\-2222\-2222*  |  Account B  |  AccountBadmin  | 

All the tasks of creating users and granting permissions are done in the AWS Management Console\. To verify permissions, the walkthrough uses the command line tools, AWS Command Line Interface \(CLI\) and AWS Tools for Windows PowerShell, so you don't need to write any code\.

## Step 0: Preparing for the walkthrough<a name="cross-acct-access-step0"></a>

1. Make sure you have two AWS accounts and that each account has one administrator user as shown in the table in the preceding section\.

   1. Sign up for an AWS account, if needed\. 

      1.  Go to [https://aws\.amazon\.com/s3/](https://aws.amazon.com/s3/) and click **Create an AWS Account**\. 

      1. Follow the on\-screen instructions\.

         AWS will notify you by email when your account is active and available for you to use\.

   1. Using Account A credentials, sign in to the [IAM console](https://console.aws.amazon.com/iam/home?#home) to create the administrator user:

      1. Create user AccountAadmin and note down the security credentials\. For instructions, see [Creating an IAM User in Your AWS Account](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html) in the *IAM User Guide*\. 

      1. Grant AccountAadmin administrator privileges by attaching a user policy giving full access\. For instructions, see [Working with Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html) in the *IAM User Guide*\. 

   1. While you are in the IAM console, note down the **IAM User Sign\-In URL** on the **Dashboard**\. All users in the account must use this URL when signing in to the AWS Management Console\.

      For more information, see [How Users Sign in to Your Account](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_how-users-sign-in.html) in *IAM User Guide*\. 

   1. Repeat the preceding step using Account B credentials and create administrator user AccountBadmin\.

1. Set up either the AWS Command Line Interface \(CLI\) or the AWS Tools for Windows PowerShell\. Make sure you save administrator user credentials as follows:
   + If using the AWS CLI, create two profiles, AccountAadmin and AccountBadmin, in the config file\.
   + If using the AWS Tools for Windows PowerShell, make sure you store credentials for the session as AccountAadmin and AccountBadmin\.

   For instructions, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\. 

1. Save the administrator user credentials, also referred to as profiles\. You can use the profile name instead of specifying credentials for each command you enter\. For more information, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\. 

   1. Add profiles in the AWS CLI credentials file for each of the administrator users in the two accounts\. 

      ```
      [AccountAadmin]
      aws_access_key_id = access-key-ID
      aws_secret_access_key = secret-access-key
      region = us-east-1
      
      [AccountBadmin]
      aws_access_key_id = access-key-ID
      aws_secret_access_key = secret-access-key
      region = us-east-1
      ```

   1. If you are using the AWS Tools for Windows PowerShell

      ```
      set-awscredentials –AccessKey AcctA-access-key-ID –SecretKey AcctA-secret-access-key –storeas AccountAadmin
      set-awscredentials –AccessKey AcctB-access-key-ID –SecretKey AcctB-secret-access-key –storeas AccountBadmin
      ```

## Step 1: Do the account a tasks<a name="access-policies-walkthrough-cross-account-permissions-acctA-tasks"></a>

### Step 1\.1: Sign in to the AWS Management Console<a name="access-policies-walkthrough-cross-account-permissions-acctA-tasks-sign-in"></a>

Using the IAM user sign\-in URL for Account A first sign in to the AWS Management Console as AccountAadmin user\. This user will create a bucket and attach a policy to it\. 

### Step 1\.2: Create a bucket<a name="access-policies-walkthrough-example2a-create-bucket"></a>

1. In the Amazon S3 console, create a bucket\. This exercise assumes the bucket is created in the US East \(N\. Virginia\) region and is named `examplebucket`\.

   For instructions, see [How Do I Create an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\. 

1. Upload a sample object to the bucket\.

   For instructions, go to [Add an Object to a Bucket](https://docs.aws.amazon.com/AmazonS3/latest/gsg/PuttingAnObjectInABucket.html) in the *Amazon Simple Storage Service Getting Started Guide*\. 

### Step 1\.3: Attach a bucket policy to grant cross\-account permissions to account b<a name="access-policies-walkthrough-example2a"></a>

The bucket policy grants the `s3:GetBucketLocation` and `s3:ListBucket` permissions to Account B\. It is assumed you are still signed into the console using AccountAadmin user credentials\.

1. Attach the following bucket policy to `examplebucket`\. The policy grants Account B permission for the `s3:GetBucketLocation` and `s3:ListBucket` actions\.

   For instructions, see [How Do I Add an S3 Bucket Policy?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html) in the *Amazon Simple Storage Service Console User Guide*\. 

   ```
   {
      "Version": "2012-10-17",
      "Statement": [
         {
            "Sid": "Example permissions",
            "Effect": "Allow",
            "Principal": {
               "AWS": "arn:aws:iam::AccountB-ID:root"
            },
            "Action": [
               "s3:GetBucketLocation",
               "s3:ListBucket"
            ],
            "Resource": [
               "arn:aws:s3:::awsexamplebucket1"
            ]
         }
      ]
   }
   ```

1. Verify Account B \(and thus its administrator user\) can perform the operations\.
   + Using the AWS CLI

     ```
     aws s3 ls s3://examplebucket --profile AccountBadmin
     aws s3api get-bucket-location --bucket examplebucket --profile AccountBadmin
     ```
   + Using the AWS Tools for Windows PowerShell

     ```
     get-s3object -BucketName example2bucket -StoredCredentials AccountBadmin 
     get-s3bucketlocation -BucketName example2bucket -StoredCredentials AccountBadmin
     ```

## Step 2: Do the account b tasks<a name="access-policies-walkthrough-cross-account-permissions-acctB-tasks"></a>

Now the Account B administrator creates a user, Dave, and delegates the permissions received from Account A\. 

### Step 2\.1: Sign in to the AWS Management Console<a name="access-policies-walkthrough-cross-account-permissions-acctB-tasks-sign-in"></a>

Using the IAM user sign\-in URL for Account B, first sign in to the AWS Management Console as AccountBadmin user\. 

### Step 2\.2: Create user dave in account b<a name="access-policies-walkthrough-example2b-create-user"></a>

In the IAM console, create a user, Dave\. 

For instructions, see [Creating IAM Users \(AWS Management Console\)](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console) in the *IAM User Guide*\. 

### Step 2\.3: Delegate permissions to user dave<a name="access-policies-walkthrough-example2-delegate-perm-userdave"></a>

Create an inline policy for the user Dave by using the following policy\. You will need to update the policy by providing your bucket name\.

It is assumed you are signed in to the console using AccountBadmin user credentials\.

```
{
   "Version": "2012-10-17",
   "Statement": [
      {
         "Sid": "Example",
         "Effect": "Allow",
         "Action": [
            "s3:ListBucket"
         ],
         "Resource": [
            "arn:aws:s3:::awsexamplebucket1"
         ]
      }
   ]
}
```

For instructions, see [Working with Inline Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_inline-using.html) in the *IAM User Guide*\.

### Step 2\.4: Test permissions<a name="access-policies-walkthrough-example2b-user-dave-access"></a>

Now Dave in Account B can list the contents of `examplebucket` owned by Account A\. You can verify the permissions using either of the following procedures\. 

**Test using the AWS CLI**

1. Add the UserDave profile to the AWS CLI config file\. For more information about the config file, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\.

   ```
   [profile UserDave]
   aws_access_key_id = access-key
   aws_secret_access_key = secret-access-key
   region = us-east-1
   ```

1. At the command prompt, enter the following AWS CLI command to verify Dave can now get an object list from the `examplebucket` owned by Account A\. Note the command specifies the UserDave profile\.

   ```
   aws s3 ls s3://examplebucket --profile UserDave
   ```

   Dave does not have any other permissions\. So if he tries any other operation—for example, the following get bucket location—Amazon S3 returns permission denied\. 

   ```
   aws s3api get-bucket-location --bucket examplebucket --profile UserDave
   ```

**Test using AWS Tools for Windows PowerShell**

1. Store Dave's credentials as AccountBDave\.

   ```
   set-awscredentials -AccessKey AccessKeyID -SecretKey SecretAccessKey -storeas AccountBDave
   ```

1. Try the List Bucket command\.

   ```
   get-s3object -BucketName example2bucket -StoredCredentials AccountBDave
   ```

   Dave does not have any other permissions\. So if he tries any other operation—for example, the following get bucket location—Amazon S3 returns permission denied\. 

   ```
   get-s3bucketlocation -BucketName example2bucket -StoredCredentials AccountBDave
   ```

## Step 3: Extra credit: Try explicit deny<a name="access-policies-walkthrough-example2-explicit-deny"></a>

You can have permissions granted via an ACL, a bucket policy, and a user policy\. But if there is an explicit deny set via either a bucket policy or a user policy, the explicit deny takes precedence over any other permissions\. For testing, let's update the bucket policy and explicitly deny Account B the `s3:ListBucket` permission\. The policy also grants `s3:ListBucket` permission, but explicit deny takes precedence, and Account B or users in Account B will not be able to list objects in `examplebucket`\.

1. Using credentials of user AccountAadmin in Account A, replace the bucket policy by the following\. 

   ```
   {
      "Version": "2012-10-17",
      "Statement": [
         {
            "Sid": "Example permissions",
            "Effect": "Allow",
            "Principal": {
               "AWS": "arn:aws:iam::AccountB-ID:root"
            },
            "Action": [
               "s3:GetBucketLocation",
               "s3:ListBucket"
            ],
            "Resource": [
               "arn:aws:s3:::awsexamplebucket1"
            ]
         },
         {
            "Sid": "Deny permission",
            "Effect": "Deny",
            "Principal": {
               "AWS": "arn:aws:iam::AccountB-ID:root"
            },
            "Action": [
               "s3:ListBucket"
            ],
            "Resource": [
               "arn:aws:s3:::awsexamplebucket1"
            ]
         }
      ]
   }
   ```

1. Now if you try to get a bucket list using AccountBadmin credentials, you will get access denied\.
   + Using the AWS CLI:

     ```
     aws s3 ls s3://examplebucket --profile AccountBadmin
     ```
   + Using the AWS Tools for Windows PowerShell:

     ```
     get-s3object -BucketName example2bucket -StoredCredentials AccountBDave
     ```

## Step 4: Clean up<a name="access-policies-walkthrough-example2-cleanup-step"></a>

1. After you are done testing, you can do the following to clean up\.

   1. Sign in to the AWS Management Console \([AWS Management Console](https://console.aws.amazon.com/)\) using Account A credentials, and do the following:
     + In the Amazon S3 console, remove the bucket policy attached to *examplebucket*\. In the bucket **Properties**, delete the policy in the **Permissions** section\. 
     + If the bucket is created for this exercise, in the Amazon S3 console, delete the objects and then delete the bucket\. 
     + In the IAM console, remove the AccountAadmin user\.

1. Sign in to the AWS Management Console \([AWS Management Console](https://console.aws.amazon.com/)\) using Account B credentials\. In the IAM console, delete user AccountBadmin\.


# Lifecycle configuration elements<a name="intro-lifecycle-rules"></a>

**Topics**
+ [ID element](#intro-lifecycle-rule-id)
+ [Status element](#intro-lifecycle-rule-status)
+ [Filter element](#intro-lifecycle-rules-filter)
+ [Elements to describe lifecycle actions](#intro-lifecycle-rules-actions)

You specify an S3 Lifecycle configuration as XML, consisting of one or more Lifecycle rules\. 

```
<LifecycleConfiguration>
    <Rule>
         ...
    </Rule>
    <Rule>
         ...
    </Rule>
</LifecycleConfiguration>
```

Each rule consists of the following:
+ Rule metadata that include a rule ID, and status indicating whether the rule is enabled or disabled\. If a rule is disabled, Amazon S3 doesn't perform any actions specified in the rule\.
+ Filter identifying objects to which the rule applies\. You can specify a filter by using an object key prefix, one or more object tags, or both\. 
+ One or more transition or expiration actions with a date or a time period in the object's lifetime when you want Amazon S3 to perform the specified action\. 

The following sections describe the XML elements in an S3 Lifecycle configuration\. For example configurations, see [Examples of lifecycle configuration](lifecycle-configuration-examples.md)\.

## ID element<a name="intro-lifecycle-rule-id"></a>

An S3 Lifecycle configuration can have up to 1,000 rules\. The <ID> element uniquely identifies a rule\. ID length is limited to 255 characters\.

## Status element<a name="intro-lifecycle-rule-status"></a>

The <Status> element value can be either Enabled or Disabled\. If a rule is disabled, Amazon S3 doesn't perform any of the actions defined in the rule\.

## Filter element<a name="intro-lifecycle-rules-filter"></a>

A Lifecycle rule can apply to all or a subset of objects in a bucket based on the <Filter> element that you specify in the Lifecycle rule\. 

You can filter objects by key prefix, object tags, or a combination of both \(in which case Amazon S3 uses a logical AND to combine the filters\)\. Consider the following examples:
+ **Specifying a filter using key prefixes** – This example shows an S3 Lifecycle rule that applies to a subset of objects based on the key name prefix \(`logs/`\)\. For example, the Lifecycle rule applies to objects `logs/mylog.txt`, `logs/temp1.txt`, and `logs/test.txt`\. The rule does not apply to the object `example.jpg`\.

  ```
  <LifecycleConfiguration>
      <Rule>
          <Filter>
             <Prefix>logs/</Prefix>
          </Filter>
          transition/expiration actions.
           ...
      </Rule>
      ...
  </LifecycleConfiguration>
  ```

  If you want to apply a Lifecycle action to a subset of objects based on different key name prefixes, specify separate rules\. In each rule, specify a prefix\-based filter\. For example, to describe a Lifecycle action for objects with key prefixes `projectA/` and `projectB/`, you specify two rules as shown following\. 

  ```
  <LifecycleConfiguration>
      <Rule>
          <Filter>
             <Prefix>projectA/</Prefix>
          </Filter>
          transition/expiration actions.
           ...
      </Rule>
  
      <Rule>
          <Filter>
             <Prefix>projectB/</Prefix>
          </Filter>
          transition/expiration actions.
           ...
      </Rule>
  </LifecycleConfiguration>
  ```

  

  For more information about object keys, see [Object keys](UsingMetadata.md#object-keys)\. 
+ **Specifying a filter based on object tags** – In the following example, the Lifecycle rule specifies a filter based on a tag \(*key*\) and value \(*value*\)\. The rule then applies only to a subset of objects with the specific tag\.

  ```
  <LifecycleConfiguration>
      <Rule>
          <Filter>
             <Tag>
                <Key>key</Key>
                <Value>value</Value>
             </Tag>
          </Filter>
          transition/expiration actions.
          ...
      </Rule>
  </LifecycleConfiguration>
  ```

  You can specify a filter based on multiple tags\. You must wrap the tags in the <AND> element shown in the following example\. The rule directs Amazon S3 to perform lifecycle actions on objects with two tags \(with the specific tag key and value\)\.

  ```
  <LifecycleConfiguration>
      <Rule>
        <Filter>
           <And>
              <Tag>
                 <Key>key1</Key>
                 <Value>value1</Value>
              </Tag>
              <Tag>
                 <Key>key2</Key>
                 <Value>value2</Value>
              </Tag>
               ...
            </And>
        </Filter>
        transition/expiration actions.
      </Rule>
  </Lifecycle>
  ```

  The Lifecycle rule applies to objects that have both of the tags specified\. Amazon S3 performs a logical AND\. Note the following:
  + Each tag must match both key and value exactly\.
  + The rule applies to a subset of objects that has all the tags specified in the rule\. If an object has additional tags specified, the rule will still apply\.
**Note**  
When you specify multiple tags in a filter, each tag key must be unique\.
+ **Specifying a filter based on both prefix and one or more tags** – In a Lifecycle rule, you can specify a filter based on both the key prefix and one or more tags\. Again, you must wrap all of these in the <And> element as shown following\.

  ```
  <LifecycleConfiguration>
      <Rule>
          <Filter>
            <And>
               <Prefix>key-prefix</Prefix>
               <Tag>
                  <Key>key1</Key>
                  <Value>value1</Value>
               </Tag>
               <Tag>
                  <Key>key2</Key>
                  <Value>value2</Value>
               </Tag>
                ...
            </And>
          </Filter>
          <Status>Enabled</Status>
          transition/expiration actions.
      </Rule>
  </LifecycleConfiguration>
  ```

  Amazon S3 combines these filters using a logical AND\. That is, the rule applies to subset of objects with a specific key prefix and specific tags\. A filter can have only one prefix, and zero or more tags\.
+ You can specify an empty filter, in which case the rule applies to all objects in the bucket\.

  ```
  <LifecycleConfiguration>
      <Rule>
          <Filter>
          </Filter>
          <Status>Enabled</Status>
          transition/expiration actions.
      </Rule>
  </LifecycleConfiguration>
  ```

## Elements to describe lifecycle actions<a name="intro-lifecycle-rules-actions"></a>

You can direct Amazon S3 to perform specific actions in an object's lifetime by specifying one or more of the following predefined actions in an S3 Lifecycle rule\. The effect of these actions depends on the versioning state of your bucket\. 
+ **Transition** action element – You specify the `Transition` action to transition objects from one storage class to another\. For more information about transitioning objects, see [Supported transitions and related constraints](lifecycle-transition-general-considerations.md#lifecycle-general-considerations-transition-sc)\. When a specified date or time period in the object's lifetime is reached, Amazon S3 performs the transition\. 

  For a versioned bucket \(versioning\-enabled or versioning\-suspended bucket\), the `Transition` action applies to the current object version\. To manage noncurrent versions, Amazon S3 defines the `NoncurrentVersionTransition` action \(described below\)\.
+ **Expiration action element** – The `Expiration` action expires objects identified in the rule and applies to eligible objects in any of the Amazon S3 storage classes\. For more information about storage classes, see [Amazon S3 storage classes](storage-class-intro.md)\. Amazon S3 makes all expired objects unavailable\. Whether the objects are permanently removed depends on the versioning state of the bucket\. 
**Important**  
Object expiration Lifecycle policies do not remove incomplete multipart uploads\. To remove incomplete multipart uploads, you must use the **AbortIncompleteMultipartUpload** Lifecycle configuration action that is described later in this section\. 
  + **Non\-versioned bucket** – The `Expiration` action results in Amazon S3 permanently removing the object\. 
  + **Versioned bucket** – For a versioned bucket \(that is, versioning\-enabled or versioning\-suspended\), there are several considerations that guide how Amazon S3 handles the `expiration` action\. For more information, see [Using versioning](Versioning.md)\. Regardless of the versioning state, the following applies:
    + The `Expiration` action applies only to the current version \(it has no impact on noncurrent object versions\)\.
    + Amazon S3 doesn't take any action if there are one or more object versions and the delete marker is the current version\.
    + If the current object version is the only object version and it is also a delete marker \(also referred as an *expired object delete marker*, where all object versions are deleted and you only have a delete marker remaining\), Amazon S3 removes the expired object delete marker\. You can also use the expiration action to direct Amazon S3 to remove any expired object delete markers\. For an example, see [Example 7: Removing expired object delete markers](lifecycle-configuration-examples.md#lifecycle-config-conceptual-ex7)\. 

    Also consider the following when setting up Amazon S3 to manage expiration:
    + **Versioning\-enabled bucket** 

      If the current object version is not a delete marker, Amazon S3 adds a delete marker with a unique version ID\. This makes the current version noncurrent, and the delete marker the current version\. 
    + **Versioning\-suspended bucket** 

      In a versioning\-suspended bucket, the expiration action causes Amazon S3 to create a delete marker with null as the version ID\. This delete marker replaces any object version with a null version ID in the version hierarchy, which effectively deletes the object\. 

In addition, Amazon S3 provides the following actions that you can use to manage noncurrent object versions in a versioned bucket \(that is, versioning\-enabled and versioning\-suspended buckets\)\.
+ **NoncurrentVersionTransition** action element – Use this action to specify how long \(from the time the objects became noncurrent\) you want the objects to remain in the current storage class before Amazon S3 transitions them to the specified storage class\. For more information about transitioning objects, see [Supported transitions and related constraints](lifecycle-transition-general-considerations.md#lifecycle-general-considerations-transition-sc)\. 
+ **NoncurrentVersionExpiration** action element – Use this action to specify how long \(from the time the objects became noncurrent\) you want to retain noncurrent object versions before Amazon S3 permanently removes them\. The deleted object can't be recovered\. 

  This delayed removal of noncurrent objects can be helpful when you need to correct any accidental deletes or overwrites\. For example, you can configure an expiration rule to delete noncurrent versions five days after they become noncurrent\. For example, suppose that on 1/1/2014 10:30 AM UTC, you create an object called `photo.gif` \(version ID 111111\)\. On 1/2/2014 11:30 AM UTC, you accidentally delete `photo.gif` \(version ID 111111\), which creates a delete marker with a new version ID \(such as version ID 4857693\)\. You now have five days to recover the original version of `photo.gif` \(version ID 111111\) before the deletion is permanent\. On 1/8/2014 00:00 UTC, the Lifecycle rule for expiration executes and permanently deletes `photo.gif` \(version ID 111111\), five days after it became a noncurrent version\. 
**Important**  
Object expiration Lifecycle policies do not remove incomplete multipart uploads\. To remove incomplete multipart uploads, you must use the **AbortIncompleteMultipartUpload** Lifecycle configuration action that is described later in this section\. 

In addition to the transition and expiration actions, you can use the following Lifecycle configuration action to direct Amazon S3 to stop incomplete multipart uploads\. 
+ **AbortIncompleteMultipartUpload** action element – Use this element to set a maximum time \(in days\) that you want to allow multipart uploads to remain in progress\. If the applicable multipart uploads \(determined by the key name `prefix` specified in the Lifecycle rule\) are not successfully completed within the predefined time period, Amazon S3 stops the incomplete multipart uploads\. For more information, see [Stopping incomplete multipart uploads using a bucket lifecycle policy](mpuoverview.md#mpu-stop-incomplete-mpu-lifecycle-config)\. 
**Note**  
You cannot specify this Lifecycle action in a rule that specifies a filter based on object tags\. 

  
+ **ExpiredObjectDeleteMarker** action element – In a versioning\-enabled bucket, a delete marker with zero noncurrent versions is referred to as the expired object delete marker\. You can use this Lifecycle action to direct S3 to remove the expired object delete markers\. For an example, see [Example 7: Removing expired object delete markers](lifecycle-configuration-examples.md#lifecycle-config-conceptual-ex7)\.
**Note**  
You cannot specify this Lifecycle action in a rule that specifies a filter based on object tags\. 

### How Amazon S3 calculates how long an object has been noncurrent<a name="non-current-days-calculations"></a>

In a versioning\-enabled bucket, you can have multiple versions of an object, there is always one current version, and zero or more noncurrent versions\. Each time you upload an object, the current version is retained as the noncurrent version and the newly added version, the successor, becomes the current version\. To determine the number of days an object is noncurrent, Amazon S3 looks at when its successor was created\. Amazon S3 uses the number of days since its successor was created as the number of days an object is noncurrent\.

**Restoring previous versions of an object when using lifecycle configurations**  
 As explained in detail in the topic [Restoring previous versions](RestoringPreviousVersions.md), you can use either of the following two methods to retrieve previous versions of an object:  
By copying a noncurrent version of the object into the same bucket\. The copied object becomes the current version of that object, and all object versions are preserved\.
By permanently deleting the current version of the object\. When you delete the current object version, you, in effect, turn the noncurrent version into the current version of that object\. 
When you are using S3 Lifecycle configuration rules with versioning\-enabled buckets, we recommend as a best practice that you use the first method\.   
Lifecycle operates under an eventually consistent model\. A current version that you permanently deleted may not disappear until the changes propagate \(Amazon S3 may be unaware of this deletion\)\. In the meantime, the lifecycle rule that you configured to expire noncurrent objects may permanently remove noncurrent objects, including the one you want to restore\. So, copying the old version, as recommended in the first method, is the safer alternative\.

The following table summarizes the behavior of the S3 Lifecycle configuration rule actions on objects in relation to the versioning state of the bucket containing the object\.


**Lifecycle actions and bucket versioning state**  

| Action | Nonversioned bucket \(versioning not enabled\) | Versioning\-enabled bucket | Versioning\-suspended bucket | 
| --- | --- | --- | --- | 
|  `Transition` When a specified date or time period in the object's lifetime is reached\.  | Amazon S3 transitions the object to the specified storage class\. | Amazon S3 transitions the current version of the object to the specified storage class\. | Same behavior as a versioning\-enabled bucket\. | 
|  `Expiration` When a specified date or time period in the object's lifetime is reached\.  | Expiration deletes the object, and the deleted object cannot be recovered\. | If the current version is not a delete marker, Amazon S3 creates a delete marker, which becomes the current version, and the existing current version is retained as a noncurrent version\. | The lifecycle creates a delete marker with null version ID, which becomes the current version\. If the version ID of the current version of the object is null, the Expiration action permanently deletes this version\. Otherwise, the current version is retained as a noncurrent version\. | 
|  `NoncurrentVersionTransition` When the specified number of days from when the object becomes noncurrent is reached\.  | NoncurrentVersionTransition has no effect\. |  Amazon S3 transitions the noncurrent object versions to the specified storage class\.  | Same behavior as a versioning\-enabled bucket\. | 
|  `NoncurrentVersionExpiration` When the specified number of days from when the object becomes noncurrent is reached\.  | NoncurrentVersionExpiration has no effect\. | NoncurrentVersionExpiration action deletes the noncurrent version of the object, and the deleted object cannot be recovered\. | Same behavior as a versioning\-enabled bucket\. | 

### Lifecycle rules: Based on an object's age<a name="intro-lifecycle-rules-number-of-days"></a>

You can specify a time period, in number of days from the creation \(or modification\) of the objects, when Amazon S3 can take the action\. 

When you specify the number of days in the `Transition` and `Expiration` actions in an S3 Lifecycle configuration, note the following:
+ It is the number of days since object creation when the action will occur\.
+ Amazon S3 calculates the time by adding the number of days specified in the rule to the object creation time and rounding the resulting time to the next day midnight UTC\. For example, if an object was created at 1/15/2014 10:30 AM UTC and you specify 3 days in a transition rule, then the transition date of the object would be calculated as 1/19/2014 00:00 UTC\. 

**Note**  
Amazon S3 maintains only the last modified date for each object\. For example, the Amazon S3 console shows the **Last Modified** date in the object **Properties** pane\. When you initially create a new object, this date reflects the date the object is created\. If you replace the object, the date changes accordingly\. So when we use the term *creation date*, it is synonymous with the term *last modified date*\. 

When specifying the number of days in the `NoncurrentVersionTransition` and `NoncurrentVersionExpiration` actions in a Lifecycle configuration, note the following:
+ It is the number of days from when the version of the object becomes noncurrent \(that is, when the object is overwritten or deleted\), that Amazon S3 will perform the action on the specified object or objects\.
+ Amazon S3 calculates the time by adding the number of days specified in the rule to the time when the new successor version of the object is created and rounding the resulting time to the next day midnight UTC\. For example, in your bucket, suppose that you have a current version of an object that was created at 1/1/2014 10:30 AM UTC\. If the new version of the object that replaces the current version is created at 1/15/2014 10:30 AM UTC, and you specify 3 days in a transition rule, the transition date of the object is calculated as 1/19/2014 00:00 UTC\. 

### Lifecycle rules: Based on a specific date<a name="intro-lifecycle-rules-date"></a>

When specifying an action in an S3 Lifecycle rule, you can specify a date when you want Amazon S3 to take the action\. When the specific date arrives, Amazon S3 applies the action to all qualified objects \(based on the filter criteria\)\. 

If you specify an S3 Lifecycle action with a date that is in the past, all qualified objects become immediately eligible for that Lifecycle action\.

**Important**  
The date\-based action is not a one\-time action\. Amazon S3 continues to apply the date\-based action even after the date has passed, as long as the rule status is `Enabled`\.  
For example, suppose that you specify a date\-based `Expiration` action to delete all objects \(assume no filter specified in the rule\)\. On the specified date, Amazon S3 expires all the objects in the bucket\. S3 also continues to expire any new objects you create in the bucket\. To stop the Lifecycle action, you must remove the action from the Lifecycle configuration, disable the rule, or delete the rule from the Lifecycle configuration\.

The date value must conform to the ISO 8601 format\. The time is always midnight UTC\. 

**Note**  
You can't create the date\-based Lifecycle rules using the Amazon S3 console, but you can view, disable, or delete such rules\. 


# List multipart uploads to an S3 Bucket using the AWS SDK for \.NET \(low\-level\)<a name="LLlistMPuploadsDotNet"></a>

To list all of the in\-progress multipart uploads on a specific bucket, use the AWS SDK for \.NET low\-level multipart upload API's `ListMultipartUploadsRequest` class\. The `AmazonS3Client.ListMultipartUploads` method returns an instance of the `ListMultipartUploadsResponse` class that provides information about the in\-progress multipart uploads\. 

An in\-progress multipart upload is a multipart upload that has been initiated using the initiate multipart upload request, but has not yet been completed or stopped\. For more information about Amazon S3 multipart uploads, see [Multipart upload overview](mpuoverview.md)\.

The following C\# example shows how to use the AWS SDK for \.NET to list all in\-progress multipart uploads on a bucket\. For information about the example's compatibility with a specific version of the AWS SDK for \.NET and instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
ListMultipartUploadsRequest request = new ListMultipartUploadsRequest
{
	 BucketName = bucketName // Bucket receiving the uploads.
};

ListMultipartUploadsResponse response = await AmazonS3Client.ListMultipartUploadsAsync(request);
```

## More info<a name="LLlistMPuploadsDotNet-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# Getting started with Amazon S3 on Outposts<a name="S3OutpostsGS"></a>

With Amazon S3 on Outposts, you can use the Amazon S3 APIs and features, such as object storage, access policies, encryption, and tagging, on AWS Outposts as you do on Amazon S3\. For information about AWS Outposts, see [ What is AWS Outposts?](https://docs.aws.amazon.com/outposts/latest/userguide/what-is-outposts.html) in the *AWS Outposts User Guide*\. 

**Topics**
+ [Order an Outpost](#OrderOutposts)
+ [Setting up S3 on Outposts](#SettingUpS3Outposts)

## Ordering your AWS Outpost<a name="OrderOutposts"></a>

To get started with Amazon S3 on Outposts, you need an Outpost with Amazon S3 capacity deployed at your facility\. For information about options for ordering an Outpost and S3 capacity, see [AWS Outposts](http://aws.amazon.com/outposts)\. For specifications, restrictions, and limitations, see [Amazon S3 on Outposts restrictions and limitations](S3OnOutpostsRestrictionsLimitations.md)\.

### Do you need a new AWS Outpost?<a name="SettingUpS3OutpostsNewOutpost"></a>

If you need to order a new Outpost with S3 capacity, see [AWS Outposts pricing](http://aws.amazon.com/outposts/pricing/) to understand the capacity option for Amazon EC2, Amazon EBS, and Amazon S3\. Current options for Amazon S3 on Outposts capacity are 48 TB and 96 TB\. 

After you select your configuration, follow the steps in [Create an Outpost and order Outpost capacity](https://docs.aws.amazon.com/outposts/latest/userguide/order-outpost-capacity.html) in the *AWS Outposts User Guide\.* 

### Do you already have an AWS Outpost?<a name="SettingUpS3OutpostsExistingOutpost"></a>

If AWS Outposts is already on your site, depending on your current Outpost configuration and storage capacity, you may be able to add Amazon S3 storage to an existing Outpost, or you may need to work with your AWS account team to add additional hardware to support Amazon S3 on Outposts\.

## Setting up S3 on Outposts<a name="SettingUpS3Outposts"></a>

After your S3 on Outposts capacity is provisioned, you can create buckets and S3 access points on your Outpost using the [ AWS Outposts console](https://console.aws.amazon.com/outposts), the Amazon S3 on Outposts REST API, the AWS Command Line Interface \(AWS CLI\), or the AWS SDKs\. You can then use APIs to store and retrieve objects from these buckets\. You can also use AWS DataSync to transfer data between your Outpost and the AWS Region\. For more information, see [Working with Amazon S3 on Outposts](WorkingWithS3Outposts.md)\.

You can manage your Amazon S3 storage on Outposts using the same services that you use in\-Region today\. These include AWS Identity and Access Management \(IAM\) and Amazon S3 Access Points to control access to objects and buckets, Amazon CloudWatch to monitor operational health, and AWS CloudTrail to track and report on object\-level and bucket\-level activity\.

After AWS enables your S3 on Outposts capacity, you can access S3 on Outposts using the AWS Outposts or Amazon S3 consoles, the Amazon S3 REST API, the AWS CLI, or the AWS SDKs\.


# Versioned object permissions<a name="VersionedObjectPermissionsandACLs"></a>

Permissions are set at the version level\. Each version has its own object owner; an AWS account that creates the object version is the owner\. So, you can set different permissions for different versions of the same object\. To do so, you must specify the version ID of the object whose permissions you want to set in a `PUT Object versionId acl` request\. For a detailed description and instructions on using ACLs, see [Identity and access management in Amazon S3](s3-access-control.md)\.

**Example Setting permissions for an object version**  
The following request sets the permission of the grantee, `BucketOwner@amazon.com`, to `FULL_CONTROL` on the key, `my-image.jpg`, version ID, 3HL4kqtJvjVBH40Nrjfkd\.  

```
 1. PUT /my-image.jpg?acl&versionId=3HL4kqtJvjVBH40Nrjfkd HTTP/1.1
 2. Host: bucket.s3.amazonaws.com
 3. Date: Wed, 28 Oct 2009 22:32:00 GMT
 4. Authorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=
 5. Content-Length: 124
 6.  
 7. <AccessControlPolicy>
 8.   <Owner>
 9.     <ID>75cc57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a</ID>
10.     <DisplayName>mtd@amazon.com</DisplayName>
11.   </Owner>
12.   <AccessControlList>
13.     <Grant>
14.       <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="CanonicalUser">
15.         <ID>a9a7b886d6fd24a52fe8ca5bef65f89a64e0193f23000e241bf9b1c61be666e9</ID>
16.         <DisplayName>BucketOwner@amazon.com</DisplayName>
17.       </Grantee>
18.       <Permission>FULL_CONTROL</Permission>
19.     </Grant>
20.   </AccessControlList>
21.   </AccessControlPolicy>
```

Likewise, to get the permissions of a specific object version, you must specify its version ID in a `GET Object versionId acl` request\. You need to include the version ID because, by default, `GET Object acl` returns the permissions of the current version of the object\. 

**Example Retrieving the permissions for a specified object version**  
In the following example, Amazon S3 returns the permissions for the key, `my-image.jpg`, version ID, DVBH40Nr8X8gUMLUo\.  

```
1. GET /my-image.jpg?versionId=DVBH40Nr8X8gUMLUo&acl HTTP/1.1
2. Host: bucket.s3.amazonaws.com
3. Date: Wed, 28 Oct 2009 22:32:00 GMT
4. Authorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU
```

For more information, see [GET Object acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETacl.html)\.


# Using the AWS SDKs, CLI, and Explorers<a name="UsingAWSSDK"></a>



You can use the AWS SDKs when developing applications with Amazon S3\. The AWS SDKs simplify your programming tasks by wrapping the underlying REST API\. The AWS Mobile SDKs and the AWS Amplify JavaScript library are also available for building connected mobile and web applications using AWS\. 

This section provides an overview of using AWS SDKs for developing Amazon S3 applications\. This section also describes how you can test the AWS SDK code examples provided in this guide\. 

**Topics**
+ [Specifying the Signature Version in Request Authentication](#specify-signature-version)
+ [Setting Up the AWS CLI](setup-aws-cli.md)
+ [Using the AWS SDK for Java](UsingTheMPJavaAPI.md)
+ [Using the AWS SDK for \.NET](UsingTheMPDotNetAPI.md)
+ [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md)
+ [Using the AWS SDK for Ruby \- Version 3](UsingTheMPRubyAPI.md)
+ [Using the AWS SDK for Python \(Boto\)](UsingTheBotoAPI.md)
+ [Using the AWS Mobile SDKs for iOS and Android](using-mobile-sdks.md)
+ [Using the AWS Amplify JavaScript Library](using-aws-amplify.md)

In addition to the AWS SDKs, AWS Explorers are available for Visual Studio and Eclipse for Java IDE\. In this case, the SDKs and the explorers are available bundled together as AWS Toolkits\. 

You can also use the AWS Command Line Interface \(AWS CLI\) to manage Amazon S3 buckets and objects\.

**AWS Toolkit for Eclipse**  
The AWS Toolkit for Eclipse includes both the AWS SDK for Java and AWS Explorer for Eclipse\. The AWS Explorer for Eclipse is an open source plugin for Eclipse for Java IDE that makes it easier for developers to develop, debug, and deploy Java applications using AWS\. The easy\-to\-use GUI enables you to access and administer your AWS infrastructure including Amazon S3\. You can perform common operations such as managing your buckets and objects and setting IAM policies, while developing applications, all from within the context of Eclipse for Java IDE\. For set up instructions, see [Set up the Toolkit](https://docs.aws.amazon.com/eclipse-toolkit/latest/user-guide/setup-install.html)\. For examples of using the explorer, see [How to Access AWS Explorer](https://docs.aws.amazon.com/eclipse-toolkit/latest/user-guide/open-aws-explorer.html)\. 

**AWS Toolkit for Visual Studio**  
AWS Explorer for Visual Studio is an extension for Microsoft Visual Studio that makes it easier for developers to develop, debug, and deploy \.NET applications using Amazon Web Services\. The easy\-to\-use GUI enables you to access and administer your AWS infrastructure including Amazon S3\. You can perform common operations such as managing your buckets and objects or setting IAM policies, while developing applications, all from within the context of Visual Studio\. For setup instructions, go to [Setting Up the AWS Toolkit for Visual Studio](https://docs.aws.amazon.com/AWSToolkitVS/latest/UserGuide/tkv_setup.html)\. For examples of using Amazon S3 using the explorer, see [Using Amazon S3 from AWS Explorer](https://docs.aws.amazon.com/AWSToolkitVS/latest/UserGuide/using-s3.html)\. 

**AWS SDKs**  
You can download only the SDKs\. For information about downloading the SDK libraries, see [Sample Code Libraries](https://aws.amazon.com/tools/)\. 

**AWS CLI**  
The AWS CLI is a unified tool to manage your AWS services, including Amazon S3\. For information about downloading the AWS CLI, see [AWS Command Line Interface](https://aws.amazon.com/cli/)\. 

## Specifying the Signature Version in Request Authentication<a name="specify-signature-version"></a>

 Amazon S3 supports only AWS Signature Version 4 in most AWS Regions\. In some of the older AWS Regions, Amazon S3 supports both Signature Version 4 and Signature Version 2\. However, Signature Version 2 is being turned off \(deprecated\)\. For more information about the end of support for Signature Version 2, see [AWS Signature Version 2 Turned Off \(Deprecated\) for Amazon S3](#UsingAWSSDK-sig2-deprecation)\.

For a list of all the Amazon S3 Regions and the signature versions they support, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.

For all AWS Regions, AWS SDKs use Signature Version 4 by default to authenticate requests\. When using AWS SDKs that were released before May 2016, you might be required to request Signature Version 4, as shown in the following table\.


| SDK | Requesting Signature Version 4 for Request Authentication | 
| --- | --- | 
| AWS CLI |  For the default profile, run the following command: <pre>$ aws configure set default.s3.signature_version s3v4</pre> For a custom profile, run the following command: <pre>$ aws configure set profile.your_profile_name.s3.signature_version s3v4</pre>  | 
| Java SDK |  Add the following in your code: <pre>System.setProperty(SDKGlobalConfiguration.ENABLE_S3_SIGV4_SYSTEM_PROPERTY, "true");</pre> Or, on the command line, specify the following: <pre>-Dcom.amazonaws.services.s3.enableV4</pre>  | 
|  JavaScript SDK |  Set the `signatureVersion` parameter to `v4` when constructing the client: <pre>var s3 = new AWS.S3({signatureVersion: 'v4'});</pre>  | 
| PHP SDK |  Set the `signature` parameter to `v4` when constructing the Amazon S3 service client for PHP SDK v2: <pre><?php <br />$client = S3Client::factory([<br />    'region' => 'YOUR-REGION',<br />    'version' => 'latest',<br />    'signature' => 'v4'<br />]);</pre> When using the PHP SDK v3, set the `signature_version` parameter to `v4` during construction of the Amazon S3 service client: <pre><?php <br />$s3 = new Aws\S3\S3Client([<br />    'version' => '2006-03-01',<br />    'region' => 'YOUR-REGION',<br />    'signature_version' => 'v4'<br />]);</pre>  | 
| Python\-Boto SDK |  Specify the following in the boto default config file: <pre>[s3] use-sigv4 = True</pre>  | 
| Ruby SDK |  Ruby SDK \- Version 1: Set the `:s3_signature_version` parameter to `:v4` when constructing the client: <pre>s3 = AWS::S3::Client.new(:s3_signature_version => :v4)</pre> Ruby SDK \- Version 3: Set the `signature_version` parameter to `v4` when constructing the client: <pre>s3 = Aws::S3::Client.new(signature_version: 'v4')</pre>  | 
| \.NET SDK |  Add the following to the code before creating the Amazon S3 client: <pre>AWSConfigsS3.UseSignatureVersion4 = true;</pre> Or, add the following to the config file: <pre><appSettings><br />   <add key="AWS.S3.UseSignatureVersion4" value="true" /><br /></appSettings></pre>  | 

 

### AWS Signature Version 2 Turned Off \(Deprecated\) for Amazon S3<a name="UsingAWSSDK-sig2-deprecation"></a>

Signature Version 2 is being turned off \(deprecated\) in Amazon S3\.  Amazon S3 will then only accept API requests that are signed using Signature Version 4\. 

This section provides answers to common questions regarding the end of support for Signature Version 2\. 

**What is Signature Version 2/4, and What Does It Mean to Sign Requests?**  
The Signature Version 2 or Signature Version 4 signing process is used to authenticate your Amazon S3 API requests\. Signing requests enables Amazon S3 to identify who is sending the request and protects your requests from bad actors\.

For more information about signing AWS requests, see [Signing AWS API Requests](https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html) in the *AWS General Reference*\. 

**What Update Are You Making?**  
We currently support Amazon S3 API requests that are signed using Signature Version 2 and Signature Version 4 processes\. After that, Amazon S3 will only accept requests that are signed using Signature Version 4\. 

For more information about signing AWS requests, see [Changes in Signature Version 4](https://docs.aws.amazon.com/general/latest/gr/sigv4_changes.html) in the *AWS General Reference*\. 

**Why Are You Making the Update?**  
Signature Version 4 provides improved security by using a signing key instead of your secret access key\. Signature Version 4 is currently supported in all AWS Regions, whereas Signature Version 2 is only supported in Regions that were launched before January 2014\. This update allows us to provide a more consistent experience across all Regions\. 

**How Do I Ensure That I'm Using Signature Version 4, and What Updates Do I Need?**  
The signature version that is used to sign your requests is usually set by the tool or the SDK on the client side\. By default, the latest versions of our AWS SDKs use Signature Version 4\. For third\-party software, contact the appropriate support team for your software to confirm what version you need\. If you are sending direct REST calls to Amazon S3, you must modify your application to use the Signature Version 4 signing process\. 

For information about which version of the AWS SDKs to use when moving to Signature Version 4, see [Moving from Signature Version 2 to Signature Version 4](#UsingAWSSDK-move-to-Sig4)\. 

For information about using Signature Version 4 with the Amazon S3 REST API, see [Authenticating Requests \(AWS Signature Version 4\)](https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html) in the *Amazon Simple Storage Service API Reference*\.

**What Happens if I Don't Make Updates?**  
Requests signed with Signature Version 2 that are made after that will fail to authenticate with Amazon S3\. Requesters will see errors stating that the request must be signed with Signature Version 4\. 

**Should I Make Changes Even if I’m Using a Presigned URL That Requires Me to Sign for More than 7 Days?**  
If you are using a presigned URL that requires you to sign for more than 7 days, no action is currently needed\. You can continue to use AWS Signature Version 2 to sign and authenticate the presigned URL\. We will follow up and provide more details on how to migrate to Signature Version 4 for a presigned URL scenario\. 

#### More Info<a name="UsingAWSSDK-sev2-deprecation-more-info"></a>
+ For more information about using Signature Version 4, see [Signing AWS API Requests](https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html)\.
+ View the list of changes between Signature Version 2 and Signature Version 4 in [Changes in Signature Version 4](https://docs.aws.amazon.com/general/latest/gr/sigv4_changes.html)\. 
+ View the post [AWS Signature Version 4 to replace AWS Signature Version 2 for signing Amazon S3 API requests](https://forums.aws.amazon.com/ann.jspa?annID=5816) in the AWS forums\.
+ If you have any questions or concerns, contact [AWS Support](https://docs.aws.amazon.com/awssupport/latest/user/getting-started.html)\.

   

### Moving from Signature Version 2 to Signature Version 4<a name="UsingAWSSDK-move-to-Sig4"></a>

If you currently use Signature Version 2 for Amazon S3 API request authentication, you should move to using Signature Version 4\. Support is ending for Signature Version 2, as described in [AWS Signature Version 2 Turned Off \(Deprecated\) for Amazon S3](#UsingAWSSDK-sig2-deprecation)\.

For information about using Signature Version 4 with the Amazon S3 REST API, see [Authenticating Requests \(AWS Signature Version 4\)](https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html) in the *Amazon Simple Storage Service API Reference*\.

The following table lists the SDKs with the necessary minimum version to use Signature Version 4 \(SigV4\)\.  If you are using presigned URLs with the AWS Java, JavaScript \(Node\.js\), or Python \(Boto/CLI\) SDKs, you must set the correct AWS Region and set Signature Version 4 in the client configuration\. For information about setting `SigV4` in the client configuration, see [Specifying the Signature Version in Request Authentication](#specify-signature-version)\.


| If you use this SDK/Product | Upgrade to this SDK version | Code change needed to the client to use Sigv4? | Link to SDK documentation | 
| --- | --- | --- | --- | 
|  AWS SDK for Java v1  | Upgrade to Java 1\.11\.201\+ or v2 in Q4 2018\. | Yes | [Specifying the Signature Version in Request Authentication](#specify-signature-version) | 
|  AWS SDK for Java v2 \(preview\)  | No SDK upgrade is needed\. | No | [AWS SDK for Java](https://aws.amazon.com/sdk-for-java/) | 
|  AWS SDK for \.NET v1   | Upgrade to 3\.1\.10 or later\. | Yes | [AWS SDK for \.NET](https://github.com/aws/aws-sdk-net/tree/aws-sdk-net-v1/) | 
|  AWS SDK for \.NET v2   | Upgrade to 3\.1\.10 or later\. | No | [AWS SDK for \.NET v2](https://github.com/aws/aws-sdk-net/tree/aws-sdk-net-v2/) | 
|  AWS SDK for \.NET v3   | Upgrade to 3\.3\.0\.0 or later\. | Yes | [AWS SDK for \.NET v3](https://github.com/aws/aws-sdk-net) | 
|  AWS SDK for JavaScript v1   | Upgrade to 2\.68\.0 or later\. | Yes | [AWS SDK for JavaScript](https://github.com/aws/aws-sdk-js) | 
|  AWS SDK for JavaScript v2   | Upgrade to 2\.68\.0 or later\. | Yes | [AWS SDK for JavaScript](https://github.com/aws/aws-sdk-js) | 
|  AWS SDK for JavaScript v3   | No action is currently needed\. Upgrade to major version V3 in Q3 2019\. | No | [AWS SDK for JavaScript](https://github.com/aws/aws-sdk-js) | 
|  AWS SDK for PHP v1   | Recommend to upgrade to the most recent version of PHP or, at least to v2\.7\.4 with the signature parameter set to v4 in the S3 client's configuration\. | Yes | [AWS SDK for PHP](https://aws.amazon.com/sdk-for-php/) | 
|  AWS SDK for PHP v2   | Recommend to upgrade to the most recent version of PHP or, at least to v2\.7\.4 with the signature parameter set to v4 in the S3 client's configuration\. | No | [AWS SDK for PHP](https://aws.amazon.com/sdk-for-php/) | 
|  AWS SDK for PHP v3   | No SDK upgrade is needed\. | No | [AWS SDK for PHP](https://aws.amazon.com/sdk-for-php/) | 
|  Boto2   | Upgrade to Boto2 v2\.49\.0\. | Yes | [Boto 2 Upgrade](https://github.com/boto/boto/commit/16729da27b95d6dbbd81bcebb43bcf099ce23fd3) | 
|  Boto3   | Upgrade to 1\.5\.71 \(Botocore\), 1\.4\.6 \(Boto3\)\. | Yes | [Boto 3 \- AWS SDK for Python](https://github.com/boto/boto3) | 
|  AWS CLI   | Upgrade to 1\.11\.108\. | Yes | [AWS Command Line Interface](https://aws.amazon.com/cli/) | 
|  AWS CLI v2 \(preview\)   | No SDK upgrade is needed\. | No | [AWS Command Line Interface version 2](https://github.com/aws/aws-cli/tree/v2) | 
|  AWS SDK for Ruby v1   | Upgrade to Ruby V3\. | Yes | [Ruby V3 for AWS](https://rubygems.org/gems/aws-sdk/versions) | 
|  AWS SDK for Ruby v2   | Upgrade to Ruby V3\. | Yes | [Ruby V3 for AWS](https://rubygems.org/gems/aws-sdk/versions) | 
|  AWS SDK for Ruby v3   | No SDK upgrade is needed\. | No | [Ruby V3 for AWS](https://rubygems.org/gems/aws-sdk/versions) | 
|  Go   | No SDK upgrade is needed\. | No | [AWS SDK for Go](https://aws.amazon.com/sdk-for-go/) | 
|  C\+\+   | No SDK upgrade is needed\. | No | [AWS SDK for C\+\+](https://aws.amazon.com/sdk-for-cpp/) | 

**AWS Tools for Windows PowerShell or AWS Tools for PowerShell Core**  
If you are using module versions *earlier* than 3\.3\.0\.0, you must upgrade to 3\.3\.0\.0\. 

To get the version information, use the `Get-Module` cmdlet: 

```
          Get-Module –Name AWSPowershell
          Get-Module –Name AWSPowershell.NetCore
```



To update the 3\.3\.0\.0 version, use the `Update-Module` cmdlet: 

```
          Update-Module –Name AWSPowershell
          Update-Module –Name AWSPowershell.NetCore
```

You can use presigned URLs that are valid for more than 7 days that you will send Signature Version 2 traffic on\.


# Amazon S3 storage classes<a name="storage-class-intro"></a>



Each object in Amazon S3 has a storage class associated with it\. For example, if you list the objects in an S3 bucket, the console shows the storage class for all the objects in the list\.

Amazon S3 offers a range of storage classes for the objects that you store\. You choose a class depending on your use case scenario and performance access requirements\. All of these storage classes offer high durability\. 

**Topics**
+ [Storage classes for frequently accessed objects](#sc-freq-data-access)
+ [Storage class for automatically optimizing data with changing or unknown access patterns](#sc-dynamic-data-access)
+ [Storage classes for infrequently accessed objects](#sc-infreq-data-access)
+ [Storage classes for archiving objects](#sc-glacier)
+ [Storage class for Amazon S3 on Outposts](#s3-outposts)
+ [Comparing the Amazon S3 storage classes](#sc-compare)
+ [Setting the storage class of an object](#sc-howtoset)

## Storage classes for frequently accessed objects<a name="sc-freq-data-access"></a>

For performance\-sensitive use cases \(those that require millisecond access time\) and frequently accessed data, Amazon S3 provides the following storage classes:
+ **S3 Standard**—The default storage class\. If you don't specify the storage class when you upload an object, Amazon S3 assigns the S3 Standard storage class\.
+ **Reduced Redundancy**—The Reduced Redundancy Storage \(RRS\) storage class is designed for noncritical, reproducible data that can be stored with less redundancy than the S3 Standard storage class\.
**Important**  
We recommend that you not use this storage class\. The S3 Standard storage class is more cost effective\. 

  For durability, RRS objects have an average annual expected loss of 0\.01 percent of objects\. If an RRS object is lost, when requests are made to that object, Amazon S3 returns a 405 error\.

## Storage class for automatically optimizing data with changing or unknown access patterns<a name="sc-dynamic-data-access"></a>

S3 Intelligent\-Tiering is an Amazon S3 storage class designed to optimize storage costs by automatically moving data to the most cost\-effective access tier, without operational overhead\. It is the only cloud storage that delivers automatic cost savings by moving data on a granular object level between access tiers when access patterns change\. S3 Intelligent\-Tiering is the perfect storage class when you want to optimize storage costs for data that has unknown or changing access patterns\. There are no retrieval fees for S3 Intelligent\-Tiering\. 

For a small monthly object monitoring and automation fee, S3 Intelligent\-Tiering monitors the access patterns and moves the objects automatically from one tier to another\. It works by storing objects in four access tiers: two low latency access tiers optimized for frequent and infrequent access, and two opt\-in archive access tiers designed for asynchronous access that are optimized for rare access\. 

Objects that are uploaded or transitioned to S3 Intelligent\-Tiering are automatically stored in the *Frequent Access* tier\. S3 Intelligent\-Tiering works by monitoring access patterns and then moving the objects that have not been accessed in 30 consecutive days to the *Infrequent Access* tier\. After you activate one or both of the archive access tiers, S3 Intelligent\-Tiering automatically moves objects that haven’t been accessed for 90 consecutive days to the *Archive Access* tier, and after 180 consecutive days of no access, to the *Deep Archive Access* tier\. 

If the objects are accessed later, the objects are moved back to the *Frequent Access* tier\. There are no retrieval fees, so you won’t see unexpected increases in storage bills when access patterns change\.

**Note**  
The S3 Intelligent\-Tiering storage class is suitable for objects larger than 128 KB that you plan to store for at least 30 days\. If the size of an object is less than 128 KB, it is not eligible for automatic tiering\. Smaller objects can be stored, but they are always charged at the frequent access tier rates in the S3 Intelligent\-Tiering storage class\.   
If you delete an object before the end of the 30\-day minimum storage duration period, you are charged for 30 days\. For pricing information, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

## Storage classes for infrequently accessed objects<a name="sc-infreq-data-access"></a>

The **S3 Standard\-IA** and **S3 One Zone\-IA** storage classes are designed for long\-lived and infrequently accessed data\. \(IA stands for *infrequent access*\.\) S3 Standard\-IA and S3 One Zone\-IA objects are available for millisecond access \(same as the S3 Standard storage class\)\. Amazon S3 charges a retrieval fee for these objects, so they are most suitable for infrequently accessed data\. For pricing information, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\. 

For example, you might choose the S3 Standard\-IA and S3 One Zone\-IA storage classes:
+ For storing backups\. 
+ For older data that is accessed infrequently, but that still requires millisecond access\. For example, when you upload data, you might choose the S3 Standard storage class, and use lifecycle configuration to tell Amazon S3 to transition the objects to the S3 Standard\-IA or S3 One Zone\-IA class\. 

  For more information about lifecycle management, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

**Note**  
The S3 Standard\-IA and S3 One Zone\-IA storage classes are suitable for objects larger than 128 KB that you plan to store for at least 30 days\. If an object is less than 128 KB, Amazon S3 charges you for 128 KB\. If you delete an object before the end of the 30\-day minimum storage duration period, you are charged for 30 days\. For pricing information, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

These storage classes differ as follows:
+ S3 Standard\-IA—Amazon S3 stores the object data redundantly across multiple geographically separated Availability Zones \(similar to the S3 Standard storage class\)\. S3 Standard\-IA objects are resilient to the loss of an Availability Zone\. This storage class offers greater availability and resiliency than the S3 One Zone\-IA class\. 
+ S3 One Zone\-IA—Amazon S3 stores the object data in only one Availability Zone, which makes it less expensive than S3 Standard\-IA\. However, the data is not resilient to the physical loss of the Availability Zone resulting from disasters, such as earthquakes and floods\. The S3 One Zone\-IA storage class is as durable as Standard\-IA, but it is less available and less resilient\. For a comparison of storage class durability and availability, see the Durability and Availability table at the end of this section\. For pricing, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\. 

We recommend the following:
+ S3 Standard\-IA—Use for your primary or only copy of data that can't be re\-created\. 
+ S3 One Zone\-IA—Use if you can re\-create the data if the Availability Zone fails, and for object replicas when setting S3 Cross\-Region Replication \(CRR\)\. 

## Storage classes for archiving objects<a name="sc-glacier"></a>

The **S3 Glacier** and **S3 Glacier Deep Archive** storage classes are designed for low\-cost data archiving\. These storage classes offer the same durability and resiliency as the S3 Standard storage class\. For a comparison of storage class durability and availability, see the table at the end of this section\.

These storage classes differ as follows:
+ **S3 Glacier**—Use for archives where portions of the data might need to be retrieved in minutes\. Data stored in the S3 Glacier storage class has a minimum storage duration period of 90 days and can be accessed in as little as 1\-5 minutes using expedited retrieval\. If you have deleted, overwritten, or transitioned to a different storage class an object before the 90\-day minimum, you are charged for 90 days\. For pricing information, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\. 
+ **S3 Glacier Deep Archive**—Use for archiving data that rarely needs to be accessed\. Data stored in the S3 Glacier Deep Archive storage class has a minimum storage duration period of 180 days and a default retrieval time of 12 hours\. If you have deleted, overwritten, or transitioned to a different storage class an object before the 180\-day minimum, you are charged for 180 days\. For pricing information, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\. 

  S3 Glacier Deep Archive is the lowest cost storage option in AWS\. Storage costs for S3 Glacier Deep Archive are less expensive than using the S3 Glacier storage class\. You can reduce S3 Glacier Deep Archive retrieval costs by using bulk retrieval, which returns data within 48 hours\. 

### Retrieving archived objects<a name="sc-glacier-restore"></a>

You can set the storage class of an object to S3 Glacier or S3 Glacier Deep Archive in the same ways that you do for the other storage classes as described in the section [Setting the storage class of an object](#sc-howtoset)\. However, the S3 Glacier and S3 Glacier Deep Archive objects are not available for real\-time access\. You must first restore the S3 Glacier and S3 Glacier Deep Archive objects before you can access them\. \(S3 Standard, RRS, S3 Standard\-IA, S3 One Zone\-IA, and S3 Intelligent\-Tiering objects are available for anytime access\.\) For more information about retrieving archived objects, see [Restoring archived objects](restoring-objects.md)\.

**Important**  
When you choose the S3 Glacier or S3 Glacier Deep Archive storage class, your objects remain in Amazon S3\. You cannot access them directly through the separate Amazon S3 Glacier service\. 

To learn more about the Amazon S3 Glacier service, see the [Amazon S3 Glacier Developer Guide](https://docs.aws.amazon.com/amazonglacier/latest/dev/)\.

## Storage class for Amazon S3 on Outposts<a name="s3-outposts"></a>

With Amazon S3 on Outposts, you can create S3 buckets on your AWS Outposts and easily store and retrieve objects on\-premises for applications that require local data access, local data processing, and data residency\. S3 on Outposts provides a new storage class, S3 Outposts \[OUTPOSTS\], and allows you to use the same APIs and features on Outposts as you do on Amazon S3 such as access policies, encryption, and tagging\. The S3 Outposts storage class is only available for objects stored in buckets on Outposts, and attempting to use this storage class with an S3 bucket in an AWS Region will result in an `InvalidStorageClass` error\. Conversely, attempting to use other S3 storage classes with S3 on Outposts will result in this same error response\. You can use S3 on Outposts through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\.

 Ojects stores in the S3 Outposts \[OUTPOSTS\] storage class will always be encrypted using [server\-side encryption with Amazon S3\-managed encryption keys \(SSE\-S3\) ](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html)\. You can also explicity choose to encrypt objects stored in S3 Outposts storage class using [server\-side encryption with customer\-provided encryption keys \(SSE\-C\)](https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html)\. For more information, see [Using Amazon S3 on Outposts](https://docs.aws.amazon.com/AmazonS3/latest/dev/S3onOutposts.html)\. 

## Comparing the Amazon S3 storage classes<a name="sc-compare"></a>

The following table compares the storage classes\. 




****  

| Storage class | Designed for | Durability \(designed for\) | Availability \(designed for\) | Availability Zones | Min storage duration | Min billable object size | Other considerations  | 
| --- | --- | --- | --- | --- | --- | --- | --- | 
|  S3 Standard  |  Frequently accessed data  |  99\.999999999%   |  99\.99%  |  >= 3  |  None  |  None  |  None  | 
|  S3 Standard\-IA  |  Long\-lived, infrequently accessed data  |  99\.999999999%   |  99\.9%  |  >= 3  |  30 days  |  128 KB  |  Per GB retrieval fees apply\.   | 
|  S3 Intelligent\-Tiering  |  Long\-lived data with changing or unknown access patterns  |  99\.999999999%  |  99\.9%  |  >= 3  |  30 days  |  None  |  Monitoring and automation fees per object apply\. No retrieval fees\.  | 
|  S3 One Zone\-IA  |  Long\-lived, infrequently accessed, non\-critical data  |  99\.999999999%   |  99\.5%  |  1  |  30 days  |  128 KB  |  Per GB retrieval fees apply\. Not resilient to the loss of the Availability Zone\.  | 
|  S3 Glacier  | Long\-term data archiving with retrieval times ranging from minutes to hours | 99\.999999999%  |  99\.99% \(after you restore objects\)  |  >= 3  |  90 days  |  40 KB  | Per GB retrieval fees apply\. You must first restore archived objects before you can access them\. For more information, see [Restoring archived objects](restoring-objects.md)\. | 
|  S3 Glacier Deep Archive  | Archiving rarely accessed data with a default retrieval time of 12 hours | 99\.999999999%  |  99\.99% \(after you restore objects\)  |  >= 3  |  180 days  |  40 KB  | Per GB retrieval fees apply\. You must first restore archived objects before you can access them\. For more information, see [Restoring archived objects](restoring-objects.md)\. | 
|  RRS \(Not recommended\)  |  Frequently accessed, non\-critical data  |  99\.99%   |  99\.99%  |  >= 3  |  None  |  None  |  None  | 

All of the storage classes except for S3 One Zone\-IA are designed to be resilient to simultaneous complete data loss in a single Availability Zone and partial loss in another Availability Zone\. 

In addition to the performance requirements of your application scenario, consider price\. For storage class pricing, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

## Setting the storage class of an object<a name="sc-howtoset"></a>

Amazon S3 APIs support setting \(or updating\) the storage class of objects as follows:
+ When creating a new object, you can specify its storage class\. For example, when creating objects using the [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html), [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html), and [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html) APIs, you add the `x-amz-storage-class` request header to specify a storage class\. If you don't add this header, Amazon S3 uses Standard, the default storage class\.
+ You can also change the storage class of an object that is already stored in Amazon S3 to any other storage class by making a copy of the object using the [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) API\. However, you cannot use [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) to copy objects that are stored in the S3 Glacier or S3 Glacier Deep Archive storage classes\.

  You copy the object in the same bucket using the same key name and specify request headers as follows:
  + Set the `x-amz-metadata-directive` header to COPY\.
  + Set the `x-amz-storage-class` to the storage class that you want to use\. 

  In a versioning\-enabled bucket, you cannot change the storage class of a specific version of an object\. When you copy it, Amazon S3 gives it a new version ID\.
+ You can direct Amazon S3 to change the storage class of objects by adding an S3 Lifecycle configuration to a bucket\. For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.
+ When setting up a replication configuration, you can set the storage class for replicated objects to any other storage class\. However, you cannot replicate objects that are stored in the S3 Glacier or S3 Glacier Deep Archive storage classes\. For more information, see [Replication configuration overview](replication-add-config.md)\.

To create and update object storage classes, you can use the Amazon S3 console, AWS SDKs, or the AWS Command Line Interface \(AWS CLI\)\. Each uses the Amazon S3 APIs to send requests to Amazon S3\.

### Restricting access policy permissions to a specific storage class<a name="restricting-storage-class"></a>

When you grant access policy permissions for Amazon S3 operations, you can use the `s3:x-amz-storage-class` condition key to restrict which storage class to use when storing uploaded objects\. For example, when you grant `s3:PUTObject` permission, you can restrict object uploads to a specific storage class\. For an example policy, see [Example 5: Restricting Object Uploads to Objects with a Specific Storage Class](amazon-s3-policy-keys.md#example-storage-class-condition-key)\. For more information about using conditions in policies and a complete list of Amazon S3 condition keys, see the following:
+ [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)
+ [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)


# Protecting data using encryption<a name="UsingEncryption"></a>

Data protection refers to protecting data while in\-transit \(as it travels to and from Amazon S3\) and at rest \(while it is stored on disks in Amazon S3 data centers\)\. You can protect data in transit using Secure Socket Layer/Transport Layer Security \(SSL/TLS\) or client\-side encryption\. You have the following options for protecting data at rest in Amazon S3:
+ **Server\-Side Encryption** – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects\. 
+ **Client\-Side Encryption** – Encrypt data client\-side and upload the encrypted data to Amazon S3\. In this case, you manage the encryption process, the encryption keys, and related tools\.

For more information about server\-side encryption and client\-side encryption, review the topics listed below\.

**Topics**
+ [Protecting data using server\-side encryption](serv-side-encryption.md)
+ [Protecting data using client\-side encryption](UsingClientSideEncryption.md)


# Track multipart upload progress<a name="HLTrackProgressMPUJava"></a>

The high\-level Java multipart upload API provides a listen interface, `ProgressListener`, to track progress when uploading an object to Amazon S3\. Progress events periodically notify the listener that bytes have been transferred\.

The following example demonstrates how to subscribe to a `ProgressEvent` event and write a handler:

**Example**  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.event.ProgressEvent;
import com.amazonaws.event.ProgressListener;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.transfer.TransferManager;
import com.amazonaws.services.s3.transfer.TransferManagerBuilder;
import com.amazonaws.services.s3.transfer.Upload;

import java.io.File;

public class HighLevelTrackMultipartUpload {

    public static void main(String[] args) throws Exception {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Object key ***";
        String filePath = "*** Path to file to upload ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();
            TransferManager tm = TransferManagerBuilder.standard()
                    .withS3Client(s3Client)
                    .build();
            PutObjectRequest request = new PutObjectRequest(bucketName, keyName, new File(filePath));

            // To receive notifications when bytes are transferred, add a  
            // ProgressListener to your request.
            request.setGeneralProgressListener(new ProgressListener() {
                public void progressChanged(ProgressEvent progressEvent) {
                    System.out.println("Transferred bytes: " + progressEvent.getBytesTransferred());
                }
            });
            // TransferManager processes all transfers asynchronously,
            // so this call returns immediately.
            Upload upload = tm.upload(request);

            // Optionally, you can wait for the upload to finish before continuing.
            upload.waitForCompletion();
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client 
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# API support for multipart upload<a name="sdksupportformpu"></a>



You can use an AWS SDK to upload an object in parts\. The following AWS SDK libraries support multipart upload:
+ [AWS SDK for Java ](https://aws.amazon.com/sdk-for-java/)
+ [AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)
+ [AWS SDK for PHP](https://aws.amazon.com/sdk-for-php/)
+ [AWS SDK for Ruby](https://aws.amazon.com/sdk-for-ruby/)
+ [AWS SDK for Python \(Boto\)](https://aws.amazon.com/sdk-for-python/)
+ [AWS SDK for JavaScript in Node\.js](https://aws.amazon.com/sdk-for-node-js/)

These libraries provide a high\-level abstraction that makes uploading multipart objects easy\. However, if your application requires, you can use the REST API directly\. The following sections in the Amazon Simple Storage Service API Reference describe the REST API for multipart upload\. 
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)
+ [Upload Part](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html)
+ [Upload Part \(Copy\)](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html)
+ [Complete Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html)
+ [Abort Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadAbort.html)
+ [List Parts](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html)
+ [List Multipart Uploads](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html)

The following sections in the AWS Command Line Interface describe the operations for multipart upload\. 
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/cli/latest/reference/s3api/create-multipart-upload.html)
+ [Upload Part](https://docs.aws.amazon.com/cli/latest/reference/s3api/upload-part.html)
+ [Upload Part \(Copy\)](https://docs.aws.amazon.com/cli/latest/reference/s3api/upload-part-copy.html)
+ [Complete Multipart Upload](https://docs.aws.amazon.com/cli/latest/reference/s3api/complete-multipart-upload.html)
+ [Abort Multipart Upload](https://docs.aws.amazon.com/cli/latest/reference/s3api/abort-multipart-upload.html)
+ [List Parts](https://docs.aws.amazon.com/cli/latest/reference/s3api/list-parts.html)
+ [List Multipart Uploads](https://docs.aws.amazon.com/cli/latest/reference/s3api/list-multipart-uploads.html)


# Specifying Server\-Side Encryption Using the AWS SDK for Java<a name="SSEUsingJavaSDK"></a>

When you use the AWS SDK for Java to upload an object, you can use server\-side encryption to encrypt it\. To request server\-side encryption, use the `ObjectMetadata` property of the `PutObjectRequest` to set the `x-amz-server-side-encryption` request header\. When you call the `putObject()` method of the `AmazonS3Client`, Amazon S3 encrypts and saves the data\.

You can also request server\-side encryption when uploading objects with the multipart upload API: 
+ When using the high\-level multipart upload API, you use the `TransferManager` methods to apply server\-side encryption to objects as you upload them\. You can use any of the upload methods that take `ObjectMetadata` as a parameter\. For more information, see [Using the AWS Java SDK for multipart upload \(high\-level API\)](usingHLmpuJava.md)\.
+ When using the low\-level multipart upload API, you specify server\-side encryption when you initiate the multipart upload\. You add the `ObjectMetadata` property by calling the `InitiateMultipartUploadRequest.setObjectMetadata()` method\. For more information, see [Upload a file](llJavaUploadFile.md)\.

You can't directly change the encryption state of an object \(encrypting an unencrypted object or decrypting an encrypted object\)\. To change an object's encryption state, you make a copy of the object, specifying the desired encryption state for the copy, and then delete the original object\. Amazon S3 encrypts the copied object only if you explicitly request server\-side encryption\. To request encryption of the copied object through the Java API, use the `ObjectMetadata` property to specify server\-side encryption in the `CopyObjectRequest`\.

**Example**  
The following example shows how to set server\-side encryption using the AWS SDK for Java\. It shows how to perform the following tasks:  
+ Upload a new object using server\-side encryption\.
+ Change an object's encryption state \(in this example, encrypting a previously unencrypted object\) by making a copy of the object\.
+ Check the encryption state of the object\.
For more information about server\-side encryption, see [Specifying Server\-Side Encryption Using the REST API](SSEUsingRESTAPI.md)\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.internal.SSEResultBase;
import com.amazonaws.services.s3.model.*;

import java.io.ByteArrayInputStream;

public class SpecifyServerSideEncryption {

    public static void main(String[] args) {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyNameToEncrypt = "*** Key name for an object to upload and encrypt ***";
        String keyNameToCopyAndEncrypt = "*** Key name for an unencrypted object to be encrypted by copying ***";
        String copiedObjectKeyName = "*** Key name for the encrypted copy of the unencrypted object ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

            // Upload an object and encrypt it with SSE.
            uploadObjectWithSSEEncryption(s3Client, bucketName, keyNameToEncrypt);

            // Upload a new unencrypted object, then change its encryption state
            // to encrypted by making a copy.
            changeSSEEncryptionStatusByCopying(s3Client,
                    bucketName,
                    keyNameToCopyAndEncrypt,
                    copiedObjectKeyName);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void uploadObjectWithSSEEncryption(AmazonS3 s3Client, String bucketName, String keyName) {
        String objectContent = "Test object encrypted with SSE";
        byte[] objectBytes = objectContent.getBytes();

        // Specify server-side encryption.
        ObjectMetadata objectMetadata = new ObjectMetadata();
        objectMetadata.setContentLength(objectBytes.length);
        objectMetadata.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
        PutObjectRequest putRequest = new PutObjectRequest(bucketName,
                keyName,
                new ByteArrayInputStream(objectBytes),
                objectMetadata);

        // Upload the object and check its encryption status.
        PutObjectResult putResult = s3Client.putObject(putRequest);
        System.out.println("Object \"" + keyName + "\" uploaded with SSE.");
        printEncryptionStatus(putResult);
    }

    private static void changeSSEEncryptionStatusByCopying(AmazonS3 s3Client,
                                                           String bucketName,
                                                           String sourceKey,
                                                           String destKey) {
        // Upload a new, unencrypted object.
        PutObjectResult putResult = s3Client.putObject(bucketName, sourceKey, "Object example to encrypt by copying");
        System.out.println("Unencrypted object \"" + sourceKey + "\" uploaded.");
        printEncryptionStatus(putResult);

        // Make a copy of the object and use server-side encryption when storing the copy.
        CopyObjectRequest request = new CopyObjectRequest(bucketName,
                sourceKey,
                bucketName,
                destKey);
        ObjectMetadata objectMetadata = new ObjectMetadata();
        objectMetadata.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
        request.setNewObjectMetadata(objectMetadata);

        // Perform the copy operation and display the copy's encryption status.
        CopyObjectResult response = s3Client.copyObject(request);
        System.out.println("Object \"" + destKey + "\" uploaded with SSE.");
        printEncryptionStatus(response);

        // Delete the original, unencrypted object, leaving only the encrypted copy in Amazon S3.
        s3Client.deleteObject(bucketName, sourceKey);
        System.out.println("Unencrypted object \"" + sourceKey + "\" deleted.");
    }

    private static void printEncryptionStatus(SSEResultBase response) {
        String encryptionStatus = response.getSSEAlgorithm();
        if (encryptionStatus == null) {
            encryptionStatus = "Not encrypted with SSE";
        }
        System.out.println("Object encryption status is: " + encryptionStatus);
    }
}
```


# Authenticating requests using the REST API<a name="S3_Authentication2"></a>

When accessing Amazon S3 using REST, you must provide the following items in your request so the request can be authenticated: 

**Request elements**
+ **AWS access key Id** – Each request must contain the access key ID of the identity you are using to send your request\. 
+ **Signature** – Each request must contain a valid request signature, or the request is rejected\. 

  A request signature is calculated using your secret access key, which is a shared secret known only to you and AWS\.
+ **Time stamp** – Each request must contain the date and time the request was created, represented as a string in UTC\. 
+ **Date** – Each request must contain the time stamp of the request\. 

  Depending on the API action you're using, you can provide an expiration date and time for the request instead of or in addition to the time stamp\. See the authentication topic for the particular action to determine what it requires\.

Following are the general steps for authenticating requests to Amazon S3\. It is assumed you have the necessary security credentials, access key ID and secret access key\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/HMACAuthProcess_You.png)


|  |  | 
| --- |--- |
|  1  |  Construct a request to AWS\.  | 
|  2  |  Calculate the signature using your secret access key\.  | 
|  3  |  Send the request to Amazon S3\. Include your access key ID and the signature in your request\. Amazon S3 performs the next three steps\.  | 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/HMACAuthProcess_AWS.png)


|  |  | 
| --- |--- |
|  4  |  Amazon S3 uses the access key ID to look up your secret access key\.  | 
|  5  |  Amazon S3 calculates a signature from the request data and the secret access key using the same algorithm that you used to calculate the signature you sent in the request\.  | 
|  6  |  If the signature generated by Amazon S3 matches the one you sent in the request, the request is considered authentic\. If the comparison fails, the request is discarded, and Amazon S3 returns an error response\.  | 

## Detailed authentication information<a name="S3_Authentication_DetailedAuthentication"></a>

For detailed information about REST authentication, see [Signing and authenticating REST requests](RESTAuthentication.md)\.


# Copy object using the REST multipart upload API<a name="CopyingObjctsUsingRESTMPUapi"></a>

The following sections in the *Amazon Simple Storage Service API Reference* describe the REST API for multipart upload\. For copying an existing object you use the Upload Part \(Copy\) API and specify the source object by adding the `x-amz-copy-source` request header in your request\. 
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)
+ [Upload Part](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html)
+ [Upload Part \(Copy\)](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html)
+ [Complete Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html)
+ [Abort Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadAbort.html)
+ [List Parts](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html)
+ [List Multipart Uploads](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html)

You can use these APIs to make your own REST requests, or you can use one the SDKs we provide\. For more information about the SDKs, see [API support for multipart upload](sdksupportformpu.md)\.


# Protecting data with server\-side encryption using AWS KMS CMKs \(SSE\-KMS\)<a name="UsingKMSEncryption"></a>

Server\-side encryption is the encryption of data at its destination by the application or service that receives it\. AWS Key Management Service \(AWS KMS\) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud\. Amazon S3 uses AWS KMS customer master keys \(CMKs\) to encrypt your Amazon S3 objects\. AWS KMS encrypts only the object data\. Any object metadata is not encrypted\. 

If you use CMKs, you use AWS KMS via the [AWS Management Console](https://console.aws.amazon.com/kms) or [AWS KMS APIs](https://docs.aws.amazon.com/kms/latest/APIReference/) to centrally create CMKs, define the policies that control how CMKs can be used, and audit their usage to prove that they are being used correctly\. You can use these CMKs to protect your data in Amazon S3 buckets\. When you use SSE\-KMS encryption with an S3 bucket, the AWS KMS CMK must be in the same Region as the bucket\.

There are additional charges for using AWS KMS CMKs\. For more information, see [AWS KMS concepts \- Customer master keys \(CMKs\)](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys) in the *AWS Key Management Service Developer Guide* and [AWS KMS pricing](https://aws.amazon.com/kms/pricing)\.

**Important**  
You need the `kms:Decrypt` permission when you upload or download an Amazon S3 object encrypted with an AWS KMS CMK\. This is in addition to the `kms:ReEncrypt`, `kms:GenerateDataKey`, and `kms:DescribeKey` permissions\. For more information, see [Failure to upload a large file to Amazon S3 with encryption using an AWS KMS key](https://aws.amazon.com/premiumsupport/knowledge-center/s3-large-file-encryption-kms-key/)\.

## AWS managed CMKs and customer managed CMKs<a name="aws-managed-customer-managed-cmks"></a>

When you use server\-side encryption with AWS KMS \(SSE\-KMS\), you can use the default [AWS managed CMK](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk), or you can specify a [customer managed CMK](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk) that you have already created\. 

If you don't specify a customer managed CMK, Amazon S3 automatically creates an AWS managed CMK in your AWS account the first time that you add an object encrypted with SSE\-KMS to a bucket\. By default, Amazon S3 uses this CMK for SSE\-KMS\. 

If you want to use a customer managed CMK for SSE\-KMS, create the CMK before you configure SSE\-KMS\. Then, when you configure SSE\-KMS for your bucket, specify the existing customer managed CMK\. 

Creating your own customer managed CMK gives you more flexibility and control\. For example, you can create, rotate, and disable customer managed CMKs\. You can also define access controls and audit the customer managed CMKs that you use to protect your data\. For more information about customer managed and AWS managed CMKs, see [AWS KMS concepts](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html) in the *AWS Key Management Service Developer Guide*\.

**Important**  
When you use an AWS KMS CMK for server\-side encryption in Amazon S3, you must choose a symmetric CMK\. Amazon S3 only supports symmetric CMKs and not asymmetric CMKs\. For more information, see [Using Symmetric and Asymmetric Keys](https://docs.aws.amazon.com/kms/latest/developerguide/symmetric-asymmetric.html) in the *AWS Key Management Service Developer Guide*\.

## Amazon S3 Bucket Keys<a name="sse-kms-bucket-keys"></a>

When you configure server\-side encryption using AWS KMS \(SSE\-KMS\), you can configure your bucket to use an S3 Bucket Key for SSE\-KMS\. This bucket\-level key for SSE\-KMS can reduce your KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS\. 

When you configure your bucket to use an S3 Bucket Key for SSE\-KMS on new objects, AWS KMS generates a bucket\-level key that is used to create unique [data keys](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys) for objects in the bucket\. This bucket key is used for a time\-limited period within Amazon S3, further reducing the need for Amazon S3 to make requests to AWS KMS to complete encryption operations\. Amazon S3 will only share an S3 Bucket Key for objects accessed by the same AWS KMS customer master key \(CMK\)\.\.

For more information about using S3 Bucket Keys, see [Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys](bucket-key.md)\.

## AWS Signature Version 4<a name="aws-signature-version-4-sse-kms"></a>

If you are uploading or accessing objects encrypted by SSE\-KMS, you need to use AWS Signature Version 4 for added security\. For more information on how to do this using an AWS SDK, see [Specifying Signature Version in request authentication](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version)\.

**Important**  
All GET and PUT requests for an object protected by AWS KMS fail if they are not made via SSL or TLS, or if they are not made using SigV4\.

## SSE\-KMS highlights<a name="sse-kms-highlights"></a>

The highlights of SSE\-KMS are as follows:
+ You can choose a customer managed CMK that you create and manage, or you can choose an AWS managed CMK that Amazon S3 creates in your AWS account and manages for you\. Like a customer managed CMK, your AWS managed CMK is unique to your AWS account and Region\. Only Amazon S3 has permission to use this CMK on your behalf\. Amazon S3 only supports symmetric CMKs\.
+ You can create, rotate, and disable auditable customer managed CMKs from the AWS KMS console\. 
+ The `ETag` in the response is not the MD5 of the object data\.
+ The data keys used to encrypt your data are also encrypted and stored alongside the data that they protect\. 
+ The security controls in AWS KMS can help you meet encryption\-related compliance requirements\.

## Requiring server\-side encryption<a name="require-sse-kms"></a>

To require server\-side encryption of all objects in a particular Amazon S3 bucket, you can use a policy\. For example, the following bucket policy denies upload object \(`s3:PutObject`\) permission to everyone if the request does not include the `x-amz-server-side-encryption` header requesting server\-side encryption with SSE\-KMS\.

```
 1. {
 2.    "Version":"2012-10-17",
 3.    "Id":"PutObjectPolicy",
 4.    "Statement":[{
 5.          "Sid":"DenyUnEncryptedObjectUploads",
 6.          "Effect":"Deny",
 7.          "Principal":"*",
 8.          "Action":"s3:PutObject",
 9.          "Resource":"arn:aws:s3:::awsexamplebucket1/*",
10.          "Condition":{
11.             "StringNotEquals":{
12.                "s3:x-amz-server-side-encryption":"aws:kms"
13.             }
14.          }
15.       }
16.    ]
17. }
```

To require that a particular AWS KMS CMK be used to encrypt the objects in a bucket, you can use the `s3:x-amz-server-side-encryption-aws-kms-key-id` condition key\. To specify the AWS KMS CMK, you must use a key Amazon Resource Name \(ARN\) that is in the "`arn:aws:kms:region:acct-id:key/key-id"` format\.

**Note**  
When you upload an object, you can specify the AWS KMS CMK using the `x-amz-server-side-encryption-aws-kms-key-id` header\. If the header is not present in the request, Amazon S3 assumes the AWS managed CMK\. Regardless, the AWS KMS key ID that Amazon S3 uses for object encryption must match the AWS KMS key ID in the policy, otherwise Amazon S3 denies the request\.

For a complete list of Amazon S3‐specific condition keys and more information about specifying condition keys, see [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)\.

## Using AWS KMS on the Amazon S3 console<a name="kms-encryption-s3-console"></a>

For more information about using the Amazon S3 console with CMKs stored in AWS KMS, see [How do I add encryption to an S3 object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-object-encryption.html) in the *Amazon Simple Storage Service Console User Guide*\.

## API support for AWS KMS in Amazon S3<a name="APISupportforKMSEncryption"></a>

To request SSE\-KMS in the object creation REST APIs, use the `x-amz-server-side-encryption` request header\. To specify the ID of the AWS KMS CMK that was used for the object, use `x-amz-server-side-encryption-aws-kms-key-id`\. To enable an S3 Bucket Key, use `x-amz-server-side-encryption-bucket-key-enabled`\. For more information, see [REST API, AWS CLI, and AWS SDK support for S3 Bucket Keys](bucket-key.md#configure-bucket-key-programmatic)\.

The Amazon S3 API also supports encryption context, with the `x-amz-server-side-encryption-context` header\. When you enable bucket keys, the encryption context is the bucket ARN\. With bucket keys disabled, you can use the object ARN as the encryption context\. For more information, see [Specifying AWS KMS in Amazon S3 using the REST API](KMSUsingRESTAPI.md)\. 

The AWS SDKs also provide wrapper APIs for you to request SSE\-KMS with Amazon S3\. For more information, see [Specifying AWS KMS in Amazon S3 using the AWS SDKs](kms-using-sdks.md)\.


# Monitoring Amazon S3<a name="monitoring-overview"></a>

Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 and your AWS solutions\. You should collect monitoring data from all of the parts of your AWS solution so that you can more easily debug a multipoint failure if one occurs\. But before you start monitoring Amazon S3, you should create a monitoring plan that includes answers to the following questions:
+ What are your monitoring goals?
+ What resources will you monitor?
+ How often will you monitor these resources?
+ What monitoring tools will you use?
+ Who will perform the monitoring tasks?
+ Who should be notified when something goes wrong?

**Topics**
+ [Monitoring tools](monitoring-automated-manual.md)
+ [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)
+ [Metrics configurations for buckets](metrics-configurations.md)
+ [Logging with Amazon S3](logging-with-S3.md)
+ [Logging Amazon S3 API calls using AWS CloudTrail](cloudtrail-logging.md)
+ [Using AWS CloudTrail to identify Amazon S3 requests](cloudtrail-request-identification.md)
+ [Tracing Amazon S3 requests using AWS X\-Ray](tracing_requests_using_xray.md)


# Locking objects using S3 Object Lock<a name="object-lock"></a>

With S3 Object Lock, you can store objects using a *write\-once\-read\-many* \(WORM\) model\. You can use it to prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely\. Object Lock helps you meet regulatory requirements that require WORM storage, or simply add another layer of protection against object changes and deletion\.

S3 Object Lock has been assessed by Cohasset Associates for use in environments that are subject to SEC 17a\-4, CFTC, and FINRA regulations\. For more information about how Object Lock relates to these regulations, see the [Cohasset Associates Compliance Assessment](https://d1.awsstatic.com/r2018/b/S3-Object-Lock/Amazon-S3-Compliance-Assessment.pdf)\.

Object Lock provides two ways to manage object retention: retention periods and legal holds\.
+ A *retention period* specifies a fixed period of time during which an object remains locked\. During this period, your object is WORM\-protected and can't be overwritten or deleted\.
+ A *legal hold* provides the same protection as a retention period, but it has no expiration date\. Instead, a legal hold remains in place until you explicitly remove it\. Legal holds are independent from retention periods\.

An object version can have both a retention period and a legal hold, one but not the other, or neither\. For more information, see [S3 Object Lock overview](object-lock-overview.md)\. 

Object Lock works only in versioned buckets, and retention periods and legal holds apply to individual object versions\. When you lock an object version, Amazon S3 stores the lock information in the metadata for that object version\. Placing a retention period or legal hold on an object protects only the version specified in the request\. It doesn't prevent new versions of the object from being created\. If you put an object into a bucket that has the same key name as an existing, protected object, Amazon S3 creates a new version of that object, stores it in the bucket as requested, and reports the request as completed successfully\. The existing, protected version of the object remains locked according to its retention configuration\.

To use S3 Object Lock, follow these basic steps:

1. Create a new bucket with Object Lock enabled\.

1. \(Optional\) Configure a default retention period for objects placed in the bucket\.

1. Place the objects that you want to lock in the bucket\.

1. Apply a retention period, a legal hold, or both, to the objects that you want to protect\.

For information about using Object Lock on the AWS Management Console, see [How Do I Lock an Amazon S3 Object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/object-lock.html) in the *Amazon Simple Storage Service Console User Guide*\.

**Topics**
+ [S3 Object Lock overview](object-lock-overview.md)
+ [Managing Amazon S3 object locks](object-lock-managing.md)


# Speeding up your website with Amazon CloudFront<a name="website-hosting-cloudfront-walkthrough"></a>

You can use [Amazon CloudFront](http://aws.amazon.com/cloudfront) to improve the performance of your website\. CloudFront makes your website files \(such as HTML, images, and video\) available from data centers around the world \(called *edge locations*\)\. When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location\. This results in faster download times than if the visitor had requested the content from a data center that is located farther away\.

CloudFront caches content at edge locations for a period of time that you specify\. If a visitor requests content that has been cached for longer than the expiration date, CloudFront checks the origin server to see if a newer version of the content is available\. If a newer version is available, CloudFront copies the new version to the edge location\. Changes that you make to the original content are replicated to edge locations as visitors request the content\. 

**Automating set up with an AWS CloudFormation template**  
For more information about using an AWS CloudFormation template to configure a secure static website that creates a CloudFront distribution to serve your website, see [Getting started with a secure static website](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/getting-started-secure-static-website-cloudformation-template.html) in the *Amazon CloudFront Developer Guide*\.

**Topics**
+ [Step 1: Create a CloudFront distribution](#create-distribution)
+ [Step 2: Update the record sets for your domain and subdomain](#update-record-sets)
+ [\(Optional\) step 3: Check the log files](#check-log-files)

## Step 1: Create a CloudFront distribution<a name="create-distribution"></a>

First, you create a CloudFront distribution\. This makes your website available from data centers around the world\.

**To create a distribution with an Amazon S3 origin**

1. Open the CloudFront console at [ https://console\.aws\.amazon\.com/cloudfront/](https://console.aws.amazon.com/cloudfront/)\.

1. Choose **Create Distribution**\.

1. On the **Select a delivery method for your content** page, under **Web**, choose **Get Started**\.

1. On the **Create Distribution** page, in the **Origin Settings** section, for **Origin Domain Name**, enter the Amazon S3 website endpoint for your bucket, for example, `example.com.s3-website.us-west-1.amazonaws.com`\.

   CloudFront fills in the **Origin ID** for you\.

1. For **Default Cache Behavior Settings**, keep the values set to the defaults\. 

   With the default settings for **Viewer Protocol Policy**, you can use HTTPS for your static website\. For more information these configuration options, see [Values that You Specify When You Create or Update a Web Distribution](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/WorkingWithDownloadDistributions.html#DownloadDistValuesYouSpecify) in the *Amazon CloudFront Developer Guide*\.

1. For **Distribution Settings**, do the following:

   1. Leave **Price Class** set to **Use All Edge Locations \(Best Performance\)**\.

   1. Set **Alternate Domain Names \(CNAMEs\)** to the root domain and `www` subdomain, for example, `example.com` and `www.example.com`\.  
**Important**  
Prior to performing this step, note the [requirements for using alternate domain names](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/CNAMEs.html#alternate-domain-names-requirements), in particular the need for a valid SSL/TLS certificate\. 

   1. For **SSL Certificate**, choose **Custom SSL Certificate \(example\.com\)**, and choose the custom certificate that covers the domain and subdomain names\.

      For more information, see [SSL Certificate](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html#DownloadDistValuesSSLCertificate) in the *Amazon CloudFront Developer Guide*\.

   1. In **Default Root Object**, enter the name of your index document, for example, `index.html`\. 

      If the URL used to access the distribution doesn't contain a file name, the CloudFront distribution returns the index document\. The **Default Root Object** should exactly match the name of the index document for your static website\. For more information, see [Configuring an index document](IndexDocumentSupport.md)\.

   1. Set **Logging** to **On**\.

   1. For **Bucket for Logs**, choose the logging bucket that you created\.

      For more information about configuring a logging bucket, see [\(Optional\) Logging web traffic](LoggingWebsiteTraffic.md)\.

   1. If you want to store the logs that are generated by traffic to the CloudFront distribution in a folder, in **Log Prefix**, enter the folder name\.

   1. Keep all other settings at their default values\.

1. Choose **Create Distribution**\.

1. To see the status of the distribution, find the distribution in the console and check the **Status** column\. 

   A status of `InProgress` indicates that the distribution is not yet fully deployed\.

   After your distribution is deployed, you can reference your content with the new CloudFront domain name\.

1. Record the value of **Domain Name** shown in the CloudFront console, for example, `dj4p1rv6mvubz.cloudfront.net`\. 

1. To verify that your CloudFront distribution is working, enter the domain name of the distribution in a web browser\.

   If your website is visible, the CloudFront distribution works\. If your website has a custom domain registered with Amazon Route 53, you will need the CloudFront domain name to update the record set in the next step\.

## Step 2: Update the record sets for your domain and subdomain<a name="update-record-sets"></a>

Now that you have successfully created a CloudFront distribution, update the alias record in Route 53 to point to the new CloudFront distribution\.

**To update the alias record to point to a CloudFront distribution**

1. Open the Route 53 console at [https://console\.aws\.amazon\.com/route53/](https://console.aws.amazon.com/route53/)\.

1. On the **Hosted Zones** page, choose the hosted zone that you created for your subdomain\.

1. Choose **Go to Record Sets**\.

1. Choose the A record that you created for your subdomain, for example, `www.example.com`\.

1. For **Alias Target**, choose the CloudFront distribution\.

1. Choose **Save Record Set**\.

1. To redirect the A record for the root domain to the CloudFront distribution, repeat this procedure\.

   The update to the record sets takes effect within 2–48 hours\. 

1. To see whether the new A records have taken effect, in a web browser, enter your subdomain URL, for example, `http://www.example.com`\. 

   If the browser no longer redirects you to the root domain \(for example, `http://example.com`\), the new A records are in place\. When the new A record has taken effect, traffic routed by the new A record to the CloudFront distribution is not redirected to the root domain\. Any visitors who reference the site by using `http://example.com` or `http://www.example.com` are redirected to the nearest CloudFront edge location, where they benefit from faster download times\.
**Tip**  
Browsers can cache redirect settings\. If you think the new A record settings should have taken effect, but your browser still redirects `http://www.example.com` to `http://example.com`, try clearing your browser history and cache, closing and reopening your browser application, or using a different web browser\. 

## \(Optional\) step 3: Check the log files<a name="check-log-files"></a>

The access logs tell you how many people are visiting the website\. They also contain valuable business data that you can analyze with other services, such as [Amazon EMR](https://docs.aws.amazon.com/emr/latest/DeveloperGuide/)\. 

CloudFront logs are stored in the bucket and folder that you choose when you create a CloudFront distribution and enable logging\. CloudFront writes logs to your log bucket within 24 hours from when the corresponding requests are made\.

**To see the log files for your website**

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Choose the name of the logging bucket for your website\.

1. Choose the CloudFront logs folder\.

1. Download the `.gzip` files written by CloudFront before opening them\.

   If you created your website only as a learning exercise, you can delete the resources that you allocated so that you no longer accrue charges\. To do so, see [Cleaning up your example resources](getting-started-cleanup.md)\. After you delete your AWS resources, your website is no longer available\.


# \(Optional\) Configuring a webpage redirect<a name="how-to-page-redirect"></a>

If your Amazon S3 bucket is configured for website hosting, you can configure a webpage redirect\. You have the following options for configuring a redirect\.

**Topics**
+ [Setting an object redirect using the Amazon S3 console](#page-redirect-using-console)
+ [Setting an object redirect using the REST API](#page-redirect-using-rest-api)
+ [Redirecting requests for a bucket's website endpoint to another host](#redirect-endpoint-host)
+ [Configuring advanced conditional redirects](#advanced-conditional-redirects)

## Setting an object redirect using the Amazon S3 console<a name="page-redirect-using-console"></a>

You can redirect requests for an object to another object or URL by setting the website redirect location in the metadata of the object\. You set the redirect by adding the `x-amz-website-redirect-location` property to the object metadata\. On the Amazon S3 console, you set the **Website Redirect Location** in the metadata of the object\. If you use the [Amazon S3 API](#page-redirect-using-rest-api), you set `x-amz-website-redirect-location`\. The website then interprets the object as a 301 redirect\. 

To redirect a request to another object, you set the redirect location to the key of the target object\. To redirect a request to an external URL, you set the redirect location to the URL that you want\. For more information about object metadata, see [System\-defined object metadata](UsingMetadata.md#SysMetadata)\.

When you set a page redirect, you can either keep or delete the source object content\. For example, if you have a `page1.html` object in your bucket, you can redirect any requests for this page to another object, `page2.html`\. You have two options:
+ Keep the content of the `page1.html` object and redirect page requests\.
+ Delete the content of `page1.html` and upload a zero\-byte object named `page1.html` to replace the existing object and redirect page requests\. 

**To redirect requests for an object**

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose the name of the bucket that you have configured as a static website \(for example, `example.com`\)\.

1. Under **Objects**, select your object\.

1. Choose **Actions**, and choose **Edit metadata**\.

1. Choose **Metadata**\.

1. Choose **Add Metadata**\.

1. Under **Type**, choose **System Defined**\.

1. In **Key**, choose **x\-amz\-website\-redirect\-location**\.

1. In **Value**, enter the key name of the object that you want to redirect to, for example, `/page2.html`\.

   For another object in the same bucket, the `/` prefix in the value is required\. You can also set the value to an external URL, for example, `http://www.example.com`\.

1. Choose **Edit metadata**\.

## Setting an object redirect using the REST API<a name="page-redirect-using-rest-api"></a>

The following Amazon S3 API actions support the `x-amz-website-redirect-location` header in the request\. Amazon S3 stores the header value in the object metadata as `x-amz-website-redirect-location`\. 
+ [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)
+ [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)
+ [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)

A bucket configured for website hosting has both the website endpoint and the REST endpoint\. A request for a page that is configured as a 301 redirect has the following possible outcomes, depending on the endpoint of the request:
+ **Region\-specific website endpoint – **Amazon S3 redirects the page request according to the value of the `x-amz-website-redirect-location` property\. 
+ **REST endpoint – **Amazon S3 doesn't redirect the page request\. It returns the requested object\.

For more information about the endpoints, see [Key differences between a website endpoint and a REST API endpoint](WebsiteEndpoints.md#WebsiteRestEndpointDiff)\.

When setting a page redirect, you can either keep or delete the object content\. For example, suppose that you have a `page1.html` object in your bucket\.
+ To keep the content of `page1.html` and only redirect page requests, you can submit a [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) request to create a new `page1.html` object that uses the existing `page1.html` object as the source\. In your request, you set the `x-amz-website-redirect-location` header\. When the request is complete, you have the original page with its content unchanged, but Amazon S3 redirects any requests for the page to the redirect location that you specify\.
+ To delete the content of the `page1.html` object and redirect requests for the page, you can send a PUT Object request to upload a zero\-byte object that has the same object key: `page1.html`\. In the PUT request, you set `x-amz-website-redirect-location` for `page1.html` to the new object\. When the request is complete, `page1.html` has no content, and requests are redirected to the location that is specified by `x-amz-website-redirect-location`\.

When you retrieve the object using the [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) action, along with other object metadata, Amazon S3 returns the `x-amz-website-redirect-location` header in the response\.

## Redirecting requests for a bucket's website endpoint to another host<a name="redirect-endpoint-host"></a>

You can redirect all requests for a website endpoint for a bucket to another host\. If you redirect all requests, any request made to the website endpoint is redirected to the specified host name\. 

For example, if your root domain is `example.com`, and you want to serve requests for both `http://example.com` and `http://www.example.com`, you can create two buckets named `example.com` and `www.example.com`\. Then, maintain the content in the `example.com` bucket, and configure the other `www.example.com` bucket to redirect all requests to the `example.com` bucket\. For more information, see [Configuring a Static Website Using a Custom Domain Name](https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html)\.

**To redirect requests for a bucket website endpoint**

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Under **Buckets**, choose the name of the bucket that you want to redirect requests from \(for example, `www.example.com`\)\.

1. Choose **Properties**\.

1. Under **Static website hosting**, choose **Edit**\.

1. Choose **Redirect requests for an object**\. 

1. In the **Host name** box, enter the website endpoint for your bucket or your custom domain\.

   For example, if you are redirecting to a root domain address, you would enter **example\.com**\.

1. For **Protocol**, choose the protocol for the redirected requests \(**none**,**http**, or **https**\)\.

   If you do not specify a protocol, the default option is **none**\.

1. Choose **Save changes**\.

## Configuring advanced conditional redirects<a name="advanced-conditional-redirects"></a>

Using advanced redirection rules, you can route requests conditionally according to specific object key names, prefixes in the request, or response codes\. For example, suppose that you delete or rename an object in your bucket\. You can add a routing rule that redirects the request to another object\. If you want to make a folder unavailable, you can add a routing rule to redirect the request to another webpage\. You can also add a routing rule to handle error conditions by routing requests that return the error to another domain when the error is processed\.

When configuring a bucket for website hosting, you have the option of specifying advanced redirection rules\. Amazon S3 has a limitation of 50 routing rules per website configuration\. If you require more than 50 routing rules, you can use object redirect\. For more information, see [Setting an object redirect using the Amazon S3 console](#page-redirect-using-console)\.

**Important**  
To create redirection rules in the new Amazon S3 console, you must use JSON\. For more information about configuring routing rules using the REST API, see [PutBucketWebsite](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketWebsite.html) in the *Amazon Simple Storage Service API Reference*\.

**To configure redirection rules for a static website**

To add redirection rules for a bucket that already has static website hosting enabled, follow these steps\.

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose the name of a bucket that you have configured as a static website\.

1. Choose **Properties**\.

1. Under **Static website hosting**, choose **Edit**\.

1. In **Redirection rules** box, enter your redirection rules in JSON\. 

   In the S3 console you describe the rules using JSON\. For general syntax and examples for specifying redirection rules, see [Routing rule elements](#configure-bucket-as-website-routing-rule-syntax)\. Amazon S3 has a limitation of 50 routing rules per website configuration\.

1. Choose **Save changes**\.

### Routing rule elements<a name="configure-bucket-as-website-routing-rule-syntax"></a>

The following is general syntax for defining the routing rules in a website configuration in XML To configure redirection rules in the new S3 console, you must use JSON\. For JSON examples, see [Redirection rules examples](#redirect-rule-examples)\.

```
<RoutingRules> =
    <RoutingRules>
         <RoutingRule>...</RoutingRule>
         [<RoutingRule>...</RoutingRule>   
         ...]
    </RoutingRules>

<RoutingRule> =
   <RoutingRule>
      [ <Condition>...</Condition> ]
      <Redirect>...</Redirect>
   </RoutingRule>

<Condition> =
   <Condition> 
      [ <KeyPrefixEquals>...</KeyPrefixEquals> ]
      [ <HttpErrorCodeReturnedEquals>...</HttpErrorCodeReturnedEquals> ]
   </Condition>
    Note: <Condition> must have at least one child element.

<Redirect> =
   <Redirect> 
      [ <HostName>...</HostName> ]
      [ <Protocol>...</Protocol> ]
      [ <ReplaceKeyPrefixWith>...</ReplaceKeyPrefixWith>  ]
      [ <ReplaceKeyWith>...</ReplaceKeyWith> ]
      [ <HttpRedirectCode>...</HttpRedirectCode> ]
   </Redirect>
    Note: <Redirect> must have at least one child element. 
           Also, you can have either ReplaceKeyPrefix with or ReplaceKeyWith, 
           but not both.
```

The following table describes the elements in the routing rule\.


|  Name  |  Description  | 
| --- | --- | 
| RoutingRules |  Container for a collection of RoutingRule elements\.  | 
| RoutingRule |  A rule that identifies a condition and the redirect that is applied when the condition is met\.  Condition: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html)  | 
| Condition |  Container for describing a condition that must be met for the specified redirect to be applied\. If the routing rule does not include a condition, the rule is applied to all requests\.  | 
| KeyPrefixEquals |  The prefix of the object key name from which requests are redirected\.  `KeyPrefixEquals` is required if `HttpErrorCodeReturnedEquals` is not specified\. If both `KeyPrefixEquals` and `HttpErrorCodeReturnedEquals` are specified, both must be true for the condition to be met\.  | 
| HttpErrorCodeReturnedEquals |  The HTTP error code that must match for the redirect to apply\. If an error occurs, and if the error code meets this value, then the specified redirect applies\. `HttpErrorCodeReturnedEquals` is required if `KeyPrefixEquals` is not specified\. If both `KeyPrefixEquals` and `HttpErrorCodeReturnedEquals` are specified, both must be true for the condition to be met\.  | 
| Redirect |  Container element that provides instructions for redirecting the request\. You can redirect requests to another host or another page, or you can specify another protocol to use\. A `RoutingRule` must have a `Redirect` element\. A `Redirect` element must contain at least one of the following sibling elements: `Protocol`, `HostName`, `ReplaceKeyPrefixWith`, `ReplaceKeyWith`, or `HttpRedirectCode`\.  | 
| Protocol |  The protocol, `http` or `https`, to be used in the `Location` header that is returned in the response\.  If one of its siblings is supplied, `Protocol` is not required\.  | 
| HostName |  The hostname to be used in the `Location` header that is returned in the response\. If one of its siblings is supplied, `HostName` is not required\.  | 
| ReplaceKeyPrefixWith |  The prefix of the object key name that replaces the value of `KeyPrefixEquals` in the redirect request\.  If one of its siblings is supplied, `ReplaceKeyPrefixWith` is not required\. It can be supplied only if `ReplaceKeyWith` is not supplied\.  | 
| ReplaceKeyWith |  The object key to be used in the `Location` header that is returned in the response\.  If one of its siblings is supplied, `ReplaceKeyWith` is not required\. It can be supplied only if `ReplaceKeyPrefixWith` is not supplied\.  | 
| HttpRedirectCode |  The HTTP redirect code to be used in the `Location` header that is returned in the response\. If one of its siblings is supplied, `HttpRedirectCode` is not required\.  | 

#### Redirection rules examples<a name="redirect-rule-examples"></a>

The following examples explain common redirection tasks:

**Important**  
To create redirection rules in the new Amazon S3 console, you must use JSON\.

**Example 1: Redirect after renaming a key prefix**  
Suppose that your bucket contains the following objects:  
+ index\.html
+ docs/article1\.html
+ docs/article2\.html
You decide to rename the folder from `docs/` to `documents/`\. After you make this change, you need to redirect requests for prefix `docs/` to `documents/`\. For example, request for `docs/article1.html` will be redirected to `documents/article1.html`\.  
In this case, you add the following routing rule to the website configuration\.  

```
  <RoutingRules>
    <RoutingRule>
    <Condition>
      <KeyPrefixEquals>docs/</KeyPrefixEquals>
    </Condition>
    <Redirect>
      <ReplaceKeyPrefixWith>documents/</ReplaceKeyPrefixWith>
    </Redirect>
    </RoutingRule>
  </RoutingRules>
```

```
[
    {
        "Condition": {
            "KeyPrefixEquals": "docs/"
        },
        "Redirect": {
            "ReplaceKeyPrefixWith": "documents/"
        }
    }
]
```

**Example 2: Redirect requests for a deleted folder to a page**  
Suppose that you delete the `images/` folder \(that is, you delete all objects with the key prefix `images/`\)\. You can add a routing rule that redirects requests for any object with the key prefix `images/` to a page named `folderdeleted.html`\.  

```
  <RoutingRules>
    <RoutingRule>
    <Condition>
       <KeyPrefixEquals>images/</KeyPrefixEquals>
    </Condition>
    <Redirect>
      <ReplaceKeyWith>folderdeleted.html</ReplaceKeyWith>
    </Redirect>
    </RoutingRule>
  </RoutingRules>
```

```
[
    {
        "Condition": {
            "KeyPrefixEquals": "images/"
        },
        "Redirect": {
            "ReplaceKeyWith": "folderdeleted.html"
        }
    }
]
```

**Example 3: Redirect for an HTTP error**  
Suppose that when a requested object is not found, you want to redirect requests to an Amazon Elastic Compute Cloud \(Amazon EC2\) instance\. Add a redirection rule so that when an HTTP status code 404 \(Not Found\) is returned, the site visitor is redirected to an Amazon EC2 instance that handles the request\.   
The following example also inserts the object key prefix `report-404/` in the redirect\. For example, if you request a page `ExamplePage.html` and it results in an HTTP 404 error, the request is redirected to a page `report-404/ExamplePage.html` on the specified Amazon EC2 instance\. If there is no routing rule and the HTTP error 404 occurs, the error document that is specified in the configuration is returned\.  

```
  <RoutingRules>
    <RoutingRule>
    <Condition>
      <HttpErrorCodeReturnedEquals>404</HttpErrorCodeReturnedEquals >
    </Condition>
    <Redirect>
      <HostName>ec2-11-22-333-44.compute-1.amazonaws.com</HostName>
      <ReplaceKeyPrefixWith>report-404/</ReplaceKeyPrefixWith>
    </Redirect>
    </RoutingRule>
  </RoutingRules>
```

```
[
    {
        "Condition": {
            "HttpErrorCodeReturnedEquals": "404"
        },
        "Redirect": {
            "HostName": "ec2-11-22-333-44.compute-1.amazonaws.com",
            "ReplaceKeyPrefixWith": "report-404/"
        }
    }
]
```


# Operators<a name="s3-glacier-select-sql-reference-operators"></a>

Amazon S3 Select and S3 Glacier Select support the following operators\.

## Logical Operators<a name="s3-glacier-select-sql-reference-loical-ops"></a>


+ `AND`
+ `NOT`
+ `OR`

## Comparison Operators<a name="s3-glacier-select-sql-reference-compare-ops"></a>


+ `<` 
+ `>` 
+ `<=`
+ `>=`
+ `=`
+ `<>`
+ `!=`
+ `BETWEEN`
+ `IN` – For example: `IN ('a', 'b', 'c')`

  

## Pattern Matching Operators<a name="s3-glacier-select-sql-reference-pattern"></a>
+ `LIKE`
+ `_` \(Matches any character\)
+ `%` \(Matches any sequence of characters\)

## Unitary Operators<a name="s3-glacier-select-sql-reference-unitary-ops"></a>
+ IS NULL
+ IS NOT NULL

## Math Operators<a name="s3-glacier-select-sql-referencemath-ops"></a>

Addition, subtraction, multiplication, division, and modulo are supported\.
+ \+
+ \-
+ \*
+ /
+ %

## Operator Precedence<a name="s3-glacier-select-sql-reference-op-Precedence"></a>

The following table shows the operators' precedence in decreasing order\.


|  Operator/Element  |  Associativity |  Required  | 
| --- | --- | --- | 
| \-  | right  | unary minus  | 
| \*, /, %  | left  | multiplication, division, modulo  | 
| \+, \-  | left  | addition, subtraction  | 
| IN |  | set membership  | 
| BETWEEN |  | range containment  | 
| LIKE |  | string pattern matching  | 
| <> |  | less than, greater than  | 
| = | right  | equality, assignment | 
| NOT | right | logical negation  | 
| AND | left | logical conjunction  | 
| OR | left | logical disjunction  | 


# Making requests<a name="MakingRequests"></a>

**Topics**
+ [About access keys](#TypesofSecurityCredentials)
+ [Request endpoints](#RequestEndpoints)
+ [Making requests to Amazon S3 over IPv6](ipv6-access.md)
+ [Making requests using the AWS SDKs](MakingAuthenticatedRequests.md)
+ [Making requests using the REST API](RESTAPI.md)

Amazon S3 is a REST service\. You can send requests to Amazon S3 using the REST API or the AWS SDK \(see [Sample Code and Libraries](https://aws.amazon.com/code)\) wrapper libraries that wrap the underlying Amazon S3 REST API, simplifying your programming tasks\. 

Every interaction with Amazon S3 is either authenticated or anonymous\. Authentication is a process of verifying the identity of the requester trying to access an Amazon Web Services \(AWS\) product\. Authenticated requests must include a signature value that authenticates the request sender\. The signature value is, in part, generated from the requester's AWS access keys \(access key ID and secret access key\)\. For more information about getting access keys, see [How Do I Get Security Credentials?](https://docs.aws.amazon.com/general/latest/gr/getting-aws-sec-creds.html) in the *AWS General Reference*\. 

If you are using the AWS SDK, the libraries compute the signature from the keys you provide\. However, if you make direct REST API calls in your application, you must write the code to compute the signature and add it to the request\. 

## About access keys<a name="TypesofSecurityCredentials"></a>

The following sections review the types of access keys that you can use to make authenticated requests\.

### AWS account access keys<a name="requestsUsingAcctCred"></a>

The account access keys provide full access to the AWS resources owned by the account\. The following are examples of access keys:
+ Access key ID \(a 20\-character, alphanumeric string\)\. For example: AKIAIOSFODNN7EXAMPLE
+ Secret access key \(a 40\-character string\)\. For example: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

The access key ID uniquely identifies an AWS account\. You can use these access keys to send authenticated requests to Amazon S3\. 

### IAM user access keys<a name="requestsUsingIAMUserCred"></a>

You can create one AWS account for your company; however, there may be several employees in the organization who need access to your organization's AWS resources\. Sharing your AWS account access keys reduces security, and creating individual AWS accounts for each employee might not be practical\. Also, you cannot easily share resources such as buckets and objects because they are owned by different accounts\. To share resources, you must grant permissions, which is additional work\.

In such scenarios, you can use AWS Identity and Access Management \(IAM\) to create users under your AWS account with their own access keys and attach IAM user policies granting appropriate resource access permissions to them\. To better manage these users, IAM enables you to create groups of users and grant group\-level permissions that apply to all users in that group\. 

These users are referred to as IAM users that you create and manage within AWS\. The parent account controls a user's ability to access AWS\. Any resources an IAM user creates are under the control of and paid for by the parent AWS account\. These IAM users can send authenticated requests to Amazon S3 using their own security credentials\. For more information about creating and managing users under your AWS account, go to the [AWS Identity and Access Management product details page](https://aws.amazon.com/iam/)\. 

### Temporary security credentials<a name="requestsUsingTempCred"></a>

In addition to creating IAM users with their own access keys, IAM also enables you to grant temporary security credentials \(temporary access keys and a security token\) to any IAM user to enable them to access your AWS services and resources\. You can also manage users in your system outside AWS\. These are referred to as federated users\. Additionally, users can be applications that you create to access your AWS resources\.

IAM provides the AWS Security Token Service API for you to request temporary security credentials\. You can use either the AWS STS API or the AWS SDK to request these credentials\. The API returns temporary security credentials \(access key ID and secret access key\), and a security token\. These credentials are valid only for the duration you specify when you request them\. You use the access key ID and secret key the same way you use them when sending requests using your AWS account or IAM user access keys\. In addition, you must include the token in each request you send to Amazon S3\. 

An IAM user can request these temporary security credentials for their own use or hand them out to federated users or applications\. When requesting temporary security credentials for federated users, you must provide a user name and an IAM policy defining the permissions you want to associate with these temporary security credentials\. The federated user cannot get more permissions than the parent IAM user who requested the temporary credentials\. 

You can use these temporary security credentials in making requests to Amazon S3\. The API libraries compute the necessary signature value using those credentials to authenticate your request\. If you send requests using expired credentials, Amazon S3 denies the request\.

For information on signing requests using temporary security credentials in your REST API requests, see [Signing and authenticating REST requests](RESTAuthentication.md)\. For information about sending requests using AWS SDKs, see [Making requests using the AWS SDKs](MakingAuthenticatedRequests.md)\. 

For more information about IAM support for temporary security credentials, see [Temporary Security Credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html) in the *IAM User Guide*\.

For added security, you can require multifactor authentication \(MFA\) when accessing your Amazon S3 resources by configuring a bucket policy\. For information, see [Adding a Bucket Policy to Require MFA](example-bucket-policies.md#example-bucket-policies-use-case-7)\. After you require MFA to access your Amazon S3 resources, the only way you can access these resources is by providing temporary credentials that are created with an MFA key\. For more information, see the [AWS Multi\-Factor Authentication](https://aws.amazon.com/mfa/) detail page and [Configuring MFA\-Protected API Access](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html) in the *IAM User Guide*\.



## Request endpoints<a name="RequestEndpoints"></a>

You send REST requests to the service's predefined endpoint\. For a list of all AWS services and their corresponding endpoints, go to [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/aws-service-information.html) in the *AWS General Reference*\.


# Specifying server\-side encryption with customer\-provided encryption keys using the REST API<a name="ServerSideEncryptionCustomerKeysSSEUsingRESTAPI"></a>

At the time of object creation with the REST API, you can specify server\-side encryption with customer\-provided encryption keys \(SSE\-C\)\. When you use SSE\-C, you must provide encryption key information using the following request headers\. 


|  Name  |  Description  | 
| --- | --- | 
| x\-amz\-server\-side​\-encryption​\-customer\-algorithm  |  Use this header to specify the encryption algorithm\. The header value must be "AES256"\.   | 
| x\-amz\-server\-side​\-encryption​\-customer\-key  |  Use this header to provide the 256\-bit, base64\-encoded encryption key for Amazon S3 to use to encrypt or decrypt your data\.   | 
| x\-amz\-server\-side​\-encryption​\-customer\-key\-MD5  |  Use this header to provide the base64\-encoded 128\-bit MD5 digest of the encryption key according to [RFC 1321](http://tools.ietf.org/html/rfc1321)\. Amazon S3 uses this header for a message integrity check to ensure that the encryption key was transmitted without error\.  | 

You can use AWS SDK wrapper libraries to add these headers to your request\. If you need to, you can make the Amazon S3 REST API calls directly in your application\. 

**Note**  
You cannot use the Amazon S3 console to upload an object and request SSE\-C\. You also cannot use the console to update \(for example, change the storage class or add metadata\) an existing object stored using SSE\-C\.

## Amazon S3 rest APIs that support SSE\-C<a name="sse-c-supported-apis"></a>

The following Amazon S3 APIs support server\-side encryption with customer\-provided encryption keys \(SSE\-C\)\.
+ **GET operation** — When retrieving objects using the GET API \(see [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html)\), you can specify the request headers\. Torrents are not supported for objects encrypted using SSE\-C\.
+ **HEAD operation** — To retrieve object metadata using the HEAD API \(see [HEAD Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html)\), you can specify these request headers\.
+ **PUT operation** — When uploading data using the PUT Object API \(see [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)\), you can specify these request headers\. 
+ **Multipart Upload **— When uploading large objects using the multipart upload API, you can specify these headers\. You specify these headers in the initiate request \(see [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)\) and each subsequent part upload request \(see [Upload Part](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html) or 

  [Upload Part \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html)\)

  \)\. For each part upload request, the encryption information must be the same as what you provided in the initiate multipart upload request\.
+ **POST operation **— When using a POST operation to upload an object \(see [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)\), instead of the request headers, you provide the same information in the form fields\.
+ **Copy operation **— When you copy an object \(see [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)\), you have both a source object and a target object:
  + If you want the target object encrypted using server\-side encryption with AWS managed keys, you must provide the `x-amz-server-side​-encryption` request header\.
  +  If you want the target object encrypted using SSE\-C, you must provide encryption information using the three headers described in the preceding table\.
  +  If the source object is encrypted using SSE\-C, you must provide encryption key information using the following headers so that Amazon S3 can decrypt the object for copying\.    
[\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeysSSEUsingRESTAPI.html)

## Presigned URLs and SSE\-C<a name="ssec-and-presignedurl"></a>

You can generate a presigned URL that can be used for operations such as upload a new object, retrieve an existing object, or object metadata\. Presigned URLs support the SSE\-C as follows:
+ When creating a presigned URL, you must specify the algorithm using the `x-amz-server-side​-encryption​-customer-algorithm` in the signature calculation\.
+ When using the presigned URL to upload a new object, retrieve an existing object, or retrieve only object metadata, you must provide all the encryption headers in your client application\. 
**Note**  
For non\-SSE\-C objects, you can generate a presigned URL and directly paste that into a browser, for example to access the data\.   
However, this is not true for SSE\-C objects because in addition to the presigned URL, you also need to include HTTP headers that are specific to SSE\-C objects\. Therefore, you can use the presigned URL for SSE\-C objects only programmatically\.

## More info<a name="sse-c-more-info"></a>
+ [Specifying server\-side encryption with customer\-provided encryption keys using the AWS SDK for \.NET](sse-c-using-dot-net-sdk.md)
+ [Specifying server\-side encryption with customer\-provided encryption keys using the AWS SDK for Java](sse-c-using-java-sdk.md)


# Monitoring metrics with Amazon CloudWatch<a name="cloudwatch-monitoring"></a>

Amazon CloudWatch metrics for Amazon S3 can help you understand and improve the performance of applications that use Amazon S3\. There are several ways that you can use CloudWatch with Amazon S3\.
+ **Daily storage metrics for buckets** ‐ Monitor bucket storage using CloudWatch, which collects and processes storage data from Amazon S3 into readable, daily metrics\. These storage metrics for Amazon S3 are reported once per day and are provided to all customers at no additional cost\.
+ **Request metrics** ‐ Monitor Amazon S3 requests to quickly identify and act on operational issues\. The metrics are available at 1\-minute intervals after some latency to process\. These CloudWatch metrics are billed at the same rate as the Amazon CloudWatch custom metrics\. For information about CloudWatch pricing, see [Amazon CloudWatch pricing](https://aws.amazon.com/cloudwatch/pricing/)\. To learn how to opt in to getting these metrics, see [Metrics configurations for buckets](metrics-configurations.md)\.

  When enabled, request metrics are reported for all object operations\. By default, these 1\-minute metrics are available at the Amazon S3 bucket level\. You can also define a filter for the metrics collected using a shared prefix or object tag\. This allows you to align metrics filters to specific business applications, workflows, or internal organizations\.
+ **Replication metrics** ‐ Monitor the total number of S3 API operations that are pending replication, the total size of objects pending replication, and the maximum replication time to the destination Region\. Replication rules that have S3 Replication Time Control \(S3 RTC\) or S3 replication metrics enabled will publish replication metrics\. 

  For more information, see [Monitoring progress with replication metrics and Amazon S3 event notifications](replication-metrics.md) or [Meeting compliance requirements using S3 Replication Time Control \(S3 RTC\)](replication-time-control.md)\.

All CloudWatch statistics are retained for a period of 15 months so that you can access historical information and gain a better perspective on how your web application or service is performing\. For more information, see [What Is Amazon CloudWatch?](https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/WhatIsCloudWatch.html) in the *Amazon CloudWatch User Guide*\.

## Metrics and dimensions<a name="metrics-dimensions"></a>

The storage metrics and dimensions that Amazon S3 sends to CloudWatch are listed below\.

## Amazon S3 CloudWatch Daily Storage Metrics for Buckets<a name="s3-cloudwatch-metrics"></a>

The `AWS/S3` namespace includes the following daily storage metrics for buckets\.


| Metric | Description | 
| --- | --- | 
| BucketSizeBytes |  The amount of data in bytes stored in a bucket in the STANDARD storage class, INTELLIGENT\_TIERING storage class, Standard \- Infrequent Access \(STANDARD\_IA\) storage class, OneZone \- Infrequent Access \(ONEZONE\_IA\), Reduced Redundancy Storage \(RRS\) class, Deep Archive Storage \(S3 Glacier Deep Archive\) class or, Glacier \(GLACIER\) storage class\. This value is calculated by summing the size of all objects in the bucket \(both current and noncurrent objects\), including the size of all parts for all incomplete multipart uploads to the bucket\.  Valid storage type filters: `StandardStorage`, `IntelligentTieringFAStorage`, `IntelligentTieringIAStorage`, `IntelligentTieringAAStorage`, `IntelligentTieringDAAStorage`, `StandardIAStorage`, `StandardIASizeOverhead`, `StandardIAObjectOverhead`, `OneZoneIAStorage`, `OneZoneIASizeOverhead`, `ReducedRedundancyStorage`, `GlacierStorage`, `GlacierStagingStorage`, `GlacierObjectOverhead`, `GlacierS3ObjectOverhead`, `DeepArchiveStorage`, `DeepArchiveObjectOverhead`, `DeepArchiveS3ObjectOverhead` and, `DeepArchiveStagingStorage` \(see the `StorageType` dimension\)  Units: Bytes Valid statistics: Average  | 
| NumberOfObjects |  The total number of objects stored in a bucket for all storage classes except for the GLACIER storage class\. This value is calculated by counting all objects in the bucket \(both current and noncurrent objects\) and the total number of parts for all incomplete multipart uploads to the bucket\. Valid storage type filters: `AllStorageTypes` \(see the `StorageType` dimension\) Units: Count Valid statistics: Average  | 

## Amazon S3 CloudWatch Request Metrics<a name="s3-request-cloudwatch-metrics"></a>

The `AWS/S3` namespace includes the following request metrics\.


| Metric | Description | 
| --- | --- | 
| AllRequests |  The total number of HTTP requests made to an Amazon S3 bucket, regardless of type\. If you're using a metrics configuration with a filter, then this metric only returns the HTTP requests made to the objects in the bucket that meet the filter's requirements\. Units: Count Valid statistics: Sum  | 
| GetRequests |  The number of HTTP GET requests made for objects in an Amazon S3 bucket\. This doesn't include list operations\. Units: Count Valid statistics: Sum  Paginated list\-oriented requests, like [List Multipart Uploads](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html), [List Parts](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html), [Get Bucket Object versions](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html), and others, are not included in this metric\.   | 
| PutRequests |  The number of HTTP PUT requests made for objects in an Amazon S3 bucket\. Units: Count Valid statistics: Sum  | 
| DeleteRequests |  The number of HTTP DELETE requests made for objects in an Amazon S3 bucket\. This also includes [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html) requests\. This metric shows the number of requests, not the number of objects deleted\. Units: Count Valid statistics: Sum  | 
| HeadRequests |  The number of HTTP HEAD requests made to an Amazon S3 bucket\. Units: Count Valid statistics: Sum  | 
| PostRequests |  The number of HTTP POST requests made to an Amazon S3 bucket\. Units: Count Valid statistics: Sum  [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html) and [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) requests are not included in this metric\.    | 
| SelectRequests |  The number of Amazon S3 [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) requests made for objects in an Amazon S3 bucket\.  Units: Count Valid statistics: Sum  | 
| SelectScannedBytes |  The number of bytes of data scanned with Amazon S3 [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) requests in an Amazon S3 bucket\.   Units: Bytes  Valid statistics: Average \(bytes per request\), Sum \(bytes per period\), Sample Count, Min, Max \(same as p100\), any percentile between p0\.0 and p99\.9  | 
| SelectReturnedBytes |  The number of bytes of data returned with Amazon S3 [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) requests in an Amazon S3 bucket\.   Units: Bytes  Valid statistics: Average \(bytes per request\), Sum \(bytes per period\), Sample Count, Min, Max \(same as p100\), any percentile between p0\.0 and p99\.9  | 
| ListRequests |  The number of HTTP requests that list the contents of a bucket\. Units: Count Valid statistics: Sum  | 
| BytesDownloaded |  The number of bytes downloaded for requests made to an Amazon S3 bucket, where the response includes a body\. Units: Bytes Valid statistics: Average \(bytes per request\), Sum \(bytes per period\), Sample Count, Min, Max \(same as p100\), any percentile between p0\.0 and p99\.9  | 
| BytesUploaded |  The number of bytes uploaded that contain a request body, made to an Amazon S3 bucket\. Units: Bytes Valid statistics: Average \(bytes per request\), Sum \(bytes per period\), Sample Count, Min, Max \(same as p100\), any percentile between p0\.0 and p99\.9  | 
| 4xxErrors |  The number of HTTP 4xx client error status code requests made to an Amazon S3 bucket with a value of either 0 or 1\. The `average` statistic shows the error rate, and the `sum` statistic shows the count of that type of error, during each period\. Units: Count Valid statistics: Average \(reports per request\), Sum \(reports per period\), Min, Max, Sample Count  | 
| 5xxErrors |  The number of HTTP 5xx server error status code requests made to an Amazon S3 bucket with a value of either 0 or 1\. The `average` statistic shows the error rate, and the `sum` statistic shows the count of that type of error, during each period\. Units: Counts Valid statistics: Average \(reports per request\), Sum \(reports per period\), Min, Max, Sample Count  | 
| FirstByteLatency |  The per\-request time from the complete request being received by an Amazon S3 bucket to when the response starts to be returned\. Units: Milliseconds Valid statistics: Average, Sum, Min, Max\(same as p100\), Sample Count, any percentile between p0\.0 and p100  | 
| TotalRequestLatency |  The elapsed per\-request time from the first byte received to the last byte sent to an Amazon S3 bucket\. This includes the time taken to receive the request body and send the response body, which is not included in `FirstByteLatency`\. Units: Milliseconds Valid statistics: Average, Sum, Min, Max\(same as p100\), Sample Count, any percentile between p0\.0 and p100  | 

## Amazon S3 CloudWatch replication metrics<a name="s3-cloudwatch-replication-metrics"></a>

You can monitor the progress of replication with S3 replication metrics by tracking bytes pending, operations pending, and replication latency\. For more information, see [Monitoring progress with replication metrics](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-metrics.html)

**Note**  
You can enable alarms for your replication metrics on Amazon CloudWatch\. When you set up alarms for your replication metrics, set the **Missing data treatment** field to **Treat missing data as ignore \(maintain the alarm state\)**\.




| Metric | Description | 
| --- | --- | 
| ReplicationLatency |  The maximum number of seconds by which the replication destination Region is behind the source Region for a given replication rule\.  Units: Seconds Valid statistics: Max  | 
| BytesPendingReplication |  The total number of bytes of objects pending replication for a given replication rule\. Units: Bytes Valid statistics: Max  | 
| OperationsPendingReplication |  The number of operations pending replication for a given replication rule\. Units: Counts Valid statistics: Max  | 

## Amazon S3 on Outposts CloudWatch metrics<a name="s3-outposts-cloudwatch-metrics"></a>

The `S3Outposts` namespace includes the following metrics for Amazon S3 on Outposts buckets\. You can monitor the total number of S3 on Outposts bytes provisioned, the total free bytes available for objects, and the total size of all objects for a given bucket\. 

**Note**  
S3 on Outposts only supports the following metrics and not other Amazon S3 metrics\.  
Since S3 on Outposts have limited capcity, you can create CloudWatch alerts that alert you when storage utilization exceeds a threshold\.




| Metric | Description | 
| --- | --- | 
| OutpostTotalBytes |  The total provisioned capacity in bytes for an outpost  Units: bytes Period: 5 minutes  | 
| OutpostFreeBytes |  The count of free bytes available on outposts to store customer data\. Units: Bytes Period: 5 minutes  | 
| BucketUsedBytes |  The total size of all objects for the given bucket\. Units: Counts Period: 5 minutes  | 

## Amazon S3 CloudWatch Dimensions<a name="s3-cloudwatch-dimensions"></a>

The following dimensions are used to filter Amazon S3 metrics\.


|  Dimension  |  Description  | 
| --- | --- | 
|  BucketName  |  This dimension filters the data you request for the identified bucket only\.  | 
|  StorageType  |  This dimension filters the data that you have stored in a bucket by the following types of storage:  [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/cloudwatch-monitoring.html)  | 
| FilterId | This dimension filters metrics configurations that you specify for request metrics on a bucket, for example, a prefix or a tag\. You specify a filter id when you create a metrics configuration\. For more information, see [Metrics Configurations for Buckets](https://docs.aws.amazon.com/AmazonS3/latest/dev/metrics-configurations.html)\. | 

## Accessing CloudWatch metrics<a name="cloudwatch-monitoring-accessing"></a>

 You can use the following procedures to view the storage metrics for Amazon S3\. To get the Amazon S3 metrics involved, you must set a start and end timestamp\. For metrics for any given 24\-hour period, set the time period to 86400 seconds, the number of seconds in a day\. Also, remember to set the `BucketName` and `StorageType` dimensions\.

For example, if you use the AWS CLI to get the average of a specific bucket's size in bytes, you could use the following command\.

```
aws cloudwatch get-metric-statistics --metric-name BucketSizeBytes --namespace AWS/S3 --start-time 2016-10-19T00:00:00Z --end-time 2016-10-20T00:00:00Z --statistics Average --unit Bytes --region us-west-2 --dimensions Name=BucketName,Value=ExampleBucket Name=StorageType,Value=StandardStorage --period 86400 --output json
```

This example produces the following output\.

```
{
    "Datapoints": [
        {
            "Timestamp": "2016-10-19T00:00:00Z", 
            "Average": 1025328.0, 
            "Unit": "Bytes"
        }
    ], 
    "Label": "BucketSizeBytes"
}
```

**To view metrics using the CloudWatch console**

1. Open the CloudWatch console at [https://console\.aws\.amazon\.com/cloudwatch/](https://console.aws.amazon.com/cloudwatch/)\.

1. In the navigation pane, choose **Metrics**\. 

1. Choose the **S3** namespace\.

1. \(Optional\) To view a metric, enter the metric name in the search box\.

1. \(Optional\) To filter by the **StorageType** dimension, enter the name of the storage class in the search box\.

**To view a list of valid metrics stored for your AWS account using the AWS CLI**
+ At a command prompt, use the following command\.

  ```
  1. aws cloudwatch list-metrics --namespace "AWS/S3"
  ```

## Related resources<a name="cloudwatch-monitoring-related-resources"></a>
+ [Amazon CloudWatch Logs API Reference](https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/)
+ [Amazon CloudWatch User Guide](https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/)
+ [list\-metrics](https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/list-metrics.html) action in the *AWS CLI Command Reference*\.
+ [get\-metric\-statistics](https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/get-metric-statistics.html) action in the *AWS CLI Command Reference*\.
+  [Metrics configurations for buckets](metrics-configurations.md)\.


# Upload an object Using the AWS SDK for Java<a name="UploadObjSingleOpJava"></a>

**Example**  
The following example creates two objects\. The first object has a text string as data, and the second object is a file\. The example creates the first object by specifying the bucket name, object key, and text data directly in a call to `AmazonS3Client.putObject()`\. The example creates the second object by using a `PutObjectRequest` that specifies the bucket name, object key, and file path\. The `PutObjectRequest` also specifies the `ContentType` header and title metadata\.  
 For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.PutObjectRequest;

import java.io.File;
import java.io.IOException;

public class UploadObject {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String stringObjKeyName = "*** String object key name ***";
        String fileObjKeyName = "*** File object key name ***";
        String fileName = "*** Path to file to upload ***";

        try {
            //This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .build();

            // Upload a text string as a new object.
            s3Client.putObject(bucketName, stringObjKeyName, "Uploaded String Object");

            // Upload a file as a new object with ContentType and title specified.
            PutObjectRequest request = new PutObjectRequest(bucketName, fileObjKeyName, new File(fileName));
            ObjectMetadata metadata = new ObjectMetadata();
            metadata.setContentType("plain/text");
            metadata.addUserMetadata("title", "someTitle");
            request.setMetadata(metadata);
            s3Client.putObject(request);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Copy an Object Using the AWS SDK for Ruby<a name="CopyingObjectUsingRuby"></a>

The following tasks guide you through using the Ruby classes to copy an object in Amazon S3, from one bucket to another or to copy an object within the same bucket\. 


**Copying Objects**  

|  |  | 
| --- |--- |
|  1  |  Use the Amazon S3 modularized gem for version 3 of the AWS SDK for Ruby, require 'aws\-sdk\-s3', and provide your AWS credentials\. For more information about how to provide your credentials, see [Making requests using AWS account or IAM user credentials](AuthUsingAcctOrUserCredentials.md)\.  | 
|  2  |  Provide the request information, such as source bucket name, source key name, destination bucket name, and destination key\.   | 

 The following Ruby code example demonstrates the preceding tasks using the `#copy_object` method to copy an object from one bucket to another\.

**Example**  

```
require 'aws-sdk-s3'

# Copies an object from one Amazon S3 bucket to another.
#
# Prerequisites:
#
# - Two S3 buckets (a source bucket and a target bucket).
# - An object in the source bucket to be copied.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param source_bucket_name [String] The source bucket's name.
# @param source_key [String] The name of the object
#   in the source bucket to be copied.
# @param target_bucket_name [String] The target bucket's name.
# @param target_key [String] The name of the copied object.
# @return [Boolean] true if the object was copied; otherwise, false.
# @example
#   s3_client = Aws::S3::Client.new(region: 'us-east-1')
#   exit 1 unless object_copied?(
#     s3_client,
#     'doc-example-bucket1',
#     'my-source-file.txt',
#     'doc-example-bucket2',
#     'my-target-file.txt'
#   )
def object_copied?(
  s3_client,
  source_bucket_name,
  source_key,
  target_bucket_name,
  target_key)

  return true if s3_client.copy_object(
    bucket: target_bucket_name,
    copy_source: source_bucket_name + '/' + source_key,
    key: target_key
  )
rescue StandardError => e
  puts "Error while copying object: #{e.message}"
end
```


# Specifying server\-side encryption with customer\-provided encryption keys using the AWS SDK for Java<a name="sse-c-using-java-sdk"></a>

The following example shows how to request server\-side encryption with customer\-provided keys \(SSE\-C\) for objects\. The example performs the following operations\. Each operation shows how to specify SSE\-C\-related headers in the request:
+ **Put object**—Uploads an object and requests server\-side encryption using a customer\-provided encryption key\.
+ **Get object**—Downloads the object uploaded in the previous step\. In the request, you provide the same encryption information that you provided when you uploaded the object\. Amazon S3 needs this information to decrypt the object so that it can return it to you\.
+ **Get object metadata**—Retrieves the object's metadata\. You provide the same encryption information used when the object was created\.
+ **Copy object**—Makes a copy of the previously uploaded object\. Because the source object is stored using SSE\-C, you must provide its encryption information in your copy request\. By default, Amazon S3 encrypts the copy of the object only if you explicitly request it\. This example directs Amazon S3 to store an encrypted copy of the object using a new SSE\-C key\.

**Note**  
This example shows how to upload an object in a single operation\. When using the Multipart Upload API to upload large objects, you provide encryption information in the same way shown in this example\. For examples of multipart uploads that use the AWS SDK for Java, see [Using the AWS Java SDK for multipart upload \(high\-level API\)](usingHLmpuJava.md) and [Using the AWS Java SDK for a multipart upload \(low\-level API\)](mpListPartsJavaAPI.md)\.

To add the required encryption information, you include an `SSECustomerKey` in your request\. For more information about the `SSECustomerKey` class, see [Specifying server\-side encryption with customer\-provided encryption keys using the REST API](ServerSideEncryptionCustomerKeysSSEUsingRESTAPI.md)\.

For information about SSE\-C, see [Protecting data using server\-side encryption with customer\-provided encryption keys \(SSE\-C\)](ServerSideEncryptionCustomerKeys.md)\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\. 

**Example**  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import javax.crypto.KeyGenerator;
import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.security.NoSuchAlgorithmException;
import java.security.SecureRandom;

public class ServerSideEncryptionUsingClientSideEncryptionKey {
    private static SSECustomerKey SSE_KEY;
    private static AmazonS3 S3_CLIENT;
    private static KeyGenerator KEY_GENERATOR;

    public static void main(String[] args) throws IOException, NoSuchAlgorithmException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ***";
        String uploadFileName = "*** File path ***";
        String targetKeyName = "*** Target key name ***";

        // Create an encryption key.
        KEY_GENERATOR = KeyGenerator.getInstance("AES");
        KEY_GENERATOR.init(256, new SecureRandom());
        SSE_KEY = new SSECustomerKey(KEY_GENERATOR.generateKey());

        try {
            S3_CLIENT = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Upload an object.
            uploadObject(bucketName, keyName, new File(uploadFileName));

            // Download the object.
            downloadObject(bucketName, keyName);

            // Verify that the object is properly encrypted by attempting to retrieve it
            // using the encryption key.
            retrieveObjectMetadata(bucketName, keyName);

            // Copy the object into a new object that also uses SSE-C.
            copyObject(bucketName, keyName, targetKeyName);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void uploadObject(String bucketName, String keyName, File file) {
        PutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName, file).withSSECustomerKey(SSE_KEY);
        S3_CLIENT.putObject(putRequest);
        System.out.println("Object uploaded");
    }

    private static void downloadObject(String bucketName, String keyName) throws IOException {
        GetObjectRequest getObjectRequest = new GetObjectRequest(bucketName, keyName).withSSECustomerKey(SSE_KEY);
        S3Object object = S3_CLIENT.getObject(getObjectRequest);

        System.out.println("Object content: ");
        displayTextInputStream(object.getObjectContent());
    }

    private static void retrieveObjectMetadata(String bucketName, String keyName) {
        GetObjectMetadataRequest getMetadataRequest = new GetObjectMetadataRequest(bucketName, keyName)
                .withSSECustomerKey(SSE_KEY);
        ObjectMetadata objectMetadata = S3_CLIENT.getObjectMetadata(getMetadataRequest);
        System.out.println("Metadata retrieved. Object size: " + objectMetadata.getContentLength());
    }

    private static void copyObject(String bucketName, String keyName, String targetKeyName)
            throws NoSuchAlgorithmException {
        // Create a new encryption key for target so that the target is saved using SSE-C.
        SSECustomerKey newSSEKey = new SSECustomerKey(KEY_GENERATOR.generateKey());

        CopyObjectRequest copyRequest = new CopyObjectRequest(bucketName, keyName, bucketName, targetKeyName)
                .withSourceSSECustomerKey(SSE_KEY)
                .withDestinationSSECustomerKey(newSSEKey);

        S3_CLIENT.copyObject(copyRequest);
        System.out.println("Object copied");
    }

    private static void displayTextInputStream(S3ObjectInputStream input) throws IOException {
        // Read one line at a time from the input stream and display each line.
        BufferedReader reader = new BufferedReader(new InputStreamReader(input));
        String line;
        while ((line = reader.readLine()) != null) {
            System.out.println(line);
        }
        System.out.println();
    }
}
```

## Other Amazon S3 operations with SSE\-C using the AWS SDK for Java<a name="sse-c-java-other-api"></a>

The example in the preceding section shows how to request server\-side encryption with customer\-provided keys \(SSE\-C\) in the PUT, GET, Head, and Copy operations\. This section describes other APIs that support SSE\-C\.

To upload large objects, you can use multipart upload API \(see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\)\. You can use either high\-level or low\-level APIs to upload large objects\. These APIs support encryption\-related headers in the request\.
+ When using the high\-level `TransferManager` API, you provide the encryption\-specific headers in the `PutObjectRequest` \(see [Using the AWS Java SDK for multipart upload \(high\-level API\)](usingHLmpuJava.md)\)\. 
+ When using the low\-level API, you provide encryption\-related information in the `InitiateMultipartUploadRequest`, followed by identical encryption information in each `UploadPartRequest`\. You do not need to provide any encryption\-specific headers in your `CompleteMultipartUploadRequest`\. For examples, see [Using the AWS Java SDK for a multipart upload \(low\-level API\)](mpListPartsJavaAPI.md)\. 

The following example uses `TransferManager` to create objects and shows how to provide SSE\-C related information\. The example does the following:
+ Creates an object using the `TransferManager.upload()` method\. In the `PutObjectRequest` instance, you provide encryption key information to request\. Amazon S3 encrypts the object using the customer\-provided encryption key\.
+ Makes a copy of the object by calling the `TransferManager.copy()` method\. The example directs Amazon S3 to encrypt the object copy using a new `SSECustomerKey`\. Because the source object is encrypted using SSE\-C, the `CopyObjectRequest` also provides the encryption key of the source object so that Amazon S3 can decrypt the object before copying it\. 

**Example**  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CopyObjectRequest;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.model.SSECustomerKey;
import com.amazonaws.services.s3.transfer.Copy;
import com.amazonaws.services.s3.transfer.TransferManager;
import com.amazonaws.services.s3.transfer.TransferManagerBuilder;
import com.amazonaws.services.s3.transfer.Upload;

import javax.crypto.KeyGenerator;
import java.io.File;
import java.security.SecureRandom;

public class ServerSideEncryptionCopyObjectUsingHLwithSSEC {

    public static void main(String[] args) throws Exception {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String fileToUpload = "*** File path ***";
        String keyName = "*** New object key name ***";
        String targetKeyName = "*** Key name for object copy ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();
            TransferManager tm = TransferManagerBuilder.standard()
                    .withS3Client(s3Client)
                    .build();

            // Create an object from a file.
            PutObjectRequest putObjectRequest = new PutObjectRequest(bucketName, keyName, new File(fileToUpload));

            // Create an encryption key.
            KeyGenerator keyGenerator = KeyGenerator.getInstance("AES");
            keyGenerator.init(256, new SecureRandom());
            SSECustomerKey sseCustomerEncryptionKey = new SSECustomerKey(keyGenerator.generateKey());

            // Upload the object. TransferManager uploads asynchronously, so this call returns immediately.
            putObjectRequest.setSSECustomerKey(sseCustomerEncryptionKey);
            Upload upload = tm.upload(putObjectRequest);

            // Optionally, wait for the upload to finish before continuing.
            upload.waitForCompletion();
            System.out.println("Object created.");

            // Copy the object and store the copy using SSE-C with a new key.
            CopyObjectRequest copyObjectRequest = new CopyObjectRequest(bucketName, keyName, bucketName, targetKeyName);
            SSECustomerKey sseTargetObjectEncryptionKey = new SSECustomerKey(keyGenerator.generateKey());
            copyObjectRequest.setSourceSSECustomerKey(sseCustomerEncryptionKey);
            copyObjectRequest.setDestinationSSECustomerKey(sseTargetObjectEncryptionKey);

            // Copy the object. TransferManager copies asynchronously, so this call returns immediately.
            Copy copy = tm.copy(copyObjectRequest);

            // Optionally, wait for the upload to finish before continuing.
            copy.waitForCompletion();
            System.out.println("Copy complete.");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Setting permissions for website access<a name="WebsiteAccessPermissionsReqd"></a>

When you configure a bucket as a static website, if you want your website to be public, you can grant public read access\. To make your bucket publicly readable, you must disable block public access settings for the bucket and write a bucket policy that grants public read access\. If your bucket contains objects that are not owned by the bucket owner, you might also need to add an object access control list \(ACL\) that grants everyone read access\.

**Note**  
On the website endpoint, if a user requests an object that doesn't exist, Amazon S3 returns HTTP response code `404 (Not Found)`\. If the object exists but you haven't granted read permission on it, the website endpoint returns HTTP response code `403 (Access Denied)`\. The user can use the response code to infer whether a specific object exists\. If you don't want this behavior, you should not enable website support for your bucket\. 

## Step 1: Edit S3 Block Public Access settings<a name="block-public-access-static-site"></a>

If you want to configure an existing bucket as a static website that has public access, you must edit Block Public Access settings for that bucket\. You might also have to edit your account\-level Block Public Access settings\. Amazon S3 applies the most restrictive combination of the bucket\-level and account\-level block public access settings\.

For example, if you allow public access for a bucket but block all public access at the account level, Amazon S3 will continue to block public access to the bucket\. In this scenario, you would have to edit your bucket\-level and account\-level Block Public Access settings\. For more information, see [Using Amazon S3 block public access](access-control-block-public-access.md)\.

By default, Amazon S3 blocks public access to your account and buckets\. If you want to use a bucket to host a static website, you can use these steps to edit your block public access settings\. 

**Warning**  
Before you complete this step, review [Using Amazon S3 Block Public Access](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) to ensure that you understand and accept the risks involved with allowing public access\. When you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket\. We recommend that you block all public access to your buckets\.

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Choose the name of the bucket that you have configured as a static website\.

1. Choose **Permissions**\.

1. Under **Block public access \(bucket settings\)**, choose **Edit**\.

1. Clear **Block *all* public access**, and choose **Save changes**\.
**Warning**  
Before you complete this step, review [Using Amazon S3 Block Public Access](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) to ensure you understand and accept the risks involved with allowing public access\. When you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket\. We recommend that you block all public access to your buckets\.  
![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/edit-public-access-clear.png)

   Amazon S3 turns off Block Public Access settings for your bucket\. To create a public, static website, you might also have to [edit the Block Public Access settings](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access-account.html) for your account before adding a bucket policy\. If account settings for Block Public Access are currently turned on, you see a note under **Block public access \(bucket settings\)**\.

## Step 2: Add a bucket policy<a name="bucket-policy-static-site"></a>

To make the objects in your bucket publicly readable, you must write a bucket policy that grants everyone `s3:GetObject` permission\. 

After you edit S3 Block Public Access settings, you can add a bucket policy to grant public read access to your bucket\. When you grant public read access, anyone on the internet can access your bucket\.

**Important**  
The following policy is an example only and allows full access to the contents of your bucket\. Before you proceed with this step, review [How can I secure the files in my Amazon S3 bucket?](https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/) to ensure that you understand the best practices for securing the files in your S3 bucket and risks involved in granting public access\.

1. Under **Buckets**, choose the name of your bucket\.

1. Choose **Permissions**\.

1. Under **Bucket Policy**, choose **Edit**\.

1. To grant public read access for your website, copy the following bucket policy, and paste it in the **Bucket policy editor**\.

   ```
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Sid": "PublicReadGetObject",
               "Effect": "Allow",
               "Principal": "*",
               "Action": [
                   "s3:GetObject"
               ],
               "Resource": [
                   "arn:aws:s3:::example.com/*"
               ]
           }
       ]
   }
   ```

1. Update the `Resource` to your bucket name\.

   In the preceding example bucket policy, *example\.com* is the bucket name\. To use this bucket policy with your own bucket, you must update this name to match your bucket name\.

1. Choose **Save changes**\.

   A message appears indicating that the bucket policy has been successfully added\.

   If you see an error that says `Policy has invalid resource`, confirm that the bucket name in the bucket policy matches your bucket name\. For information about adding a bucket policy, see [How do I add an S3 bucket policy?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html)

   If you get an error message and cannot save the bucket policy, check your account and bucket Block Public Access settings to confirm that you allow public access to the bucket\.

### Object access control lists<a name="object-acl"></a>

You can use a bucket policy to grant public read permission to your objects\. However, the bucket policy applies only to objects that are owned by the bucket owner\. If your bucket contains objects that aren't owned by the bucket owner, the bucket owner should use the object access control list \(ACL\) to grant public READ permission on those objects\.

To make an object publicly readable using an ACL, grant READ permission to the `AllUsers` group, as shown in the following grant element\. Add this grant element to the object ACL\. For information about managing ACLs, see [Managing Access with ACLs](S3_ACLs_UsingACLs.md)\.

```
1. <Grant>
2.   <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
3.           xsi:type="Group">
4.     <URI>http://acs.amazonaws.com/groups/global/AllUsers</URI>
5.   </Grantee>
6.   <Permission>READ</Permission>
7. </Grant>
```


# Upload an object using the AWS SDK for C\+\+<a name="UploadObjSingleCpp"></a>

**Example**  
The following C\+\+ code example puts a string into an Amazon S3 object using `PutObjectRequest` request by taking a text string as sample object data on a specified S3 bucket, and AWS Region\(optional\)\.  

```
#include <aws/core/Aws.h>
#include <aws/s3/S3Client.h>
#include <aws/s3/model/PutObjectRequest.h>
#include <iostream>
#include <fstream>
#include <awsdoc/s3/s3_examples.h>

bool AwsDoc::S3::PutObjectBuffer(const Aws::String& bucketName,
    const Aws::String& objectName,
    const std::string& objectContent,
    const Aws::String& region)
{
    Aws::Client::ClientConfiguration config;
    
    if (!region.empty())
    {
        config.region = region;
    }

    Aws::S3::S3Client s3_client(config);

    Aws::S3::Model::PutObjectRequest request;
    request.SetBucket(bucketName);
    request.SetKey(objectName);

    const std::shared_ptr<Aws::IOStream> input_data =
        Aws::MakeShared<Aws::StringStream>("");
    *input_data << objectContent.c_str();

    request.SetBody(input_data);

    Aws::S3::Model::PutObjectOutcome outcome = s3_client.PutObject(request);

    if (!outcome.IsSuccess()) {
        std::cout << "Error: PutObjectBuffer: " << 
            outcome.GetError().GetMessage() << std::endl;

        return false;
    }
    else
    {
        std::cout << "Success: Object '" << objectName << "' with content '"
            << objectContent << "' uploaded to bucket '" << bucketName << "'.";

        return true;
    }
}

int main()
{
    Aws::SDKOptions options;
    Aws::InitAPI(options);
    {
        const Aws::String bucket_name = "my-bucket";
        const Aws::String object_name = "my-file.txt";
        const std::string object_content = "This is my sample text content.";
        const Aws::String region = "us-east-1";

        if (!AwsDoc::S3::PutObjectBuffer(bucket_name, object_name, object_content, region)) 
        {
            return 1;
        }
    }
    Aws::ShutdownAPI(options);

    return 0;
}
```


# Generate a presigned object URL using AWS SDK for \.NET<a name="ShareObjectPreSignedURLDotNetSDK"></a>

**Example**  
The following example generates a presigned URL that you can give to others so that they can retrieve an object\. For more information, see [Share an object with others](ShareObjectPreSignedURL.md)\.   
For instructions about how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;

namespace Amazon.DocSamples.S3
{
    class GenPresignedURLTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string objectKey = "*** object key ***";
        // Specify how long the presigned URL lasts, in hours
        private const double timeoutDuration = 12;
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            string urlString = GeneratePreSignedURL(timeoutDuration);
        }
        static string GeneratePreSignedURL(double duration)
        {
            string urlString = "";
            try
            {
                GetPreSignedUrlRequest request1 = new GetPreSignedUrlRequest
                {
                    BucketName = bucketName,
                    Key = objectKey,
                    Expires = DateTime.UtcNow.AddHours(duration)
                };
                urlString = s3Client.GetPreSignedURL(request1);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            return urlString;
        }
    }
}
```


# Using the AWS Command Line Interface for multipart upload<a name="UsingCLImpUpload"></a>

The following sections in the AWS Command Line Interface describe the operations for multipart upload\. 
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/cli/latest/reference/s3api/create-multipart-upload.html)
+ [Upload Part](https://docs.aws.amazon.com/cli/latest/reference/s3api/upload-part.html)
+ [Upload Part \(Copy\)](https://docs.aws.amazon.com/cli/latest/reference/s3api/upload-part-copy.html)
+ [Complete Multipart Upload](https://docs.aws.amazon.com/cli/latest/reference/s3api/complete-multipart-upload.html)
+ [Abort Multipart Upload](https://docs.aws.amazon.com/cli/latest/reference/s3api/abort-multipart-upload.html)
+ [List Parts](https://docs.aws.amazon.com/cli/latest/reference/s3api/list-parts.html)
+ [List Multipart Uploads](https://docs.aws.amazon.com/cli/latest/reference/s3api/list-multipart-uploads.html)

You can also use the REST API to make your own REST requests, or you can use one the SDKs we provide\. For more information about the REST API, see [Using the REST API for multipart upload](UsingRESTAPImpUpload.md)\. For more information about the SDKs, see [API support for multipart upload](sdksupportformpu.md)\.


# Stop multipart uploads<a name="HLAbortMPUploadsJava"></a>

**Example**  
The following example uses the high\-level Java API \(the `TransferManager` class\) to stop all in\-progress multipart uploads that were initiated on a specific bucket over a week ago\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.transfer.TransferManager;
import com.amazonaws.services.s3.transfer.TransferManagerBuilder;

import java.util.Date;

public class HighLevelAbortMultipartUpload {

    public static void main(String[] args) {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();
            TransferManager tm = TransferManagerBuilder.standard()
                    .withS3Client(s3Client)
                    .build();

            // sevenDays is the duration of seven days in milliseconds.
            long sevenDays = 1000 * 60 * 60 * 24 * 7;
            Date oneWeekAgo = new Date(System.currentTimeMillis() - sevenDays);
            tm.abortMultipartUploads(bucketName, oneWeekAgo);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client couldn't 
            // parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# AWS Billing reports for Amazon S3<a name="aws-billing-reports"></a>

Your monthly bill from AWS separates your usage information and cost by AWS service and function\. There are several AWS billing reports available, the monthly report, the cost allocation report, and detailed billing reports\. For information about how to see your billing reports, see [Viewing Your Bill](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/getting-viewing-bill.html) in the *AWS Billing and Cost Management User Guide*\.

You can also download a usage report that gives more detail about your Amazon S3 storage usage than the billing reports\. For more information, see [AWS usage report for Amazon S3](aws-usage-report.md)\.

The following table lists the charges associated with Amazon S3 usage\. 


**Amazon S3 usage charges**  

| Charge | Comments | 
| --- | --- | 
|  Storage  |  You pay for storing objects in your S3 buckets\. The rate you’re charged depends on your objects' size, how long you stored the objects during the month, and the storage class—STANDARD, INTELLIGENT\_TIERING, STANDARD\_IA \(IA for infrequent access\), ONEZONE\_IA, S3 Glacier, S3 Glacier Deep Archive or Reduced Redundancy Storage \(RRS\)\. For more information about storage classes, see [Amazon S3 storage classes](storage-class-intro.md)\.  | 
| Monitoring and Automation | You pay a monthly monitoring and automation fee per object stored in the INTELLIGENT\_TIERING storage class to monitor access patterns and move objects between access tiers in INTELLIGENT\_TIERING\. | 
|  Requests  |  You pay for requests, for example, GET requests, made against your S3 buckets and objects\. This includes lifecycle requests\. The rates for requests depend on what kind of request you’re making\. For information about request pricing, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\.  | 
|  Retrievals  |  You pay for retrieving objects that are stored in STANDARD\_IA, ONEZONE\_IA, S3 Glacier and S3 Glacier Deep Archive storage\.  | 
|  Early Deletes  |  If you delete an object stored in INTELLIGENT\_TIERING, STANDARD\_IA, ONEZONE\_IA, S3 Glacier, or S3 Glacier Deep Archive storage before the minimum storage commitment has passed, you pay an early deletion fee for that object\.  | 
|  Storage Management  |  You pay for the storage management features \(Amazon S3 inventory, analytics, and object tagging\) that are enabled on your account’s buckets\.  | 
|  Bandwidth  |  You pay for all bandwidth into and out of Amazon S3, except for the following: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/aws-billing-reports.html) You also pay a fee for any data transferred using Amazon S3 Transfer Acceleration\.   | 

For detailed information on Amazon S3 usage charges for storage, data transfer, and services, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/) and the [Amazon S3 FAQ](https://aws.amazon.com/s3/faqs/#billing)\.

For information on understanding codes and abbreviations used in the billing and usage reports for Amazon S3, see [Understanding your AWS billing and usage reports for Amazon S3](aws-usage-report-understand.md)\.

## More Info<a name="aws-billing-reports-more-info"></a>
+ [AWS usage report for Amazon S3](aws-usage-report.md)
+ [Using cost allocation S3 bucket tags](CostAllocTagging.md)
+ [AWS Billing and Cost Management](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html)
+ [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)
+ [Amazon S3 FAQ](https://aws.amazon.com/s3/faqs/#billing)
+ [S3 Glacier Pricing](https://aws.amazon.com/glacier/pricing/)


# Upload an object using a presigned URL \(AWS SDK for Ruby\)<a name="UploadObjectPreSignedURLRubySDK"></a>

The following tasks guide you through using a Ruby script to upload an object using a presigned URL for SDK for Ruby \- Version 3\.


**Uploading objects \- SDK for Ruby \- version 3**  

|  |  | 
| --- |--- |
|  1  |  Create an instance of the `Aws::S3::Resource` class\.  | 
|  2  |  Provide a bucket name and an object key by calling the `#bucket[]` and the `#object[]` methods of your `Aws::S3::Resource` class instance\. Generate a presigned URL by creating an instance of the `URI` class, and use it to parse the `.presigned_url` method of your `Aws::S3::Resource` class instance\. You must specify `:put` as an argument to `.presigned_url`, and you must specify `PUT` to `Net::HTTP::Session#send_request` if you want to upload an object\.  | 
|  3  |  Anyone with the presigned URL can upload an object\.  The upload creates an object or replaces any existing object with the same key that is specified in the presigned URL\.  | 

The following Ruby code example demonstrates the preceding tasks for SDK for Ruby \- Version 3\.

**Example**  

```
require 'aws-sdk-s3'
require 'net/http'

# Uploads an object to a bucket in Amazon Simple Storage Service (Amazon S3)
#   by using a presigned URL.
#
# Prerequisites:
#
# - An S3 bucket.
# - An object in the bucket to upload content to.
#
# @param s3_client [Aws::S3::Resource] An initialized S3 resource.
# @param bucket_name [String] The name of the bucket.
# @param object_key [String] The name of the object.
# @param object_content [String] The content to upload to the object.
# @param http_client [Net::HTTP] An initialized HTTP client.
#   This is especially useful for testing with mock HTTP clients.
#   If not specified, a default HTTP client is created.
# @return [Boolean] true if the object was uploaded; otherwise, false.
# @example
#   exit 1 unless object_uploaded_to_presigned_url?(
#     Aws::S3::Resource.new(region: 'us-east-1'),
#     'doc-example-bucket',
#     'my-file.txt',
#     'This is the content of my-file.txt'
#   )
def object_uploaded_to_presigned_url?(
  s3_resource,
  bucket_name,
  object_key,
  object_content,
  http_client = nil
)
  object = s3_resource.bucket(bucket_name).object(object_key)
  url = URI.parse(object.presigned_url(:put))

  if http_client.nil?
    Net::HTTP.start(url.host) do |http|
      http.send_request(
        'PUT',
        url.request_uri,
        object_content,
        'content-type' => ''
      )
    end
  else
    http_client.start(url.host) do |http|
      http.send_request(
        'PUT',
        url.request_uri,
        object_content,
        'content-type' => ''
      )
    end
  end
  content = object.get.body
  puts "The presigned URL for the object '#{object_key}' in the bucket " \
    "'#{bucket_name}' is:\n\n"
  puts url
  puts "\nUsing this presigned URL to get the content that " \
    "was just uploaded to this object, the object\'s content is:\n\n"
  puts content.read
  return true
rescue StandardError => e
  puts "Error uploading to presigned URL: #{e.message}"
  return false
end
```


# Making requests using AWS account or IAM user credentials \- AWS SDK for Ruby<a name="AuthUsingAcctOrUserCredRuby"></a>

Before you can use version 3 of the AWS SDK for Ruby to make calls to Amazon S3, you must set the AWS access credentials that the SDK uses to verify your access to your buckets and objects\. If you have shared credentials set up in the AWS credentials profile on your local system, version 3 of the SDK for Ruby can use those credentials without your having to declare them in your code\. For more information about setting up shared credentials, see [Making requests using AWS account or IAM user credentials](AuthUsingAcctOrUserCredentials.md)\.

The following Ruby code snippet uses the credentials in a shared AWS credentials file on a local computer to authenticate a request to get all of the object key names in a specific bucket\. It does the following:

1. Creates an instance of the `Aws::S3::Client` class\. 

1. Makes a request to Amazon S3 by enumerating objects in a bucket using the `list_objects_v2` method of `Aws::S3::Client`\. The client generates the necessary signature value from the credentials in the AWS credentials file on your computer, and includes it in the request it sends to Amazon S3\.

1. Prints the array of object key names to the terminal\.

**Example**  

```
require 'aws-sdk-s3'

# Prints the list of objects in an Amazon S3 bucket.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param bucket_name [String] The bucket's name.
# @return [Boolean] true if all operations succeed; otherwise, false.
# @example
#   s3_client = Aws::S3::Client.new(region: 'us-east-1')
#   exit 1 unless list_bucket_objects?(s3_client, 'doc-example-bucket')
def list_bucket_objects?(s3_client, bucket_name)
  puts "Accessing the bucket named '#{bucket_name}'..."
  objects = s3_client.list_objects_v2(
    bucket: bucket_name,
    max_keys: 50
  )

  if objects.count.positive?
    puts 'The object keys in this bucket are (first 50 objects):'
    objects.contents.each do |object|
      puts object.key
    end
  else
    puts 'No objects found in this bucket.'
  end

  return true
rescue StandardError => e
  puts "Error while accessing the bucket named '#{bucket_name}': #{e.message}"
  return false
end
```

If you don't have a local AWS credentials file, you can still create the `Aws::S3::Client` resource and run code against Amazon S3 buckets and objects\. Requests that are sent using version 3 of the SDK for Ruby are anonymous, with no signature by default\. Amazon S3 returns an error if you send anonymous requests for a resource that's not publicly available\.

You can use and expand the previous code snippet for SDK for Ruby applications, as in the following more robust example\.

```
require 'aws-sdk-s3'

# Prints a list of objects in an Amazon S3 bucket.
#
# Prerequisites:
#
# - An Amazon S3 bucket.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param bucket_name [String] The bucket's name.
# @return [Boolean] true if all operations succeed; otherwise, false.
# @example
#   s3_client = Aws::S3::Client.new(region: 'us-east-1')
#   exit 1 unless can_list_bucket_objects?(s3_client, 'doc-example-bucket')
def list_bucket_objects?(s3_client, bucket_name)
  puts "Accessing the bucket named '#{bucket_name}'..."
  objects = s3_client.list_objects_v2(
    bucket: bucket_name,
    max_keys: 50
  )

  if objects.count.positive?
    puts 'The object keys in this bucket are (first 50 objects):'
    objects.contents.each do |object|
      puts object.key
    end
  else
    puts 'No objects found in this bucket.'
  end

  return true
rescue StandardError => e
  puts "Error while accessing the bucket named '#{bucket_name}': #{e.message}"
end
```


# Enabling logging using the console<a name="enable-logging-console"></a>

For information about enabling [Amazon S3 server access logging](ServerLogs.md) in the [AWS Management Console](https://console.aws.amazon.com/s3/), see [ How Do I Enable Server Access Logging for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/server-access-logging.html) in the *Amazon Simple Storage Service Console User Guide*\. 

When you enable logging on a bucket, the console both enables logging on the source bucket and adds a grant in the target bucket's access control list \(ACL\) granting write permission to the Log Delivery group\. 

For information about how to enable logging programmatically, see [Enabling logging programmatically](enable-logging-programming.md)\.

For information about the log record format, including the list of fields and their descriptions, see [Amazon S3 Server Access Log Format](LogFormat.md)\.


# Upload a file to an S3 Bucket using the AWS SDK for \.NET \(low\-level API\)<a name="LLuploadFileDotNet"></a>

The following C\# example shows how to use the low\-level AWS SDK for \.NET multipart upload API to upload a file to an S3 bucket\. For information about Amazon S3 multipart uploads, see [Multipart upload overview](mpuoverview.md)\.

**Note**  
When you use the AWS SDK for \.NET API to upload large objects, a timeout might occur while data is being written to the request stream\. You can set an explicit timeout using the `UploadPartRequest`\. 

The following C\# example uploads a file to an S3 bucket using the low\-level multipart upload API\. For information about the example's compatibility with a specific version of the AWS SDK for \.NET and instructions for creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.Runtime;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.IO;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class UploadFileMPULowLevelAPITest
    {
        private const string bucketName = "*** provide bucket name ***";
        private const string keyName = "*** provide a name for the uploaded object ***";
        private const string filePath = "*** provide the full path name of the file to upload ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            Console.WriteLine("Uploading an object");
            UploadObjectAsync().Wait(); 
        }

        private static async Task UploadObjectAsync()
        {
            // Create list to store upload part responses.
            List<UploadPartResponse> uploadResponses = new List<UploadPartResponse>();

            // Setup information required to initiate the multipart upload.
            InitiateMultipartUploadRequest initiateRequest = new InitiateMultipartUploadRequest
            {
                BucketName = bucketName,
                Key = keyName
            };

            // Initiate the upload.
            InitiateMultipartUploadResponse initResponse =
                await s3Client.InitiateMultipartUploadAsync(initiateRequest);

            // Upload parts.
            long contentLength = new FileInfo(filePath).Length;
            long partSize = 5 * (long)Math.Pow(2, 20); // 5 MB

            try
            {
                Console.WriteLine("Uploading parts");
        
                long filePosition = 0;
                for (int i = 1; filePosition < contentLength; i++)
                {
                    UploadPartRequest uploadRequest = new UploadPartRequest
                        {
                            BucketName = bucketName,
                            Key = keyName,
                            UploadId = initResponse.UploadId,
                            PartNumber = i,
                            PartSize = partSize,
                            FilePosition = filePosition,
                            FilePath = filePath
                        };

                    // Track upload progress.
                    uploadRequest.StreamTransferProgress +=
                        new EventHandler<StreamTransferProgressArgs>(UploadPartProgressEventCallback);

                    // Upload a part and add the response to our list.
                    uploadResponses.Add(await s3Client.UploadPartAsync(uploadRequest));

                    filePosition += partSize;
                }

                // Setup to complete the upload.
                CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest
                    {
                        BucketName = bucketName,
                        Key = keyName,
                        UploadId = initResponse.UploadId
                     };
                completeRequest.AddPartETags(uploadResponses);

                // Complete the upload.
                CompleteMultipartUploadResponse completeUploadResponse =
                    await s3Client.CompleteMultipartUploadAsync(completeRequest);
            }
            catch (Exception exception)
            {
                Console.WriteLine("An AmazonS3Exception was thrown: { 0}", exception.Message);

                // Abort the upload.
                AbortMultipartUploadRequest abortMPURequest = new AbortMultipartUploadRequest
                {
                    BucketName = bucketName,
                    Key = keyName,
                    UploadId = initResponse.UploadId
                };
               await s3Client.AbortMultipartUploadAsync(abortMPURequest);
            }
        }
        public static void UploadPartProgressEventCallback(object sender, StreamTransferProgressArgs e)
        {
            // Process event. 
            Console.WriteLine("{0}/{1}", e.TransferredBytes, e.TotalBytes);
        }
    }
}
```

## More info<a name="LLuploadFileDotNet-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# Specifying Server\-Side Encryption Using the AWS SDK for \.NET<a name="SSEUsingDotNetSDK"></a>

When you upload an object, you can direct Amazon S3 to encrypt it\. To change the encryption state of an existing object, you make a copy of the object and delete the source object\. By default, the copy operation encrypts the target only if you explicitly request server\-side encryption of the target object\. To specify server\-side encryption in the `CopyObjectRequest`, add the following:

```
 ServerSideEncryptionMethod = ServerSideEncryptionMethod.AES256
```

## <a name="SSEUsingDotNetExample"></a>

For a working sample of how to copy an object, see [Copy an Amazon S3 Object in a Single Operation Using the AWS SDK for \.NET](CopyingObjectUsingNetSDK.md)\. 

The following example uploads an object\. In the request, the example directs Amazon S3 to encrypt the object\. The example then retrieves object metadata and verifies the encryption method that was used\. For information about creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class SpecifyServerSideEncryptionTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string keyName = "*** key name for object created ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            WritingAnObjectAsync().Wait();
        }

        static async Task WritingAnObjectAsync()
        {
            try
            {
                var putRequest = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName,
                    ContentBody = "sample text",
                    ServerSideEncryptionMethod = ServerSideEncryptionMethod.AES256
                };

                var putResponse = await client.PutObjectAsync(putRequest);

                // Determine the encryption state of an object.
                GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest
                {
                    BucketName = bucketName,
                    Key = keyName
                };
                GetObjectMetadataResponse response = await client.GetObjectMetadataAsync(metadataRequest);
                ServerSideEncryptionMethod objectEncryption = response.ServerSideEncryptionMethod;

                Console.WriteLine("Encryption method used: {0}", objectEncryption.ToString());
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}
```


# Upload an object using the CLI<a name="UploadObjSingleOpCLI"></a>

You can use AWS SDK to upload an object\. However, if your application requires it, you can send AWS Command Line Interface requests directly\. You can send a PUT request to upload data in a single operation\. For more information, see [PUT Object](https://docs.aws.amazon.com/cli/latest/reference/s3api/put-object.html) in the AWS CLI Command Reference guide\.


# Data Types<a name="s3-glacier-select-sql-reference-data-types"></a>

Amazon S3 Select and S3 Glacier Select support several primitive data types\.

## Data Type Conversions<a name="s3-glacier-select-sql-reference-data-conversion"></a>

The general rule is to follow the `CAST` function if defined\. If `CAST` is not defined, then all input data is treated as a string\. It must be cast into the relevant data types when necessary\.

For more information about the `CAST` function, see [CAST](s3-glacier-select-sql-reference-conversion.md#s3-glacier-select-sql-reference-cast)\.

## Supported Data Types<a name="s3-glacier-select-sql-reference-supported-data-types"></a>

Amazon S3 Select and S3 Glacier Select support the following set of primitive data types\.


|  Name  |  Description  |  Examples  | 
| --- | --- | --- | 
| bool | TRUE or FALSE | FALSE | 
| int, integer | 8\-byte signed integer in the range \-9,223,372,036,854,775,808 to 9,223,372,036,854,775,807\.  | 100000 | 
| string | UTF8\-encoded variable\-length string\. The default limit is one character\. The maximum character limit is 2,147,483,647\.  | 'xyz' | 
| float | 8\-byte floating point number\.  | CAST\(0\.456 AS FLOAT\) | 
| decimal, numeric |  Base\-10 number, with maximum precision of 38 \(that is, the maximum number of significant digits\), and with scale within the range of \-231 to 231\-1 \(that is, the base\-10 exponent\)\. Amazon S3 Select ignores scale and precision when you provide both at the same time\.   | 123\.456  | 
| timestamp |  Time stamps represent a specific moment in time, always include a local offset, and are capable of arbitrary precision\. In the text format, time stamps follow the [W3C note on date and time formats](https://www.w3.org/TR/NOTE-datetime), but they must end with the literal "T" if not at least whole\-day precision\. Fractional seconds are allowed, with at least one digit of precision, and an unlimited maximum\. Local\-time offsets can be represented as either hour:minute offsets from UTC, or as the literal "Z" to denote a local time of UTC\. They are required on time stamps with time and are not allowed on date values\.  | CAST\('2007\-04\-05T14:30Z' AS TIMESTAMP\) | 


# Managing websites with the AWS SDK for \.NET<a name="ConfigWebSiteDotNet"></a>

The following example shows how to use the AWS SDK for \.NET to manage website configuration for a bucket\. To add a website configuration to a bucket, you provide a bucket name and a website configuration\. The website configuration must include an index document and can contain an optional error document\. These documents must be stored in the bucket\. For more information, see [PUT Bucket website](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTwebsite.html)\. For more information about the Amazon S3 website feature, see [Hosting a static website using Amazon S3](WebsiteHosting.md)\. 

**Example**  
The following C\# code example adds a website configuration to the specified bucket\. The configuration specifies both the index document and the error document names\. For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class WebsiteConfigTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string indexDocumentSuffix = "*** index object key ***"; // For example, index.html.
        private const string errorDocument = "*** error object key ***"; // For example, error.html.
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;
        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            AddWebsiteConfigurationAsync(bucketName, indexDocumentSuffix, errorDocument).Wait();
        }

        static async Task AddWebsiteConfigurationAsync(string bucketName,
                                            string indexDocumentSuffix,
                                            string errorDocument)
        {
            try
            {
                // 1. Put the website configuration.
                PutBucketWebsiteRequest putRequest = new PutBucketWebsiteRequest()
                {
                    BucketName = bucketName,
                    WebsiteConfiguration = new WebsiteConfiguration()
                    {
                        IndexDocumentSuffix = indexDocumentSuffix,
                        ErrorDocument = errorDocument
                    }
                };
                PutBucketWebsiteResponse response = await client.PutBucketWebsiteAsync(putRequest);

                // 2. Get the website configuration.
                GetBucketWebsiteRequest getRequest = new GetBucketWebsiteRequest()
                {
                    BucketName = bucketName
                };
                GetBucketWebsiteResponse getResponse = await client.GetBucketWebsiteAsync(getRequest);
                Console.WriteLine("Index document: {0}", getResponse.WebsiteConfiguration.IndexDocumentSuffix);
                Console.WriteLine("Error document: {0}", getResponse.WebsiteConfiguration.ErrorDocument);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}
```


# Selecting content from objects using the REST API<a name="SelectObjectContentUsingRestApi"></a>

You can use the AWS SDK to select content from objects\. However, if your application requires it, you can send REST requests directly\. For more information about the request and response format, see [SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html)\.


# Replicating delete markers between buckets<a name="delete-marker-replication"></a>

By default, when Amazon S3 Replication is enabled and an object is deleted in the source bucket, Amazon S3 adds a delete marker in the source bucket only\. This action protects data from malicious deletions\. 

If you have *delete marker replication* enabled, these markers are copied to the destination buckets, and Amazon S3 behaves as if the object was deleted in both source and destination buckets\. For more information about how delete markers work, see [Working with delete markers](https://docs.aws.amazon.com/AmazonS3/latest/dev/DeleteMarker.html)\.

**Note**  
Delete marker replication is not supported for tag\-based replication rules\. Delete marker replication also does not adhere to the 15\-minute SLA granted when using S3 Replication Time Control\.

If you are not using the latest replication configuration version, delete operations will affect replication differently\. For more information, see [How delete operations affect replication](https://docs.aws.amazon.com/AmazonS3/latest/dev/Replication.html#replication-delete-op)\.

## Enabling delete marker replication<a name="enabling-delete-marker-replication"></a>

You can start using delete marker replication with a new or existing replication rule\. You can apply it to an entire S3 bucket or to Amazon S3 objects that have a specific prefix\.

To enable delete marker replication using the Amazon S3 console, see [How do I add a replication rule to an S3 bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html) in the *Amazon Simple Storage Service Console User Guide*\. This topic provides instructions for enabling delete marker replication in your replication configuration when buckets are owned by the same or different AWS accounts\.

To enable delete marker replication using the AWS Command Line Interface \(AWS CLI\), you must add a replication configuration to the source bucket with `DeleteMarkerReplication` enabled\. 

In the following example configuration, delete markers are replicated to the destination bucket *DOC\-EXAMPLE\-BUCKET* for objects under the prefix *Tax*\.

```
{
    "Rules": [
        {
            "Status": "Enabled",
            "Filter": {
                "Prefix": "Tax"
            },
            "DeleteMarkerReplication": {
                "Status": "Enabled"
            },
            "Destination": {
                "Bucket": "arn:aws:s3:::DOC-EXAMPLE-BUCKET"
            },
            "Priority": 1
        }
    ],
    "Role": "IAM-Role-ARN"
}
```

For full instructions on creating replication rules through the AWS CLI, see [Example 1: Configuring replication when the source and destination buckets are owned by the same account](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-walkthrough1.html) in the [Replication walkthroughs](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-example-walkthroughs.html) section\.


# Performance Design Patterns for Amazon S3<a name="optimizing-performance-design-patterns"></a>

When designing applications to upload and retrieve objects from Amazon S3, use our best practices design patterns for achieving the best performance for your application\. We also offer [Performance Guidelines](optimizing-performance-guidelines.md) for you to consider when planning your application architecture\.

To optimize performance, you can use the following design patterns\.

**Topics**
+ [Using Caching for Frequently Accessed Content](#optimizing-performance-caching)
+ [Timeouts and Retries for Latency\-Sensitive Applications](#optimizing-performance-timeouts-retries)
+ [Horizontal Scaling and Request Parallelization for High Throughput](#optimizing-performance-parallelization)
+ [Using Amazon S3 Transfer Acceleration to Accelerate Geographically Disparate Data Transfers](#optimizing-performance-acceleration)

## Using Caching for Frequently Accessed Content<a name="optimizing-performance-caching"></a>

Many applications that store data in Amazon S3 serve a “working set” of data that is repeatedly requested by users\. If a workload is sending repeated GET requests for a common set of objects, you can use a cache such as [Amazon CloudFront](https://docs.aws.amazon.com/cloudfront/index.html), [Amazon ElastiCache](https://docs.aws.amazon.com/elasticache/index.html), or [AWS Elemental MediaStore](https://docs.aws.amazon.com/mediastore/index.html) to optimize performance\. Successful cache adoption can result in low latency and high data transfer rates\. Applications that use caching also send fewer direct requests to Amazon S3, which can help reduce request costs\.

Amazon CloudFront is a fast content delivery network \(CDN\) that transparently caches data from Amazon S3 in a large set of geographically distributed points of presence \(PoPs\)\. When objects might be accessed from multiple Regions, or over the internet, CloudFront allows data to be cached close to the users that are accessing the objects\. This can result in high performance delivery of popular Amazon S3 content\. For information about CloudFront, see the [Amazon CloudFront Developer Guide](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/)\.

Amazon ElastiCache is a managed, in\-memory cache\. With ElastiCache, you can provision Amazon EC2 instances that cache objects in memory\. This caching results in orders of magnitude reduction in GET latency and substantial increases in download throughput\. To use ElastiCache, you modify application logic to both populate the cache with hot objects and check the cache for hot objects before requesting them from Amazon S3\. For examples of using ElastiCache to improve Amazon S3 GET performance, see the blog post [Turbocharge Amazon S3 with Amazon ElastiCache for Redis](https://aws.amazon.com/blogs/storage/turbocharge-amazon-s3-with-amazon-elasticache-for-redis/)\.

AWS Elemental MediaStore is a caching and content distribution system specifically built for video workflows and media delivery from Amazon S3\. MediaStore provides end\-to\-end storage APIs specifically for video, and is recommended for performance\-sensitive video workloads\. For information about MediaStore, see the [AWS Elemental MediaStore User Guide](https://docs.aws.amazon.com/mediastore/latest/ug/)\. 

## Timeouts and Retries for Latency\-Sensitive Applications<a name="optimizing-performance-timeouts-retries"></a>

There are certain situations where an application receives a response from Amazon S3 indicating that a retry is necessary\. Amazon S3 maps bucket and object names to the object data associated with them\. If an application generates high request rates \(typically sustained rates of over 5,000 requests per second to a small number of objects\), it might receive HTTP 503 *slowdown* responses\. If these errors occur, each AWS SDK implements automatic retry logic using exponential backoff\. If you are not using an AWS SDK, you should implement retry logic when receiving the HTTP 503 error\. For information about back\-off techniques, see [Error Retries and Exponential Backoff in AWS](https://docs.aws.amazon.com/general/latest/gr/api-retries.html) in the *Amazon Web Services General Reference*\.

Amazon S3 automatically scales in response to sustained new request rates, dynamically optimizing performance\. While Amazon S3 is internally optimizing for a new request rate, you will receive HTTP 503 request responses temporarily until the optimization completes\. After Amazon S3 internally optimizes performance for the new request rate, all requests are generally served without retries\. 

For latency\-sensitive applications, Amazon S3 advises tracking and aggressively retrying slower operations\. When you retry a request, we recommend using a new connection to Amazon S3 and performing a fresh DNS lookup\. 

When you make large variably sized requests \(for example, more than 128 MB\), we advise tracking the throughput being achieved and retrying the slowest 5 percent of the requests\. When you make smaller requests \(for example, less than 512 KB\), where median latencies are often in the tens of milliseconds range, a good guideline is to retry a GET or PUT operation after 2 seconds\. If additional retries are needed, the best practice is to back off\. For example, we recommend issuing one retry after 2 seconds and a second retry after an additional 4 seconds\.

If your application makes fixed\-size requests to Amazon S3, you should expect more consistent response times for each of these requests\. In this case, a simple strategy is to identify the slowest 1 percent of requests and to retry them\. Even a single retry is frequently effective at reducing latency\.

If you are using AWS Key Management Service \(AWS KMS\) for server\-side encryption, see [Limits](https://docs.aws.amazon.com/kms/latest/developerguide/limits.html) in the *AWS Key Management Service Developer Guide* for information about the request rates that are supported for your use case\.

## Horizontal Scaling and Request Parallelization for High Throughput<a name="optimizing-performance-parallelization"></a>

Amazon S3 is a very large distributed system\. To help you take advantage of its scale, we encourage you to horizontally scale parallel requests to the Amazon S3 service endpoints\. In addition to distributing the requests within Amazon S3, this type of scaling approach helps distribute the load over multiple paths through the network\.

For high\-throughput transfers, Amazon S3 advises using applications that use multiple connections to GET or PUT data in parallel\. For example, this is supported by [Amazon S3 Transfer Manager](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/examples-s3-transfermanager.html) in the AWS Java SDK, and most of the other AWS SDKs provide similar constructs\. For some applications, you can achieve parallel connections by launching multiple requests concurrently in different application threads, or in different application instances\. The best approach to take depends on your application and the structure of the objects that you are accessing\.

You can use the AWS SDKs to issue GET and PUT requests directly rather than employing the management of transfers in the AWS SDK\. This approach lets you tune your workload more directly, while still benefiting from the SDK’s support for retries and its handling of any HTTP 503 responses that might occur\. As a general rule, when you download large objects within a Region from Amazon S3 to [Amazon EC2](https://docs.aws.amazon.com/ec2/index.html), we suggest making concurrent requests for byte ranges of an object at the granularity of 8–16 MB\. Make one concurrent request for each 85–90 MB/s of desired network throughput\. To saturate a 10 Gb/s network interface card \(NIC\), you might use about 15 concurrent requests over separate connections\. You can scale up the concurrent requests over more connections to saturate faster NICs, such as 25 Gb/s or 100 Gb/s NICs\. 

Measuring performance is important when you tune the number of requests to issue concurrently\. We recommend starting with a single request at a time\. Measure the network bandwidth being achieved and the use of other resources that your application uses in processing the data\. You can then identify the bottleneck resource \(that is, the resource with the highest usage\), and hence the number of requests that are likely to be useful\. For example, if processing one request at a time leads to a CPU usage of 25 percent, it suggests that up to four concurrent requests can be accommodated\. Measurement is essential, and it is worth confirming resource use as the request rate is increased\. 

If your application issues requests directly to Amazon S3 using the REST API, we recommend using a pool of HTTP connections and re\-using each connection for a series of requests\. Avoiding per\-request connection setup removes the need to perform TCP slow\-start and Secure Sockets Layer \(SSL\) handshakes on each request\. For information about using the REST API, see the [Amazon Simple Storage Service API Reference](https://docs.aws.amazon.com/AmazonS3/latest/API/)\.

Finally, it’s worth paying attention to DNS and double\-checking that requests are being spread over a wide pool of Amazon S3 IP addresses\. DNS queries for Amazon S3 cycle through a large list of IP endpoints\. But caching resolvers or application code that reuses a single IP address do not benefit from address diversity and the load balancing that follows from it\. Network utility tools such as the `netstat` command line tool can show the IP addresses being used for communication with Amazon S3, and we provide guidelines for DNS configurations to use\. For more information about these guidelines, see [DNS considerations](DNSConsiderations.md)\.

## Using Amazon S3 Transfer Acceleration to Accelerate Geographically Disparate Data Transfers<a name="optimizing-performance-acceleration"></a>

[Amazon S3 Transfer Acceleration](transfer-acceleration.md) is effective at minimizing or eliminating the latency caused by geographic distance between globally dispersed clients and a regional application using Amazon S3\. Transfer Acceleration uses the globally distributed edge locations in CloudFront for data transport\. The AWS edge network has points of presence in more than 50 locations\. Today, it is used to distribute content through CloudFront and to provide rapid responses to DNS queries made to [Amazon Route 53](https://docs.aws.amazon.com/route53/index.html)\. 

The edge network also helps to accelerate data transfers into and out of Amazon S3\. It is ideal for applications that transfer data across or between continents, have a fast internet connection, use large objects, or have a lot of content to upload\. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path\. In general, the farther away you are from an Amazon S3 Region, the higher the speed improvement you can expect from using Transfer Acceleration\. 

You can set up Transfer Acceleration on new or existing buckets\. You can use a separate Amazon S3 Transfer Acceleration endpoint to use the AWS edge locations\. The best way to test whether Transfer Acceleration helps client request performance is to use the [Amazon S3 Transfer Acceleration Speed Comparison tool](https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html)\. Network configurations and conditions vary from time to time and from location to location\. So you are charged only for transfers where Amazon S3 Transfer Acceleration can potentially improve your upload performance\. For information about using Transfer Acceleration with different AWS SDKs, see [Amazon S3 Transfer Acceleration Examples](transfer-acceleration-examples.md)\. 


# Specifying Server\-Side Encryption Using the AWS SDK for Ruby<a name="SSEUsingRubySDK"></a>

When using the AWS SDK for Ruby to upload an object, you can specify that the object be stored encrypted at rest with server\-side encryption \(SSE\)\. When you read the object back, it is automatically decrypted\.

The following AWS SDK for Ruby – Version 3 example demonstrates how to specify that a file uploaded to Amazon S3 be encrypted at rest\.

```
require 'aws-sdk-s3'

# Uploads a file to an Amazon S3 bucket and then encrypts the file server-side
#   by using the 256-bit Advanced Encryption Standard (AES-256) block cipher.
#
# Prerequisites:
#
# - An Amazon S3 bucket.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param bucket_name [String] The name of the bucket.
# @param object_key [String] The name for the uploaded object.
# @param object_content [String] The content to upload into the object.
# @return [Boolean] true if the file was successfully uploaded and then
#   encrypted; otherwise, false.
# @example
#   exit 1 unless upload_file_encrypted_aes256_at_rest?(
#     Aws::S3::Client.new(region: 'us-east-1'),
#     'doc-example-bucket',
#     'my-file.txt',
#     'This is the content of my-file.txt.'
#   )
def upload_file_encrypted_aes256_at_rest?(
  s3_client,
  bucket_name,
  object_key,
  object_content
)
  s3_client.put_object(
    bucket: bucket_name,
    key: object_key,
    body: object_content,
    server_side_encryption: 'AES256'
  )
  return true
rescue StandardError => e
  puts "Error uploading object: #{e.message}"
  return false
end
```

For an example that shows how to upload an object without SSE, see [Upload an object using the AWS SDK for Ruby](UploadObjSingleOpRuby.md)\.

## Determining the Encryption Algorithm Used<a name="DeterminingEncryptionAlgorithmUsed03"></a>

The following code example demonstrates how to determine the encryption state of an existing object\.

```
require 'aws-sdk-s3'

# Gets the server-side encryption state of an object in an Amazon S3 bucket.
#
# Prerequisites:
#
# - An Amazon S3 bucket.
# - An object within that bucket.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param bucket_name [String] The bucket's name.
# @param object_key [String] The object's key.
# @return [String] The server-side encryption state.
# @example
#   s3_client = Aws::S3::Client.new(region: 'us-east-1')
#   puts get_server_side_encryption_state(
#     s3_client,
#     'doc-example-bucket',
#     'my-file.txt'
#   )
def get_server_side_encryption_state(s3_client, bucket_name, object_key)
  response = s3_client.get_object(
    bucket: bucket_name,
    key: object_key
  )
  encryption_state = response.server_side_encryption
  encryption_state.nil? ? 'not set' : encryption_state
rescue StandardError => e
  "unknown or error: #{e.message}"
end
```

If server\-side encryption is not used for the object that is stored in Amazon S3, the method returns null\.

## Changing Server\-Side Encryption of an Existing Object \(Copy Operation\)<a name="ChangingServer-SideEncryptionofanExistingObjectCopyOperation03"></a>

To change the encryption state of an existing object, make a copy of the object and delete the source object\. By default, the copy methods do not encrypt the target unless you explicitly request server\-side encryption\. You can request the encryption of the target object by specifying the `server_side_encryption` value in the options hash argument as shown in the following Ruby code example\. The code example demonstrates how to copy an object and encrypt the copy\. 

```
require 'aws-sdk-s3'

# Copies an object from one Amazon S3 bucket to another,
#   changing the object's server-side encryption state during 
#   the copy operation.
#
# Prerequisites:
#
# - A bucket containing an object to be copied.
# - A separate bucket to copy the object into.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param source_bucket_name [String] The source bucket's name.
# @param source_object_key [String] The name of the object to be copied.
# @param target_bucket_name [String] The target bucket's name.
# @param target_object_key [String] The name of the copied object.
# @param encryption_type [String] The server-side encryption type for
#   the copied object.
# @return [Boolean] true if the object was copied with the specified
#   server-side encryption; otherwise, false.
# @example
#   s3_client = Aws::S3::Client.new(region: 'us-east-1')
#   if object_copied_with_encryption?(
#     s3_client,
#     'doc-example-bucket1',
#     'my-source-file.txt',
#     'doc-example-bucket2',
#     'my-target-file.txt',
#     'AES256'
#   )
#     puts 'Copied.'
#   else
#     puts 'Not copied.'
#   end
def object_copied_with_encryption?(
  s3_client,
  source_bucket_name,
  source_object_key,
  target_bucket_name,
  target_object_key,
  encryption_type
)
  response = s3_client.copy_object(
    bucket: target_bucket_name,
    copy_source: source_bucket_name + '/' + source_object_key,
    key: target_object_key,
    server_side_encryption: encryption_type
  )
  return true if response.copy_object_result
rescue StandardError => e
  puts "Error while copying object: #{e.message}"
end
```

For a sample of how to copy an object without encryption, see [Copy an Object Using the AWS SDK for Ruby](CopyingObjectUsingRuby.md)\. 


# Object subresources<a name="ObjectAndSubResource"></a>

Amazon S3 defines a set of subresources associated with buckets and objects\. Subresources are subordinates to objects\. This means that subresources don't exist on their own\. They are always associated with some other entity, such as an object or a bucket\. 

 The following table lists the subresources associated with Amazon S3 objects\.


| Subresource | Description | 
| --- | --- | 
| acl | Contains a list of grants identifying the grantees and the permissions granted\. When you create an object, the acl identifies the object owner as having full control over the object\. You can retrieve an object ACL or replace it with an updated list of grants\. Any update to an ACL requires you to replace the existing ACL\. For more information about ACLs, see [Managing Access with ACLs](S3_ACLs_UsingACLs.md)\. | 
| torrent | Amazon S3 supports the BitTorrent protocol\. Amazon S3 uses the torrent subresource to return the torrent file associated with the specific object\. To retrieve a torrent file, you specify the torrent subresource in your GET request\. Amazon S3 creates a torrent file and returns it\. You can only retrieve the torrent subresource, you cannot create, update, or delete the torrent subresource\. For more information, see [Using BitTorrent with Amazon S3](S3Torrent.md)\.   Amazon S3 does not support the BitTorrent protocol in AWS Regions launched after May 30, 2016\.   | 


# Get an object Using the AWS SDK for PHP<a name="RetrieveObjSingleOpPHP"></a>

This topic explains how to use a class from the AWS SDK for PHP to retrieve an Amazon S3 object\. You can retrieve an entire object or a byte range from the object\. We assume that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\. 

When retrieving an object, you can optionally override the response header values by adding the response keys, `ResponseContentType`, `ResponseContentLanguage`, `ResponseContentDisposition`, `ResponseCacheControl`, and `ResponseExpires`, to the `getObject()` method, as shown in the following PHP code example:

**Example**  

```
1. $result = $s3->getObject([
2.     'Bucket'                     => $bucket,
3.     'Key'                        => $keyname,
4.     'ResponseContentType'        => 'text/plain',
5.     'ResponseContentLanguage'    => 'en-US',
6.     'ResponseContentDisposition' => 'attachment; filename=testing.txt',
7.     'ResponseCacheControl'       => 'No-cache',
8.     'ResponseExpires'            => gmdate(DATE_RFC2822, time() + 3600),
9. ]);
```

For more information about retrieving objects, see [Getting objects](GettingObjectsUsingAPIs.md)\. 

The following PHP example retrieves an object and displays the content of the object in the browser\. The example shows how to use the `getObject()` method\. For information about running the PHP examples in this guide, see [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\. 

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

try {
    // Get the object.
    $result = $s3->getObject([
        'Bucket' => $bucket,
        'Key'    => $keyname
    ]);

    // Display the object in the browser.
    header("Content-Type: {$result['ContentType']}");
    echo $result['Body'];
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}
```

## Related Resources<a name="RelatedResources-RetrieveObjSingleOpPHP"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Getting objects<a name="GettingObjectsUsingAPIs"></a>

**Topics**
+ [Related resources](#RelatedResources013)
+ [Get an object Using the AWS SDK for Java](RetrievingObjectUsingJava.md)
+ [Get an object Using the AWS SDK for \.NET](RetrievingObjectUsingNetSDK.md)
+ [Get an object Using the AWS SDK for PHP](RetrieveObjSingleOpPHP.md)
+ [Get an object Using the REST API](RetrieveObjSingleOpREST.md)
+ [Share an object with others](ShareObjectPreSignedURL.md)

 You can retrieve objects directly from Amazon S3\. You have the following options when retrieving an object: 
+ **Retrieve an entire object—**A single GET operation can return you the entire object stored in Amazon S3\. 
+ **Retrieve object in parts—**Using the `Range` HTTP header in a GET request, you can retrieve a specific range of bytes in an object stored in Amazon S3\. 

  You resume fetching other parts of the object whenever your application is ready\. This resumable download is useful when you need only portions of your object data\. It is also useful where network connectivity is poor and you need to react to failures\.
**Note**  
Amazon S3 doesn't support retrieving multiple ranges of data per GET request\.

 When you retrieve an object, its metadata is returned in the response headers\. There are times when you want to override certain response header values returned in a GET response\. For example, you might override the `Content-Disposition` response header value in your GET request\. The REST GET Object API \(see [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html)\) allows you to specify query string parameters in your GET request to override these values\. 

The AWS SDKs for Java, \.NET, and PHP also provide necessary objects you can use to specify values for these response headers in your GET request\. 

When retrieving objects that are stored encrypted using server\-side encryption, you must provide appropriate request headers\. For more information, see [Protecting data using encryption](UsingEncryption.md)\.

## Related resources<a name="RelatedResources013"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Put object ACL<a name="batch-ops-put-object-acl"></a>

The Put Object Acl operation replaces the Amazon S3 access control lists \(ACLs\) for each object that is listed in the manifest\. Using ACLs, you can define who can access an object and what actions they can perform\.

S3 Batch Operations support custom ACLs that you define and canned ACLs that Amazon S3 provides with a predefined set of access permissions\.

If the objects in your manifest are in a versioned bucket, you can apply the ACLs to specific versions of each object\. You do this by specifying a version ID for each object in the manifest\. If you don't include a version ID for any object, then S3 Batch Operations applies the ACL to the latest version of the object\.

**Note**  
If you want to limit public access to all objects in a bucket, you should use Amazon S3 block public access instead of S3 Batch Operations\. Block public access can limit public access on a per\-bucket or account\-wide basis with a single, simple operation that takes effect quickly\. This make it a better choice when your goal is to control public access to all objects in a bucket or account\. Use S3 Batch Operations when you need to apply a customized ACL to each object in the manifest\. For more information about Amazon S3 block public access, see [Using Amazon S3 block public access](access-control-block-public-access.md)\.

## Restrictions and limitations<a name="batch-ops-put-object-acl-restrictions"></a>
+ The role that you specify to run the Put Object Acl job must have permissions to perform the underlying Amazon S3 PUT Object acl operation\. For more information about the permissions required, see [PUT Object acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUTacl.html) in the *Amazon Simple Storage Service API Reference*\.
+ S3 Batch Operations use the Amazon S3 PUT Object acl operation to apply the specified ACL to each object in the manifest\. Therefore, all restrictions and limitations that apply to the underlying PUT Object acl operation also apply to S3 Batch Operations Put Object Acl jobs\. For more information, see the [Related resources](#batch-ops-put-object-acl-related-resources) section of this page\.

## Related resources<a name="batch-ops-put-object-acl-related-resources"></a>
+ [Managing Access with ACLs](S3_ACLs_UsingACLs.md)
+ [GET Object ACL](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETacl.html) in the *Amazon Simple Storage Service API Reference*


# Amazon S3 resources<a name="RelatedResources012"></a>

Following is a table that lists related resources that you'll find useful as you work with this service\.


|  Resource  |  Description  | 
| --- | --- | 
|  [ Amazon Simple Storage Service Getting Started Guide](https://docs.aws.amazon.com/AmazonS3/latest/gsg/)  | The Getting Started Guide provides a quick tutorial of the service based on a simple use case\.  | 
|  [ Amazon Simple Storage Service API Reference](https://docs.aws.amazon.com/AmazonS3/latest/API/)  | The API Reference describes Amazon S3 operations in detail\. | 
|  [Amazon S3Technical FAQ](https://aws.amazon.com/s3/faqs/)  |  The FAQ covers the top questions developers have asked about this product\.   | 
|  [AWS Developer Resource Center](https://aws.amazon.com/resources/)  | A central starting point to find documentation, code samples, release notes, and other information to help you build innovative applications with AWS\.  | 
|   [AWS Management Console](https://aws.amazon.com/console/)   |   The console allows you to perform most of the functions of Amazon S3without programming\.  | 
|    [https://forums\.aws\.amazon\.com/](https://forums.aws.amazon.com/)   |  A community\-based forum for developers to discuss technical questions related to AWS\.   | 
|   [AWS Support Center](https://aws.amazon.com/support)   |   The home page for AWS Technical Support, including access to our Developer Forums, Technical FAQs, Service Status page, and Premium Support\.   | 
|  [AWS Premium Support](https://aws.amazon.com/premiumsupport/)  |  The primary web page for information about AWS Premium Support, a one\-on\-one, fast\-response support channel to help you build and run applications on AWS Infrastructure Services\.  | 
|  [Amazon S3 product information](https://aws.amazon.com/s3/)  | The primary web page for information about Amazon S3\.  | 
|  [Contact Us](https://aws.amazon.com/contact-us/)  |  A central contact point for inquiries concerning AWS billing, account, events, abuse, etc\.   | 
|   [Conditions of Use](https://aws.amazon.com/legal)   |  Detailed information about the copyright and trademark usage at Amazon\.com and other topics\.   | 


# Metrics configurations for buckets<a name="metrics-configurations"></a>

With Amazon CloudWatch request metrics for Amazon S3, you can receive 1\-minute CloudWatch metrics, set CloudWatch alarms, and access CloudWatch dashboards to view near\-real\-time operations and performance of your Amazon S3 storage\. For applications that depend on cloud storage, these metrics let you quickly identify and act on operational issues\. When enabled, these 1\-minute metrics are available at the Amazon S3 bucket\-level, by default\.

If you want to get the CloudWatch request metrics for the objects in a bucket, you must create a metrics configuration for the bucket\. You can also define a filter for the metrics collected using a shared prefix or object tags\. This allows you to align metrics filters to specific business applications, workflows, or internal organizations\.

For more information about the CloudWatch metrics that are available and the differences between storage and request metrics, see [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)\.

Keep the following in mind when using metrics configurations:
+ You can have a maximum of 1,000 metrics configurations per bucket\.
+ You can choose which objects in a bucket to include in metrics configurations by using filters\. Filtering on a shared prefix or object tag allows you to align metrics filters to specific business applications, workflows, or internal organizations\. To request metrics for the entire bucket, create a metrics configuration without a filter\.
+ Metrics configurations are necessary only to enable request metrics\. Bucket\-level daily storage metrics are always turned on, and are provided at no additional cost\. Currently, it's not possible to get daily storage metrics for a filtered subset of objects\.
+ Each metrics configuration enables the full set of [available request metrics](cloudwatch-monitoring.md#s3-request-cloudwatch-metrics)\. Operation\-specific metrics \(such as `PostRequests`\) are reported only if there are requests of that type for your bucket or your filter\.
+ Request metrics are reported for object\-level operations\. They are also reported for operations that list bucket contents, like [GET Bucket \(List Objects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html), [GET Bucket Object Versions](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html), and [List Multipart Uploads](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListMPUpload.html), but they are not reported for other operations on buckets\.
+ Request metrics support filtering by prefixes, but storage metrics do not\.

## Best\-effort CloudWatch metrics delivery<a name="metrics-configurations-delivery"></a>

 CloudWatch metrics are delivered on a best\-effort basis\. Most requests for an Amazon S3 object that have request metrics result in a data point being sent to CloudWatch\.

The completeness and timeliness of metrics is not guaranteed\. The data point for a particular request might be returned with a timestamp that is later than when the request was actually processed\. Or the data point for a minute might be delayed before being available through CloudWatch, or it might not be delivered at all\. CloudWatch request metrics give you an idea of the nature of traffic against your bucket in near\-real time\. It is not meant to be a complete accounting of all requests\.

It follows from the best\-effort nature of this feature that the reports available at the [Billing & Cost Management Dashboard](https://console.aws.amazon.com/billing/home?#/) might include one or more access requests that do not appear in the bucket metrics\.

## Filtering metrics configurations<a name="metrics-configurations-filter"></a>

When working with CloudWatch metric configurations, you have the option of filtering the configuration into groups of related objects within a single bucket\. You can filter objects in a bucket for inclusion in a metrics configuration based on one or more of the following elements:
+ **Object key name prefix** – Although the Amazon S3 data model is a flat structure, you can infer hierarchy by using a prefix\. The Amazon S3 console supports these prefixes with the concept of folders\. If you filter by prefix, objects that have the same prefix are included in the metrics configuration\.
+ **Tag** – You can add tags, which are key\-value name pairs, to objects\. Tags help you find and organize objects easily\. You can also use tags as a filter for metrics configurations\.

If you specify a filter, only requests that operate on single objects can match the filter and be included in the reported metrics\. Requests like [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html) and `List` requests don't return any metrics for configurations with filters\.

To request more complex filtering, choose two or more elements\. Only objects that have all of those elements are included in the metrics configuration\. If you don't set filters, all of the objects in the bucket are included in the metrics configuration\.

## How to add metrics configurations<a name="add-metrics-configurations"></a>

You can add metrics configurations to a bucket through the Amazon S3 console, with the AWS CLI, or with the Amazon S3 REST API\. For information about how to do this in the AWS Management Console, see the [How Do I Configure Request Metrics for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-metrics.html) in the *Amazon Simple Storage Service Console User Guide*\.

**To add metrics configurations using the AWS CLI**

1. Install and set up the AWS CLI\. For instructions, see [Getting Set Up with the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html) in the *AWS Command Line Interface User Guide*\.

1. Open a terminal\.

1. Run the following command to add a metrics configuration\.

   ```
   aws s3api put-bucket-metrics-configuration --endpoint https://s3.us-west-2.amazonaws.com --bucket bucket-name --id metrics-config-id --metrics-configuration '{"Id":"metrics-config-id","Filter":{"Prefix":"prefix1"}}'
   ```

1. To verify that the configuration was added, run the following command\.

   ```
   aws s3api get-bucket-metrics-configuration --endpoint https://s3.us-west-2.amazonaws.com --bucket bucket-name --id metrics-config-id
   ```

   This returns the following response\.

   ```
   {
       "MetricsConfiguration": {
           "Filter": {
               "Prefix": "prefix1"
           },
           "Id": "metrics-config-id"
       }
   }
   ```

You can also add metrics configurations programmatically with the Amazon S3 REST API\. For more information, see the following topics in the *Amazon Simple Storage Service API Reference*:
+ [PUT Bucket Metric Configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTMetricConfiguration.html)
+ [GET Bucket Metric Configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETMetricConfiguration.html)
+ [List Bucket Metric Configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTListBucketMetricsConfiguration.html)
+ [DELETE Bucket Metric Configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTDeleteBucketMetricsConfiguration.html)


# Compliance Validation for Amazon S3<a name="s3-compliance"></a>

The security and compliance of Amazon S3 is assessed by third\-party auditors as part of multiple AWS compliance programs, including the following:
+ System and Organization Controls \(SOC\)
+ Payment Card Industry Data Security Standard \(PCI DSS\)
+ Federal Risk and Authorization Management Program \(FedRAMP\)
+ Health Insurance Portability and Accountability Act \(HIPAA\)

AWS provides a frequently updated list of AWS services in scope of specific compliance programs at [AWS Services in Scope by Compliance Program](https://aws.amazon.com/compliance/services-in-scope/)\. 

Third\-party audit reports are available for you to download using AWS Artifact\. For more information, see [Downloading Reports in AWS Artifact](https://docs.aws.amazon.com/artifact/latest/ug/downloading-documents.html)\. 

For more information about AWS compliance programs, see [AWS Compliance Programs](https://aws.amazon.com/compliance/programs/)\.

Your compliance responsibility when using Amazon S3 is determined by the sensitivity of your data, your organization’s compliance objectives, and applicable laws and regulations\. If your use of Amazon S3 is subject to compliance with standards like HIPAA, PCI, or FedRAMP, AWS provides resources to help:
+ [Security and Compliance Quick Start Guides](https://aws.amazon.com/quickstart/?awsf.quickstart-homepage-filter=categories%23security-identity-compliance) that discuss architectural considerations and steps for deploying security\- and compliance\-focused baseline environments on AWS\. 
+ [Architecting for HIPAA Security and Compliance](https://d0.awsstatic.com/whitepapers/compliance/AWS_HIPAA_Compliance_Whitepaper.pdf) outlines how companies use AWS to help them meet HIPAA requirements\.
+ [AWS Compliance Resources](https://aws.amazon.com/compliance/resources/) provide several different workbooks and guides that might apply to your industry and location\.
+ [AWS Config](https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config.html) can be used to assess how well your resource configurations comply with internal practices, industry guidelines, and regulations\.
+ [AWS Security Hub](https://docs.aws.amazon.com/securityhub/latest/userguide/what-is-securityhub.html) provides you with a comprehensive view of your security state within AWS and helps you check your compliance with security industry standards and best practices\. 
+ [S3 Object Lock](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html) can help you meet technical requirements of financial services regulators \(such as the SEC, FINRA, and CFTC\) that require write once, read many \(WORM\) data storage for certain types of books and records information\. 
+ [ Amazon S3 inventory](storage-inventory.md) can help you audit and report on the replication and encryption status of your objects for business, compliance, and regulatory needs\.


# Creating an S3 Batch Operations job<a name="batch-ops-create-job"></a>

With S3 Batch Operations, you can perform large\-scale Batch Operations on a list of specific Amazon S3 objects\. You can create S3 Batch Operations jobs using the AWS Management Console, AWS Command Line Interface \(AWS CLI\), AWS SDKs, or REST API\. 

This section describes the information that you need to create an S3 Batch Operations job\. It also describes the results of a `Create Job` request\.

**Note**  
For step\-by\-step instructions for creating a job using the Amazon S3 console, see [Creating an S3 Batch Operations Job](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/batch-ops-create-job.html) in the *Amazon Simple Storage Service Console User Guide\.*

## Creating a job request<a name="batch-ops-create-job-request-elements"></a>

To create an S3 Batch Operations job, you must provide the following information:

**Operation**  
Specify the operation that you want S3 Batch Operations to run against the objects in the manifest\. Each operation type accepts parameters that are specific to that operation\. This enables you to perform the same tasks as if you performed the operation one\-by\-one on each object\.

**Manifest**  
The manifest is a list of all of the objects that you want S3 Batch Operations to run the specified action on\. You can use a CSV\-formatted [ Amazon S3 inventory](storage-inventory.md) report as a manifest or use your own customized CSV list of objects\.   
For more information about manifests, see [Specifying a manifest](batch-ops-basics.md#specify-batchjob-manifest)\.

**Priority**  
Use job priorities to indicate the relative priority of this job to others running in your account\. A higher number indicates higher priority\.  
 Job priorities only have meaning relative to the priorities that are set for other jobs in the same account and Region\. So you can choose whatever numbering system works for you\. For example, you might want to assign all `Initiate Restore Object` jobs a priority of 1, all `PUT Object Copy` jobs a priority of 2, and all `Put Object ACL` jobs a priority of 3\.   
S3 Batch Operations prioritize jobs according to priority numbers, but strict ordering isn't guaranteed\. Thus, you shouldn't use job priorities to ensure that any one job will start or finish before any other job\. If you need to ensure strict ordering, wait until one job has finished before starting the next\. 

**RoleArn**  
Specify an AWS Identity and Access Management \(IAM\) role to run the job\. The IAM role that you use must have sufficient permissions to perform the operation that is specified in the job\. For example, to run a `PUT Object Copy` job, the IAM role must have `s3:GetObject` permissions for the source bucket and `s3:PutObject` permissions for the destination bucket\. The role also needs permissions to read the manifest and write the job\-completion report\.   
For more information about IAM roles, see [IAM Roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) in the *IAM User Guide*\.   
For more information about Amazon S3 permissions, see [Amazon S3 Actions](using-with-s3-actions.md)\.

**Report**  
Specify whether you want S3 Batch Operations to generate a completion report\. If you request a job\-completion report, you must also provide the parameters for the report in this element\. The necessary information includes the bucket where you want to store the report, the format of the report, whether you want the report to include the details of all tasks or only failed tasks, and an optional prefix string\.

**Tags \(Optional\)**  
You can label and control access to your S3 Batch Operations jobs by adding *tags*\. Tags can be used to identify who is responsible for a Batch Operations job\. You can create jobs with tags attached to them, and you can add tags to jobs after you create them\. For example, you could grant an IAM user permission to invoke `CreateJob` provided that the job is created with the tag `"Department=Finance"`\.   
For more information, see [Controlling access and labeling jobs using tags](batch-ops-managing-jobs.md#batch-ops-job-tags)\.

**Description \(Optional\)**  
To track and monitor your job, you can also provide a description of up to 256 characters\. Amazon S3 includes this description whenever it returns information about a job or displays job details on the Amazon S3 console\. You can then easily sort and filter jobs according to the descriptions that you assigned\. Descriptions don't need to be unique, so you can use descriptions as categories \(for example, "Weekly Log Copy Jobs"\) to help you track groups of similar jobs\.

## Creating a job response<a name="batch-ops-create-job-response-elements"></a>

If the `Create Job` request succeeds, Amazon S3 returns a job ID\. The job ID is a unique identifier that Amazon S3 generates automatically so that you can identify your Batch Operations job and monitor its status\.

When you create a job through the AWS CLI, AWS SDKs, or REST API, you can set S3 Batch Operations to begin processing the job automatically\. The job runs as soon as it's ready and not waiting behind higher\-priority jobs\. 

When you create a job through the AWS Management Console, you must review the job details and confirm that you want to run it before Batch Operations can begin to process it\. After you confirm that you want to run the job, it progresses as though you had created it through one of the other methods\. If a job remains in the suspended state for over 30 days, it will fail\.

For examples, see [S3 Batch Operations examples using the AWS CLI](batch-ops-examples-cli.md)\.


# Performance Guidelines for Amazon S3<a name="optimizing-performance-guidelines"></a>

When building applications that upload and retrieve objects from Amazon S3, follow our best practices guidelines to optimize performance\. We also offer more detailed [Performance Design Patterns](optimizing-performance-design-patterns.md)\. 

To obtain the best performance for your application on Amazon S3, we recommend the following guidelines\.

**Topics**
+ [Measure Performance](#optimizing-performance-guidelines-measure)
+ [Scale Storage Connections Horizontally](#optimizing-performance-guidelines-scale)
+ [Use Byte\-Range Fetches](#optimizing-performance-guidelines-get-range)
+ [Retry Requests for Latency\-Sensitive Applications](#optimizing-performance-guidelines-retry)
+ [Combine Amazon S3 \(Storage\) and Amazon EC2 \(Compute\) in the Same AWS Region](#optimizing-performance-guidelines-combine)
+ [Use Amazon S3 Transfer Acceleration to Minimize Latency Caused by Distance](#optimizing-performance-guidelines-acceleration)
+ [Use the Latest Version of the AWS SDKs](#optimizing-performance-guidelines-sdk)

## Measure Performance<a name="optimizing-performance-guidelines-measure"></a>

When optimizing performance, look at network throughput, CPU, and DRAM requirements\. Depending on the mix of demands for these different resources, it might be worth evaluating different [Amazon EC2](https://docs.aws.amazon.com/ec2/index.html) instance types\. For more information about instance types, see [Instance Types](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html) in the *Amazon EC2 User Guide for Linux Instances*\. 

It’s also helpful to look at DNS lookup time, latency, and data transfer speed using HTTP analysis tools when measuring performance\.

## Scale Storage Connections Horizontally<a name="optimizing-performance-guidelines-scale"></a>

Spreading requests across many connections is a common design pattern to horizontally scale performance\. When you build high performance applications, think of Amazon S3 as a very large distributed system, not as a single network endpoint like a traditional storage server\. You can achieve the best performance by issuing multiple concurrent requests to Amazon S3\. Spread these requests over separate connections to maximize the accessible bandwidth from Amazon S3\. Amazon S3 doesn't have any limits for the number of connections made to your bucket\. 

## Use Byte\-Range Fetches<a name="optimizing-performance-guidelines-get-range"></a>

Using the `Range` HTTP header in a [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) request, you can fetch a byte\-range from an object, transferring only the specified portion\. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object\. This helps you achieve higher aggregate throughput versus a single whole\-object request\. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted\. For more information, see [Getting objects](GettingObjectsUsingAPIs.md)\.

Typical sizes for byte\-range requests are 8 MB or 16 MB\. If objects are PUT using a multipart upload, it’s a good practice to GET them in the same part sizes \(or at least aligned to part boundaries\) for best performance\. GET requests can directly address individual parts; for example, `GET ?partNumber=N.`

## Retry Requests for Latency\-Sensitive Applications<a name="optimizing-performance-guidelines-retry"></a>

Aggressive timeouts and retries help drive consistent latency\. Given the large scale of Amazon S3, if the first request is slow, a retried request is likely to take a different path and quickly succeed\. The AWS SDKs have configurable timeout and retry values that you can tune to the tolerances of your specific application\.

## Combine Amazon S3 \(Storage\) and Amazon EC2 \(Compute\) in the Same AWS Region<a name="optimizing-performance-guidelines-combine"></a>

Although S3 bucket names are [globally unique](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html), each bucket is stored in a Region that you select when you create the bucket\. To optimize performance, we recommend that you access the bucket from Amazon EC2 instances in the same AWS Region when possible\. This helps reduce network latency and data transfer costs\.

For more information about data transfer costs, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\.

## Use Amazon S3 Transfer Acceleration to Minimize Latency Caused by Distance<a name="optimizing-performance-guidelines-acceleration"></a>

[Amazon S3 Transfer Acceleration](transfer-acceleration.md) manages fast, easy, and secure transfers of files over long geographic distances between the client and an S3 bucket\. Transfer Acceleration takes advantage of the globally distributed edge locations in [Amazon CloudFront](https://docs.aws.amazon.com/cloudfront/index.html)\. As the data arrives at an edge location, it is routed to Amazon S3 over an optimized network path\. Transfer Acceleration is ideal for transferring gigabytes to terabytes of data regularly across continents\. It's also useful for clients that upload to a centralized bucket from all over the world\.

You can use the [Amazon S3 Transfer Acceleration Speed Comparison tool](https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html) to compare accelerated and non\-accelerated upload speeds across Amazon S3 Regions\. The Speed Comparison tool uses multipart uploads to transfer a file from your browser to various Amazon S3 Regions with and without using Amazon S3 Transfer Acceleration\.

## Use the Latest Version of the AWS SDKs<a name="optimizing-performance-guidelines-sdk"></a>

The AWS SDKs provide built\-in support for many of the recommended guidelines for optimizing Amazon S3 performance\. The SDKs provide a simpler API for taking advantage of Amazon S3 from within an application and are regularly updated to follow the latest best practices\. For example, the SDKs include logic to automatically retry requests on HTTP 503 errors and are investing in code to respond and adapt to slow connections\. 

The SDKs also provide the [Transfer Manager](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/examples-s3-transfermanager.html), which automates horizontally scaling connections to achieve thousands of requests per second, using byte\-range requests where appropriate\. It’s important to use the latest version of the AWS SDKs to obtain the latest performance optimization features\.

You can also optimize performance when you are using HTTP REST API requests\. When using the REST API, you should follow the same best practices that are part of the SDKs\. Allow for timeouts and retries on slow requests, and multiple connections to allow fetching of object data in parallel\. For information about using the REST API, see the [Amazon Simple Storage Service API Reference](https://docs.aws.amazon.com/AmazonS3/latest/API/)\.


# Document history<a name="WhatsNew"></a>
+ **Current API version:** 2006\-03\-01

The following table describes the important changes in each release of the *Amazon Simple Storage Service API Reference* and the *Amazon Simple Storage Service Developer Guide*\. For notification about updates to this documentation, you can subscribe to an RSS feed\.

| Change | Description | Date | 
| --- |--- |--- |
| [Strong consistency](#WhatsNew) | Amazon S3 provides strong read\-after\-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions\. In addition, read operations on Amazon S3 Select, Amazon S3 Access Control Lists, Amazon S3 Object Tags, and object metadata \(e\.g\. HEAD object\) are strongly consistent\. For more information, see [Amazon S3 data consistency model](https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html)\. | December 1, 2020 | 
| [Amazon S3 replica modification sync](#WhatsNew) | Amazon S3 replica modification sync keeps object metadata such as tags, ACLs, and Object Lock settings in sync between source objects and replicas\. When this feature is enabled, Amazon S3 replicates metadata changes made to either the source object or the replica copies\. For more information, see [Replicating metadata changes with replica modification sync](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-for-metadata-changes.html)\. | December 1, 2020 | 
| [Amazon S3 Bucket Keys](#WhatsNew) | Amazon S3 Bucket Keys reduce the cost of Amazon S3 server\-side encryption using AWS Key Management Service \(SSE\-KMS\)\. This new bucket\-level key for server\-side encryption can reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS\. For more information, see [Reducing the cost of SSE\-KMS using S3 Bucket Keys](https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-key.html)\. | December 1, 2020 | 
| [Amazon S3 Storage Lens](#WhatsNew) | Amazon S3 Storage Lens aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format\. You can use the dashboard to visualize insights and trends, flag outliers, and provides recommendations for optimizing storage costs and applying data protection best practices\. You can use S3 Storage Lens through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\. For more information, see [ Assessing your storage activity and usage with S3 Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens.html)\.  | November 18, 2020 | 
| [Tracing S3 requests using AWS X\-Ray](#WhatsNew) | Amazon S3 integrates with X\-Ray to get one request chain integrates with X\-Ray to propagate [ trace context](https://www.w3.org/TR/trace-context/#:~:text=Trace%20context%20is%20split%20into,design%20focuses%20on%20fast%20parsing) and give you one request chain with [upstream and downstream](https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html) nodes\. For more information, see [ Tracing requests using X\-Ray](https://docs.aws.amazon.com/AmazonS3/latest/dev/tracing_requests_using_xray.html)\.  | November 16, 2020 | 
| [S3 replication metrics](#WhatsNew) | S3 replication metrics provide detailed metrics for the replication rules in your replication configuration\. For more information, see [Replication metrics and Amazon S3 event notifications](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-metrics.html)\. | November 9, 2020 | 
| [S3 Intelligent\-Tiering Archive Access and Deep Archive Access](#WhatsNew) | S3 Intelligent\-Tiering Archive Access and Deep Archive Access are additional storage tiers under S3 Intelligent\-Tiering\. For more information, see [Storage class for automatically optimizing frequently and infrequently accessed objects ](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-dynamic-data-access)\. | November 9, 2020 | 
| [Delete marker replication](#WhatsNew) | With delete marker replication you can ensure delete markers are copied to your destination buckets for your replication rules\. For more information, see [Using delete marker replication](https://docs.aws.amazon.com/AmazonS3/latest/dev/delete-marker-replication.html)\. | November 9, 2020 | 
| [S3 Object Ownership](#WhatsNew) | Object Ownership is an S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets\. For more information, see [Using S3 Object Ownership](https://docs.aws.amazon.com/AmazonS3/latest/dev/about-object-ownership.html)\. | October 2, 2020 | 
| [Amazon S3 on Outposts](#WhatsNew) | With Amazon S3 on Outposts, you can create S3 buckets on your AWS Outposts and easily store and retrieve objects on\-premises for applications that require local data access, local data processing, and data residency\. You can use S3 on Outposts through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\. For more information, see [Using Amazon S3 on Outposts](https://docs.aws.amazon.com/AmazonS3/latest/dev/S3onOutposts.html)\. | September 30, 2020 | 
| [Bucket owner condition](#WhatsNew) | You can use Amazon S3 bucket owner condition to ensure that the buckets you use in your S3 operations belong to the AWS accounts you expect\. For more information, see [Bucket owner condition](https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-owner-condition.html)\. | September 11, 2020 | 
| [S3 Batch Operations support for Object Lock Retention ](#WhatsNew) | You can now use Batch Operations with S3 Object Lock to apply retention settings to many Amazon S3 objects at once\. For more information, see [Setting S3 Object Lock Retention Dates with S3 Batch Operations](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-retention-date.html)\. | May 4, 2020 | 
| [S3 Batch Operations support for Object Lock Legal Hold](#WhatsNew) | You can now use Batch Operations with S3 Object Lock to add legal hold to many Amazon S3 objects at once\. For more information, see [Using S3 Batch Operations for setting S3 Object Lock Legal Hold](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-legal-hold.html)\. | May 4, 2020 | 
| [Job Tags for S3 Batch Operations](#WhatsNew) | You can add tags to your S3 Batch Operations jobs to control and label those jobs\. For more information, see [Tags for S3 Batch Operations Jobs](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-managing-jobs.html#batch-ops-job-tags)\. | March 16, 2020 | 
| [Amazon S3 Access Points](#WhatsNew) | Amazon S3 Access Points simplify managing data access at scale for shared datasets in S3\. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations\. For more information, see [Managing Data Access with Amazon S3 Access Points](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-points.html)\. | December 2, 2019 | 
| [Access Analyzer for Amazon S3](#WhatsNew) | Access Analyzer for Amazon S3 alerts you to S3 buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization\. For more information, see [Using Access Analyzer for Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html)\. | December 2, 2019 | 
| [S3 Replication Time Control \(S3 RTC\)](#WhatsNew) | S3 Replication Time Control \(S3 RTC\) replicates most objects that you upload to Amazon S3 in seconds, and 99\.99 percent of those objects within 15 minutes\. For more information, see [Replicating Objects Using S3 Replication Time Control \(S3 RTC\)](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-time-control.html)\. | November 20, 2019 | 
| [Same\-Region replication](#WhatsNew) | Same\-Region replication \(SRR\) is used to copy objects across Amazon S3 buckets in the same AWS Region\. For information about both cross\-Region and same\-Region replication, see [Replication](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html)\. | September 18, 2019 | 
| [Cross\-Region replication support for S3 Object Lock](#WhatsNew) | Cross\-Region replication now supports Object Lock\. For more information, see [Cross\-Region Replication](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html#replication-requirements) and [What Does Amazon S3 Replicate?](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-what-is-isnot-replicated.html)\. | May 28, 2019 | 
| [S3 Batch Operations ](#WhatsNew) | Using S3 Batch Operations you can perform large\-scale Batch Operations on Amazon S3 objects\. S3 Batch Operations can run a single operation on lists of objects that you specify\. A single job can perform the specified operation on billions of objects containing exabytes of data\. For more information, see [Performing S3 Batch Operations](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops.html)\. | April 30, 2019 | 
| [Asia Pacific \(Hong Kong\) Region](#WhatsNew) | Amazon S3 is now available in the Asia Pacific \(Hong Kong\)  Region\. For more information about Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\. | April 24, 2019 | 
| [Added a new field to the server access logs](#WhatsNew) | Amazon S3 added the following new field to the server access logs: Transport Layer Security \(TLS\) version\. For more information, see [Amazon S3 Server Access Log Format](https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html)\. | March 28, 2019 | 
| [ New archive storage class ](#WhatsNew) | Amazon S3 now offers a new archive storage class, DEEP\_ARCHIVE, for storing rarely accessed objects\. For more information, see [Storage Classes](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)\.  | March 27, 2019 | 
| [Added new fields to the server access logs](#WhatsNew) | Amazon S3 added the following new fields to the server access logs: Host Id, Signature Version, Cipher Suite, Authentication Type, and Host Header\. For more information, see [Amazon S3 Server Access Log Format](https://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html)\. | March 5, 2019 | 
| [Support for Parquet\-formatted Amazon S3 inventory files](#WhatsNew) | Amazon S3 now supports the [Apache Parquet \(Parquet\)](https://parquet.apache.org/) format in addition to the [Apache optimized row columnar \(ORC\)](https://orc.apache.org/) and comma\-separated values \(CSV\) file formats for inventory output files\. For more information, see [Amazon S3 Inventory](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html)\. | December 4, 2018 | 
| [S3 Object Lock](#WhatsNew) | Amazon S3 now offers Object Lock functionality that provides Write Once Read Many protections for Amazon S3 objects\. For more information, see [Locking Objects](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html)\. | November 26, 2018 | 
| [Restore speed upgrade](#WhatsNew) | Using Amazon S3 restore speed upgrade you can change the speed of a restoration from the S3 Glacier storage class to a faster speed while the restoration is in progress\. For more information, see [Restoring Archived Objects](https://docs.aws.amazon.com/AmazonS3/latest/dev/restoring-objects.html)\.  | November 26, 2018 | 
| [Restore event notifications](#WhatsNew) |  Amazon S3 event notifications now support initiation and completion events when restoring objects from the S3 Glacier storage class\. For more information, see [Event Notifications](https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html)\.  | November 26, 2018 | 
| [PUT directly to the S3 Glacier storage class](#WhatsNew) | The Amazon S3 PUT operation now supports specifying S3 Glacier as the storage class when creating objects\. Previously, you had to transition objects to the S3 Glacier storage class from another Amazon S3 storage class\. Also, when using S3 Cross\-Region Replication \(CRR\), you can now specify S3 Glacier as the storage class for replicated objects\. For more information about the S3 Glacier storage class, see [Storage Classes](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)\. For more information about specifying the storage class for replicated objects, [Replication Configuration Overview](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-add-config.html)\. For more information about the direct PUT to S3 Glacier REST API changes, see [Document History: PUT directly to S3 Glacier ](https://docs.aws.amazon.com/AmazonS3/latest/API/WhatsNew.html)\. | November 26, 2018 | 
| [New storage class](#WhatsNew) |  Amazon S3 now offers a new storage class named INTELLIGENT\_TIERING that is designed for long\-lived data with changing or unknown access patterns\. For more information, see [Storage Classes](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)\.  | November 26, 2018 | 
| [Amazon S3 Block Public Access](#WhatsNew) |  Amazon S3 now includes the ability to block public access to buckets and objects on a per\-bucket or account\-wide basis\. For more information, see [Using Amazon S3 Block Public Access](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html)\.  | November 15, 2018 | 
| [ Filtering enhancements in Cross\-Region Replication \(CRR\) rules ](#WhatsNew) |  In a CRR rule configuration, you can specify an object filter to choose a subset of objects to apply the rule to\. Previously, you could filter only on an object key prefix\. In this release, you can filter on an object key prefix, one or more object tags, or both\. For more information, see [CRR Setup: Replication Configuration Overview](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-add-config.html)\.  | September 19, 2018 | 
| [New Amazon S3 Select features](#WhatsNew) | Amazon S3 Select now supports Apache Parquet input, queries on nested JSON objects, and two new Amazon CloudWatch monitoring metrics \(`SelectScannedBytes` and `SelectReturnedBytes`\)\. | September 5, 2018 | 
| [Updates now available over RSS](#WhatsNew) | You can now subscribe to an RSS feed to receive notifications about updates to the Amazon Simple Storage Service Developer Guide\. | June 19, 2018 | 

## Earlier Updates<a name="WhatsNew-earlier-doc-history"></a>

The following table describes the important changes in each release of the *Amazon Simple Storage Service Developer Guide* before June 19, 2018\. 


| Change | Description | Date | 
| --- | --- | --- | 
|  Code examples update  |  Code examples updated: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/WhatsNew.html)  | April 30, 2018 | 
|  Amazon S3 now reports S3 Glacier and ONEZONE\_IA storage classes to Amazon CloudWatch Logs storage metrics  |  In addition to reporting actual bytes, these storage metrics include per\-object overhead bytes for applicable storage classes \(ONEZONE\_IA, STANDARD\_IA, and S3 Glacier \): [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/WhatsNew.html) For more information about storage metrics, see [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)\.  | April 30, 2018 | 
| New storage class  |  Amazon S3 now offers a new storage class, ONEZONE\_IA \(IA, for infrequent access\) for storing objects\. For more information, see [Amazon S3 storage classes](storage-class-intro.md)\.   |  April 4, 2018  | 
| Amazon S3 Select | Amazon S3 now supports retrieving object content based on an SQL expression\. For more information, see [Selecting content from objects](selecting-content-from-objects.md)\.  |  April 4, 2018 | 
| Asia Pacific \(Osaka\-Local\) Region |  Amazon S3 is now available in the Asia Pacific \(Osaka\-Local\) Region\. For more information about Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.  You can use the Asia Pacific \(Osaka\-Local\) Region only in conjunction with the Asia Pacific \(Tokyo\) Region\. To request access to Asia Pacific \(Osaka\-Local\) Region, contact your sales representative\.   |  February 12, 2018 | 
| Amazon S3 inventory creation timestamp |  Amazon S3 inventory now includes a timestamp of the date and start time of the creation of the Amazon S3 inventory report\. You can use the timestamp to determine changes in your Amazon S3 storage from the start time of when the inventory report was generated\.  |  January 16, 2018 | 
| Europe \(Paris\) Region |  Amazon S3 is now available in the Europe \(Paris\) Region\. For more information about Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.  |  December 18, 2017 | 
| China \(Ningxia\) Region |  Amazon S3 is now available in the China \(Ningxia\) Region\. For more information about Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.  |  December 11, 2017 | 
| Querying archives with SQL | Amazon S3 now supports querying S3 Glacier data archives with SQL\. For more information, see [Querying archived objects programmatically](querying-glacier-archives.md)\.  |  November 29, 2017 | 
| Support for ORC\-formatted Amazon S3 inventory files | Amazon S3 now supports the [Apache optimized row columnar \(ORC\)](https://orc.apache.org/) format in addition to comma\-separated values \(CSV\) file format for inventory output files\. Also, you can now query Amazon S3 inventory using standard SQL by using Amazon Athena, Amazon Redshift Spectrum, and other tools such as [Presto](https://prestodb.io/), [Apache Hive](https://hive.apache.org/), and [Apache Spark](https://databricks.com/spark/about/)\. For more information, see [ Amazon S3 inventory](storage-inventory.md)\.  |  November 17, 2017 | 
| Default encryption for S3 buckets |  Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket\. You can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket\. The objects are encrypted using server\-side encryption with either Amazon S3\-managed keys \(SSE\-S3\) or AWS KMS\-managed keys \(SSE\-KMS\)\. For more information, see [ Setting default server\-side encryption behavior for Amazon S3 buckets](bucket-encryption.md)\.  |  November 06, 2017 | 
| Encryption status in Amazon S3 inventory | Amazon S3 now supports including encryption status in Amazon S3 inventory so you can see how your objects are encrypted at rest for compliance auditing or other purposes\. You can also configure to encrypt S3 inventory with server\-side encryption \(SSE\) or SSE\-KMS so that all inventory files are encrypted accordingly\. For more information, see [ Amazon S3 inventory](storage-inventory.md)\.  |  November 06, 2017 | 
| Cross\-Region Replication \(CRR\) enhancements | Cross\-Region Replication now supports the following: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/WhatsNew.html)  |  November 06, 2017 | 
| Europe \(London\) Region |  Amazon S3 is now available in the Europe \(London\) Region\. For more information about Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.  |  December 13, 2016 | 
| Canada \(Central\) Region |  Amazon S3 is now available in the Canada \(Central\) Region\. For more information about Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.  |  December 8, 2016 | 
| Object tagging  |  Amazon S3 now supports object tagging\. Object tagging enables you to categorize storage\. Object key name prefixes also enable you to categorize storage, object tagging adds another dimension to it\.  There are added benefits tagging offers\. These include: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/WhatsNew.html) For more information, see [Object tagging](object-tagging.md)\.    |  November 29, 2016 | 
| Amazon S3 lifecycle now supports tag\-based filters |  Amazon S3 now supports tag\-based filtering in lifecycle configuration\. You can now specify lifecycle rules in which you can specify a key prefix, one or more object tags, or a combination of both to select a subset of objects to which the lifecycle rule applies\. For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.  |  November 29, 2016 | 
| CloudWatch request metrics for buckets  | Amazon S3 now supports CloudWatch metrics for requests made on buckets\. When you enable these metrics for a bucket, the metrics report at 1\-minute intervals\. You can also configure which objects in a bucket will report these request metrics\. For more information, see [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)\. | November 29, 2016 | 
| Amazon S3 Inventory |  Amazon S3 now supports storage inventory\. Amazon S3 inventory provides a flat\-file output of your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or a shared prefix \(that is, objects that have names that begin with a common string\)\. For more information, see [ Amazon S3 inventory](storage-inventory.md)\.   |  November 29, 2016 | 
| Amazon S3 Analytics – Storage Class Analysis | The new Amazon S3 analytics – storage class analysis feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD\_IA \(IA, for infrequent access\) storage class\. After storage class analysis observes the infrequent access patterns of a filtered set of data over a period of time, you can use the analysis results to help you improve your lifecycle policies\. This feature also includes a detailed daily analysis of your storage usage at the specified bucket, prefix, or tag level that you can export to an S3 bucket\. For more information, see [Amazon S3 analytics – Storage Class Analysis](analytics-storage-class.md) in the *Amazon Simple Storage Service Developer Guide*\.   |  November 29, 2016  | 
| New Expedited and Bulk data retrievals when restoring archived objects from S3 Glacier  |  Amazon S3 now supports Expedited and Bulk data retrievals in addition to Standard retrievals when restoring objects archived to S3 Glacier\. For more information, see [Restoring archived objects](restoring-objects.md)\.  |  November 21, 2016 | 
| CloudTrail object logging |  CloudTrail supports logging Amazon S3 object level API operations such as `GetObject`, `PutObject`, and `DeleteObject`\. You can configure your event selectors to log object level API operations\. For more information, see [Logging Amazon S3 API calls using AWS CloudTrail](cloudtrail-logging.md)\.  |  November 21, 2016 | 
| US East \(Ohio\) Region |  Amazon S3 is now available in the US East \(Ohio\) Region\. For more information about Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.  |  October 17, 2016 | 
| IPv6 support for Amazon S3 Transfer Acceleration  |  Amazon S3 now supports Internet Protocol version 6 \(IPv6\) for Amazon S3 Transfer Acceleration\. You can connect to Amazon S3 over IPv6 by using the new dual\-stack for Transfer Acceleration endpoint\. For more information, see [Getting Started with Amazon S3 Transfer Acceleration](transfer-acceleration.md#transfer-acceleration-getting-started)\.   |  October 6, 2016 | 
| IPv6 support |  Amazon S3 now supports Internet Protocol version 6 \(IPv6\)\. You can access Amazon S3 over IPv6 by using dual\-stack endpoints\. For more information, see [Making requests to Amazon S3 over IPv6](ipv6-access.md)\.   |  August 11, 2016 | 
| Asia Pacific \(Mumbai\) Region |  Amazon S3 is now available in the Asia Pacific \(Mumbai\) Region\. For more information about Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.  |  June 27, 2016 | 
| Amazon S3 Transfer Acceleration | Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations\.  For more information, see [Amazon S3 Transfer Acceleration](transfer-acceleration.md)\.   |  April 19, 2016 | 
| Lifecycle support to remove expired object delete markers |  Lifecycle configuration `Expiration` action now allows you to direct Amazon S3 to remove expired object delete markers in a versioned bucket\. For more information, see [Elements to describe lifecycle actions](intro-lifecycle-rules.md#intro-lifecycle-rules-actions)\.    |  March 16, 2016 | 
| Bucket lifecycle configuration now supports action to stop incomplete multipart uploads  |  Bucket lifecycle configuration now supports the `AbortIncompleteMultipartUpload` action that you can use to direct Amazon S3 to stop multipart uploads that don't complete within a specified number of days after being initiated\. When a multipart upload becomes eligible for a stop operation, Amazon S3 deletes any uploaded parts and stops the multipart upload\. For conceptual information, see the following topics in the *Amazon Simple Storage Service Developer Guide*: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/WhatsNew.html) The following API operations have been updated to support the new action: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/WhatsNew.html)     |  March 16, 2016  | 
| Asia Pacific \(Seoul\) Region |  Amazon S3 is now available in the Asia Pacific \(Seoul\) Region\. For more information about Amazon S3 Regions and endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) in the *AWS General Reference*\.  | January 6, 2016 | 
| New condition key and a Multipart Upload change |  IAM policies now support an Amazon S3 `s3:x-amz-storage-class` condition key\. For more information, see [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)\. You no longer need to be the initiator of a multipart upload to upload parts and complete the upload\. For more information, see [Multipart upload API and permissions](mpuAndPermissions.md)\.  |  December 14, 2015  | 
| Renamed the US Standard Region  | Changed the Region name string from "US Standard" to "US East \(N\. Virginia\)\." This is only a Region name update, there is no change in the functionality\. |  December 11, 2015  | 
| New storage class  |  Amazon S3 now offers a new storage class, STANDARD\_IA \(IA, for infrequent access\) for storing objects\. This storage class is optimized for long\-lived and less frequently accessed data\. For more information, see [Amazon S3 storage classes](storage-class-intro.md)\.  Lifecycle configuration feature updates now allow you to transition objects to the STANDARD\_IA storage class\. For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\. Previously, the Cross\-Region Replication feature used the storage class of the source object for object replicas\. Now, when you configure Cross\-Region Replication, you can specify a storage class for the object replica created in the destination bucket\. For more information, see [Replication](replication.md)\.  |  September 16, 2015  | 
| AWS CloudTrail integration |  New AWS CloudTrail integration allows you to record Amazon S3 API activity in your S3 bucket\. You can use CloudTrail to track S3 bucket creations or deletions, access control modifications, or lifecycle policy changes\. For more information, see [Logging Amazon S3 API calls using AWS CloudTrail](cloudtrail-logging.md)\.  |  September 1, 2015  | 
| Bucket limit increase |  Amazon S3 now supports bucket limit increases\. By default, customers can create up to 100 buckets in their AWS account\. Customers who need additional buckets can increase that limit by submitting a service limit increase\. For information about how to increase your bucket limit, go to [AWS Service Limits](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) in the *AWS General Reference*\. For more information, see [Creating a bucket](UsingBucket.md#create-bucket-intro) and [Bucket restrictions and limitations](BucketRestrictions.md)\.  |  August 4, 2015  | 
| Consistency model update |  Amazon S3 now supports read\-after\-write consistency for new objects added to Amazon S3 in the US East \(N\. Virginia\) Region\. Prior to this update, all Regions except US East \(N\. Virginia\) Region supported read\-after\-write consistency for new objects uploaded to Amazon S3\. With this enhancement, Amazon S3 now supports read\-after\-write consistency in all Regions for new objects added to Amazon S3\. Read\-after\-write consistency allows you to retrieve objects immediately after creation in Amazon S3\. For more information, see [Regions](Introduction.md#Regions)\.  |  August 4, 2015  | 
| Event notifications |  Amazon S3 event notifications have been updated to add notifications when objects are deleted and to add filtering on object names with prefix and suffix matching\. For more information, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.  |  July 28, 2015  | 
| Amazon CloudWatch integration |  New Amazon CloudWatch integration allows you to monitor and set alarms on your Amazon S3 usage through CloudWatch metrics for Amazon S3\. Supported metrics include total bytes for standard storage, total bytes for reduced\-redundancy storage, and total number of objects for a given S3 bucket\. For more information, see [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)\.  |  July 28, 2015  | 
| Support for deleting and emptying non\-empty buckets |  Amazon S3 now supports deleting and emptying non\-empty buckets\. For more information, see [Deleting or emptying a bucket](delete-or-empty-bucket.md)\.  |  July 16, 2015  | 
| Bucket policies for Amazon VPC endpoints |  Amazon S3 has added support for bucket policies for virtual private cloud \(VPC\) \(VPC\) endpoints\. You can use S3 bucket policies to control access to buckets from specific VPC endpoints, or specific VPCs\. VPC endpoints are easy to configure, are highly reliable, and provide a secure connection to Amazon S3 without requiring a gateway or a NAT instance\. For more information, see [Example Bucket Policies for VPC Endpoints for Amazon S3](example-bucket-policies-vpc-endpoint.md)\.  |  April 29, 2015  | 
| Event notifications |  Amazon S3 event notifications have been updated to support the switch to resource\-based permissions for AWS Lambda functions\. For more information, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.  |  April 9, 2015  | 
| Cross\-Region Replication |  Amazon S3 now supports Cross\-Region Replication\. Cross\-Region Replication is the automatic, asynchronous copying of objects across buckets in different AWS Regions\. For more information, see [Replication](replication.md)\.  |  March 24, 2015  | 
| Event notifications |  Amazon S3 now supports new event types and destinations in a bucket notification configuration\. Prior to this release, Amazon S3 supported only the *s3:ReducedRedundancyLostObject* event type and an Amazon SNS topic as the destination\. For more information about the new event types, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.  |  November 13, 2014  | 
| Server\-side encryption with customer\-provided encryption keys | Server\-side encryption with AWS Key Management Service \(AWS KMS\) Amazon S3 now supports server\-side encryption using AWS KMS\. This feature allows you to manage the envelope key through AWS KMS, and Amazon S3 calls AWS KMS to access the envelope key within the permissions you set\.  For more information about server\-side encryption with AWS KMS, see [Protecting Data Using Server\-Side Encryption with AWS Key Management Service](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html)\.   |  November 12, 2014  | 
| Europe \(Frankfurt\) Region |  Amazon S3 is now available in the Europe \(Frankfurt\) Region\.  | October 23, 2014 | 
| Server\-side encryption with customer\-provided encryption keys |  Amazon S3 now supports server\-side encryption using customer\-provided encryption keys \(SSE\-C\)\. Server\-side encryption enables you to request Amazon S3 to encrypt your data at rest\. When using SSE\-C, Amazon S3 encrypts your objects with the custom encryption keys that you provide\. Since Amazon S3 performs the encryption for you, you get the benefits of using your own encryption keys without the cost of writing or executing your own encryption code\.  For more information about SSE\-C, see [Server\-Side Encryption \(Using Customer\-Provided Encryption Keys\)](https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html)\.  | June 12, 2014 | 
| Lifecycle support for versioning |  Prior to this release, lifecycle configuration was supported only on nonversioned buckets\. Now you can configure lifecycle on both nonversioned and versioning\-enabled buckets\. For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.   | May 20, 2014 | 
| Access control topics revised |  Revised Amazon S3 access control documentation\. For more information, see [Identity and access management in Amazon S3](s3-access-control.md)\.   | April 15, 2014 | 
| Server access logging topic revised |  Revised server access logging documentation\. For more information, see [Amazon S3 server access logging](ServerLogs.md)\.   | November 26, 2013 | 
| \.NET SDK samples updated to version 2\.0 |  \.NET SDK samples in this guide are now compliant to version 2\.0\.  | November 26, 2013  | 
| SOAP Support Over HTTP Deprecated |  SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\.  | September 20, 2013 | 
| IAM policy variable support |  The IAM access policy language now supports variables\. When a policy is evaluated, any policy variables are replaced with values that are supplied by context\-based information from the authenticated user’s session\. You can use policy variables to define general purpose policies without explicitly listing all the components of the policy\. For more information about policy variables, see [IAM Policy Variables Overview](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html) in the *IAM User Guide*\.  For examples of policy variables in Amazon S3, see [User Policy Examples](example-policies-s3.md)\.   | April 3, 2013 | 
| Console support for Requester Pays | You can now configure your bucket for Requester Pays by using the Amazon S3 console\. For more information, see [Configure Requester Pays by using the Amazon S3 console](configure-requester-pays-console.md)\. | December 31, 2012  | 
| Root domain support for website hosting | Amazon S3 now supports hosting static websites at the root domain\. Visitors to your website can access your site from their browser without specifying "www" in the web address \(e\.g\., "example\.com"\)\. Many customers already host static websites on Amazon S3 that are accessible from a "www" subdomain \(e\.g\., "www\.example\.com"\)\. Previously, to support root domain access, you needed to run your own web server to proxy root domain requests from browsers to your website on Amazon S3\. Running a web server to proxy requests introduces additional costs, operational burden, and another potential point of failure\. Now, you can take advantage of the high availability and durability of Amazon S3 for both "www" and root domain addresses\. For more information, see [Hosting a static website using Amazon S3](WebsiteHosting.md)\. | December 27, 2012  | 
| Console revision | Amazon S3 console has been updated\. The documentation topics that refer to the console have been revised accordingly\. | December 14, 2012  | 
| Support for Archiving Data to S3 Glacier  |  Amazon S3 now supports a storage option that enables you to utilize S3 Glacier's low\-cost storage service for data archival\. To archive objects, you define archival rules identifying objects and a timeline when you want Amazon S3 to archive these objects to S3 Glacier\. You can easily set the rules on a bucket using the Amazon S3 console or programmatically using the Amazon S3 API or AWS SDKs\. For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.   | November 13, 2012  | 
| Support for Website Page Redirects |  For a bucket that is configured as a website, Amazon S3 now supports redirecting a request for an object to another object in the same bucket or to an external URL\. For more information, see [\(Optional\) Configuring a webpage redirect](how-to-page-redirect.md)\.  For information about hosting websites, see [Hosting a static website using Amazon S3](WebsiteHosting.md)\.   | October 4, 2012  | 
| Support for Cross\-Origin Resource Sharing \(CORS\) |  Amazon S3 now supports Cross\-Origin Resource Sharing \(CORS\)\. CORS defines a way in which client web applications that are loaded in one domain can interact with or access resources in a different domain\. With CORS support in Amazon S3, you can build rich client\-side web applications on top of Amazon S3 and selectively allow cross\-domain access to your Amazon S3 resources\. For more information, see [Cross\-origin resource sharing \(CORS\)](cors.md)\.  | August 31, 2012 | 
| Support for Cost Allocation Tags | Amazon S3 now supports cost allocation tagging, which allows you to label S3 buckets so you can more easily track their cost against projects or other criteria\. For more information about using tagging for buckets, see [Using cost allocation S3 bucket tags](CostAllocTagging.md)\. | August 21, 2012 | 
| Support for MFA\-protected API access in bucket policies  |  Amazon S3 now supports MFA\-protected API access, a feature that can enforce AWS Multi\-Factor Authentication for an extra level of security when accessing your Amazon S3 resources\. It is a security feature that requires users to prove physical possession of an MFA device by providing a valid MFA code\. For more information, go to [AWS Multi\-Factor Authentication](https://aws.amazon.com/iam/details/mfa/)\. You can now require MFA authentication for any requests to access your Amazon S3 resources\.  To enforce MFA authentication, Amazon S3 now supports the `aws:MultiFactorAuthAge` key in a bucket policy\. For an example bucket policy, see [Adding a Bucket Policy to Require MFA](example-bucket-policies.md#example-bucket-policies-use-case-7)\. | July 10, 2012 | 
| Object Expiration support  | You can use Object Expiration to schedule automatic removal of data after a configured time period\. You set object expiration by adding lifecycle configuration to a bucket\. | 27 December 2011   | 
| New Region supported | Amazon S3 now supports the South America \(São Paulo\) Region\. For more information, see [Accessing a bucket](UsingBucket.md#access-bucket-intro)\. | December 14, 2011 | 
| Multi\-Object Delete | Amazon S3 now supports Multi\-Object Delete API that enables you to delete multiple objects in a single request\. With this feature, you can remove large numbers of objects from Amazon S3 more quickly than using multiple individual DELETE requests\. For more information, see [Deleting objects](DeletingObjects.md)\. | December 7, 2011 | 
| New Region supported | Amazon S3 now supports the US West \(Oregon\) Region\. For more information, see [Buckets and Regions](UsingBucket.md#access-bucket-intro)\. | November 8, 2011 | 
| Documentation Update | Documentation bug fixes\. | November 8, 2011 | 
|  Documentation Update  |  In addition to documentation bug fixes, this release includes the following enhancements: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/WhatsNew.html)  | October 17, 2011 | 
|  Server\-side encryption support  |  Amazon S3 now supports server\-side encryption\. It enables you to request Amazon S3 to encrypt your data at rest, that is, encrypt your object data when Amazon S3 writes your data to disks in its data centers\. In addition to REST API updates, the AWS SDK for Java and \.NET provide necessary functionality to request server\-side encryption\. You can also request server\-side encryption when uploading objects using AWS Management Console\. To learn more about data encryption, go to [Using Data Encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html)\.  | October 4, 2011 | 
|  Documentation Update   |  In addition to documentation bug fixes, this release includes the following enhancements: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/WhatsNew.html)  | September 22, 2011 | 
|  Support for sending requests using temporary security credentials   |  In addition to using your AWS account and IAM user security credentials to send authenticated requests to Amazon S3, you can now send requests using temporary security credentials you obtain from AWS Identity and Access Management \(IAM\)\. You can use the AWS Security Token Service API or the AWS SDK wrapper libraries to request these temporary credentials from IAM\. You can request these temporary security credentials for your own use or hand them out to federated users and applications\. This feature enables you to manage your users outside AWS and provide them with temporary security credentials to access your AWS resources\.  For more information, see [Making requests](MakingRequests.md)\. For more information about IAM support for temporary security credentials, see [Temporary Security Credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html) in the *IAM User Guide*\.  | August 3, 2011 | 
|  Multipart Upload API extended to enable copying objects up to 5 TB  |  Prior to this release, Amazon S3 API supported copying objects of up to 5 GB in size\. To enable copying objects larger than 5 GB, Amazon S3 now extends the multipart upload API with a new operation, `Upload Part (Copy)`\. You can use this multipart upload operation to copy objects up to 5 TB in size\. For more information, see [Copying objects](CopyingObjectsExamples.md)\.  For conceptual information about multipart upload API, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\.   | June 21, 2011 | 
|  SOAP API calls over HTTP disabled  |  To increase security, SOAP API calls over HTTP are disabled\. Authenticated and anonymous SOAP requests must be sent to Amazon S3 using SSL\.  | June 6, 2011 | 
|  IAM enables cross\-account delegation   |  Previously, to access an Amazon S3 resource, an IAM user needed permissions from both the parent AWS account and the Amazon S3 resource owner\. With cross\-account access, the IAM user now only needs permission from the owner account\. That is, If a resource owner grants access to an AWS account, the AWS account can now grant its IAM users access to these resources\.  For more information, see [Creating a Role to Delegate Permissions to an IAM User](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html) in the *IAM User Guide*\.  For more information on specifying principals in a bucket policy, see [Principals](s3-bucket-user-policy-specifying-principal-intro.md)\.  | June 6, 2011 | 
|  New link  |  This service's endpoint information is now located in the AWS General Reference\. For more information, go to Regions and Endpoints in [AWS General Reference](https://docs.aws.amazon.com/general/latest/gr/rande.html)\.  | March 1, 2011 | 
|  Support for hosting static websites in Amazon S3  |  Amazon S3 introduces enhanced support for hosting static websites\. This includes support for index documents and custom error documents\. When using these features, requests to the root of your bucket or a subfolder \(e\.g\., `http://mywebsite.com/subfolder`\) returns your index document instead of the list of objects in your bucket\. If an error is encountered, Amazon S3 returns your custom error message instead of an Amazon S3 error message\. For more information, see [Hosting a static website using Amazon S3](WebsiteHosting.md)\.  |  February 17, 2011  | 
|  Response Header API Support  |  The GET Object REST API now allows you to change the response headers of the REST GET Object request for each request\. That is, you can alter object metadata in the response, without altering the object itself\. For more information, see [Getting objects](GettingObjectsUsingAPIs.md)\.  |  January 14, 2011  | 
|  Large object support  |  Amazon S3 has increased the maximum size of an object you can store in an S3 bucket from 5 GB to 5 TB\. If you are using the REST API you can upload objects of up to 5 GB size in a single PUT operation\. For larger objects, you must use the Multipart Upload REST API to upload objects in parts\. For more information, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\.  |  December 9, 2010  | 
|  Multipart upload  |  Multipart upload enables faster, more flexible uploads into Amazon S3\. It allows you to upload a single object as a set of parts\. For more information, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\.  |  November 10, 2010  | 
|  Canonical ID support in bucket policies  |  You can now specify canonical IDs in bucket policies\. For more information, see [Policies and Permissions in Amazon S3](access-policy-language-overview.md)   |  September 17, 2010  | 
|  Amazon S3 works with IAM  |  This service now integrates with AWS Identity and Access Management \(IAM\)\. For more information, go to [AWS Services That Work with IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html) in the *IAM User Guide*\.   |  September 2, 2010  | 
| Notifications | The Amazon S3 notifications feature enables you to configure a bucket so that Amazon S3 publishes a message to an Amazon Simple Notification Service \(Amazon SNS\) topic when Amazon S3 detects a key event on a bucket\. For more information, see [Setting Up Notification of Bucket Events](NotificationHowTo.md)\. | July 14, 2010 | 
| Bucket policies | Bucket policies is an access management system you use to set access permissions across buckets, objects, and sets of objects\. This functionality supplements and in many cases replaces access control lists\. For more information, see [Using Bucket Policies and User Policies](using-iam-policies.md)\. | July 6, 2010 | 
| Path\-style syntax available in all Regions | Amazon S3 now supports the path\-style syntax for any bucket in the US Classic Region, or if the bucket is in the same Region as the endpoint of the request\. For more information, see [Virtual Hosting](VirtualHosting.md)\. | June 9, 2010 | 
| New endpoint for Europe \(Ireland\) | Amazon S3 now provides an endpoint for Europe \(Ireland\): http://s3\-eu\-west\-1\.amazonaws\.com\. | June 9, 2010 | 
| Console | You can now use Amazon S3 through the AWS Management Console\. You can read about all of the Amazon S3 functionality in the console in the Amazon Simple Storage Service Console User Guide\.  | June 9, 2010 | 
| Reduced Redundancy | Amazon S3 now enables you to reduce your storage costs by storing objects in Amazon S3 with reduced redundancy\. For more information, see [Reduced Redundancy Storage](Introduction.md#RRS)\. | May 12, 2010 | 
| New Region supported | Amazon S3 now supports the Asia Pacific \(Singapore\) Region\. For more information, see [Buckets and Regions](UsingBucket.md#access-bucket-intro)\. | April 28, 2010 | 
| Object Versioning | This release introduces object versioning\. All objects now can have a key and a version\. If you enable versioning for a bucket, Amazon S3 gives all objects added to a bucket a unique version ID\. This feature enables you to recover from unintended overwrites and deletions\. For more information, see [Versioning](Introduction.md#Versions) and [Using Versioning](Versioning.md)\. | February 8, 2010 | 
| New Region supported | Amazon S3 now supports the US West \(N\. California\) Region\. The new endpoint for requests to this Region is s3\-us\-west\-1\.amazonaws\.com\. For more information, see [Buckets and Regions](UsingBucket.md#access-bucket-intro)\. | December 2, 2009 | 
| AWS SDK for \.NET | AWS now provides libraries, sample code, tutorials, and other resources for software developers who prefer to build applications using \.NET language\-specific API operations instead of REST or SOAP\. These libraries provide basic functions \(not included in the REST or SOAP APIs\), such as request authentication, request retries, and error handling so that it's easier to get started\. For more information about language\-specific libraries and resources, see [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)\.  | November 11, 2009 | 



# Example 3: Changing the replica owner when the source and destination buckets are owned by different accounts<a name="replication-walkthrough-3"></a>

When the *source* and *destination* buckets in a replication configuration are owned by different AWS accounts, you can tell Amazon S3 to change replica ownership to the AWS account that owns the *destination* bucket\. This example explains how to use the Amazon S3 console and the AWS CLI to change replica ownership\. For more information, see [Changing the replica owner](replication-change-owner.md)\. 

**Topics**

## Change the replica owner when buckets are owned by different accounts \(console\)<a name="replication-ex3-console"></a>

For step\-by\-step instructions, see [Configuring a Replication Rule When the Destination Bucket is in a Different AWS Account](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html) in the *Amazon Simple Storage Service Console User Guide*\. 

## Change the replica owner when buckets are owned by different accounts \(AWS CLI\)<a name="replication-ex3-cli"></a>

To change replica ownership using the AWS CLI, you create buckets, enable versioning on the buckets, create an IAM role that gives Amazon S3 permission to replicate objects, and add the replication configuration to the source bucket\. In the replication configuration you direct Amazon S3 to change replica owner\. You also test the setup\.

**To change replica ownership when source and destination buckets are owned by different AWS accounts \(AWS CLI\)**

1. In this example, you create the *source* and *destination* buckets in two different AWS accounts\. Configure the AWS CLI with two named profiles\. This example uses profiles named `acctA` and `acctB`, respectively\. For more information about setting credential profiles, see [Named Profiles](https://docs.aws.amazon.com/cli/latest/userguide/cli-multiple-profiles.html) in the *AWS Command Line Interface User Guide\.*
**Important**  
The profiles you use for this exercise must have the necessary permissions\. For example, in the replication configuration, you specify the IAM role that Amazon S3 can assume\. You can do this only if the profile you use has the `iam:PassRole` permission\. If you use administrator user credentials to create a named profile then you can perform all the tasks\. For more information, see [Granting a User Permissions to Pass a Role to an AWS Service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html) in the *IAM User Guide*\. 

   You will need to make sure these profiles have necessary permissions\. For example, the replication configuration includes an IAM role that Amazon S3 can assume\. The named profile you use to attach such configuration to a bucket can do so only if it has the `iam:PassRole` permission\. If you specify administrator user credentials when creating these named profiles, they have all the permissions\. For more information, see [Granting a User Permissions to Pass a Role to an AWS Service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html) in the *IAM User Guide*\. 

1. Create the *source* bucket and enable versioning\. This example creates the *source* bucket in the US East \(N\. Virginia\) \(us\-east\-1\) Region\.

   ```
   aws s3api create-bucket \
   --bucket source \
   --region us-east-1 \
   --profile acctA
   ```

   ```
   aws s3api put-bucket-versioning \
   --bucket source \
   --versioning-configuration Status=Enabled \
   --profile acctA
   ```

1. Create a *destination* bucket and enable versioning\. This example creates the *destination* bucket in the US West \(Oregon\) \(us\-west\-2\) Region\. Use an AWS account profile different from the one you used for the *source* bucket\.

   ```
   aws s3api create-bucket \
   --bucket destination \
   --region us-west-2 \
   --create-bucket-configuration LocationConstraint=us-west-2 \
   --profile acctB
   ```

   ```
   aws s3api put-bucket-versioning \
   --bucket destination \
   --versioning-configuration Status=Enabled \
   --profile acctB
   ```

1. You must add permissions to your *destination* bucket policy to allow changing the replica ownership\.

   1.  Save the following policy to `destination-bucket-policy.json`

      ```
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Sid": "destination_bucket_policy_sid",
            "Principal": {
              "AWS": "source-bucket-owner-account-id"
            },
            "Action": [
              "s3:ReplicateObject",
              "s3:ReplicateDelete",
              "s3:ObjectOwnerOverrideToBucketOwner",
              "s3:ReplicateTags",
              "s3:GetObjectVersionTagging"
            ],
            "Effect": "Allow",
            "Resource": [
              "arn:aws:s3:::destination/*"
            ]
          }
        ]
      }
      ```

   1. Put the above policy to *destination* bucket:

      ```
      aws s3api put-bucket-policy --region $ {destination_region} --bucket $ {destination} --policy file://destination_bucket_policy.json
      ```

1. Create an IAM role\. You specify this role in the replication configuration that you add to the *source* bucket later\. Amazon S3 assumes this role to replicate objects on your behalf\. You create an IAM role in two steps:
   + Create a role\.
   + Attach a permissions policy to the role\.

   1. Create an IAM role\.

      1. Copy the following trust policy and save it to a file named `S3-role-trust-policy.json` in the current directory on your local computer\. This policy grants Amazon S3 permissions to assume the role\.

         ```
         {
            "Version":"2012-10-17",
            "Statement":[
               {
                  "Effect":"Allow",
                  "Principal":{
                     "Service":"s3.amazonaws.com"
                  },
                  "Action":"sts:AssumeRole"
               }
            ]
         }
         ```

      1. Run the following AWS CLI command to create a role\.

         ```
         $ aws iam create-role \
         --role-name replicationRole \
         --assume-role-policy-document file://s3-role-trust-policy.json  \
         --profile acctA
         ```

   1. Attach a permissions policy to the role\.

      1. Copy the following permissions policy and save it to a file named `s3-role-perm-pol-changeowner.json` in the current directory on your local computer\. This policy grants permissions for various Amazon S3 bucket and object actions\. In the following steps, you create an IAM role and attach this policy to the role\. 

         ```
         {
            "Version":"2012-10-17",
            "Statement":[
               {
                  "Effect":"Allow",
                  "Action":[
                     "s3:GetObjectVersionForReplication",
                     "s3:GetObjectVersionAcl"
                  ],
                  "Resource":[
                     "arn:aws:s3:::source/*"
                  ]
               },
               {
                  "Effect":"Allow",
                  "Action":[
                     "s3:ListBucket",
                     "s3:GetReplicationConfiguration"
                  ],
                  "Resource":[
                     "arn:aws:s3:::source"
                  ]
               },
               {
                  "Effect":"Allow",
                  "Action":[
                     "s3:ReplicateObject",
                     "s3:ReplicateDelete",
                     "s3:ObjectOwnerOverrideToBucketOwner",
                     "s3:ReplicateTags",
                     "s3:GetObjectVersionTagging"
                  ],
                  "Resource":"arn:aws:s3:::destination/*"
               }
            ]
         }
         ```

      1. To create a policy and attach it to the role, run the following command\.

         ```
         $ aws iam put-role-policy \
         --role-name replicationRole \
         --policy-document file://s3-role-perm-pol-changeowner.json \
         --policy-name replicationRolechangeownerPolicy \
         --profile acctA
         ```

1. Add a replication configuration to your source bucket\.

   1. The AWS CLI requires specifying the replication configuration as JSON\. Save the following JSON in a file named `replication.json` in the current directory on your local computer\. In the configuration, the addition of `AccessControlTranslation` to indicate change in replica ownership\.

      ```
      {
         "Role":"IAM-role-ARN",
         "Rules":[
            {
               "Status":"Enabled",
               "Priority":1,
               "DeleteMarkerReplication":{
                  "Status":"Disabled"
               },
               "Filter":{
               },
               "Status":"Enabled",
               "Destination":{
                  "Bucket":"arn:aws:s3:::destination",
                  "Account":"destination-bucket-owner-account-id",
                  "AccessControlTranslation":{
                     "Owner":"Destination"
                  }
               }
            }
         ]
      }
      ```

   1. Edit the JSON by providing values for the *destination* bucket owner account ID and *IAM\-role\-ARN*\. Save the changes\.

   1. To add the replication configuration to the source bucket, run the following command\. Provide the *source* bucket name\.

      ```
      $ aws s3api put-bucket-replication \
      --replication-configuration file://replication.json \
      --bucket source \
      --profile acctA
      ```

1. Check replica ownership in the Amazon S3 console\.

   1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\. 

   1. Add objects to the *source* bucket\. Verify that the *destination* bucket contains the object replicas and that the ownership of the replicas has changed to the AWS account that owns the *destination* bucket\.

## Change the replica owner when buckets are owned by different accounts \(AWS SDK\)<a name="replication-ex3-sdk"></a>

 For a code example to add replication configuration, see [Configure replication when buckets are owned by the same account \(AWS SDK\)](replication-walkthrough1.md#replication-ex1-sdk)\. You need to modify the replication configuration appropriately\. For conceptual information, see [Changing the replica owner](replication-change-owner.md)\. 


# Additional considerations for lifecycle configuration<a name="lifecycle-additional-considerations"></a>

When configuring the lifecycle of objects, you need to understand the following guidelines for transitioning objects, setting expiration dates, and other object configurations\.

**Topics**
+ [Transitioning objects using Amazon S3 Lifecycle](lifecycle-transition-general-considerations.md)
+ [Understanding object expiration](lifecycle-expire-general-considerations.md)
+ [Lifecycle and other bucket configurations](lifecycle-and-other-bucket-config.md)


# Managing ACLs in the AWS Management Console<a name="manage-acls-using-console"></a>

AWS Management Console provides a UI for you to grant ACL\-based access permissions to your buckets and objects\. For information on setting ACL\-based access permissions in the console, see [How Do I Set ACL Bucket Permissions?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-bucket-permissions.html) and [How Do I Set Permissions on an Object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/set-object-permissions.html) in the *Amazon Simple Storage Service Console User Guide*\. 


# Example 4: Bucket owner granting cross\-account permission to objects it does not own<a name="example-walkthroughs-managing-access-example4"></a>

**Topics**
+ [Background: Cross\-account permissions and using IAM roles](#access-policies-walkthrough-example4-overview)
+ [Step 0: Preparing for the walkthrough](#access-policies-walkthrough-example4-step0)
+ [Step 1: Do the account a tasks](#access-policies-walkthrough-example4-step1)
+ [Step 2: Do the account b tasks](#access-policies-walkthrough-example4-step2)
+ [Step 3: Do the account C tasks](#access-policies-walkthrough-example4-step3)
+ [Step 4: Clean up](#access-policies-walkthrough-example4-step6)
+ [Related resources](#RelatedResources-managing-access-example4)

 In this example scenario, you own a bucket and you have enabled other AWS accounts to upload objects\. That is, your bucket can have objects that other AWS accounts own\. 

Now, suppose as a bucket owner, you need to grant cross\-account permission on objects, regardless of who the owner is, to a user in another account\. For example, that user could be a billing application that needs to access object metadata\. There are two core issues:
+ The bucket owner has no permissions on those objects created by other AWS accounts\. So for the bucket owner to grant permissions on objects it does not own, the object owner, the AWS account that created the objects, must first grant permission to the bucket owner\. The bucket owner can then delegate those permissions\.
+ Bucket owner account can delegate permissions to users in its own account \(see [Example 3: Bucket owner granting its users permissions to objects it does not own ](example-walkthroughs-managing-access-example3.md)\), but it cannot delegate permissions to other AWS accounts, because cross\-account delegation is not supported\. 

In this scenario, the bucket owner can create an AWS Identity and Access Management \(IAM\) role with permission to access objects, and grant another AWS account permission to assume the role temporarily enabling it to access objects in the bucket\. 

## Background: Cross\-account permissions and using IAM roles<a name="access-policies-walkthrough-example4-overview"></a>

 IAM roles enable several scenarios to delegate access to your resources, and cross\-account access is one of the key scenarios\. In this example, the bucket owner, Account A, uses an IAM role to temporarily delegate object access cross\-account to users in another AWS account, Account C\. Each IAM role you create has two policies attached to it:
+ A trust policy identifying another AWS account that can assume the role\.
+ An access policy defining what permissions—for example, `s3:GetObject`—are allowed when someone assumes the role\. For a list of permissions you can specify in a policy, see [Amazon S3 Actions](using-with-s3-actions.md)\.

The AWS account identified in the trust policy then grants its user permission to assume the role\. The user can then do the following to access objects:
+ Assume the role and, in response, get temporary security credentials\. 
+ Using the temporary security credentials, access the objects in the bucket\.

For more information about IAM roles, go to [IAM Roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) in *IAM User Guide*\. 

The following is a summary of the walkthrough steps:

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/access-policy-ex4.png)

1. Account A administrator user attaches a bucket policy granting Account B conditional permission to upload objects\.

1. Account A administrator creates an IAM role, establishing trust with Account C, so users in that account can access Account A\. The access policy attached to the role limits what user in Account C can do when the user accesses Account A\.

1. Account B administrator uploads an object to the bucket owned by Account A, granting full\-control permission to the bucket owner\.

1. Account C administrator creates a user and attaches a user policy that allows the user to assume the role\.

1. User in Account C first assumes the role, which returns the user temporary security credentials\. Using those temporary credentials, the user then accesses objects in the bucket\.

For this example, you need three accounts\. The following table shows how we refer to these accounts and the administrator users in these accounts\. Per IAM guidelines \(see [About using an administrator user to create resources and grant permissions](example-walkthroughs-managing-access.md#about-using-root-credentials)\) we do not use the account root credentials in this walkthrough\. Instead, you create an administrator user in each account and use those credentials in creating resources and granting them permissions


| AWS account ID | Account referred to as | Administrator user in the account  | 
| --- | --- | --- | 
|  *1111\-1111\-1111*  |  Account A  |  AccountAadmin  | 
|  *2222\-2222\-2222*  |  Account B  |  AccountBadmin  | 
|  *3333\-3333\-3333*  |  Account C  |  AccountCadmin  | 



## Step 0: Preparing for the walkthrough<a name="access-policies-walkthrough-example4-step0"></a>

**Note**  
You may want to open a text editor and write down some of the information as you walk through the steps\. In particular, you will need account IDs, canonical user IDs, IAM User Sign\-in URLs for each account to connect to the console, and Amazon Resource Names \(ARNs\) of the IAM users, and roles\. 

1. Make sure you have three AWS accounts and each account has one administrator user as shown in the table in the preceding section\.

   1. Sign up for AWS accounts, as needed\. We refer to these accounts as Account A, Account B, and Account C\.

      1.  Go to [https://aws\.amazon\.com/s3/](https://aws.amazon.com/s3/) and click **Create an AWS Account**\. 

      1. Follow the on\-screen instructions\.

         AWS will notify you by email when your account is active and available for you to use\.

   1. Using Account A credentials, sign in to the [IAM console](https://console.aws.amazon.com/iam/home?#home) and do the following to create an administrator user:
      + Create user AccountAadmin and note down security credentials\. For more information about adding users, see [Creating an IAM User in Your AWS Account](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html) in the *IAM User Guide*\. 
      + Grant AccountAadmin administrator privileges by attaching a user policy giving full access\. For instructions, see [Working with Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html) in the *IAM User Guide*\. 
      + In the IAM Console **Dashboard**, note down the** IAM User Sign\-In URL**\. Users in this account must use this URL when signing in to the AWS Management Console\. For more information, go to [How Users Sign In to Your Account](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_how-users-sign-in.html) in *IAM User Guide*\. 

   1. Repeat the preceding step to create administrator users in Account B and Account C\.

1. For Account C, note down the canonical user ID\. 

   When you create an IAM role in Account A, the trust policy grants Account C permission to assume the role by specifying the account ID\. You can find account information as follows:

   1. Use your AWS account ID or account alias, your IAM user name, and your password to sign in to the [Amazon S3 Console](https://console.aws.amazon.com/s3/)\.

   1. Choose the name of an Amazon S3 bucket to view the details about that bucket\.

   1. Choose the **Permissions** tab and then choose **Access Control List**\. 

   1. In the **Access for your AWS account** section, in the **Account** column is a long identifier, such as `c1daexampleaaf850ea79cf0430f33d72579fd1611c97f7ded193374c0b163b6`\. This is your canonical user ID\.

1. When creating a bucket policy, you will need the following information\. Note down these values:
   + **Canonical user ID of Account A** – When the Account A administrator grants conditional upload object permission to the Account B administrator, the condition specifies the canonical user ID of the Account A user that must get full\-control of the objects\. 
**Note**  
The canonical user ID is the Amazon S3–only concept\. It is a 64\-character obfuscated version of the account ID\. 
   + **User ARN for Account B administrator** – You can find the user ARN in the IAM console\. You will need to select the user and find the user's ARN in the **Summary** tab\.

     In the bucket policy, you grant AccountBadmin permission to upload objects and you specify the user using the ARN\. Here's an example ARN value:

     ```
     arn:aws:iam::AccountB-ID:user/AccountBadmin
     ```

1. Set up either the AWS Command Line Interface \(CLI\) or the AWS Tools for Windows PowerShell\. Make sure you save administrator user credentials as follows:
   + If using the AWS CLI, create profiles, AccountAadmin and AccountBadmin, in the config file\.
   + If using the AWS Tools for Windows PowerShell, make sure you store credentials for the session as AccountAadmin and AccountBadmin\.

   For instructions, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\.

## Step 1: Do the account a tasks<a name="access-policies-walkthrough-example4-step1"></a>

In this example, Account A is the bucket owner\. So user AccountAadmin in Account A will create a bucket, attach a bucket policy granting the Account B administrator permission to upload objects, create an IAM role granting Account C permission to assume the role so it can access objects in the bucket\. 

### Step 1\.1: Sign in to the AWS Management Console<a name="access-policies-walkthrough-cross-account-permissions-acctA-tasks-sign-in-example4"></a>

Using the IAM User Sign\-in URL for Account A, first sign in to the AWS Management Console as AccountAadmin user\. This user will create a bucket and attach a policy to it\. 

### Step 1\.2: Create a bucket and attach a bucket policy<a name="access-policies-walkthrough-example2d-step1-1"></a>

In the Amazon S3 console, do the following:

1. Create a bucket\. This exercise assumes the bucket name is `examplebucket`\.

   For instructions, see [How Do I Create an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\. 

1. Attach the following bucket policy granting conditional permission to the Account B administrator permission to upload objects\.

   You need to update the policy by providing your own values for *examplebucket*, *AccountB\-ID*, and the *CanonicalUserId\-of\-AWSaccountA\-BucketOwner*\. 

   ```
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Sid": "111",
               "Effect": "Allow",
               "Principal": {
                   "AWS": "arn:aws:iam::AccountB-ID:user/AccountBadmin"
               },
               "Action": "s3:PutObject",
               "Resource": "arn:aws:s3:::awsexamplebucket1/*"
           },
           {
               "Sid": "112",
               "Effect": "Deny",
               "Principal": {
                   "AWS": "arn:aws:iam::AccountB-ID:user/AccountBadmin"
               },
               "Action": "s3:PutObject",
               "Resource": "arn:aws:s3:::awsexamplebucket1/*",
               "Condition": {
                   "StringNotEquals": {
                       "s3:x-amz-grant-full-control": "id=CanonicalUserId-of-AWSaccountA-BucketOwner"
                   }
               }
           }
       ]
   }
   ```

### Step 1\.3: Create an IAM role to allow account C cross\-account access in account a<a name="access-policies-walkthrough-example2d-step1-2"></a>

In the IAM console, create an IAM role \("examplerole"\) that grants Account C permission to assume the role\. Make sure you are still signed in as the Account A administrator because the role must be created in Account A\.

1. Before creating the role, prepare the managed policy that defines the permissions that the role requires\. You attach this policy to the role in a later step\.

   1. In the navigation pane on the left, click **Policies** and then click **Create Policy**\.

   1. Next to **Create Your Own Policy**, click **Select**\.

   1. Enter `access-accountA-bucket` in the **Policy Name** field\.

   1. Copy the following access policy and paste it into the **Policy Document** field\. The access policy grants the role `s3:GetObject` permission so when Account C user assumes the role, it can only perform the `s3:GetObject` operation\.

      ```
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::awsexamplebucket1/*"
          }
        ]
      }
      ```

   1. Click **Create Policy**\.

      The new policy appears in the list of managed policies\.

1. In the navigation pane on the left, click **Roles** and then click **Create New Role**\.

1. Under **Select Role Type**, select **Role for Cross\-Account Access**, and then click the **Select** button next to **Provide access between AWS accounts you own**\.

1. Enter the Account C account ID\.

   For this walkthrough you do not need to require users to have multi\-factor authentication \(MFA\) to assume the role, so leave that option unselected\.

1. Click **Next Step** to set the permissions that will be associated with the role\.

1. 

   Select the box next to the `access-accountA-bucket` policy that you created and then click **Next Step**\.

   The Review page appears so you can confirm the settings for the role before it's created\. One very important item to note on this page is the link that you can send to your users who need to use this role\. Users who click the link go straight to the Switch Role page with the Account ID and Role Name fields already filled in\. You can also see this link later on the Role Summary page for any cross\-account role\.

1. Enter `examplerole` for the role name, and then click **Next Step**\.

1. After reviewing the role, click **Create Role**\.

   The `examplerole` role is displayed in the list of roles\.

1. Click the role name `examplerole`\.

1. Select the **Trust Relationships** tab\.

1. Click **Show policy document** and verify the trust policy shown matches the following policy\.

   The following trust policy establishes trust with Account C, by allowing it the `sts:AssumeRole` action\. For more information, go to [AssumeRole](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html) in the *AWS Security Token Service API Reference*\.

   ```
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Sid": "",
         "Effect": "Allow",
         "Principal": {
           "AWS": "arn:aws:iam::AccountC-ID:root"
         },
         "Action": "sts:AssumeRole"
       }
     ]
   }
   ```

1. Note down the Amazon Resource Name \(ARN\) of the `examplerole` role you created\. 

   Later in the following steps, you attach a user policy to allow an IAM user to assume this role, and you identify the role by the ARN value\. 

## Step 2: Do the account b tasks<a name="access-policies-walkthrough-example4-step2"></a>

The examplebucket owned by Account A needs objects owned by other accounts\. In this step, the Account B administrator uploads an object using the command line tools\.
+ Using the put\-object AWS CLI command, upload an object to the `examplebucket`\. 

  ```
  aws s3api put-object --bucket examplebucket --key HappyFace.jpg --body HappyFace.jpg --grant-full-control id="canonicalUserId-ofTheBucketOwner" --profile AccountBadmin
  ```

  Note the following:
  + The `--Profile` parameter specifies AccountBadmin profile, so the object is owned by Account B\.
  + The parameter `grant-full-control` grants the bucket owner full\-control permission on the object as required by the bucket policy\.
  + The `--body` parameter identifies the source file to upload\. For example, if the file is on the C: drive of a Windows computer, you specify `c:\HappyFace.jpg`\. 

## Step 3: Do the account C tasks<a name="access-policies-walkthrough-example4-step3"></a>

In the preceding steps, Account A has already created a role, `examplerole`, establishing trust with Account C\. This allows users in Account C to access Account A\. In this step, Account C administrator creates a user \(Dave\) and delegates him the `sts:AssumeRole` permission it received from Account A\. This will allow Dave to assume the `examplerole` and temporarily gain access to Account A\. The access policy that Account A attached to the role will limit what Dave can do when he accesses Account A—specifically, get objects in `examplebucket`\.

### Step 3\.1: Create a user in account C and delegate permission to assume `examplerole`<a name="cross-acct-access-using-role-step3-1"></a>

1. Using the IAM user sign\-in URL for Account C, first sign in to the AWS Management Console as AccountCadmin user\. 

   

1. In the IAM console, create a user Dave\. 

   For instructions, see [Creating IAM Users \(AWS Management Console\)](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console) in the *IAM User Guide*\. 

1. Note down the Dave credentials\. Dave will need these credentials to assume the `examplerole` role\.

1. Create an inline policy for the Dave IAM user to delegate the `sts:AssumeRole` permission to Dave on the `examplerole` role in account A\. 

   1. In the navigation pane on the left, click **Users**\.

   1. Click the user name Dave\.

   1. On the user details page, select the **Permissions** tab and then expand the **Inline Policies** section\.

   1. Choose **click here** \(or **Create User Policy**\)\.

   1. Click **Custom Policy**, and then click **Select**\.

   1. Enter a name for the policy in the **Policy Name** field\.

   1. Copy the following policy into the **Policy Document** field\.

      You will need to update the policy by providing the Account A ID\.

      ```
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Action": ["sts:AssumeRole"],
            "Resource": "arn:aws:iam::AccountA-ID:role/examplerole"
          }
        ]
      }
      ```

   1. Click **Apply Policy**

1. Save Dave's credentials to the config file of the AWS CLI by adding another profile, AccountCDave\.

   ```
   [profile AccountCDave]
   aws_access_key_id = UserDaveAccessKeyID
   aws_secret_access_key = UserDaveSecretAccessKey
   region = us-west-2
   ```

### Step 3\.2: Assume role \(examplerole\) and access objects<a name="cross-acct-access-using-role-step3-2"></a>

Now Dave can access objects in the bucket owned by Account A as follows:
+ Dave first assumes the `examplerole` using his own credentials\. This will return temporary credentials\.
+ Using the temporary credentials, Dave will then access objects in Account A's bucket\.

1. At the command prompt, run the following AWS CLI `assume-role` command using the AccountCDave profile\. 

   You will need to update the ARN value in the command by providing the Account A ID where `examplerole` is defined\.

   ```
   aws sts assume-role --role-arn arn:aws:iam::accountA-ID:role/examplerole --profile AccountCDave --role-session-name test
   ```

   In response, AWS Security Token Service \(STS\) returns temporary security credentials \(access key ID, secret access key, and a session token\)\.

1. Save the temporary security credentials in the AWS CLI config file under the `TempCred` profile\.

   ```
   [profile TempCred]
   aws_access_key_id = temp-access-key-ID
   aws_secret_access_key = temp-secret-access-key
   aws_session_token = session-token
   region = us-west-2
   ```

1. At the command prompt, run the following AWS CLI command to access objects using the temporary credentials\. For example, the command specifies the head\-object API to retrieve object metadata for the `HappyFace.jpg` object\.

   ```
   aws s3api get-object --bucket examplebucket --key HappyFace.jpg SaveFileAs.jpg --profile TempCred
   ```

   Because the access policy attached to `examplerole` allows the actions, Amazon S3 processes the request\. You can try any other action on any other object in the bucket\.

   If you try any other action—for example, `get-object-acl`—you will get permission denied because the role is not allowed that action\.

   ```
   aws s3api get-object-acl --bucket examplebucket --key HappyFace.jpg --profile TempCred
   ```

   We used user Dave to assume the role and access the object using temporary credentials\. It could also be an application in Account C that accesses objects in `examplebucket`\. The application can obtain temporary security credentials, and Account C can delegate the application permission to assume `examplerole`\.

## Step 4: Clean up<a name="access-policies-walkthrough-example4-step6"></a>

1. After you are done testing, you can do the following to clean up\.

   1. Sign in to the AWS Management Console \([AWS Management Console](https://console.aws.amazon.com/)\) using account A credentials, and do the following:
     + In the Amazon S3 console, remove the bucket policy attached to *examplebucket*\. In the bucket **Properties**, delete the policy in the **Permissions** section\. 
     + If the bucket is created for this exercise, in the Amazon S3 console, delete the objects and then delete the bucket\. 
     + In the IAM console, remove the `examplerole` you created in Account A\. 
     + In the IAM console, remove the AccountAadmin user\.

1. Sign in to the AWS Management Console \([AWS Management Console](https://console.aws.amazon.com/)\) using Account B credentials\. In the IAM console, delete user AccountBadmin\.

1. Sign in to the AWS Management Console \([AWS Management Console](https://console.aws.amazon.com/)\) using Account C credentials\. In the IAM console, delete user AccountCadmin and user Dave\.

## Related resources<a name="RelatedResources-managing-access-example4"></a>
+ [Creating a Role to Delegate Permissions to an IAM User](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html) in the *IAM User Guide*\.
+ [Tutorial: Delegate Access Across AWS Accounts Using IAM Roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial-cross-account-with-roles.html) in the *IAM User Guide*\.
+ [Working with Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html) in the *IAM User Guide*\.


# Share an object with others<a name="ShareObjectPreSignedURL"></a>

All objects by default are private\. Only the object owner has permission to access these objects\. However, the object owner can optionally share objects with others by creating a presigned URL, using their own security credentials, to grant time\-limited permission to download the objects\. 

When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method \(GET to download the object\) and expiration date and time\. The presigned URLs are valid only for the specified duration\. 

Anyone who receives the presigned URL can then access the object\. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a presigned URL\. 

**Note**  
Anyone with valid security credentials can create a presigned URL\. However, in order to successfully access an object, the presigned URL must be created by someone who has permission to perform the operation that the presigned URL is based upon\.
The credentials that you can use to create a presigned URL include:  
IAM instance profile: Valid up to 6 hours
AWS Security Token Service : Valid up to 36 hours when signed with permanent credentials, such as the credentials of the AWS account root user or an IAM user
IAM user: Valid up to 7 days when using AWS Signature Version 4  
To create a presigned URL that's valid for up to 7 days, first designate IAM user credentials \(the access key and secret access key\) to the SDK that you're using\. Then, generate a presigned URL using AWS Signature Version 4\.
If you created a presigned URL using a temporary token, then the URL expires when the token expires, even if the URL was created with a later expiration time\.
Since presigned URLs grant access to your Amazon S3 buckets to whoever has the URL, we recommend that you protect them appropriately\. For more details about protecting presigned URLs, see [Limiting presigned URL capabilities](https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html#PresignedUrlUploadObject-LimitCapabilities)\.

You can generate a presigned URL programmatically using the [REST API](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html#query-string-auth-v4-signing-example), the [AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/reference/s3/presign.html), and the AWS SDK for Java, \.NET, [Ruby](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/S3/Presigner.html), [PHP](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html#_createPresignedRequest), [Node\.js](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#getSignedUrl-property), [Python](http://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.generate_presigned_url), and [Go[(https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/s3-example-presigned-urls.html)\.

**Topics**
+ [Generate a presigned object URL using AWS explorer for Visual Studio](ShareObjectPreSignedURLVSExplorer.md)
+ [Generate a presigned object URL using the AWS SDK for Java](ShareObjectPreSignedURLJavaSDK.md)
+ [Generate a presigned object URL using AWS SDK for \.NET](ShareObjectPreSignedURLDotNetSDK.md)
+ [Upload an object using a presigned URL \(AWS SDK for PHP\)](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/s3-presigned-url.html)



# Protecting data using server\-side encryption with Amazon S3\-managed encryption keys \(SSE\-S3\)<a name="UsingServerSideEncryption"></a>

Server\-side encryption protects data at rest\. Amazon S3 encrypts each object with a unique key\. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly\. Amazon S3 server\-side encryption uses one of the strongest block ciphers available to encrypt your data, 256\-bit Advanced Encryption Standard \(AES\-256\)\.

If you need server\-side encryption for all of the objects that are stored in a bucket, use a bucket policy\. For example, the following bucket policy denies permissions to upload an object unless the request includes the `x-amz-server-side-encryption` header to request server\-side encryption:

```
 1. {
 2.   "Version": "2012-10-17",
 3.   "Id": "PutObjectPolicy",
 4.   "Statement": [
 5.     {
 6.       "Sid": "DenyIncorrectEncryptionHeader",
 7.       "Effect": "Deny",
 8.       "Principal": "*",
 9.       "Action": "s3:PutObject",
10.       "Resource": "arn:aws:s3:::awsexamplebucket1/*",
11.       "Condition": {
12.         "StringNotEquals": {
13.           "s3:x-amz-server-side-encryption": "AES256"
14.         }
15.       }
16.     },
17.     {
18.       "Sid": "DenyUnencryptedObjectUploads",
19.       "Effect": "Deny",
20.       "Principal": "*",
21.       "Action": "s3:PutObject",
22.       "Resource": "arn:aws:s3:::awsexamplebucket1/*",
23.       "Condition": {
24.         "Null": {
25.           "s3:x-amz-server-side-encryption": "true"
26.         }
27.       }
28.     }
29.   ]
30. }
```

**Note**  
Server\-side encryption encrypts only the object data, not object metadata\. 

## API Support for Server\-Side Encryption<a name="APISupportforServer-SideEncryption"></a>

To request server\-side encryption using the object creation REST APIs, provide the `x-amz-server-side-encryption` request header\. For information about the REST APIs, see [Specifying Server\-Side Encryption Using the REST API](SSEUsingRESTAPI.md)\.

The following Amazon S3 APIs support this header:
+ PUT operations—Specify the request header when uploading data using the PUT API\. For more information, see [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)\.
+ Initiate Multipart Upload—Specify the header in the initiate request when uploading large objects using the multipart upload API \. For more information, see [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)\.
+ COPY operations—When you copy an object, you have both a source object and a target object\. For more information, see [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)\.

**Note**  
When using a POST operation to upload an object, instead of providing the request header, you provide the same information in the form fields\. For more information, see [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)\. 

The AWS SDKs also provide wrapper APIs that you can use to request server\-side encryption\. You can also use the AWS Management Console to upload objects and request server\-side encryption\.

## More Info<a name="encryption-server-side-more-info"></a>
+  [ Setting default server\-side encryption behavior for Amazon S3 buckets](bucket-encryption.md) 


# Buckets and access control<a name="BucketAccess"></a>

Each bucket has an associated access control policy\. This policy governs the creation, deletion and enumeration of objects within the bucket\. For more information, see [Identity and access management in Amazon S3](s3-access-control.md)\. 


# How Amazon S3 Authorizes a Request<a name="how-s3-evaluates-access-control"></a>

**Topics**
+ [Related Topics](#access-control-how-s3-evaluates-related-topics)
+ [How Amazon S3 Authorizes a Request for a Bucket Operation](access-control-auth-workflow-bucket-operation.md)
+ [How Amazon S3 Authorizes a Request for an Object Operation](access-control-auth-workflow-object-operation.md)

When Amazon S3 receives a request—for example, a bucket or an object operation—it first verifies that the requester has the necessary permissions\. Amazon S3 evaluates all the relevant access policies, user policies, and resource\-based policies \(bucket policy, bucket ACL, object ACL\) in deciding whether to authorize the request\. The following are some of the example scenarios: 
+  If the requester is an IAM principal, Amazon S3 must determine if the parent AWS account to which the principal belongs has granted the principal necessary permission to perform the operation\. In addition, if the request is for a bucket operation, such as a request to list the bucket content, Amazon S3 must verify that the bucket owner has granted permission for the requester to perform the operation\. 
**Note**  
To perform a specific operation on a resource, an IAM principal needs permission from both the parent AWS account to which it belongs and the AWS account that owns the resource\.
+ If the request is for an operation on an object that the bucket owner does not own, in addition to making sure the requester has permissions from the object owner, Amazon S3 must also check the bucket policy to ensure the bucket owner has not set explicit deny on the object\. 
**Note**  
 A bucket owner \(who pays the bill\) can explicitly deny access to objects in the bucket regardless of who owns it\. The bucket owner can also delete any object in the bucket

In order to determine whether the requester has permission to perform the specific operation, Amazon S3 does the following, in order, when it receives a request:

1. Converts all the relevant access policies \(user policy, bucket policy, ACLs\) at run time into a set of policies for evaluation\.

1. Evaluates the resulting set of policies in the following steps\. In each step, Amazon S3 evaluates a subset of policies in a specific context, based on the context authority\. 

   1. **User context** – In the user context, the parent account to which the user belongs is the context authority\.

      Amazon S3 evaluates a subset of policies owned by the parent account\. This subset includes the user policy that the parent attaches to the user\. If the parent also owns the resource in the request \(bucket, object\), Amazon S3 also evaluates the corresponding resource policies \(bucket policy, bucket ACL, and object ACL\) at the same time\. 

      A user must have permission from the parent account to perform the operation\.

      This step applies only if the request is made by a user in an AWS account\. If the request is made using root credentials of an AWS account, Amazon S3 skips this step\.

   1. **Bucket context** – In the bucket context, Amazon S3 evaluates policies owned by the AWS account that owns the bucket\. 

      If the request is for a bucket operation, the requester must have permission from the bucket owner\. If the request is for an object, Amazon S3 evaluates all the policies owned by the bucket owner to check if the bucket owner has not explicitly denied access to the object\. If there is an explicit deny set, Amazon S3 does not authorize the request\. 

   1. **Object context** – If the request is for an object, Amazon S3 evaluates the subset of policies owned by the object owner\. 

      

 The following sections describe in detail and provide examples:
+ [How Amazon S3 Authorizes a Request for a Bucket Operation ](access-control-auth-workflow-bucket-operation.md)
+ [How Amazon S3 Authorizes a Request for an Object Operation ](access-control-auth-workflow-object-operation.md)

## Related Topics<a name="access-control-how-s3-evaluates-related-topics"></a>

We recommend you first review the introductory topics that explain the options for managing access to your Amazon S3 resources\. For more information, see [Introduction to managing access to Amazon S3 resources](s3-access-control.md#intro-managing-access-s3-resources)\. You can then use the following topics for more information about specific access policy options\. 
+  [Using Bucket Policies and User Policies](using-iam-policies.md) 
+  [Managing Access with ACLs](S3_ACLs_UsingACLs.md) 


# Making requests using AWS account or IAM user credentials<a name="AuthUsingAcctOrUserCredentials"></a>

You can use your AWS account or IAM user security credentials to send authenticated requests to Amazon S3\. This section provides examples of how you can send authenticated requests using the AWS SDK for Java, AWS SDK for \.NET, and AWS SDK for PHP\. For a list of available AWS SDKs, go to [Sample Code and Libraries](https://aws.amazon.com/code/)\. 

**Topics**
+ [Making requests using AWS account or IAM user credentials \- AWS SDK for Java](AuthUsingAcctOrUserCredJava.md)
+ [Making requests using AWS account or IAM user credentials \- AWS SDK for \.NET](AuthUsingAcctOrUserCredDotNet.md)
+ [Making requests using AWS account or IAM user credentials \- AWS SDK for PHP](AuthUsingAcctOrUserCredPHP3.md)
+ [Making requests using AWS account or IAM user credentials \- AWS SDK for Ruby](AuthUsingAcctOrUserCredRuby.md)

Each of these AWS SDKs uses an SDK\-specific credentials provider chain to find and use credentials and perform actions on behalf of the credentials owner\. What all these credentials provider chains have in common is that they all look for your local AWS credentials file\. 

The easiest way to configure credentials for your AWS SDKs is to use an AWS credentials file\. If you use the AWS Command Line Interface \(AWS CLI\), you may already have a local AWS credentials file configured\. Otherwise, use the following procedure to set up a credentials file:

**To create a local AWS credentials file**

1. Sign in to the AWS Management Console and open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.

1. Create a new user with permissions limited to the services and actions that you want your code to have access to\. For more information about creating a new IAM user, see [Creating IAM Users \(Console\)](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console), and follow the instructions through step 8\.

1. Choose **Download \.csv** to save a local copy of your AWS credentials\.

1. On your computer, navigate to your home directory, and create an `.aws` directory\. On Unix\-based systems, such as Linux or OS X, this is in the following location:

   ```
   ~/.aws
   ```

   On Windows, this is in the following location:

   ```
   %HOMEPATH%\.aws
   ```

1. In the `.aws` directory, create a new file named `credentials`\.

1. Open the credentials \.csv file that you downloaded from the IAM console, and copy its contents into the `credentials` file using the following format:

   ```
   [default]
   aws_access_key_id = your_access_key_id
   aws_secret_access_key = your_secret_access_key
   ```

1. Save the `credentials` file, and delete the \.csv file that you downloaded in step 3\.

Your shared credentials file is now configured on your local computer, and it's ready to be used with the AWS SDKs\.


# Performing S3 Batch Operations<a name="batch-ops"></a>

You can use S3 Batch Operations to perform large\-scale batch operations on Amazon S3 objects\. S3 Batch Operations can perform a single operation on lists of Amazon S3 objects that you specify\. A single job can perform a specified operation on billions of objects containing exabytes of data\. Amazon S3 tracks progress, sends notifications, and stores a detailed completion report of all actions, providing a fully managed, auditable, and serverless experience\. You can use S3 Batch Operations through the AWS console, AWS CLI, AWS SDKs, or REST API\.

Use S3 Batch Operations to copy objects and set object tags or access control lists \(ACLs\)\. You can also initiate object restores from Amazon S3 Glacier or invoke an AWS Lambda function to perform custom actions using your objects\. You can perform these operations on a custom list of objects, or you can use an Amazon S3 inventory report to easily generate lists of objects\. Amazon S3 Batch Operations use the same Amazon S3 APIs that you already use with Amazon S3, so you'll find the interface familiar\. 

**Topics**
+ [Terminology](#batch-ops-terminology)
+ [S3 Batch Operations basics](batch-ops-basics.md)
+ [Creating an S3 Batch Operations job](batch-ops-create-job.md)
+ [Operations](batch-ops-operations.md)
+ [Managing S3 Batch Operations jobs](batch-ops-managing-jobs.md)
+ [S3 Batch Operations examples](batch-ops-examples.md)

[![AWS Videos](http://img.youtube.com/vi/https://www.youtube.com/embed/hUv34voEftc//0.jpg)](http://www.youtube.com/watch?v=https://www.youtube.com/embed/hUv34voEftc/)

## Terminology<a name="batch-ops-terminology"></a>

This section uses the terms *jobs*, *operations*, and *tasks*, which are defined as follows:

**Job**  
A job is the basic unit of work for S3 Batch Operations\. A job contains all of the information necessary to run the specified operation on the objects listed in the manifest\. After you provide this information and request that the job begin, the job executes the operation for each object in the manifest\. 

**Operation**  
The operation is the type of API [action](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations.html), such as copying objects, that you want the Batch Operations job to run\. Each job performs a single type of operation across all objects that are specified in the manifest\.

**Task**  
A task is the unit of execution for a job\. A task represents a single call to an Amazon S3 or AWS Lambda API operation to perform the job's operation on a single object\. Over the course of a job's lifetime, S3 Batch Operations create one task for each object specified in the manifest\.


# Using versioning<a name="Versioning"></a>

Versioning is a means of keeping multiple variants of an object in the same bucket\. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket\. With versioning, you can easily recover from both unintended user actions and application failures\. When you enable versioning for a bucket, if Amazon S3 receives multiple write requests for the same object simultaneously, it stores all of the objects\.

If you enable versioning for a bucket, Amazon S3 automatically generates a unique version ID for the object being stored\. In one bucket, for example, you can have two objects with the same key, but different version IDs, such as `photo.gif` \(version 111111\) and `photo.gif `\(version 121212\)\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_Enabled.png)

Versioning\-enabled buckets enable you to recover objects from accidental deletion or overwrite\. For example:
+ If you delete an object, instead of removing it permanently, Amazon S3 inserts a delete marker, which becomes the current object version\. You can always restore the previous version\. For more information, see [Deleting object versions](DeletingObjectVersions.md)\.
+ If you overwrite an object, it results in a new object version in the bucket\. You can always restore the previous version\.

**Important**  
If you have an object expiration lifecycle policy in your non\-versioned bucket and you want to maintain the same permanent delete behavior when you enable versioning, you must add a noncurrent expiration policy\. The noncurrent expiration lifecycle policy will manage the deletes of the noncurrent object versions in the version\-enabled bucket\. \(A version\-enabled bucket maintains one current and zero or more noncurrent object versions\.\) For more information, see [ How Do I Create a Lifecycle Policy for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html) in the *Amazon Simple Storage Service Console User Guide*\. 

Buckets can be in one of three states: unversioned \(the default\), versioning\-enabled, or versioning\-suspended\.

**Important**  
Once you version\-enable a bucket, it can never return to an unversioned state\. You can, however, suspend versioning on that bucket\.

The versioning state applies to all \(never some\) of the objects in that bucket\. The first time you enable a bucket for versioning, objects in it are thereafter always versioned and given a unique version ID\. Note the following:
+ Objects stored in your bucket before you set the versioning state have a version ID of `null`\. When you enable versioning, existing objects in your bucket do not change\. What changes is how Amazon S3 handles the objects in future requests\. For more information, see [Managing objects in a versioning\-enabled bucket](manage-objects-versioned-bucket.md)\.
+ The bucket owner \(or any user with appropriate permissions\) can suspend versioning to stop accruing object versions\. When you suspend versioning, existing objects in your bucket do not change\. What changes is how Amazon S3 handles objects in future requests\. For more information, see [Managing objects in a versioning\-suspended bucket](VersionSuspendedBehavior.md)\.

## How to configure versioning on a bucket<a name="how-to-enable-disable-versioning-intro"></a>

You can configure bucket versioning using any of the following methods:
+ Configure versioning using the Amazon S3 console\.
+ Configure versioning programmatically using the AWS SDKs\.

  Both the console and the SDKs call the REST API that Amazon S3 provides to manage versioning\. 
**Note**  
If you need to, you can also make the Amazon S3 REST API calls directly from your code\. However, this can be cumbersome because it requires you to write code to authenticate your requests\. 

  Each bucket you create has a *versioning* subresource \(see [Bucket configuration options](UsingBucket.md#bucket-config-options-intro)\) associated with it\. By default, your bucket is unversioned, and accordingly the versioning subresource stores empty versioning configuration\.

  ```
  <VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
  </VersioningConfiguration>
  ```

  To enable versioning, you send a request to Amazon S3 with a versioning configuration that includes a status\. 

  ```
  <VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
    <Status>Enabled</Status> 
  </VersioningConfiguration>
  ```

  To suspend versioning, you set the status value to `Suspended`\. 

The bucket owner, an AWS account that created the bucket \(root account\), and authorized users can configure the versioning state of a bucket\. For more information about permissions, see [Identity and access management in Amazon S3](s3-access-control.md)\. 

For an example of configuring versioning, see [Examples of enabling bucket versioning](manage-versioning-examples.md)\.

## MFA delete<a name="MultiFactorAuthenticationDelete"></a>

You can optionally add another layer of security by configuring a bucket to enable MFA \(multi\-factor authentication\) Delete, which requires additional authentication for either of the following operations:
+ Change the versioning state of your bucket
+ Permanently delete an object version

 MFA Delete requires two forms of authentication together:
+ Your security credentials
+ The concatenation of a valid serial number, a space, and the six\-digit code displayed on an approved authentication device

MFA Delete thus provides added security in the event, for example, your security credentials are compromised\. 

To enable or disable MFA Delete, you use the same API that you use to configure versioning on a bucket\. Amazon S3 stores the MFA Delete configuration in the same *versioning* subresource that stores the bucket's versioning status\.

 MFA Delete can help prevent accidental bucket deletions by doing the following: 
+ Requiring the user who initiates the delete action to prove physical possession of an MFA device with an MFA code\.
+ Adding an extra layer of friction and security to the delete action\.

```
<VersioningConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
  <Status>VersioningState</Status>
  <MfaDelete>MfaDeleteState</MfaDelete>  
</VersioningConfiguration>
```

**Note**  
 The bucket owner, the AWS account that created the bucket \(root account\), and all authorized IAM users can enable versioning, but only the bucket owner \(root account\) can enable MFA Delete\. For more information, see the AWS blog post on [ MFA Delete and Versioning](http://aws.amazon.com/blogs/security/securing-access-to-aws-using-mfa-part-3/)\.

To use MFA Delete, you can use either a hardware or virtual MFA device to generate an authentication code\. The following example shows a generated authentication code displayed on a hardware device\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/MFADevice.png)

**Note**  
MFA Delete and MFA\-protected API access are features intended to provide protection for different scenarios\. You configure MFA Delete on a bucket to ensure that data in your bucket cannot be accidentally deleted\. MFA\-protected API access is used to enforce another authentication factor \(MFA code\) when accessing sensitive Amazon S3 resources\. You can require any operations against these Amazon S3 resources be done with temporary credentials created using MFA\. For an example, see [Adding a Bucket Policy to Require MFA](example-bucket-policies.md#example-bucket-policies-use-case-7)\.   
For more information on how to purchase and activate an authentication device, see [https://aws\.amazon\.com/iam/details/mfa/](https://aws.amazon.com/iam/details/mfa/)\.

## Related topics<a name="versioning-related-topics"></a>

For more information, see the following topics:
+ [Examples of enabling bucket versioning](manage-versioning-examples.md)
+ [Managing objects in a versioning\-enabled bucket](manage-objects-versioned-bucket.md)
+ [Managing objects in a versioning\-suspended bucket](VersionSuspendedBehavior.md)
+ [Significant Increases in HTTP 503 Responses to Amazon S3 Requests to Buckets with Versioning Enabled](troubleshooting-by-symptom.md#troubleshooting-by-symptom-increase-503-reponses) 


# Walkthrough: Controlling access to a bucket with user policies<a name="walkthrough1"></a>

This walkthrough explains how user permissions work with Amazon S3\. In this example, you create a bucket with folders\. You then create AWS Identity and Access Management \(IAM\) users in your AWS account and grant those users incremental permissions on your Amazon S3 bucket and the folders in it\. 

**Topics**
+ [The basics of buckets and folders](#walkthrough-background1)
+ [Walkthrough summary](#walkthrough-scenario)
+ [Preparing for the walkthrough](#walkthrough-what-you-need)
+ [Step 1: Create a bucket](#walkthrough1-create-bucket)
+ [Step 2: Create IAM users and a group](#walkthrough1-add-users)
+ [Step 3: Verify that IAM users have no permissions](#walkthrough1-verify-no-user-permissions)
+ [Step 4: Grant group\-level permissions](#walkthrough-group-policy)
+ [Step 5: Grant IAM user alice specific permissions](#walkthrough-grant-user1-permissions)
+ [Step 6: Grant IAM user bob specific permissions](#walkthrough1-grant-permissions-step5)
+ [Step 7: Secure the private folder](#walkthrough-secure-private-folder-explicit-deny)
+ [Step 8: Clean up](#walkthrough-cleanup)
+ [Related resources](#RelatedResources-walkthrough1)

## The basics of buckets and folders<a name="walkthrough-background1"></a>

The Amazon S3 data model is a flat structure: You create a bucket, and the bucket stores objects\. There is no hierarchy of subbuckets or subfolders, but you can emulate a folder hierarchy\. Tools like the Amazon S3 console can present a view of these logical folders and subfolders in your bucket, as shown in the following image\.



![\[Console screenshot showing a hierarchy of buckets, folders, and objects.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-10.png)

The console shows that a bucket named `companybucket` has three folders, `Private`, `Development`, and `Finance`, and an object, `s3-dg.pdf`\. The console uses the object names \(keys\) to create a logical hierarchy with folders and subfolders\. Consider the following examples:
+ When you create the `Development` folder, the console creates an object with the key `Development/`\. Note the trailing slash \(`/`\) delimiter\.
+ When you upload an object named `Projects1.xls` in the `Development` folder, the console uploads the object and gives it the key `Development/Projects1.xls`\. 

  In the key, `Development` is the [prefix](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix) and `/` is the delimiter\. The Amazon S3 API supports prefixes and delimiters in its operations\. For example, you can get a list of all objects from a bucket with a specific prefix and delimiter\. On the console, when you open the `Development` folder, the console lists the objects in that folder\. In the following example, the `Development` folder contains one object\. 

     
![\[Console screenshot showing a hierarchy of buckets, folders, and objects.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-50.png)

   

  When the console lists the `Development` folder in the `companybucket` bucket, it sends a request to Amazon S3 in which it specifies a prefix of `Development` and a delimiter of `/` in the request\. The console's response looks just like a folder list in your computer's file system\. The preceding example shows that the bucket `companybucket` has an object with the key `Development/Projects1.xls`\.

The console is using object keys to infer a logical hierarchy\. Amazon S3 has no physical hierarchy; it only has buckets that contain objects in a flat file structure\. When you create objects using the Amazon S3 API, you can use object keys that imply a logical hierarchy\. When you create a logical hierarchy of objects, you can manage access to individual folders, as this walkthrough demonstrates\.

Before you start, be sure that you are familiar with the concept of the *root\-level* bucket content\. Suppose that your `companybucket` bucket has the following objects:
+ `Private/privDoc1.txt`
+ `Private/privDoc2.zip`
+ `Development/project1.xls`
+ `Development/project2.xls`
+ `Finance/Tax2011/document1.pdf`
+ `Finance/Tax2011/document2.pdf`
+ `s3-dg.pdf`

These object keys create a logical hierarchy with `Private`, `Development`, and the `Finance` as root\-level folders and `s3-dg.pdf` as a root\-level object\. When you choose the bucket name on the Amazon S3 console, the root\-level items appear as shown in the following image\. The console shows the top\-level prefixes \(`Private/`, `Development/`, and `Finance/`\) as root\-level folders\. The object key `s3-dg.pdf` has no prefix, and so it appears as a root\-level item\.

![\[Console screenshot of the objects tab with the s3-dg.pdf object in the list.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-10.png)



## Walkthrough summary<a name="walkthrough-scenario"></a>

In this walkthrough, you create a bucket with three folders \(`Private`, `Development`, and `Finance`\) in it\. 

You have two users, Alice and Bob\. You want Alice to access only the `Development` folder, and you want Bob to access only the `Finance` folder\. You want to keep the `Private` folder content private\. In the walkthrough, you manage access by creating IAM users \(the example uses the user names Alice and Bob\) and granting them the necessary permissions\. 

IAM also supports creating user groups and granting group\-level permissions that apply to all users in the group\. This helps you better manage permissions\. For this exercise, both Alice and Bob need some common permissions\. So you also create a group named `Consultants` and then add both Alice and Bob to the group\. You first grant permissions by attaching a group policy to the group\. Then you add user\-specific permissions by attaching policies to specific users\.

**Note**  
The walkthrough uses `companybucket` as the bucket name, Alice and Bob as the IAM users, and `Consultants` as the group name\. Because Amazon S3 requires that bucket names be globally unique, you must replace the bucket name with a name that you create\.

## Preparing for the walkthrough<a name="walkthrough-what-you-need"></a>

 In this example, you use your AWS account credentials to create IAM users\. Initially, these users have no permissions\. You incrementally grant these users permissions to perform specific Amazon S3 actions\. To test these permissions, you sign in to the console with each user's credentials\. As you incrementally grant permissions as an AWS account owner and test permissions as an IAM user, you need to sign in and out, each time using different credentials\. You can do this testing with one browser, but the process will go faster if you can use two different browsers\. Use one browser to connect to the AWS Management Console with your AWS account credentials and another to connect with the IAM user credentials\. 

 To sign in to the AWS Management Console with your AWS account credentials, go to [https://console\.aws\.amazon\.com/](https://console.aws.amazon.com/)\.  An IAM user cannot sign in using the same link\. An IAM user must use an IAM\-enabled sign\-in page\. As the account owner, you can provide this link to your users\. 

For more information about IAM, see [The AWS Management Console Sign\-in Page](https://docs.aws.amazon.com/IAM/latest/UserGuide/console.html) in the *IAM User Guide*\.

### To provide a sign\-in link for IAM users<a name="walkthrough-sign-in-user-credentials"></a>

1. Sign in to the AWS Management Console and open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.

1. In the **Navigation** pane, choose **IAM Dashboard **\.

1. Note the URL under **IAM users sign in link:**\. You will give this link to IAM users to sign in to the console with their IAM user name and password\.

## Step 1: Create a bucket<a name="walkthrough1-create-bucket"></a>

In this step, you sign in to the Amazon S3 console with your AWS account credentials, create a bucket, add folders \(`Development`, `Finance`, and `Private`\) to the bucket, and upload one or two sample documents in each folder\. 

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Create a bucket\. 

   For step\-by\-step instructions, see [How Do I Create an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\.

1. Upload one document to the bucket\.

   This exercise assumes that you have the `s3-dg.pdf` document at the root level of this bucket\. If you upload a different document, substitute its file name for `s3-dg.pdf`\.

1. Add three folders named `Private`, `Finance`, and `Development` to the bucket\.

   For step\-by\-step instructions to create a folder, see [Creating a Folder](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-folder.html) in the *Amazon Simple Storage Service Console User Guide*\.

1. Upload one or two documents to each folder\. 

   For this exercise, assume that you have uploaded a couple of documents in each folder, resulting in the bucket having objects with the following keys:
   + `Private/privDoc1.txt`
   + `Private/privDoc2.zip`
   + `Development/project1.xls`
   + `Development/project2.xls`
   + `Finance/Tax2011/document1.pdf`
   + `Finance/Tax2011/document2.pdf`
   + `s3-dg.pdf`

   

   For step\-by\-step instructions, see [How Do I Upload Files and Folders to an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html) in the *Amazon Simple Storage Service Console User Guide*\. 

## Step 2: Create IAM users and a group<a name="walkthrough1-add-users"></a>

Now use the IAM console to add two IAM users, Alice and Bob, to your AWS account\. Also create an administrative group named `Consultants`, and then add both users to the group\. 

**Warning**  
When you add users and a group, do not attach any policies that grant permissions to these users\. At first, these users don't have any permissions\. In the following sections, you grant permissions incrementally\. First you must ensure that you have assigned passwords to these IAM users\. You use these user credentials to test Amazon S3 actions and verify that the permissions work as expected\.

For step\-by\-step instructions for creating a new IAM user, see [Creating an IAM User in Your AWS Account](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html) in the *IAM User Guide*\. When you create the users for this walkthrough, select **AWS Management Console access** and clear **Programmatic access**\.

For step\-by\-step instructions for creating an administrative group, see [Creating Your First IAM Admin User and Group](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html) in the *IAM User Guide*\.



## Step 3: Verify that IAM users have no permissions<a name="walkthrough1-verify-no-user-permissions"></a>

If you are using two browsers, you can now use the second browser to sign in to the console using one of the IAM user credentials\.

1. Using the IAM user sign\-in link \(see [To provide a sign\-in link for IAM users](#walkthrough-sign-in-user-credentials)\), sign in to the AWS Management Console using either of the IAM user credentials\.

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

    Verify the following console message telling you that access is denied\.   
![\[Console screenshot showing an access denied error message.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-20.png)

Now, you can begin granting incremental permissions to the users\. First, you attach a group policy that grants permissions that both users must have\. 

## Step 4: Grant group\-level permissions<a name="walkthrough-group-policy"></a>

You want the users to be able to do the following:
+ List all buckets owned by the parent account\. To do so, Bob and Alice must have permission for the `s3:ListAllMyBuckets` action\.
+ List root\-level items, folders, and objects in the `companybucket` bucket\. To do so, Bob and Alice must have permission for the `s3:ListBucket` action on the `companybucket` bucket\.

First, you create a policy that grants these permissions, and then you attach it to the `Consultants` group\. 

### Step 4\.1: Grant permission to list all buckets<a name="walkthrough1-grant-permissions-step1"></a>

In this step, you create a managed policy that grants the users minimum permissions to enable them to list all buckets owned by the parent account\. Then you attach the policy to the `Consultants` group\. When you attach the managed policy to a user or a group, you grant the user or group permission to obtain a list of buckets owned by the parent AWS account\.

1. Sign in to the AWS Management Console and open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.
**Note**  
Because you are granting user permissions, sign in using your AWS account credentials, not as an IAM user\.

1. Create the managed policy\.

   1. In the navigation pane on the left, choose **Policies**, and then choose **Create Policy**\.

   1. Choose the **JSON** tab\.

   1. Copy the following access policy and paste it into the policy text field\.

      ```
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Sid": "AllowGroupToSeeBucketListInTheConsole",
            "Action": ["s3:ListAllMyBuckets"],
            "Effect": "Allow",
            "Resource": ["arn:aws:s3:::*"]
          }
        ]
      }
      ```

      A policy is a JSON document\. In the document, a `Statement` is an array of objects, each describing a permission using a collection of name\-value pairs\. The preceding policy describes one specific permission\. The `Action` specifies the type of access\. In the policy, the `s3:ListAllMyBuckets` is a predefined Amazon S3 action\. This action covers the Amazon S3 GET Service operation, which returns list of all buckets owned by the authenticated sender\. The `Effect` element value determines whether specific permission is allowed or denied\.

   1. Choose **Review Policy**\. On the next page, enter `AllowGroupToSeeBucketListInTheConsole` in the **Name** field, and then choose **Create policy**\.
**Note**  
The **Summary** entry displays a message stating that the policy does not grant any permissions\. For this walkthrough, you can safely ignore this message\.

1. Attach the `AllowGroupToSeeBucketListInTheConsole` managed policy that you created to the `Consultants` group\.

   For step\-by\-step instructions for attaching a managed policy, see [Adding and Removing IAM Identity Permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html#attach-managed-policy-console) in the *IAM User Guide*\. 

   You attach policy documents to IAM users and groups in the IAM console\. Because you want both users to be able to list the buckets, you attach the policy to the group\. 

1. Test the permission\.

   1. Using the IAM user sign\-in link \(see [To provide a sign\-in link for IAM users](#walkthrough-sign-in-user-credentials)\), sign in to the console using any one of IAM user credentials\.

   1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

      The console should now list all the buckets but not the objects in any of the buckets\.  
![\[Console screenshot showing a list of buckets.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-30.png)

### Step 4\.2: Enable users to list root\-level content of a bucket<a name="walkthrough1-grant-permissions-step2"></a>

Next, you allow all users in the `Consultants` group to list the root\-level `companybucket` bucket items\. When a user chooses the company bucket on the Amazon S3 console, the user can see the root\-level items in the bucket\.

![\[Console screenshot showing the contents of companybucket.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-10.png)

**Note**  
This example uses `companybucket` for illustration\. You must use the name of the bucket that you created\.

To understand the request that the console sends to Amazon S3 when you choose a bucket name, the response that Amazon S3 returns, and how the console interprets the response, it is necessary to examine it a little more closely\.

When you choose a bucket name, the console sends the [GET Bucket \(List Objects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html) request to Amazon S3\. This request includes the following parameters:
+ The `prefix` parameter with an empty string as its value\. 
+ The `delimiter` parameter with `/` as its value\. 

The following is an example request\.

```
GET ?prefix=&delimiter=/ HTTP/1.1 
Host: companybucket.s3.amazonaws.com
Date: Wed, 01 Aug  2012 12:00:00 GMT
Authorization: AWS AKIAIOSFODNN7EXAMPLE:xQE0diMbLRepdf3YB+FIEXAMPLE=
```

Amazon S3 returns a response that includes the following `<ListBucketResult/>` element\.

```
<ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  <Name>companybucket</Name>
  <Prefix></Prefix>
  <Delimiter>/</Delimiter>
   ...
  <Contents>
    <Key>s3-dg.pdf</Key>
    ...
  </Contents>
  <CommonPrefixes>
    <Prefix>Development/</Prefix>
  </CommonPrefixes>
  <CommonPrefixes>
    <Prefix>Finance/</Prefix>
  </CommonPrefixes>
  <CommonPrefixes>
    <Prefix>Private/</Prefix>
  </CommonPrefixes>
</ListBucketResult>
```

The key `s3-dg.pdf` object does not contain the slash \(`/`\) delimiter, and Amazon S3 returns the key in the `<Contents>` element\. However, all other keys in the example bucket contain the `/` delimiter\. Amazon S3 groups these keys and returns a `<CommonPrefixes>` element for each of the distinct prefix values `Development/`, `Finance/`, and `Private/` that is a substring from the beginning of these keys to the first occurrence of the specified `/` delimiter\. 

The console interprets this result and displays the root\-level items as three folders and one object key\. 

![\[Console screenshot showing the contents of companybucket with three folders and one pdf file.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-10.png)

If Bob or Alice opens the **Development** folder, the console sends the [GET Bucket \(List Objects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html) request to Amazon S3 with the `prefix` and the `delimiter` parameters set to the following values:
+ The `prefix` parameter with the value `Development/`\.
+ The `delimiter` parameter with the "`/`" value\. 

In response, Amazon S3 returns the object keys that start with the specified prefix\. 

```
<ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  <Name>companybucket</Name>
  <Prefix>Development</Prefix>
  <Delimiter>/</Delimiter>
   ...
  <Contents>
    <Key>Project1.xls</Key>
    ...
  </Contents>
  <Contents>
    <Key>Project2.xls</Key>
    ...
  </Contents> 
</ListBucketResult>
```

The console shows the object keys\.

![\[Console screenshot showing the development folder containing two xls files.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-60.png)

Now, return to granting users permission to list the root\-level bucket items\. To list bucket content, users need permission to call the `s3:ListBucket` action, as shown in the following policy statement\. To ensure that they see only the root\-level content, you add a condition that users must specify an empty `prefix` in the request—that is, they are not allowed to double\-click any of the root\-level folders\. Finally, you add a condition to require folder\-style access by requiring user requests to include the `delimiter` parameter with the value "`/`"\. 

```
{
  "Sid": "AllowRootLevelListingOfCompanyBucket",
  "Action": ["s3:ListBucket"],
  "Effect": "Allow",
  "Resource": ["arn:aws:s3:::companybucket"],
  "Condition":{ 
         "StringEquals":{
             "s3:prefix":[""], "s3:delimiter":["/"]
                        }
              }
}
```

When you choose a bucket on the Amazon S3 console, the console first sends the [GET Bucket location](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html) request to find the AWS Region where the bucket is deployed\. Then the console uses the Region\-specific endpoint for the bucket to send the [GET Bucket \(List Objects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html) request\. As a result, if users are going to use the console, you must grant permission for the `s3:GetBucketLocation` action as shown in the following policy statement\.

```
{
   "Sid": "RequiredByS3Console",
   "Action": ["s3:GetBucketLocation"],
   "Effect": "Allow",
   "Resource": ["arn:aws:s3:::*"]
}
```

**To enable users to list root\-level bucket content**

1. Sign in to the AWS Management Console and open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.

   Use your AWS account credentials, not the credentials of an IAM user, to sign in to the console\.

1. Replace the existing `AllowGroupToSeeBucketListInTheConsole` managed policy that is attached to the `Consultants` group with the following policy, which also allows the `s3:ListBucket` action\. Remember to replace *companybucket* in the policy `Resource` with the name of your bucket\. 

   For step\-by\-step instructions, see [Editing IAM Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-edit.html) in the *IAM User Guide*\. When following the step\-by\-step instructions, be sure to follow the steps for applying your changes to all principal entities that the policy is attached to\. 

   ```
   {
     "Version": "2012-10-17",                 
     "Statement": [
        {
          "Sid": "AllowGroupToSeeBucketListAndAlsoAllowGetBucketLocationRequiredForListBucket",
          "Action": [ "s3:ListAllMyBuckets", "s3:GetBucketLocation" ],
          "Effect": "Allow",
          "Resource": [ "arn:aws:s3:::*"  ]
        },
        {
          "Sid": "AllowRootLevelListingOfCompanyBucket",
          "Action": ["s3:ListBucket"],
          "Effect": "Allow",
          "Resource": ["arn:aws:s3:::companybucket"],
          "Condition":{ 
                "StringEquals":{
                       "s3:prefix":[""], "s3:delimiter":["/"]
                              }
                      }
        }
     ] 
   }
   ```

1. Test the updated permissions\.

   1. Using the IAM user sign\-in link \(see [To provide a sign\-in link for IAM users](#walkthrough-sign-in-user-credentials)\), sign in to the AWS Management Console\. 

      Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

   1. Choose the bucket that you created, and the console shows the root\-level bucket items\. If you choose any folders in the bucket, you won't be able to see the folder content because you haven't yet granted those permissions\.  
![\[Console screenshot showing the company bucket containing three folders.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-10.png)

This test succeeds when users use the Amazon S3 console\. When you choose a bucket on the console, the console implementation sends a request that includes the `prefix` parameter with an empty string as its value and the `delimiter` parameter with "`/`" as its value\.

### Step 4\.3: Summary of the group policy<a name="walkthrough-group-policy-summary"></a>

The net effect of the group policy that you added is to grant the IAM users Alice and Bob the following minimum permissions:
+ List all buckets owned by the parent account\.
+ See root\-level items in the `companybucket` bucket\. 

However, the users still can't do much\. Next, you grant user\-specific permissions, as follows:
+ Allow Alice to get and put objects in the `Development` folder\.
+ Allow Bob to get and put objects in the `Finance` folder\.

For user\-specific permissions, you attach a policy to the specific user, not to the group\. In the following section, you grant Alice permission to work in the `Development` folder\. You can repeat the steps to grant similar permission to Bob to work in the `Finance` folder\.

## Step 5: Grant IAM user alice specific permissions<a name="walkthrough-grant-user1-permissions"></a>

Now you grant additional permissions to Alice so that she can see the content of the `Development` folder and get and put objects in that folder\.

### Step 5\.1: Grant IAM user alice permission to list the development folder content<a name="walkthrough-grant-user1-permissions-listbucket"></a>

For Alice to list the `Development` folder content, you must apply a policy to the Alice user that grants permission for the `s3:ListBucket` action on the `companybucket` bucket, provided the request includes the prefix `Development/`\. You want this policy to be applied only to the user Alice, so you use an inline policy\. For more information about inline policies, see [Managed Policies and Inline Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html) in the *IAM User Guide*\.

1. Sign in to the AWS Management Console and open the IAM console at [https://console\.aws\.amazon\.com/iam/](https://console.aws.amazon.com/iam/)\.

   Use your AWS account credentials, not the credentials of an IAM user, to sign in to the console\.

1. Create an inline policy to grant the user Alice permission to list the `Development` folder content\.

   1. In the navigation pane on the left, choose **Users**\.

   1. Choose the user name **Alice**\.

   1. On the user details page, choose the **Permissions** tab and then choose **Add inline policy**\.

   1. Choose the **JSON** tab\.

   1. Copy the following policy and paste it into the policy text field\.

      ```
      {
          "Version": "2012-10-17",  
          "Statement": [
          {
            "Sid": "AllowListBucketIfSpecificPrefixIsIncludedInRequest",
            "Action": ["s3:ListBucket"],
            "Effect": "Allow",
            "Resource": ["arn:aws:s3:::companybucket"],
            "Condition":{  "StringLike":{"s3:prefix":["Development/*"] }
             }
          }
        ]
      }
      ```

   1. Choose **Review Policy**\. On the next page, enter a name in the **Name** field, and then choose **Create policy**\.

1. Test the change to Alice's permissions:

   1. Using the IAM user sign\-in link \(see [To provide a sign\-in link for IAM users](#walkthrough-sign-in-user-credentials)\), sign in to the AWS Management Console\. 

   1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

   1. On the Amazon S3 console, verify that Alice can see the list of objects in the `Development/` folder in the bucket\. 

      When the user chooses the `/Development` folder to see the list of objects in it, the Amazon S3 console sends the `ListObjects` request to Amazon S3 with the prefix `/Development`\. Because the user is granted permission to see the object list with the prefix `Development` and delimiter `/`, Amazon S3 returns the list of objects with the key prefix `Development/`, and the console displays the list\.  
![\[Console screenshot showing the development folder containing two xls files.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/walkthrough-60.png)

### Step 5\.2: Grant IAM user alice permissions to get and put objects in the development folder<a name="walkthrough-grant-user1-permissions-get-put-object"></a>

For Alice to get and put objects in the `Development` folder, she needs permission to call the `s3:GetObject` and `s3:PutObject` actions\. The following policy statements grant these permissions, provided that the request includes the `prefix` parameter with a value of `Development/`\.

```
{
    "Sid":"AllowUserToReadWriteObjectData",
    "Action":["s3:GetObject", "s3:PutObject"],
    "Effect":"Allow",
    "Resource":["arn:aws:s3:::companybucket/Development/*"]
 }
```



1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

   Use your AWS account credentials, not the credentials of an IAM user, to sign in to the console\.

1. Edit the inline policy that you created in the previous step\. 

   1. In the navigation pane on the left, choose **Users**\.

   1. Choose the user name Alice\.

   1. On the user details page, choose the **Permissions** tab and expand the **Inline Policies** section\.

   1. Next to the name of the policy that you created in the previous step, choose **Edit Policy**\.

   1. Copy the following policy and paste it into the policy text field, replacing the existing policy\.

      ```
      {
           "Version": "2012-10-17",
           "Statement":[
            {
               "Sid":"AllowListBucketIfSpecificPrefixIsIncludedInRequest",
               "Action":["s3:ListBucket"],
               "Effect":"Allow",
               "Resource":["arn:aws:s3:::companybucket"],
               "Condition":{
                  "StringLike":{"s3:prefix":["Development/*"]
                  }
               }
            },
            {
              "Sid":"AllowUserToReadWriteObjectDataInDevelopmentFolder", 
              "Action":["s3:GetObject", "s3:PutObject"],
              "Effect":"Allow",
              "Resource":["arn:aws:s3:::companybucket/Development/*"]
            }
         ]
      }
      ```

1. Test the updated policy:

   1. Using the IAM user sign\-in link \(see [To provide a sign\-in link for IAM users](#walkthrough-sign-in-user-credentials)\), sign into the AWS Management Console\. 

   1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

   1. On the Amazon S3 console, verify that Alice can now add an object and download an object in the `Development` folder\. 

### Step 5\.3: Explicitly deny IAM user alice permissions to any other folders in the bucket<a name="walkthrough-grant-user1-explicit-deny-other-access"></a>

User Alice can now list the root\-level content in the `companybucket` bucket\. She can also get and put objects in the `Development` folder\. If you really want to tighten the access permissions, you could explicitly deny Alice access to any other folders in the bucket\. If there is any other policy \(bucket policy or ACL\) that grants Alice access to any other folders in the bucket, this explicit deny overrides those permissions\. 

You can add the following statement to the user Alice policy that requires all requests that Alice sends to Amazon S3 to include the `prefix` parameter, whose value can be either `Development/*` or an empty string\. 



```
{
   "Sid": "ExplicitlyDenyAnyRequestsForAllOtherFoldersExceptDevelopment",
   "Action": ["s3:ListBucket"],
   "Effect": "Deny",
   "Resource": ["arn:aws:s3:::companybucket"],
   "Condition":{  "StringNotLike": {"s3:prefix":["Development/*",""] },
                  "Null"         : {"s3:prefix":false }
    }
}
```

There are two conditional expressions in the `Condition` block\. The result of these conditional expressions is combined by using the logical `AND`\. If both conditions are true, the result of the combined condition is true\. Because the `Effect` in this policy is `Deny`, when the `Condition` evaluates to true, users can't perform the specified `Action`\.
+ The `Null` conditional expression ensures that requests from Alice include the `prefix` parameter\. 

  The `prefix` parameter requires folder\-like access\. If you send a request without the `prefix` parameter, Amazon S3 returns all the object keys\. 

  If the request includes the `prefix` parameter with a null value, the expression evaluates to true, and so the entire `Condition` evaluates to true\. You must allow an empty string as value of the `prefix` parameter\. From the preceding discussion, recall that allowing the null string allows Alice to retrieve root\-level bucket items as the console does in the preceding discussion\. For more information, see [Step 4\.2: Enable users to list root\-level content of a bucket](#walkthrough1-grant-permissions-step2)\. 
+ The `StringNotLike` conditional expression ensures that if the value of the `prefix` parameter is specified and is not `Development/*`, the request fails\. 

Follow the steps in the preceding section and again update the inline policy that you created for user Alice\.

Copy the following policy and paste it into the policy text field, replacing the existing policy\.

```
{
   "Version": "2012-10-17",
   "Statement":[
      {
         "Sid":"AllowListBucketIfSpecificPrefixIsIncludedInRequest",
         "Action":["s3:ListBucket"],
         "Effect":"Allow",
         "Resource":["arn:aws:s3:::companybucket"],
         "Condition":{
            "StringLike":{"s3:prefix":["Development/*"]
            }
         }
      },
      {
        "Sid":"AllowUserToReadWriteObjectDataInDevelopmentFolder", 
        "Action":["s3:GetObject", "s3:PutObject"],
        "Effect":"Allow",
        "Resource":["arn:aws:s3:::companybucket/Development/*"]
      },
      {
         "Sid": "ExplicitlyDenyAnyRequestsForAllOtherFoldersExceptDevelopment",
         "Action": ["s3:ListBucket"],
         "Effect": "Deny",
         "Resource": ["arn:aws:s3:::companybucket"],
         "Condition":{  "StringNotLike": {"s3:prefix":["Development/*",""] },
                        "Null"         : {"s3:prefix":false }
          }
      }
   ]
}
```

## Step 6: Grant IAM user bob specific permissions<a name="walkthrough1-grant-permissions-step5"></a>

Now you want to grant Bob permission to the `Finance` folder\. Follow the steps that you used earlier to grant permissions to Alice, but replace the `Development` folder with the `Finance` folder\. For step\-by\-step instructions, see [Step 5: Grant IAM user alice specific permissions](#walkthrough-grant-user1-permissions)\. 

## Step 7: Secure the private folder<a name="walkthrough-secure-private-folder-explicit-deny"></a>

In this example, you have only two users\. You granted all the minimum required permissions at the group level and granted user\-level permissions only when you really need to permissions at the individual user level\. This approach helps minimize the effort of managing permissions\. As the number of users increases, managing permissions can become cumbersome\. For example, you don't want any of the users in this example to access the content of the `Private` folder\. How do you ensure that you don't accidentally grant a user permission to it? You add a policy that explicitly denies access to the folder\. An explicit deny overrides any other permissions\. 

To ensure that the `Private` folder remains private, you can add the following two deny statements to the group policy:
+ Add the following statement to explicitly deny any action on resources in the `Private` folder \(`companybucket/Private/*`\)\.

  ```
  {
    "Sid": "ExplictDenyAccessToPrivateFolderToEveryoneInTheGroup",
    "Action": ["s3:*"],
    "Effect": "Deny",
    "Resource":["arn:aws:s3:::companybucket/Private/*"]
  }
  ```
+ You also deny permission for the list objects action when the request specifies the `Private/` prefix\. On the console, if Bob or Alice opens the `Private` folder, this policy causes Amazon S3 to return an error response\.

  ```
  {
    "Sid": "DenyListBucketOnPrivateFolder",
    "Action": ["s3:ListBucket"],
    "Effect": "Deny",
    "Resource": ["arn:aws:s3:::*"],
    "Condition":{
        "StringLike":{"s3:prefix":["Private/"]}
     }
  }
  ```

Replace the `Consultants` group policy with an updated policy that includes the preceding deny statements\. After the updated policy is applied, none of the users in the group can access the `Private` folder in your bucket\. 

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

   Use your AWS account credentials, not the credentials of an IAM user, to sign in to the console\.

1. Replace the existing `AllowGroupToSeeBucketListInTheConsole` managed policy that is attached to the `Consultants` group with the following policy\. Remember to replace `companybucket` in the policy with the name of your bucket\. 

   For instructions, see [Editing Customer Managed Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-edit.html#edit-managed-policy-console) in the *IAM User Guide*\. When following the instructions, make sure to follow the directions for applying your changes to all principal entities that the policy is attached to\. 

   ```
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Sid": "AllowGroupToSeeBucketListAndAlsoAllowGetBucketLocationRequiredForListBucket",
         "Action": ["s3:ListAllMyBuckets", "s3:GetBucketLocation"],
         "Effect": "Allow",
         "Resource": ["arn:aws:s3:::*"]
       },
       {
         "Sid": "AllowRootLevelListingOfCompanyBucket",
         "Action": ["s3:ListBucket"],
         "Effect": "Allow",
         "Resource": ["arn:aws:s3:::companybucket"],
         "Condition":{
             "StringEquals":{"s3:prefix":[""]}
          }
       },
       {
         "Sid": "RequireFolderStyleList",
         "Action": ["s3:ListBucket"],
         "Effect": "Deny",
         "Resource": ["arn:aws:s3:::*"],
         "Condition":{
             "StringNotEquals":{"s3:delimiter":"/"}
          }
        },
       {
         "Sid": "ExplictDenyAccessToPrivateFolderToEveryoneInTheGroup",
         "Action": ["s3:*"],
         "Effect": "Deny",
         "Resource":["arn:aws:s3:::companybucket/Private/*"]
       },
       {
         "Sid": "DenyListBucketOnPrivateFolder",
         "Action": ["s3:ListBucket"],
         "Effect": "Deny",
         "Resource": ["arn:aws:s3:::*"],
         "Condition":{
             "StringLike":{"s3:prefix":["Private/"]}
          }
       }
     ]
   }
   ```



## Step 8: Clean up<a name="walkthrough-cleanup"></a>

To clean up, open the IAM console and remove the users Alice and Bob\. For step\-by\-step instructions, see [Deleting an IAM User](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_manage.html#id_users_deleting) in the *IAM User Guide*\.

To ensure that you aren't charged further for storage, you should also delete the objects and the bucket that you created for this exercise\.

## Related resources<a name="RelatedResources-walkthrough1"></a>
+ [Managing IAM Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html) in the *IAM User Guide*\.


# Manage an object's lifecycle using the AWS SDK for Ruby<a name="manage-lifecycle-using-ruby"></a>

You can use the AWS SDK for Ruby to manage S3 Lifecycle configuration on a bucket by using the class [ AWS::S3::BucketLifecycleConfiguration](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/S3/BucketLifecycle.html)\. For more information about using the AWS SDK for Ruby with Amazon S3, see [Using the AWS SDK for Ruby \- Version 3](UsingTheMPRubyAPI.md)\. For more information about managing lifecycle configuration, see [Object lifecycle management](object-lifecycle-mgmt.md)\. 


# Copy an Object Using the REST API<a name="CopyingObjectUsingREST"></a>

This example describes how to copy an object using REST\. For more information about the REST API, go to [PUT Object \(Copy\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)\.

This example copies the `flotsam` object from the `pacific` bucket to the `jetsam` object of the `atlantic` bucket, preserving its metadata\.

```
1. PUT /jetsam HTTP/1.1
2. Host: atlantic.s3.amazonaws.com
3. x-amz-copy-source: /pacific/flotsam
4. Authorization: AWS AKIAIOSFODNN7EXAMPLE:ENoSbxYByFA0UGLZUqJN5EUnLDg=
5. Date: Wed, 20 Feb 2008 22:12:21 +0000
```

The signature was generated from the following information\.

```
1. PUT\r\n
2. \r\n
3. \r\n
4. Wed, 20 Feb 2008 22:12:21 +0000\r\n
5. 
6. x-amz-copy-source:/pacific/flotsam\r\n
7. /atlantic/jetsam
```

Amazon S3 returns the following response that specifies the ETag of the object and when it was last modified\.

```
 1. HTTP/1.1 200 OK
 2. x-amz-id-2: Vyaxt7qEbzv34BnSu5hctyyNSlHTYZFMWK4FtzO+iX8JQNyaLdTshL0KxatbaOZt
 3. x-amz-request-id: 6B13C3C5B34AF333
 4. Date: Wed, 20 Feb 2008 22:13:01 +0000
 5. 
 6. Content-Type: application/xml
 7. Transfer-Encoding: chunked
 8. Connection: close
 9. Server: AmazonS3
10. <?xml version="1.0" encoding="UTF-8"?>
11. 
12. <CopyObjectResult>
13.    <LastModified>2008-02-20T22:13:01</LastModified>
14.    <ETag>"7e9c608af58950deeb370c98608ed097"</ETag>
15. </CopyObjectResult>
```


# Managing tags using the AWS SDK for \.NET<a name="tagging-manage-dotnet"></a>

The following example shows how to use the AWS SDK for \.NET to set the tags for a new object and retrieve or replace the tags for an existing object\. For more information about object tagging, see [Object tagging](object-tagging.md)\. 

For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    public class ObjectTagsTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string keyName = "*** key name for the new object ***";
        private const string filePath = @"*** file path ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            PutObjectWithTagsTestAsync().Wait();
        }

        static async Task PutObjectWithTagsTestAsync()
        {
            try
            {
                // 1. Put an object with tags.
                var putRequest = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName,
                    FilePath = filePath,
                    TagSet = new List<Tag>{
                        new Tag { Key = "Keyx1", Value = "Value1"},
                        new Tag { Key = "Keyx2", Value = "Value2" }
                    }
                };

                PutObjectResponse response = await client.PutObjectAsync(putRequest);
                // 2. Retrieve the object's tags.
                GetObjectTaggingRequest getTagsRequest = new GetObjectTaggingRequest
                {
                    BucketName = bucketName,
                    Key = keyName
                };

                GetObjectTaggingResponse objectTags = await client.GetObjectTaggingAsync(getTagsRequest);
                for (int i = 0; i < objectTags.Tagging.Count; i++)
                    Console.WriteLine("Key: {0}, Value: {1}", objectTags.Tagging[i].Key, objectTags.Tagging[i].Value);


                // 3. Replace the tagset.

                Tagging newTagSet = new Tagging();
                newTagSet.TagSet = new List<Tag>{
                    new Tag { Key = "Key3", Value = "Value3"},
                    new Tag { Key = "Key4", Value = "Value4" }
                };


                PutObjectTaggingRequest putObjTagsRequest = new PutObjectTaggingRequest()
                {
                    BucketName = bucketName,
                    Key = keyName,
                    Tagging = newTagSet
                };
                PutObjectTaggingResponse response2 = await client.PutObjectTaggingAsync(putObjTagsRequest);

                // 4. Retrieve the object's tags.
                GetObjectTaggingRequest getTagsRequest2 = new GetObjectTaggingRequest();
                getTagsRequest2.BucketName = bucketName;
                getTagsRequest2.Key = keyName;
                GetObjectTaggingResponse objectTags2 = await client.GetObjectTaggingAsync(getTagsRequest2);
                for (int i = 0; i < objectTags2.Tagging.Count; i++)
                    Console.WriteLine("Key: {0}, Value: {1}", objectTags2.Tagging[i].Key, objectTags2.Tagging[i].Value);

            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine(
                        "Error encountered ***. Message:'{0}' when writing an object"
                        , e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine(
                    "Encountered an error. Message:'{0}' when writing an object"
                    , e.Message);
            }
        }
    }
}
```


# Request redirection and the REST API<a name="Redirects"></a>

Amazon S3 uses the Domain Name System \(DNS\) to route requests to facilities that can process them\. This system works effectively, but temporary routing errors can occur\. If a request arrives at the wrong Amazon S3 location, Amazon S3 responds with a temporary redirect that tells the requester to resend the request to a new endpoint\. If a request is incorrectly formed, Amazon S3 uses permanent redirects to provide direction on how to perform the request correctly\.

**Important**  
To use this feature, you must have an application that can handle Amazon S3 redirect responses\. The only exception is for applications that work exclusively with buckets that were created without `<CreateBucketConfiguration>`\. For more information about location constraints, see [Accessing a bucket](UsingBucket.md#access-bucket-intro)\.  
For all Regions that launched after March 20, 2019, if a request arrives at the wrong Amazon S3 location, Amazon S3 returns an HTTP 400 Bad Request error\.  
For more information about enabling or disabling an AWS Region, see [AWS Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html) in the *AWS General Reference*\.

**Topics**
+ [DNS routing](#DNSRouting)
+ [Temporary request redirection](#TemporaryRedirection)
+ [Permanent request redirection](#RedirectsPermanentRedirection)
+ [Request redirection examples](#redirect-examples)

## DNS routing<a name="DNSRouting"></a>

DNS routing routes requests to appropriate Amazon S3 facilities\. The following figure and procedure show an example of DNS routing\.

![\[Diagram showing steps that occur when a DNS server routes requests from the client to facility B.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/DNS_virthost.png)

**DNS routing request steps**

1. The client makes a DNS request to get an object stored on Amazon S3\.

1. The client receives one or more IP addresses for facilities that can process the request\. In this example, the IP address is for Facility B\.

1. The client makes a request to Amazon S3 Facility B\.

1. Facility B returns a copy of the object to the client\.

## Temporary request redirection<a name="TemporaryRedirection"></a>

A temporary redirect is a type of error response that signals to the requester that they should resend the request to a different endpoint\. Due to the distributed nature of Amazon S3, requests can be temporarily routed to the wrong facility\. This is most likely to occur immediately after buckets are created or deleted\.

For example, if you create a new bucket and immediately make a request to the bucket, you might receive a temporary redirect, depending on the location constraint of the bucket\. If you created the bucket in the US East \(N\. Virginia\) AWS Region, you will not see the redirect because this is also the default Amazon S3 endpoint\.

However, if the bucket is created in any other Region, any requests for the bucket go to the default endpoint while the bucket's DNS entry is propagated\. The default endpoint redirects the request to the correct endpoint with an HTTP 302 response\. Temporary redirects contain a URI to the correct facility, which you can use to immediately resend the request\.

**Important**  
Don't reuse an endpoint provided by a previous redirect response\. It might appear to work \(even for long periods of time\), but it might provide unpredictable results and will eventually fail without notice\.

The following figure and procedure shows an example of a temporary redirect\.

![\[Diagram showing steps that occur when a client sends a request to B and is redirected to C.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/DNS_virthost_redirect.png)

**Temporary request redirection steps**

1. The client makes a DNS request to get an object stored on Amazon S3\.

1. The client receives one or more IP addresses for facilities that can process the request\.

1. The client makes a request to Amazon S3 Facility B\.

1. Facility B returns a redirect indicating the object is available from Location C\.

1. The client resends the request to Facility C\.

1. Facility C returns a copy of the object\.

## Permanent request redirection<a name="RedirectsPermanentRedirection"></a>

A permanent redirect indicates that your request addressed a resource inappropriately\. For example, permanent redirects occur if you use a path\-style request to access a bucket that was created using `<CreateBucketConfiguration>`\. For more information, see [Accessing a bucket](UsingBucket.md#access-bucket-intro)\.

To help you find these errors during development, this type of redirect does not contain a Location HTTP header that allows you to automatically follow the request to the correct location\. Consult the resulting XML error document for help using the correct Amazon S3 endpoint\.

## Request redirection examples<a name="redirect-examples"></a>

The following are examples of temporary request redirection responses\.

### REST API temporary redirect response<a name="RedirectsTemporaryRedirection-response-rest-ex1"></a>

```
 1. HTTP/1.1 307 Temporary Redirect
 2. Location: http://awsexamplebucket1.s3-gztb4pa9sq.amazonaws.com/photos/puppy.jpg?rk=e2c69a31
 3. Content-Type: application/xml
 4. Transfer-Encoding: chunked
 5. Date: Fri, 12 Oct 2007 01:12:56 GMT
 6. Server: AmazonS3
 7. 
 8. <?xml version="1.0" encoding="UTF-8"?>
 9. <Error>
10.   <Code>TemporaryRedirect</Code>
11.   <Message>Please re-send this request to the specified temporary endpoint.
12.   Continue to use the original request endpoint for future requests.</Message>
13.   <Endpoint>awsexamplebucket1.s3-gztb4pa9sq.amazonaws.com</Endpoint>
14. </Error>
```

### SOAP API temporary redirect response<a name="RedirectsTemporaryRedirection-respose-soap-ex2"></a>

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

```
 1. <soapenv:Body>
 2.   <soapenv:Fault>
 3.     <Faultcode>soapenv:Client.TemporaryRedirect</Faultcode>
 4.     <Faultstring>Please re-send this request to the specified temporary endpoint.
 5.     Continue to use the original request endpoint for future requests.</Faultstring>
 6.     <Detail>
 7.       <Bucket>images</Bucket>
 8.       <Endpoint>s3-gztb4pa9sq.amazonaws.com</Endpoint>
 9.     </Detail>
10.   </soapenv:Fault>
11. </soapenv:Body>
```


# Track the progress of a multipart upload to an S3 Bucket using the AWS SDK for \.NET \(high\-level API\)<a name="HLTrackProgressMPUDotNet"></a>

The following C\# example uploads a file to an S3 bucket using the `TransferUtility` class, and tracks the progress of the upload\. For information about the example's compatibility with a specific version of the AWS SDK for \.NET and instructions for creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Transfer;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class TrackMPUUsingHighLevelAPITest
    {
        private const string bucketName = "*** provide the bucket name ***";
        private const string keyName = "*** provide the name for the uploaded object ***";
        private const string filePath = " *** provide the full path name of the file to upload **";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;


        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            TrackMPUAsync().Wait();
        }

        private static async Task TrackMPUAsync()
        {
            try
            {
                var fileTransferUtility = new TransferUtility(s3Client);

                // Use TransferUtilityUploadRequest to configure options.
                // In this example we subscribe to an event.
                var uploadRequest =
                    new TransferUtilityUploadRequest
                    {
                        BucketName = bucketName,
                        FilePath = filePath,
                        Key = keyName
                    };

                uploadRequest.UploadProgressEvent +=
                    new EventHandler<UploadProgressArgs>
                        (uploadRequest_UploadPartProgressEvent);

                await fileTransferUtility.UploadAsync(uploadRequest);
                Console.WriteLine("Upload completed");
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        static void uploadRequest_UploadPartProgressEvent(object sender, UploadProgressArgs e)
        {
            // Process event.
            Console.WriteLine("{0}/{1}", e.TransferredBytes, e.TotalBytes);
        }
    }
}
```

## More info<a name="HLTrackProgressMPUDotNet-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# Examples of creating a bucket<a name="create-bucket-get-location-example"></a>

**Topics**
+ [Using the Amazon S3 Console](#create-bucket-get-location-console)
+ [Using the AWS SDK for Java](#create-bucket-get-location-java)
+ [Using the AWS SDK for \.NET](#create-bucket-get-location-dotnet)
+ [Using the AWS SDK for Ruby Version 3](#create-bucket-get-location-ruby)
+ [Using Other AWS SDKs](#create-bucket-using-other-sdks)

The following code examples create a bucket programmatically using the AWS SDKs for Java, \.NET, and Ruby\. The code examples perform the following tasks:
+ Create a bucket, if the bucket doesn't already exist—The examples create a bucket by performing the following tasks:
  + Create a client by explicitly specifying an AWS Region \(the example uses the `s3.eu-west-1` Region\)\. Accordingly, the client communicates with Amazon S3 using the `s3.eu-west-1.amazonaws.com` endpoint\. You can specify any other AWS Region\. For a list of AWS Regions, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\. 
  + Send a create bucket request by specifying only a bucket name\. The create bucket request doesn't specify another AWS Region\. The client sends a request to Amazon S3 to create the bucket in the Region you specified when creating the client\. Once you have created a bucket, you can't change its Region\.
**Note**  
If you explicitly specify an AWS Region in your create bucket request that is different from the Region you specified when you created the client, you might get an error\. For more information, see [Creating a bucket](UsingBucket.md#create-bucket-intro)\.

    The SDK libraries send the PUT bucket request to Amazon S3 to create the bucket\. For more information, see [PUT Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html)\.
+ Retrieve information about the location of the bucket—Amazon S3 stores bucket location information in the *location* subresource that is associated with the bucket\. The SDK libraries send the GET Bucket location request \(see [GET Bucket location](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html)\) to retrieve this information\.

  

## Using the Amazon S3 Console<a name="create-bucket-get-location-console"></a>

To create a bucket using the Amazon S3 console, see [How do I create an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\.

## Using the AWS SDK for Java<a name="create-bucket-get-location-java"></a>

**Example**  
This example shows how to create an Amazon S3 bucket using the AWS SDK for Java\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CreateBucketRequest;
import com.amazonaws.services.s3.model.GetBucketLocationRequest;

import java.io.IOException;

public class CreateBucket2 {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            if (!s3Client.doesBucketExistV2(bucketName)) {
                // Because the CreateBucketRequest object doesn't specify a region, the
                // bucket is created in the region specified in the client.
                s3Client.createBucket(new CreateBucketRequest(bucketName));

                // Verify that the bucket was created by retrieving it and checking its location.
                String bucketLocation = s3Client.getBucketLocation(new GetBucketLocationRequest(bucketName));
                System.out.println("Bucket location: " + bucketLocation);
            }
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Using the AWS SDK for \.NET<a name="create-bucket-get-location-dotnet"></a>

For information about how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

**Example**  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using Amazon.S3.Util;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CreateBucketTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            CreateBucketAsync().Wait();
        }

        static async Task CreateBucketAsync()
        {
            try
            {
                if (!(await AmazonS3Util.DoesS3BucketExistAsync(s3Client, bucketName)))
                {
                    var putBucketRequest = new PutBucketRequest
                    {
                        BucketName = bucketName,
                        UseClientRegion = true
                    };

                    PutBucketResponse putBucketResponse = await s3Client.PutBucketAsync(putBucketRequest);
                }
                // Retrieve the bucket location.
                string bucketLocation = await FindBucketLocationAsync(s3Client);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
        static async Task<string> FindBucketLocationAsync(IAmazonS3 client)
        {
            string bucketLocation;
            var request = new GetBucketLocationRequest()
            {
                BucketName = bucketName
            };
            GetBucketLocationResponse response = await client.GetBucketLocationAsync(request);
            bucketLocation = response.Location.ToString();
            return bucketLocation;
        }
    }
}
```

## Using the AWS SDK for Ruby Version 3<a name="create-bucket-get-location-ruby"></a>

For information about how to create and test a working sample, see [Using the AWS SDK for Ruby \- Version 3](UsingTheMPRubyAPI.md)\. 

**Example**  

```
require 'aws-sdk-s3'

# Creates a bucket in Amazon S3.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param bucket_name [String] The bucket's name.
# @return [Boolean] true if the bucket was created; otherwise, false.
# @example
#   s3_client = Aws::S3::Client.new(region: 'us-east-1')
#   exit 1 unless bucket_created?(s3_client, 'doc-example-bucket')
def bucket_created?(s3_client, bucket_name)
  s3_client.create_bucket(bucket: bucket_name)
rescue StandardError => e
  puts "Error while creating the bucket named '#{bucket_name}': #{e.message}"
end
```

## Using Other AWS SDKs<a name="create-bucket-using-other-sdks"></a>

For information about using other AWS SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code/)\. 


# Managing object tags<a name="tagging-managing"></a>

This section explains how you can add object tags programmatically using the AWS SDK for Java or the Amazon S3 console\.

**Topics**
+ [Managing object tags using the console](tagging-manage-console.md)
+ [Managing tags using the AWS SDK for Java](tagging-manage-javasdk.md)
+ [Managing tags using the AWS SDK for \.NET](tagging-manage-dotnet.md)


# Making requests using AWS account or IAM user credentials \- AWS SDK for \.NET<a name="AuthUsingAcctOrUserCredDotNet"></a>

To send authenticated requests using your AWS account or IAM user credentials:
+ Create an instance of the `AmazonS3Client` class\. 
+ Run one of the `AmazonS3Client` methods to send requests to Amazon S3\. The client generates the necessary signature from the credentials that you provide and includes it in the request it sends to Amazon S3\. 

The following C\# example shows how to perform the preceding tasks\. For information about running the \.NET examples in this guide and for instructions on how to store your credentials in a configuration file, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

**Example**  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class MakeS3RequestTest
    {
        private const string bucketName = "*** bucket name ***"; 
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            using (client = new AmazonS3Client(bucketRegion))
            {
                Console.WriteLine("Listing objects stored in a bucket");
                ListingObjectsAsync().Wait();
            }
        }

        static async Task ListingObjectsAsync()
        {
            try
            {
                ListObjectsRequest request = new ListObjectsRequest
                {
                    BucketName = bucketName,
                    MaxKeys = 2
                };
                do
                {
                    ListObjectsResponse response = await client.ListObjectsAsync(request);
                    // Process the response.
                    foreach (S3Object entry in response.S3Objects)
                    {
                        Console.WriteLine("key = {0} size = {1}",
                            entry.Key, entry.Size);
                    }

                    // If the response is truncated, set the marker to get the next 
                    // set of keys.
                    if (response.IsTruncated)
                    {
                        request.Marker = response.NextMarker;
                    }
                    else
                    {
                        request = null;
                    }
                } while (request != null);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}
```

**Note**  
You can create the `AmazonS3Client` client without providing your security credentials\. Requests sent using this client are anonymous requests, without a signature\. Amazon S3 returns an error if you send anonymous requests for a resource that is not publicly available\.

For working examples, see [Working with Amazon S3 objects](UsingObjects.md) and [Working with Amazon S3 Buckets](UsingBucket.md)\. You can test these examples using your AWS Account or an IAM user credentials\. 

For example, to list all the object keys in your bucket, see [Listing Keys Using the AWS SDK for \.NET](ListingObjectKeysUsingNetSDK.md)\. 

## Related resources<a name="RelatedResources003"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Multipart upload API and permissions<a name="mpuAndPermissions"></a>

An individual must have the necessary permissions to use the multipart upload operations\. You can use access control lists \(ACLs\), the bucket policy, or the user policy to grant individuals permissions to perform these operations\. The following table lists the required permissions for various multipart upload operations when using ACLs, a bucket policy, or a user policy\. 


| Action | Required Permissions | 
| --- | --- | 
|  Create Multipart Upload  |  You must be allowed to perform the `s3:PutObject` action on an object to create multipart upload\.  The bucket owner can allow other principals to perform the `s3:PutObject` action\.   | 
|  Initiate Multipart Upload  |  You must be allowed to perform the `s3:PutObject` action on an object to initiate multipart upload\.  The bucket owner can allow other principals to perform the `s3:PutObject` action\.   | 
| Initiator | Container element that identifies who initiated the multipart upload\. If the initiator is an AWS account, this element provides the same information as the Owner element\. If the initiator is an IAM User, this element provides the user ARN and display name\. | 
| Upload Part | You must be allowed to perform the `s3:PutObject` action on an object to upload a part\.  The bucket owner must allow the initiator to perform the `s3:PutObject` action on an object in order for the initiator to upload a part for that object\. | 
| Upload Part \(Copy\) | You must be allowed to perform the `s3:PutObject` action on an object to upload a part\. Because you are uploading a part from an existing object, you must be allowed `s3:GetObject` on the source object\.  For the initiator to upload a part for an object, the owner of the bucket must allow the initiator to perform the `s3:PutObject` action on the object\. | 
| Complete Multipart Upload | You must be allowed to perform the `s3:PutObject` action on an object to complete a multipart upload\.  The bucket owner must allow the initiator to perform the `s3:PutObject` action on an object in order for the initiator to complete a multipart upload for that object\. | 
| Stop Multipart Upload | You must be allowed to perform the `s3:AbortMultipartUpload` action to stop a multipart upload\.  By default, the bucket owner and the initiator of the multipart upload are allowed to perform this action\. If the initiator is an IAM user, that user's AWS account is also allowed to stop that multipart upload\. In addition to these defaults, the bucket owner can allow other principals to perform the `s3:AbortMultipartUpload` action on an object\. The bucket owner can deny any principal the ability to perform the `s3:AbortMultipartUpload` action\. | 
| List Parts | You must be allowed to perform the `s3:ListMultipartUploadParts` action to list parts in a multipart upload\. By default, the bucket owner has permission to list parts for any multipart upload to the bucket\. The initiator of the multipart upload has the permission to list parts of the specific multipart upload\. If the multipart upload initiator is an IAM user, the AWS account controlling that IAM user also has permission to list parts of that upload\.  In addition to these defaults, the bucket owner can allow other principals to perform the `s3:ListMultipartUploadParts` action on an object\. The bucket owner can also deny any principal the ability to perform the `s3:ListMultipartUploadParts` action\. | 
| List Multipart Uploads | You must be allowed to perform the `s3:ListBucketMultipartUploads` action on a bucket to list multipart uploads in progress to that bucket\. In addition to the default, the bucket owner can allow other principals to perform the `s3:ListBucketMultipartUploads` action on the bucket\. | 
| AWS KMS Encrypt and Decrypt related permissions |  To perform a multipart upload with encryption using an AWS Key Management Service \(AWS KMS\) customer master key \(CMK\), the requester must have permission to the `kms:Decrypt` and `kms:GenerateDataKey*` actions on the key\. These permissions are required because Amazon S3 must decrypt and read data from the encrypted file parts before it completes the multipart upload\.  For more information, see [Uploading a large file to Amazon S3 with encryption using an AWS KMS CMK](http://aws.amazon.com/premiumsupport/knowledge-center/s3-large-file-encryption-kms-key/) in the *AWS Knowledge Center*\. If your IAM user or role is in the same AWS account as the AWS KMS CMK, then you must have these permissions on the key policy\. If your IAM user or role belongs to a different account than the CMK, then you must have the permissions on both the key policy and your IAM user or role\.  | 

For information on the relationship between ACL permissions and permissions in access policies, see [Mapping of ACL Permissions and Access Policy Permissions](acl-overview.md#acl-access-policy-permission-mapping)\. For information on IAM users, go to [Working with Users and Groups](https://docs.aws.amazon.com/IAM/latest/UserGuide/)\.


# Working with Amazon S3 objects<a name="UsingObjects"></a>

Amazon S3 is an object store that uses unique key\-values to store as many objects as you want\. You store these objects in one or more buckets, and each object can be up to 5 TB in size\. An object consists of the following:
+ ****Key**** – The name that you assign to an object\. You use the object key to retrieve the object\.

  For more information, see [Object key and metadata](UsingMetadata.md)\.
+ ****Version ID** **– Within a bucket, a key and version ID uniquely identify an object\. 

  The version ID is a string that Amazon S3 generates when you add an object to a bucket\. For more information, see [Object Versioning](ObjectVersioning.md)\.
+ ****Value**** – The content that you are storing\.

  An object value can be any sequence of bytes\. Objects can range in size from zero to 5 TB\. For more information, see [Uploading objects](UploadingObjects.md)\. 
+ ****Metadata**** – A set of name\-value pairs with which you can store information regarding the object\.

  You can assign metadata, referred to as user\-defined metadata, to your objects in Amazon S3\. Amazon S3 also assigns system\-metadata to these objects, which it uses for managing objects\. For more information, see [Object key and metadata](UsingMetadata.md)\.
+ ****Subresources** **– Amazon S3 uses the subresource mechanism to store object\-specific additional information\. 

  Because subresources are subordinates to objects, they are always associated with some other entity such as an object or a bucket\. For more information, see [Object subresources](ObjectAndSubResource.md)\.
+ ****Access control information** **– You can control access to the objects you store in Amazon S3\.

  Amazon S3 supports both the resource\-based access control, such as an access control list \(ACL\) and bucket policies, and user\-based access control\. For more information, see [Identity and access management in Amazon S3](s3-access-control.md)\.

For more information about working with objects, see the following sections\. Your Amazon S3 resources \(for example, buckets and objects\) are private by default\. You must explicitly grant permission for others to access these resources\. For example, you might want to share a video or a photo stored in your Amazon S3 bucket on your website\. That works only if you either make the object public or use a presigned URL on your website\. For more information about sharing objects, see [Share an object with others](ShareObjectPreSignedURL.md)\.

**Topics**
+ [Object key and metadata](UsingMetadata.md)
+ [Amazon S3 storage classes](storage-class-intro.md)
+ [Object subresources](ObjectAndSubResource.md)
+ [Object Versioning](ObjectVersioning.md)
+ [Object tagging](object-tagging.md)
+ [Object lifecycle management](object-lifecycle-mgmt.md)
+ [Cross\-origin resource sharing \(CORS\)](cors.md)
+ [Operations on objects](ObjectOperations.md)


# Amazon S3 error best practices<a name="ErrorBestPractices"></a>

When designing an application for use with Amazon S3, it is important to handle Amazon S3 errors appropriately\. This section describes issues to consider when designing your application\.

## Retry InternalErrors<a name="UsingErrorsRetry"></a>

Internal errors are errors that occur within the Amazon S3 environment\. 

Requests that receive an InternalError response might not have processed\. For example, if a PUT request returns InternalError, a subsequent GET might retrieve the old value or the updated value\. 

If Amazon S3 returns an InternalError response, retry the request\.

## Tune application for repeated SlowDown errors<a name="UsingErrorsSlowDown"></a>

As with any distributed system, S3 has protection mechanisms which detect intentional or unintentional resource over\-consumption and react accordingly\. SlowDown errors can occur when a high request rate triggers one of these mechanisms\. Reducing your request rate will decrease or eliminate errors of this type\. Generally speaking, most users will not experience these errors regularly; however, if you would like more information or are experiencing high or unexpected SlowDown errors, please post to our Amazon S3 developer forum  [https://forums\.aws\.amazon\.com/](https://forums.aws.amazon.com/) or sign up for AWS Premium Support [https://aws\.amazon\.com/premiumsupport/](https://aws.amazon.com/premiumsupport/)\.

## Isolate errors<a name="UsingErrorsIsolate"></a>

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

 Amazon S3 provides a set of error codes that are used by both the SOAP and REST API\. The SOAP API returns standard Amazon S3 error codes\. The REST API is designed to look like a standard HTTP server and interact with existing HTTP clients \(e\.g\., browsers, HTTP client libraries, proxies, caches, and so on\)\. To ensure the HTTP clients handle errors properly, we map each Amazon S3 error to an HTTP status code\. 

 HTTP status codes are less expressive than Amazon S3 error codes and contain less information about the error\. For example, the `NoSuchKey` and `NoSuchBucket` Amazon S3 errors both map to the `HTTP 404 Not Found` status code\. 

 Although the HTTP status codes contain less information about the error, clients that understand HTTP, but not the Amazon S3 API, will usually handle the error correctly\. 

 Therefore, when handling errors or reporting Amazon S3 errors to end users, use the Amazon S3 error code instead of the HTTP status code as it contains the most information about the error\. Additionally, when debugging your application, you should also consult the human readable <Details> element of the XML error response\. 


# Example 4: Replicating encrypted objects<a name="replication-walkthrough-4"></a>

By default, Amazon S3 doesn't replicate objects that are stored at rest using server\-side encryption with AWS Key Management Service \(AWS KMS\) customer master keys \(CMKs\)\. To replicate encrypted objects, you modify the bucket replication configuration to tell Amazon S3 to replicate these objects\. This example explains how to use the Amazon S3 console and the AWS Command Line Interface \(AWS CLI\) to change the bucket replication configuration to enable replicating encrypted objects\. For more information, see [Replicating objects created with server\-side encryption \(SSE\) using encryption keys stored in AWS KMS](replication-config-for-kms-objects.md)\. 

**Topics**

## Replicate encrypted objects \(console\)<a name="replication-ex4-console"></a>

For step\-by\-step instructions, see [How Do I Add a Replication Rule to an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html) in the *Amazon Simple Storage Service Console User Guide*\. This topic provides instructions for setting replication configuration when buckets are owned by same and different AWS accounts\.

## Replicate encrypted objects \(AWS CLI\)<a name="replication-ex4-cli"></a>

To replicate encrypted objects with the AWS CLI, you create buckets, enable versioning on the buckets, create an IAM role that gives Amazon S3 permission to replicate objects, and add the replication configuration to the source bucket\. The replication configuration provides information related to replicating objects encrypted using KMS keys\. The IAM role permissions include necessary permissions to replicate the encrypted objects\. You also test the setup\.

**To replicate server\-side encrypted objects \(AWS CLI\)**

1. In this example, we create both the *source* and *destination* buckets in the same AWS account\. Set a credentials profile for the AWS CLI\. In this example, we use the profile name `acctA`\. For more information about setting credential profiles, see [Named Profiles](https://docs.aws.amazon.com/cli/latest/userguide/cli-multiple-profiles.html) in the AWS Command Line Interface User Guide\. 

1. Create the *source* bucket and enable versioning on it\. In this example, we create the *source* bucket in the US East \(N\. Virginia\) \(us\-east\-1\) Region\.

   ```
   aws s3api create-bucket \
   --bucket source \
   --region us-east-1 \
   --profile acctA
   ```

   ```
   aws s3api put-bucket-versioning \
   --bucket source \
   --versioning-configuration Status=Enabled \
   --profile acctA
   ```

1. Create the *destination* bucket and enable versioning on it\. In this example, we create the *destination* bucket in the US West \(Oregon\) \(us\-west\-2\) Region\. 
**Note**  
To set up replication configuration when both *source* and *destination* buckets are in the same AWS account, you use the same profile\. In this example, we use `acctA`\. To test replication configuration when the buckets are owned by different AWS accounts, you specify different profiles for each\. 

   

   ```
   aws s3api create-bucket \
   --bucket destination \
   --region us-west-2 \
   --create-bucket-configuration LocationConstraint=us-west-2 \
   --profile acctA
   ```

   ```
   aws s3api put-bucket-versioning \
   --bucket destination \
   --versioning-configuration Status=Enabled \
   --profile acctA
   ```

1. Create an IAM role\. You specify this role in the replication configuration that you add to the *source* bucket later\. Amazon S3 assumes this role to replicate objects on your behalf\. You create an IAM role in two steps:
   + Create a role
   + Attach a permissions policy to the role

   1. Create an IAM role\.

      1. Copy the following trust policy and save it to a file called `s3-role-trust-policy-kmsobj.json` in the current directory on your local computer\. This policy grants Amazon S3 service principal permissions to assume the role so Amazon S3 can perform tasks on your behalf\.

         ```
         {
            "Version":"2012-10-17",
            "Statement":[
               {
                  "Effect":"Allow",
                  "Principal":{
                     "Service":"s3.amazonaws.com"
                  },
                  "Action":"sts:AssumeRole"
               }
            ]
         }
         ```

      1. Create a role\.

         ```
         $ aws iam create-role \
         --role-name replicationRolekmsobj \
         --assume-role-policy-document file://s3-role-trust-policy-kmsobj.json  \
         --profile acctA
         ```

   1. Attach a permissions policy to the role\. This policy grants permissions for various Amazon S3 bucket and object actions\. 

      1. Copy the following permissions policy and save it to a file named `s3-role-permissions-policykmsobj.json` in the current directory on your local computer\. You create an IAM role and attach the policy to it later\. 
**Important**  
 In the permissions policy, you specify the AWS KMS key IDs that will be used for encryption of *source* and `destination` buckets\. You must create two separate AWS KMS CMKs for the `source` and `destination` buckets\. AWS KMS CMKs are never shared outside the AWS Region in which they were created\. 

         ```
         {
            "Version":"2012-10-17",
            "Statement":[
               {
                  "Action":[
                     "s3:ListBucket",
                     "s3:GetReplicationConfiguration",
                     "s3:GetObjectVersionForReplication",
                     "s3:GetObjectVersionAcl"
                  ],
                  "Effect":"Allow",
                  "Resource":[
                     "arn:aws:s3:::source",
                     "arn:aws:s3:::source/*"
                  ]
               },
               {
                  "Action":[
                     "s3:ReplicateObject",
                     "s3:ReplicateDelete",
                     "s3:ReplicateTags",
                     "s3:GetObjectVersionTagging"
                  ],
                  "Effect":"Allow",
                  "Condition":{
                     "StringLikeIfExists":{
                        "s3:x-amz-server-side-encryption":[
                           "aws:kms",
                           "AES256"
                        ],
                        "s3:x-amz-server-side-encryption-aws-kms-key-id":[
                           "AWS KMS key IDs(in ARN format) to use for encrypting object replicas"  
                        ]
                     }
                  },
                  "Resource":"arn:aws:s3:::destination/*"
               },
               {
                  "Action":[
                     "kms:Decrypt"
                  ],
                  "Effect":"Allow",
                  "Condition":{
                     "StringLike":{
                        "kms:ViaService":"s3.us-east-1.amazonaws.com",
                        "kms:EncryptionContext:aws:s3:arn":[
                           "arn:aws:s3:::source/*"
                        ]
                     }
                  },
                  "Resource":[
                     "AWS KMS key IDs(in ARN format) used to encrypt source objects." 
                  ]
               },
               {
                  "Action":[
                     "kms:Encrypt"
                  ],
                  "Effect":"Allow",
                  "Condition":{
                     "StringLike":{
                        "kms:ViaService":"s3.us-west-2.amazonaws.com",
                        "kms:EncryptionContext:aws:s3:arn":[
                           "arn:aws:s3:::destination/*"
                        ]
                     }
                  },
                  "Resource":[
                     "AWS KMS key IDs(in ARN format) to use for encrypting object replicas" 
                  ]
               }
            ]
         }
         ```

      1. Create a policy and attach it to the role\.

         ```
         $ aws iam put-role-policy \
         --role-name replicationRolekmsobj \
         --policy-document file://s3-role-permissions-policykmsobj.json \
         --policy-name replicationRolechangeownerPolicy \
         --profile acctA
         ```

1. Add the following replication configuration to the *source* bucket\. It tells Amazon S3 to replicate objects with the `Tax/` prefix to the *destination* bucket\. 
**Important**  
In the replication configuration you specify the IAM role that Amazon S3 can assume\. You can do this only if you have the `iam:PassRole` permission\. The profile you specify in the CLI command must have the permission\. For more information, see [Granting a User Permissions to Pass a Role to an AWS Service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html) in the *IAM User Guide*\.

   ```
    <ReplicationConfiguration>
     <Role>IAM-Role-ARN</Role>
     <Rule>
       <Status>Enabled</Status>
       <Priority>1</Priority>
       <DeleteMarkerReplication>
          <Status>Disabled</Status>
       </DeleteMarkerReplication>
       <Filter>
          <Prefix>Tax</Prefix>
       </Filter>
       <Status>Enabled</Status>
       <SourceSelectionCriteria>
         <SseKmsEncryptedObjects>
           <Status>Enabled</Status>
         </SseKmsEncryptedObjects>
       </SourceSelectionCriteria>
       <Destination>
         <Bucket>arn:aws:s3:::dest-bucket-name</Bucket>
         <EncryptionConfiguration>
           <ReplicaKmsKeyID>AWS KMS key IDs to use for encrypting object replicas</ReplicaKmsKeyID>
         </EncryptionConfiguration>
       </Destination>
     </Rule>
   </ReplicationConfiguration>
   ```

   To add replication configuration to the *source* bucket, do the following:

   1. The AWS CLI requires you to specify the replication configuration as JSON\. Save the following JSON in a file \(`replication.json`\) in the current directory on your local computer\. 

      ```
      {
         "Role":"IAM-Role-ARN",
         "Rules":[
            {
               "Status":"Enabled",
               "Priority":1,
               "DeleteMarkerReplication":{
                  "Status":"Disabled"
               },
               "Filter":{
                  "Prefix":"Tax"
               },
               "Destination":{
                  "Bucket":"arn:aws:s3:::destination",
                  "EncryptionConfiguration":{
                     "ReplicaKmsKeyID":"AWS KMS key IDs(in ARN format)  to use for encrypting object replicas"
                  }
               },
               "SourceSelectionCriteria":{
                  "SseKmsEncryptedObjects":{
                     "Status":"Enabled"
                  }
               }
            }
         ]
      }
      ```

   1. Edit the JSON to provide values for the *destination* bucket, *KMS ID ARN* and *IAM\-role\-ARN*\. Save the changes\.

   1. Add the replication configuration to your *source* bucket\. Be sure to provide the *source* bucket name\.

      ```
      $ aws s3api put-bucket-replication \
      --replication-configuration file://replication.json \
      --bucket source \
      --profile acctA
      ```

1. Test the setup to verify that encrypted objects are replicated\. In the Amazon S3 console:

   1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

   1. In the *source* bucket, create a folder named `Tax`\. 

   1. Add sample objects to the folder\. Be sure to choose the encryption option and specify your AWS KMS CMK to encrypt the objects\. 

   1. Verify that the *destination* bucket contains the object replicas and that they are encrypted using the AWS KMS CMK that you specified in the configuration\.

## Replicate encrypted objects \(AWS SDK\)<a name="replication-ex4-sdk"></a>

 For a code example to add replication configuration, see [Configure replication when buckets are owned by the same account \(AWS SDK\)](replication-walkthrough1.md#replication-ex1-sdk)\. You need to modify the replication configuration appropriately\. 

For conceptual information, see [Replicating objects created with server\-side encryption \(SSE\) using encryption keys stored in AWS KMS](replication-config-for-kms-objects.md)\. 


# Lifecycle and other bucket configurations<a name="lifecycle-and-other-bucket-config"></a>

In addition to S3 Lifecycle configurations, you can associate other configurations with your bucket\. This section explains how S3 Lifecycle configuration relates to other bucket configurations\.

## Lifecycle and versioning<a name="lifecycle-versioning-support-intro"></a>

You can add S3 Lifecycle configurations to unversioned buckets and versioning\-enabled buckets\. For more information, see [Object Versioning](ObjectVersioning.md)\. 

A versioning\-enabled bucket maintains one current object version, and zero or more noncurrent object versions\. You can define separate Lifecycle rules for current and noncurrent object versions\.

For more information, see [Lifecycle configuration elements](intro-lifecycle-rules.md)\. For information about S3 Versioning, see [Object Versioning](ObjectVersioning.md)\.

## Lifecycle configuration on MFA\-enabled buckets<a name="lifecycle-general-considerations-mfa-enabled-bucket"></a>

Lifecycle configuration on multi\-factor authentication \(MFA\)\-enabled buckets is not supported\.

## Lifecycle and logging<a name="lifecycle-general-considerations-logging"></a>

Amazon S3 Lifecycle actions are not captured by AWS CloudTrail object level logging\. CloudTrail captures API requests made to external Amazon S3 endpoints, whereas S3 Lifecycle actions are performed using internal Amazon S3 endpoints\. Amazon S3 server access logs can be enabled in an S3 bucket to capture S3 Lifecycle\-related actions such as object transition to another storage class and object expiration resulting in permanent deletion or logical deletion\. For more information, see [Amazon S3 server access logging](ServerLogs.md)\.

If you have logging enabled on your bucket, Amazon S3 server access logs report the results of the following operations\.


| Operation log | Description | 
| --- | --- | 
|  `S3.EXPIRE.OBJECT`  |  Amazon S3 permanently deletes the object due to the Lifecycle expiration action\.  | 
|  `S3.CREATE.DELETEMARKER`  |  Amazon S3 logically deletes the current version and adds a delete marker in a Versioning enabled bucket\.  | 
|  `S3.TRANSITION_SIA.OBJECT`  |  Amazon S3 transitions the object to the S3 Standard\-IA storage class\.  | 
|  `S3.TRANSITION_ZIA.OBJECT`  |  Amazon S3 transitions the object to the S3 One Zone\-IA storage class\.  | 
|  `S3.TRANSITION_INT.OBJECT`  |  Amazon S3 transitions the object to the Intelligent\-Tiering storage class\.  | 
|  `S3.TRANSITION.OBJECT`  |  Amazon S3 initiates the transition of object to the S3 Glacier storage class\.  | 
|  `S3.TRANSITION_GDA.OBJECT`  |  Amazon S3 initiates the transition of object to the S3 Glacier Deep Archive storage class\.  | 
|  `S3.DELETE.UPLOAD`  |  Amazon S3 aborts incomplete multipart upload\.  | 

**Note**  
Amazon S3 server access log records are generally delivered on a best effort basis and cannot be used for complete accounting of all Amazon S3 requests\. 

### More info<a name="lifecycle-general-considerations-logging-more-info"></a>
+ [Lifecycle configuration elements](intro-lifecycle-rules.md) 
+ [Transitioning to the S3 Glacier and S3 Glacier Deep Archive storage classes \(object archival\)](lifecycle-transition-general-considerations.md#before-deciding-to-archive-objects)
+ [Setting lifecycle configuration on a bucket](how-to-set-lifecycle-configuration-intro.md) 


# Upload an object using a presigned URL \(AWS SDK for Java\)<a name="PresignedUrlUploadObjectJavaSDK"></a>

You can use the AWS SDK for Java to generate a presigned URL that you, or anyone you give the URL, can use to upload an object to Amazon S3\. When you use the URL to upload an object, Amazon S3 creates the object in the specified bucket\. If an object with the same key that is specified in the presigned URL already exists in the bucket, Amazon S3 replaces the existing object with the uploaded object\. To successfully complete an upload, you must do the following:
+ Specify the HTTP PUT verb when creating the `GeneratePresignedUrlRequest` and `HttpURLConnection` objects\.
+ Interact with the `HttpURLConnection` object in some way after finishing the upload\. The following example accomplishes this by using the `HttpURLConnection` object to check the HTTP response code\.

For more information about presigned URLs, see [Uploading objects using presigned URLs](PresignedUrlUploadObject.md)\.

**Example**  
This example generates a presigned URL and uses it to upload sample data as an object\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.HttpMethod;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.GeneratePresignedUrlRequest;
import com.amazonaws.services.s3.model.S3Object;

import java.io.IOException;
import java.io.OutputStreamWriter;
import java.net.HttpURLConnection;
import java.net.URL;

public class GeneratePresignedUrlAndUploadObject {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String objectKey = "*** Object key ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Set the pre-signed URL to expire after one hour.
            java.util.Date expiration = new java.util.Date();
            long expTimeMillis = expiration.getTime();
            expTimeMillis += 1000 * 60 * 60;
            expiration.setTime(expTimeMillis);

            // Generate the pre-signed URL.
            System.out.println("Generating pre-signed URL.");
            GeneratePresignedUrlRequest generatePresignedUrlRequest = new GeneratePresignedUrlRequest(bucketName, objectKey)
                    .withMethod(HttpMethod.PUT)
                    .withExpiration(expiration);
            URL url = s3Client.generatePresignedUrl(generatePresignedUrlRequest);

            // Create the connection and use it to upload the new object using the pre-signed URL.
            HttpURLConnection connection = (HttpURLConnection) url.openConnection();
            connection.setDoOutput(true);
            connection.setRequestMethod("PUT");
            OutputStreamWriter out = new OutputStreamWriter(connection.getOutputStream());
            out.write("This text uploaded as an object via presigned URL.");
            out.close();

            // Check the HTTP response code. To complete the upload and make the object available, 
            // you must interact with the connection object in some way.
            connection.getResponseCode();
            System.out.println("HTTP response code: " + connection.getResponseCode());

            // Check to make sure that the object was uploaded successfully.
            S3Object object = s3Client.getObject(bucketName, objectKey);
            System.out.println("Object " + object.getKey() + " created in bucket " + object.getBucketName());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Using Bucket Policies and User Policies<a name="using-iam-policies"></a>

Bucket policies and user policies are two access policy options available for granting permission to your Amazon S3 resources\. Both use JSON\-based access policy language\. 

The topics in this section describe the key policy language elements, with emphasis on Amazon S3–specific details, and provide example bucket and user policies\.

**Important**  
Bucket policies are limited to 20 KB in size\.  
We recommend that you first review the introductory topics that explain the basic concepts and options available for you to manage access to your Amazon S3 resources\. For more information, see [Introduction to managing access to Amazon S3 resources](s3-access-control.md#intro-managing-access-s3-resources)\. 

**Topics**
+ [Policies and Permissions in Amazon S3](access-policy-language-overview.md)
+ [Bucket Policy Examples](example-bucket-policies.md)
+ [User Policy Examples](example-policies-s3.md)


# Replication status information<a name="replication-status"></a>

Replication status can help you determine the current state of an object being replicated\. The replication status of a source object will return either `PENDING`, `COMPLETED`, or `FAILED`\. The replication status of a replica will return `REPLICA`\.

**Topics**
+ [Replication status overview](#replication-status-overview)
+ [Replication status if replicating to multiple destination buckets](#replication-status-multiple-destinations)
+ [Replication status if Amazon S3 replica modification sync is enabled](#replication-status-multiple-destinations)
+ [Finding replication status](#replication-status-usage)

## Replication status overview<a name="replication-status-overview"></a>

In replication, you have a source bucket on which you configure replication and destination where Amazon S3 replicates objects\. When you request an object \(using `GET` object\) or object metadata \(using `HEAD` object\) from these buckets, Amazon S3 returns the `x-amz-replication-status` header in the response: 
+ When you request an object from the source bucket, Amazon S3 returns the `x-amz-replication-status` header if the object in your request is eligible for replication\. 

  For example, suppose that you specify the object prefix `TaxDocs` in your replication configuration to tell Amazon S3 to replicate only objects with the key name prefix `TaxDocs`\. Any objects that you upload that have this key name prefix—for example, `TaxDocs/document1.pdf`—will be replicated\. For object requests with this key name prefix, Amazon S3 returns the `x-amz-replication-status` header with one of the following values for the object's replication status: `PENDING`, `COMPLETED`, or `FAILED`\.
**Note**  
If object replication fails after you upload an object, you can't retry replication\. You must upload the object again\. Objects transition to a `FAILED` state for issues such as missing replication role permissions, AWS KMS permissions, or bucket permissions\. For temporary failures, such as if a bucket or Region is unavailable, replication status will not transition to `FAILED`, but will remain `PENDING`\. After the resource is back online, S3 will resume replicating those objects\.
+ When you request an object from a destination bucket, if the object in your request is a replica that Amazon S3 created, Amazon S3 returns the `x-amz-replication-status` header with the value `REPLICA`\.

## Replication status if replicating to multiple destination buckets<a name="replication-status-multiple-destinations"></a>

When you replicate objects to multiple destination buckets, the `x-amz-replication-status` header acts differently\. The header of the source object only returns a value of `COMPLETED` when replication is successful to all destinations\. The header remains at the `PENDING` value until replication has completed for all destinations\. If one or more destinations fail replication, the header returns `FAILED`\.

## Replication status if Amazon S3 replica modification sync is enabled<a name="replication-status-multiple-destinations"></a>

When your replication rules enable Amazon S3 replica modification sync replicas can report statuses other than `REPLICA`\. If metadata changes are in the process of replicating the `x-amz-replication-status` header will return `PENDING`\. replica modification sync fails to replicate metadata the header will return `FAILED`\. If metadata is replicated correctly the replicas will return header `REPLICA`\.

## Finding replication status<a name="replication-status-usage"></a>

To get the replication status of the objects in a bucket, you can use the Amazon S3 inventory tool\. Amazon S3 sends a CSV file to the destination bucket that you specify in the inventory configuration\. You can also use Amazon Athena to query the replication status in the inventory report\. For more information about Amazon S3 inventory, see [ Amazon S3 inventory](storage-inventory.md)\.

You can also find the object replication status using the console, the AWS Command Line Interface \(AWS CLI\), or the AWS SDK\. 
+ **Console** – Select the object, and under the **Overview** header, view the object properties, including replication status\. 
+ **AWS CLI** – Use the `head-object` command to retrieve object metadata, as follows\.

  ```
  aws s3api head-object --bucket source-bucket --key object-key --version-id object-version-id           
  ```

  The command returns object metadata, including the `ReplicationStatus` as shown in the following example response\.

  ```
  {
     "AcceptRanges":"bytes",
     "ContentType":"image/jpeg",
     "LastModified":"Mon, 23 Mar 2015 21:02:29 GMT",
     "ContentLength":3191,
     "ReplicationStatus":"COMPLETED",
     "VersionId":"jfnW.HIMOfYiD_9rGbSkmroXsFj3fqZ.",
     "ETag":"\"6805f2cfc46c0f04559748bb039d69ae\"",
     "Metadata":{
  
     }
  }
  ```
+ **AWS SDKs** – The following code examples get replication status using the AWS SDK for Java and AWS SDK for \.NET, respectively\. 
  + AWS SDK for Java

    ```
    GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest(bucketName, key);
    ObjectMetadata metadata = s3Client.getObjectMetadata(metadataRequest);
    
    System.out.println("Replication Status : " + metadata.getRawMetadataValue(Headers.OBJECT_REPLICATION_STATUS));
    ```
  + AWS SDK for \.NET

    ```
    GetObjectMetadataRequest getmetadataRequest = new GetObjectMetadataRequest
        {
             BucketName = sourceBucket,
             Key        = objectKey
        };
    
    GetObjectMetadataResponse getmetadataResponse = client.GetObjectMetadata(getmetadataRequest);
    Console.WriteLine("Object replication status: {0}", getmetadataResponse.ReplicationStatus);
    ```

**Note**  
Before deleting an object from a source bucket that has replication enabled, check the object's replication status to ensure that the object has been replicated\.   
If lifecycle configuration is enabled on the source bucket, Amazon S3 suspends lifecycle actions until it marks the objects status as either `COMPLETED` or `FAILED`\.

### Related topics<a name="replication-status-related-topics"></a>

[Replication](replication.md)


# Managing ACLs Using the AWS SDK for Java<a name="acl-using-java-sdk"></a>

This section provides examples of how to configure access control list \(ACL\) grants on buckets and objects\. The first example creates a bucket with a canned ACL \(see [Canned ACL](acl-overview.md#canned-acl)\), creates a list of custom permission grants, and then replaces the canned ACL with an ACL containing the custom grants\. The second example shows how to modify an ACL using the `AccessControlList.grantPermission()` method\.

## Setting ACL Grants<a name="set-acl-java-create-resource"></a>

**Example**  
This example creates a bucket\. In the request, the example specifies a canned ACL that grants the Log Delivery group permission to write logs to the bucket\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.io.IOException;
import java.util.ArrayList;

public class CreateBucketWithACL {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String userEmailForReadPermission = "*** user@example.com ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .build();

            // Create a bucket with a canned ACL. This ACL will be replaced by the setBucketAcl()
            // calls below. It is included here for demonstration purposes.
            CreateBucketRequest createBucketRequest = new CreateBucketRequest(bucketName, clientRegion.getName())
                    .withCannedAcl(CannedAccessControlList.LogDeliveryWrite);
            s3Client.createBucket(createBucketRequest);

            // Create a collection of grants to add to the bucket.
            ArrayList<Grant> grantCollection = new ArrayList<Grant>();

            // Grant the account owner full control.
            Grant grant1 = new Grant(new CanonicalGrantee(s3Client.getS3AccountOwner().getId()), Permission.FullControl);
            grantCollection.add(grant1);

            // Grant the LogDelivery group permission to write to the bucket.
            Grant grant2 = new Grant(GroupGrantee.LogDelivery, Permission.Write);
            grantCollection.add(grant2);

            // Save grants by replacing all current ACL grants with the two we just created.
            AccessControlList bucketAcl = new AccessControlList();
            bucketAcl.grantAllPermissions(grantCollection.toArray(new Grant[0]));
            s3Client.setBucketAcl(bucketName, bucketAcl);

            // Retrieve the bucket's ACL, add another grant, and then save the new ACL.
            AccessControlList newBucketAcl = s3Client.getBucketAcl(bucketName);
            Grant grant3 = new Grant(new EmailAddressGrantee(userEmailForReadPermission), Permission.Read);
            newBucketAcl.grantAllPermissions(grant3);
            s3Client.setBucketAcl(bucketName, newBucketAcl);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Configuring ACL Grants on an Existing Object<a name="set-acl-java-existing-resource-example"></a>

**Example**  
This example updates the ACL on an object\. The example performs the following tasks:   
+ Retrieves an object's ACL
+ Clears the ACL by removing all existing permissions
+ Adds two permissions: full access to the owner, and WRITE\_ACP \(see [What Permissions Can I Grant?](acl-overview.md#permissions)\) to a user identified by an email address
+ Saves the ACL to the object

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.AccessControlList;
import com.amazonaws.services.s3.model.CanonicalGrantee;
import com.amazonaws.services.s3.model.EmailAddressGrantee;
import com.amazonaws.services.s3.model.Permission;

import java.io.IOException;

public class ModifyACLExistingObject {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ***";
        String emailGrantee = "*** user@example.com ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Get the existing object ACL that we want to modify.
            AccessControlList acl = s3Client.getObjectAcl(bucketName, keyName);

            // Clear the existing list of grants.
            acl.getGrantsAsList().clear();

            // Grant a sample set of permissions, using the existing ACL owner for Full Control permissions.
            acl.grantPermission(new CanonicalGrantee(acl.getOwner().getId()), Permission.FullControl);
            acl.grantPermission(new EmailAddressGrantee(emailGrantee), Permission.WriteAcp);

            // Save the modified ACL back to the object.
            s3Client.setObjectAcl(bucketName, keyName, acl);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Enabling logging programmatically<a name="enable-logging-programming"></a>

You can enable or disable logging programmatically by using either the Amazon S3 API or the AWS SDKs\. To do so, you both enable logging on the bucket and grant the Log Delivery group permission to write logs to the target bucket\.

**Topics**
+ [Enabling logging](#enabling-logging-general)
+ [Granting the log delivery group WRITE and READ\_ACP permissions](#grant-log-delivery-permissions-general)
+ [Example: AWS SDK for \.NET](#enable-logging-dotnetsdk-exmaple)

## Enabling logging<a name="enabling-logging-general"></a>



To enable logging, you submit a [PUT Bucket logging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlogging.html) request to add the logging configuration on the source bucket\. The request specifies the target bucket and, optionally, the prefix to be used with all log object keys\. The following example identifies `logbucket` as the target bucket and `logs/` as the prefix\. 

```
1. <BucketLoggingStatus xmlns="http://doc.s3.amazonaws.com/2006-03-01">
2.   <LoggingEnabled>
3.     <TargetBucket>logbucket</TargetBucket>
4.     <TargetPrefix>logs/</TargetPrefix>
5.   </LoggingEnabled>
6. </BucketLoggingStatus>
```

The log objects are written and owned by the Log Delivery account, and the bucket owner is granted full permissions on the log objects\. In addition, you can optionally grant permissions to other users so that they can access the logs\. For more information, see [PUT Bucket logging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlogging.html)\. 

Amazon S3 also provides the [GET Bucket logging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlogging.html) API to retrieve logging configuration on a bucket\. To delete the logging configuration, you send the PUT Bucket logging request with an empty `BucketLoggingStatus`\. 

```
1. <BucketLoggingStatus xmlns="http://doc.s3.amazonaws.com/2006-03-01">
2. </BucketLoggingStatus>
```

You can use either the Amazon S3 API or the AWS SDK wrapper libraries to enable logging on a bucket\.

## Granting the log delivery group WRITE and READ\_ACP permissions<a name="grant-log-delivery-permissions-general"></a>



Amazon S3 writes the log files to the target bucket as a member of the predefined Amazon S3 group Log Delivery\. These writes are subject to the usual access control restrictions\. You must grant `s3:GetObjectAcl` and `s3:PutObject` permissions to this group by adding grants to the access control list \(ACL\) of the target bucket\. The Log Delivery group is represented by the following URL\. 

```
1. http://acs.amazonaws.com/groups/s3/LogDelivery
```

To grant `WRITE` and `READ_ACP` permissions, add the following grants\. For information about ACLs, see [Managing Access with ACLs](S3_ACLs_UsingACLs.md)\.

```
 1. <Grant>
 2.     <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  xsi:type="Group">
 3.         <URI>http://acs.amazonaws.com/groups/s3/LogDelivery</URI> 
 4.     </Grantee>
 5.     <Permission>WRITE</Permission>
 6. </Grant>
 7. <Grant>
 8.     <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  xsi:type="Group">
 9.         <URI>http://acs.amazonaws.com/groups/s3/LogDelivery</URI> 
10.     </Grantee>
11.     <Permission>READ_ACP</Permission>
12. </Grant>
```

For examples of adding ACL grants programmatically using the AWS SDKs, see [Managing ACLs Using the AWS SDK for JavaConfiguring ACL Grants on an Existing Object](acl-using-java-sdk.md) and [Managing ACLs Using the AWS SDK for \.NET ](acl-using-dot-net-sdk.md)\.

## Example: AWS SDK for \.NET<a name="enable-logging-dotnetsdk-exmaple"></a>

The following C\# example enables logging on a bucket\. You need to create two buckets, a source bucket and a target bucket\. The example first grants the Log Delivery group the necessary permission to write logs to the target bucket and then enables logging on the source bucket\. For more information, see [Enabling logging programmatically](#enable-logging-programming)\. For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

**Example**  

```
 1. using Amazon;
 2. using Amazon.S3;
 3. using Amazon.S3.Model;
 4. using System;
 5. using System.Threading.Tasks;
 6. 
 7. namespace Amazon.DocSamples.S3
 8. {
 9.     class ServerAccesLoggingTest
10.     {
11.         private const string bucketName = "*** bucket name for which to enable logging ***"; 
12.         private const string targetBucketName = "*** bucket name where you want access logs stored ***"; 
13.         private const string logObjectKeyPrefix = "Logs";
14.         // Specify your bucket region (an example region is shown).
15.         private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
16.         private static IAmazonS3 client;
17. 
18.         public static void Main()
19.         {
20.             client = new AmazonS3Client(bucketRegion);
21.             EnableLoggingAsync().Wait();
22.         }
23. 
24.         private static async Task EnableLoggingAsync()
25.         {
26.             try
27.             {
28.                 // Step 1 - Grant Log Delivery group permission to write log to the target bucket.
29.                 await GrantPermissionsToWriteLogsAsync();
30.                 // Step 2 - Enable logging on the source bucket.
31.                 await EnableDisableLoggingAsync();
32.             }
33.             catch (AmazonS3Exception e)
34.             {
35.                 Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
36.             }
37.             catch (Exception e)
38.             {
39.                 Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
40.             }
41.         }
42. 
43.         private static async Task GrantPermissionsToWriteLogsAsync()
44.         {
45.             var bucketACL = new S3AccessControlList();
46.             var aclResponse = client.GetACL(new GetACLRequest { BucketName = targetBucketName });
47.             bucketACL = aclResponse.AccessControlList;
48.             bucketACL.AddGrant(new S3Grantee { URI = "http://acs.amazonaws.com/groups/s3/LogDelivery" }, S3Permission.WRITE);
49.             bucketACL.AddGrant(new S3Grantee { URI = "http://acs.amazonaws.com/groups/s3/LogDelivery" }, S3Permission.READ_ACP);
50.             var setACLRequest = new PutACLRequest
51.             {
52.                 AccessControlList = bucketACL,
53.                 BucketName = targetBucketName
54.             };
55.             await client.PutACLAsync(setACLRequest);
56.         }
57. 
58.         private static async Task EnableDisableLoggingAsync()
59.         {
60.             var loggingConfig = new S3BucketLoggingConfig
61.             {
62.                 TargetBucketName = targetBucketName,
63.                 TargetPrefix = logObjectKeyPrefix
64.             };
65. 
66.             // Send request.
67.             var putBucketLoggingRequest = new PutBucketLoggingRequest
68.             {
69.                 BucketName = bucketName,
70.                 LoggingConfig = loggingConfig
71.             };
72.             await client.PutBucketLoggingAsync(putBucketLoggingRequest);
73.         }
74.     }
75. }
```

For more information about logging basics, see [Amazon S3 server access logging](ServerLogs.md)\.

For an example CloudFormation template, see [Log Access Requests for a Specific S3 Bucket](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html#aws-properties-s3-bucket--examples--Log_Access_Requests_for_a_Specific_S3_Bucket) in the *AWS CloudFormation User Guide*


# SQL Reference for Amazon S3 Select and S3 Glacier Select<a name="s3-glacier-select-sql-reference"></a>

This reference contains a description of the structured query language \(SQL\) elements that are supported by Amazon S3 Select and S3 Glacier Select\.

**Topics**
+ [SELECT Command](s3-glacier-select-sql-reference-select.md)
+ [Data Types](s3-glacier-select-sql-reference-data-types.md)
+ [Operators](s3-glacier-select-sql-reference-operators.md)
+ [Reserved Keywords](s3-glacier-select-sql-reference-keyword-list.md)
+ [SQL Functions](s3-glacier-select-sql-reference-sql-functions.md)


# Example walkthroughs \- Hosting websites on Amazon S3<a name="hosting-websites-on-s3-examples"></a>

This section provides two examples\. In the first example, you configure a bucket for website hosting, upload a sample index document, and test the website using the Amazon S3 website endpoint for the bucket\. The second example shows how you can use your own domain, such as `example.com`, instead of the Amazon S3 bucket website endpoint, and serve content from an Amazon S3 bucket configured as a website\. The example also shows how Amazon S3 offers the root domain support\.

**Topics**
+ [Configuring a static website](HostingWebsiteOnS3Setup.md)
+ [Configuring a static website using a custom domain registered with Route 53](website-hosting-custom-domain-walkthrough.md)


# Amazon S3 Resources<a name="s3-arn-format"></a>

The following common Amazon Resource Name \(ARN\) format identifies resources in AWS:

```
arn:partition:service:region:namespace:relative-id
```

For information about ARNs, see [Amazon Resource Names \(ARNs\)](https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html) in the *AWS General Reference*\. 

For information about resources, see [IAM JSON Policy Elements: Resource](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_resource.html) in the *IAM User Guide*\.

An Amazon S3 ARN excludes the AWS Region and namespace, but includes the following:
+ **Partition** ‐ `aws` is a common partition name\. If your resources are in the China \(Beijing\) Region, `aws-cn` is the partition name\.
+ **Service** ‐ `s3`\.
+ **Relative ID** ‐ `bucket-name` or a `bucket-name/object-key`\. You can use wild cards\. 

The ARN format for Amazon S3 resources reduces to the following:

```
1. arn:aws:s3:::bucket_name/key_name
```

For a complete list of Amazon S3 resources, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\. 

To find the ARN for an S3 bucket, you can look at the Amazon S3 console **Bucket Policy** or **CORS configuration** permissions pages\. For more information, see the following topics in the *Amazon Simple Storage Service Console User Guide*:
+ [How Do I Add an S3 Bucket Policy?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html)
+ [How Do I Allow Cross\-Domain Resource Sharing with CORS?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-cors-configuration.html)

## Amazon S3 ARN Examples<a name="s3-arn-examples"></a>

The following are examples of Amazon S3 resource ARNs\. 

**Bucket Name and Object Key Specified**  
The following ARN identifies the `/developers/design_info.doc` object in the `examplebucket` bucket\.

```
1. arn:aws:s3:::examplebucket/developers/design_info.doc
```

**Wildcards**  
You can use wildcards as part of the resource ARN\. You can use wildcard characters \(`*` and `?`\) within any ARN segment \(the parts separated by colons\)\. An asterisk \(`*`\) represents any combination of zero or more characters, and a question mark \(`?`\) represents any single character\. You can use multiple `*` or `?` characters in each segment, but a wildcard cannot span segments\. 
+ The following ARN uses the wildcard `*` in the relative\-ID part of the ARN to identify all objects in the `examplebucket` bucket\. 

  ```
  1. arn:aws:s3:::examplebucket/*
  ```
+ The following ARN uses `*` to indicate all Amazon S3 resources \(all S3 buckets and objects in your account\)\.

  ```
  arn:aws:s3:::*
  ```
+ The following ARN uses both wildcards, `*` and `?`, in the `relative-ID` part\. It identifies all objects in buckets such as `example1bucket`, `example2bucket`, `example3bucket`, and so on\.

  ```
  1. arn:aws:s3:::example?bucket/*
  ```

**Policy Variables**  
You can use policy variables in Amazon S3 ARNs\. At policy evaluation time, these predefined variables are replaced by their corresponding values\. Suppose that you organize your bucket as a collection of folders, one folder for each of your users\. The folder name is the same as the user name\. To grant users permission to their folders, you can specify a policy variable in the resource ARN:

```
arn:aws:s3:::bucket_name/developers/${aws:username}/
```

At runtime, when the policy is evaluated, the variable `${aws:username}` in the resource ARN is substituted with the user name making the request\. 


# S3 Batch Operations examples using the AWS CLI<a name="batch-ops-examples-cli"></a>

The S3 Batch Operations feature tracks progress, sends notifications, and stores a detailed completion report of all actions, providing a fully managed, auditable, serverless experience\. You can use S3 Batch Operations through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\. For more information, see [S3 Batch Operations basics](batch-ops-basics.md)\.

The following examples show how you can use S3 Batch Operations with the AWS Command Line Interface \(AWS CLI\)\.

**Topics**
+ [Creating and managing S3 Batch Operations jobs](#batch-ops-example-cli-create)
+ [Managing tags on S3 Batch Operations jobs](#batch-ops-example-cli-job-tags)
+ [Using S3 Batch Operations with S3 Object Lock](#batchops-example-cli-object-lock)

## Creating and managing S3 Batch Operations jobs<a name="batch-ops-example-cli-create"></a>

You can use the AWS CLI to create and manage your S3 Batch Operations jobs\. You can get a description of a Batch Operations job, update its status or priority, and find out which jobs are *Active* and *Complete*\. 

**Topics**
+ [Create an S3 Batch Operations job](#batch-ops-example-cli-job-create)
+ [Get the description of an S3 Batch Operations job](#batch-ops-example-cli-job-description)
+ [Get a list of Active and Complete jobs](#batch-ops-example-cli-active-jobs)
+ [Update the job priority](#batch-ops-example-cli-update-job-priority)
+ [Update the job status](#batch-ops-example-cli-update-job-status)

### Create an S3 Batch Operations job<a name="batch-ops-example-cli-job-create"></a>

The following example creates an S3 Batch Operations `S3PutObjectTagging` job using the AWS CLI\. 

**To create a Batch Operations `S3PutObjectTagging` job**

1. Create an AWS Identity and Access Management \(IAM\) role, and assign permissions\. This role grants Amazon S3 permission to add object tags, for which you create a job in the next step\.

   1. Create an IAM role as follows\.

      ```
      aws iam create-role \
       --role-name S3BatchJobRole \
       --assume-role-policy-document '{
         "Version":"2012-10-17",
         "Statement":[
            {
               "Effect":"Allow",
               "Principal":{
                  "Service":"batchoperations.s3.amazonaws.com"
               },
               "Action":"sts:AssumeRole"
            }
         ]
      }'
      ```

      Record the role's Amazon Resource Name \(ARN\)\. You need the ARN when you create a job\.

   1. Create an IAM policy with permissions, and attach it to the IAM role that you created in the previous step\. For more information about permissions, see [Granting permissions for Amazon S3 Batch Operations](batch-ops-iam-role-policies.md)\.

      ```
      aws iam put-role-policy \
        --role-name S3BatchJobRole \
        --policy-name PutObjectTaggingBatchJobPolicy \
        --policy-document '{
        "Version":"2012-10-17",
        "Statement":[
          {
            "Effect":"Allow",
            "Action":[
              "s3:PutObjectTagging",
              "s3:PutObjectVersionTagging"
            ],
            "Resource": "arn:aws:s3:::{{TargetResource}}/*"
          },
          {
            "Effect": "Allow",
            "Action": [
              "s3:GetObject",
              "s3:GetObjectVersion",
              "s3:GetBucketLocation"
            ],
            "Resource": [
              "arn:aws:s3:::{{ManifestBucket}}/*"
            ]
          },
          {
            "Effect":"Allow",
            "Action":[
              "s3:PutObject",
              "s3:GetBucketLocation"
            ],
            "Resource":[
              "arn:aws:s3:::{{ReportBucket}}/*"
            ]
          }
        ]
      }'
      ```

      

1. Create an `S3PutObjectTagging` job\. 

   The `manifest.csv` file provides a list of bucket and object key values\. The job applies the specified tags to objects identified in the manifest\. The `ETag` is the ETag of the `manifest.csv` object, which you can get from the Amazon S3 console\. The request specifies the `no-confirmation-required` parameter\. Therefore, Amazon S3 makes the job eligible for execution without you having to confirm it using the `udpate-job-status` command\.

   ```
   aws s3control create-job \
       --region us-west-2 \
       --account-id acct-id \
       --operation '{"S3PutObjectTagging": { "TagSet": [{"Key":"keyOne", "Value":"ValueOne"}] }}' \
       --manifest '{"Spec":{"Format":"S3BatchOperations_CSV_20180820","Fields":["Bucket","Key"]},"Location":{"ObjectArn":"arn:aws:s3:::my_manifests/manifest.csv","ETag":"60e460c9d1046e73f7dde5043ac3ae85"}}' \
       --report '{"Bucket":"arn:aws:s3:::bucket-where-completion-report-goes","Prefix":"final-reports", "Format":"Report_CSV_20180820","Enabled":true,"ReportScope":"AllTasks"}' \
       --priority 42 \
       --role-arn IAM-role \
       --client-request-token $(uuidgen) \
       --description "job Description" \
       --no-confirmation-required
   ```

   In response, Amazon S3 returns a job ID \(for example, `00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c`\)\. You need the ID in the next commands\.

### Get the description of an S3 Batch Operations job<a name="batch-ops-example-cli-job-description"></a>

The following example gets the description of an S3 Batch Operations job using the AWS CLI\. 

```
aws s3control describe-job \
    --region us-west-2 \
    --account-id acct-id \
    --job-id 00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c
```



### Get a list of Active and Complete jobs<a name="batch-ops-example-cli-active-jobs"></a>

The following AWS CLI example gets a list of `Active` and `Complete` jobs\. 

```
aws s3control list-jobs \
    --region us-west-2 \
    --account-id acct-id \
    --job-statuses '["Active","Complete"]' \
    --max-results 20
```



### Update the job priority<a name="batch-ops-example-cli-update-job-priority"></a>

The following example updates the job priority using the AWS CLI\. A higher number indicates a higher execution priority\.

```
aws s3control update-job-priority \
    --region us-west-2 \
    --account-id acct-id \
    --priority 98 \
    --job-id 00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c
```



### Update the job status<a name="batch-ops-example-cli-update-job-status"></a>
+ If you didn't specify the `--no-confirmation-required` parameter in the previous `create-job` example, the job remains in a suspended state until you confirm the job by setting its status to `Ready`\. Amazon S3 then makes the job eligible for execution\.

  ```
  aws s3control update-job-status \
      --region us-west-2 \
      --account-id 181572960644 \
      --job-id 00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c \
      --requested-job-status 'Ready'
  ```

  
+ Cancel the job by setting the job status to `Cancelled`\.

  ```
  aws s3control update-job-status \
       --region us-west-2 \
       --account-id 181572960644 \
       --job-id 00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c \
       --status-update-reason "No longer needed" \
       --requested-job-status Cancelled
  ```

  

## Managing tags on S3 Batch Operations jobs<a name="batch-ops-example-cli-job-tags"></a>

You can label and control access to your S3 Batch Operations jobs by adding *tags*\. Tags can be used to identify who is responsible for a Batch Operations job\. You can create jobs with tags attached to them, and you can add tags to jobs after they are created\. For more information, see [Controlling access and labeling jobs using tags](batch-ops-managing-jobs.md#batch-ops-job-tags)\.

**Topics**
+ [Create a Batch Operations job with tags](#batch-ops-example-cli-job-tags-create-job)
+ [Delete the tags from a Batch Operations job](#batch-ops-example-cli-job-tags-delete-job-tagging)
+ [Get the tags of a Batch Operations job](#batch-ops-example-cli-job-tags-get-job-tagging)
+ [Put tags in a Batch Operations job](#batch-ops-example-cli-job-tags-put-job-tagging)

### Create an S3 Batch Operations job with tags<a name="batch-ops-example-cli-job-tags-create-job"></a>

The following AWS CLI example creates an S3 Batch Operations `S3PutObjectCopy` job using job tags as labels for the job\. 

1. Select the action or `OPERATION` that you want the Batch Operations job to perform, and choose your `TargetResource`\.

   ```
   read -d '' OPERATION <<EOF
   {
     "S3PutObjectCopy": {
       "TargetResource": "arn:aws:s3:::destination-bucket"
     }
   }
   EOF
   ```

1. Identify the job `TAGS` that you want for the job\. In this case, you apply two tags, `department` and `FiscalYear`, with the values `Marketing` and `2020` respectively\.

   ```
   read -d '' TAGS <<EOF
   [
     {
       "Key": "department",
       "Value": "Marketing"
     },
     {
       "Key": "FiscalYear",
       "Value": "2020"
     }
   ]
   EOF
   ```

1. Specify the `MANIFEST` for the Batch Operations job\.

   ```
   read -d '' MANIFEST <<EOF
   {
     "Spec": {
       "Format": "EXAMPLE_S3BatchOperations_CSV_20180820",
       "Fields": [
         "Bucket",
         "Key"
       ]
     },
     "Location": {
       "ObjectArn": "arn:aws:s3:::example-bucket/example_manifest.csv",
       "ETag": "example-5dc7a8bfb90808fc5d546218"
     }
   }
   EOF
   ```

1. Configure the `REPORT` for the Batch Operations job\.

   ```
   read -d '' REPORT <<EOF
   {
     "Bucket": "arn:aws:s3:::example-report-bucket",
     "Format": "Example_Report_CSV_20180820",
     "Enabled": true,
     "Prefix": "reports/copy-with-replace-metadata",
     "ReportScope": "AllTasks"
   }
   EOF
   ```

1. Run the`create-job` action to create your Batch Operations job with inputs set in the preceding steps\.

   ```
   aws \
       s3control create-job \
       --account-id 123456789012 \
       --manifest "${MANIFEST//$'\n'}" \
       --operation "${OPERATION//$'\n'/}" \
       --report "${REPORT//$'\n'}" \
       --priority 10 \
       --role-arn arn:aws:iam::123456789012:role/batch-operations-role \
       --tags "${TAGS//$'\n'/}" \
       --client-request-token "$(uuidgen)" \
       --region us-west-2 \
       --description "Copy with Replace Metadata";
   ```

### Delete the tags from an S3 Batch Operations job<a name="batch-ops-example-cli-job-tags-delete-job-tagging"></a>

The following example deletes the tags from a Batch Operations job using the AWS CLI\.

```
aws \
    s3control delete-job-tagging \
    --account-id 123456789012 \
    --job-id Example-e25a-4ed2-8bee-7f8ed7fc2f1c \
    --region us-east-1;
```

### Get the job tags of an S3 Batch Operations job<a name="batch-ops-example-cli-job-tags-get-job-tagging"></a>

The following example gets the tags of a Batch Operations job using the AWS CLI\.

```
aws \
    s3control get-job-tagging \
    --account-id 123456789012 \
    --job-id Example-e25a-4ed2-8bee-7f8ed7fc2f1c \
    --region us-east-1;
```

### Put job tags in an existing S3 Batch Operations job<a name="batch-ops-example-cli-job-tags-put-job-tagging"></a>

The following is an example of using `s3control put-job-tagging` to add job tags to your S3 Batch Operations job using the AWS CLI\.

**Note**  
If you send this request with an empty tag set, S3 Batch Operations deletes the existing tag set on the object\. Also, if you use this method, you are charged for a Tier 1 Request \(PUT\)\. For more information, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing)\.  
To delete existing tags for your Batch Operations job, the `DeleteJobTagging` action is preferred because it achieves the same result without incurring charges\.

1. Identify the job `TAGS` that you want for the job\. In this case, you apply two tags, `department` and `FiscalYear`, with the values `Marketing` and `2020` respectively\.

   ```
   read -d '' TAGS <<EOF
   [
     {
       "Key": "department",
       "Value": "Marketing"
     },
     {
       "Key": "FiscalYear",
       "Value": "2020"
     }
   ]
   EOF
   ```

1. Run the `put-job-tagging` action with the required parameters\.

   ```
   aws \
       s3control put-job-tagging \
       --account-id 123456789012 \
       --tags "${TAGS//$'\n'/}" \
       --job-id Example-e25a-4ed2-8bee-7f8ed7fc2f1c \
       --region us-east-1;
   ```

## Using S3 Batch Operations with S3 Object Lock<a name="batchops-example-cli-object-lock"></a>

You can use S3 Batch Operations with S3 Object Lock to manage retention or enable a legal hold for many Amazon S3 objects at once\. You specify the list of target objects in your manifest and submit it to Batch Operations for completion\. For more information, see [Managing S3 Object Lock retention dates](batch-ops-retention-date.md) and [Managing S3 Object Lock legal hold](batch-ops-legal-hold.md)\. 

The following examples show how to create an IAM role with S3 Batch Operations permissions and update the role permissions to create jobs that enable Object Lock using the AWS CLI\. In the examples, replace any variable values with those that suit your needs\. You must also have a `CSV` manifest identifying the objects for your S3 Batch Operations job\. For more information, see [Specifying a manifest](batch-ops-basics.md#specify-batchjob-manifest)\.

In this section, you perform two steps:

1. Create an IAM role and assign S3 Batch Operations permissions to run\.

   This step is required for all S3 Batch Operations jobs\.

   ```
   export AWS_PROFILE='aws-user'
   
   read -d '' bops_trust_policy <<EOF
   {
     "Version": "2012-10-17", 
     "Statement": [ 
       { 
         "Effect": "Allow", 
         "Principal": { 
           "Service": [
             "batchoperations.s3.amazonaws.com"
           ]
         }, 
         "Action": "sts:AssumeRole" 
       } 
     ]
   }
   EOF
   aws iam create-role --role-name bops-objectlock --assume-role-policy-document "${bops_trust_policy}"
   ```

1. Set up S3 Batch Operations with S3 Object Lock to run\.

   In this step, you allow the role to do the following:

   1. Run Object Lock on the S3 bucket that contains the target objects that you want Batch Operations to run on\.

   1. Read the S3 bucket where the manifest CSV file and the objects are located\.

   1. Write the results of the S3 Batch Operations job to the reporting bucket\.

   ```
   read -d '' bops_permissions <<EOF
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Effect": "Allow",
               "Action": "s3:GetBucketObjectLockConfiguration",
               "Resource": [
                   "arn:aws:s3:::{{ManifestBucket}}"
               ]
           },
           {
               "Effect": "Allow",
               "Action": [
                   "s3:GetObject",
                   "s3:GetObjectVersion",
                   "s3:GetBucketLocation"
               ],
               "Resource": [
                   "arn:aws:s3:::{{ManifestBucket}}/*"
               ]
           },
           {
               "Effect": "Allow",
               "Action": [
                   "s3:PutObject",
                   "s3:GetBucketLocation"
               ],
               "Resource": [
                   "arn:aws:s3:::{{ReportBucket}}/*"
               ]
           }
       ]
   }
   EOF
   
   aws iam put-role-policy --role-name bops-objectlock --policy-name object-lock-permissions --policy-document "${bops_permissions}"
   ```

**Topics**
+ [Use S3 Batch Operations to set S3 Object Lock retention](#batch-ops-cli-object-lock-retention-example)
+ [Use S3 Batch Operations to turn off S3 Object Lock legal hold](#batch-ops-cli-object-lock-legalhold-example)

### Use S3 Batch Operations to set S3 Object Lock retention<a name="batch-ops-cli-object-lock-retention-example"></a>

The following example allows the rule to set S3 Object Lock retention for your objects in the manifest bucket\.

 You update the role to include `s3:PutObjectRetention` permissions so that you can run Object Lock retention on the objects in your bucket\.

```
export AWS_PROFILE='aws-user'

read -d '' retention_permissions <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObjectRetention"
            ],
            "Resource": [
                "arn:aws:s3:::{{ManifestBucket}}/*"
            ]
        }
    ]
}
EOF

aws iam put-role-policy --role-name bops-objectlock --policy-name retention-permissions --policy-document "${retention_permissions}"
```

#### Use S3 Batch Operations with S3 Object Lock retention compliance mode<a name="batch-ops-cli-object-lock-compliance-example"></a>

The following example builds on the previous examples of creating a trust policy, and setting S3 Batch Operations and S3 Object Lock configuration permissions on your objects\. This example sets the retention mode to `COMPLIANCE` and the `retain until date` to January 1, 2025\. It creates a job that targets objects in the manifest bucket and reports the results in the reports bucket that you identified\.

```
export AWS_PROFILE='aws-user'
export AWS_DEFAULT_REGION='us-west-2'
export ACCOUNT_ID=123456789012
export ROLE_ARN='arn:aws:iam::123456789012:role/bops-objectlock'

read -d '' OPERATION <<EOF
{
  "S3PutObjectRetention": {
    "Retention": {
      "RetainUntilDate":"2025-01-01T00:00:00",
      "Mode":"COMPLIANCE"
    }
  }
}
EOF

read -d '' MANIFEST <<EOF
{
  "Spec": {
    "Format": "S3BatchOperations_CSV_20180820",
    "Fields": [
      "Bucket",
      "Key"
    ]
  },
  "Location": {
    "ObjectArn": "arn:aws:s3:::ManifestBucket/compliance-objects-manifest.csv",
    "ETag": "Your-manifest-ETag"
  }
}
EOF

read -d '' REPORT <<EOF
{
  "Bucket": "arn:aws:s3:::ReportBucket",
  "Format": "Report_CSV_20180820",
  "Enabled": true,
  "Prefix": "reports/compliance-objects-bops",
  "ReportScope": "AllTasks"
}
EOF

aws \
    s3control create-job \
    --account-id "${ACCOUNT_ID}" \
    --manifest "${MANIFEST//$'\n'}" \
    --operation "${OPERATION//$'\n'/}" \
    --report "${REPORT//$'\n'}" \
    --priority 10 \
    --role-arn "${ROLE_ARN}" \
    --client-request-token "$(uuidgen)" \
    --region "${AWS_DEFAULT_REGION}" \
    --description "Set compliance retain-until to 1 Jul 2030";
```



The following example extends the `COMPLIANCE` mode's `retain until date` to January 15, 2025\.

```
export AWS_PROFILE='aws-user'
export AWS_DEFAULT_REGION='us-west-2'
export ACCOUNT_ID=123456789012
export ROLE_ARN='arn:aws:iam::123456789012:role/bops-objectlock'

read -d '' OPERATION <<EOF
{
  "S3PutObjectRetention": {
    "Retention": {
      "RetainUntilDate":"2025-01-15T00:00:00",
      "Mode":"COMPLIANCE"
    }
  }
}
EOF

read -d '' MANIFEST <<EOF
{
  "Spec": {
    "Format": "S3BatchOperations_CSV_20180820",
    "Fields": [
      "Bucket",
      "Key"
    ]
  },
  "Location": {
    "ObjectArn": "arn:aws:s3:::ManifestBucket/compliance-objects-manifest.csv",
    "ETag": "Your-manifest-ETag"
  }
}
EOF

read -d '' REPORT <<EOF
{
  "Bucket": "arn:aws:s3:::ReportBucket",
  "Format": "Report_CSV_20180820",
  "Enabled": true,
  "Prefix": "reports/compliance-objects-bops",
  "ReportScope": "AllTasks"
}
EOF

aws \
    s3control create-job \
    --account-id "${ACCOUNT_ID}" \
    --manifest "${MANIFEST//$'\n'}" \
    --operation "${OPERATION//$'\n'/}" \
    --report "${REPORT//$'\n'}" \
    --priority 10 \
    --role-arn "${ROLE_ARN}" \
    --client-request-token "$(uuidgen)" \
    --region "${AWS_DEFAULT_REGION}" \
    --description "Extend compliance retention to 15 Jan 2020";
```

#### Use S3 Batch Operations with S3 Object Lock retention governance mode<a name="batch-ops-cli-object-lock-governance-example"></a>

The following example builds on the previous example of creating a trust policy, and setting S3 Batch Operations and S3 Object Lock configuration permissions\. It shows how to apply S3 Object Lock retention governance with the `retain until date` of January 30, 2025, across multiple objects\. It creates a Batch Operations job that uses the manifest bucket and reports the results in the reports bucket\.



```
export AWS_PROFILE='aws-user'
export AWS_DEFAULT_REGION='us-west-2'
export ACCOUNT_ID=123456789012
export ROLE_ARN='arn:aws:iam::123456789012:role/bops-objectlock'

read -d '' OPERATION <<EOF
{
  "S3PutObjectRetention": {
    "Retention": {
      "RetainUntilDate":"2025-01-30T00:00:00",
      "Mode":"GOVERNANCE"
    }
  }
}
EOF

read -d '' MANIFEST <<EOF
{
  "Spec": {
    "Format": "S3BatchOperations_CSV_20180820",
    "Fields": [
      "Bucket",
      "Key"
    ]
  },
  "Location": {
    "ObjectArn": "arn:aws:s3:::ManifestBucket/governance-objects-manifest.csv",
    "ETag": "Your-manifest-ETag"
  }
}
EOF

read -d '' REPORT <<EOF
{
  "Bucket": "arn:aws:s3:::ReportBucketT",
  "Format": "Report_CSV_20180820",
  "Enabled": true,
  "Prefix": "reports/governance-objects",
  "ReportScope": "AllTasks"
}
EOF

aws \
    s3control create-job \
    --account-id "${ACCOUNT_ID}" \
    --manifest "${MANIFEST//$'\n'}" \
    --operation "${OPERATION//$'\n'/}" \
    --report "${REPORT//$'\n'}" \
    --priority 10 \
    --role-arn "${ROLE_ARN}" \
    --client-request-token "$(uuidgen)" \
    --region "${AWS_DEFAULT_REGION}" \
    --description "Put governance retention";
```



The following example builds on the previous example of creating a trust policy, and setting S3 Batch Operations and S3 Object Lock configuration permissions\. It shows how to bypass retention governance across multiple objects and creates a Batch Operations job that uses the manifest bucket and reports the results in the reports bucket\.

```
export AWS_PROFILE='aws-user'

read -d '' bypass_governance_permissions <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:BypassGovernanceRetention"
            ],
            "Resource": [
                "arn:aws:s3:::ManifestBucket/*"
            ]
        }
    ]
}
EOF

aws iam put-role-policy --role-name bops-objectlock --policy-name bypass-governance-permissions --policy-document "${bypass_governance_permissions}"

export AWS_PROFILE='aws-user'
export AWS_DEFAULT_REGION='us-west-2'
export ACCOUNT_ID=123456789012
export ROLE_ARN='arn:aws:iam::123456789012:role/bops-objectlock'

read -d '' OPERATION <<EOF
{
  "S3PutObjectRetention": {
    "BypassGovernanceRetention": true,
    "Retention": {
    }
  }
}
EOF

read -d '' MANIFEST <<EOF
{
  "Spec": {
    "Format": "S3BatchOperations_CSV_20180820",
    "Fields": [
      "Bucket",
      "Key"
    ]
  },
  "Location": {
    "ObjectArn": "arn:aws:s3:::ManifestBucket/governance-objects-manifest.csv",
    "ETag": "Your-manifest-ETag"
  }
}
EOF

read -d '' REPORT <<EOF
{
  "Bucket": "arn:aws:s3:::REPORT_BUCKET",
  "Format": "Report_CSV_20180820",
  "Enabled": true,
  "Prefix": "reports/bops-governance",
  "ReportScope": "AllTasks"
}
EOF

aws \
    s3control create-job \
    --account-id "${ACCOUNT_ID}" \
    --manifest "${MANIFEST//$'\n'}" \
    --operation "${OPERATION//$'\n'/}" \
    --report "${REPORT//$'\n'}" \
    --priority 10 \
    --role-arn "${ROLE_ARN}" \
    --client-request-token "$(uuidgen)" \
    --region "${AWS_DEFAULT_REGION}" \
    --description "Remove governance retention";
```

### Use S3 Batch Operations to turn off S3 Object Lock legal hold<a name="batch-ops-cli-object-lock-legalhold-example"></a>

The following example builds on the previous examples of creating a trust policy, and setting S3 Batch Operations and S3 Object Lock configuration permissions\. It shows how to disable Object Lock legal hold on objects using Batch Operations\. 

The example first updates the role to grant `s3:PutObjectLegalHold` permissions, creates a Batch Operations job that turns off \(removes\) legal hold from the objects identified in the manifest, and then reports on it\.

```
export AWS_PROFILE='aws-user'

read -d '' legal_hold_permissions <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObjectLegalHold"
            ],
            "Resource": [
                "arn:aws:s3:::ManifestBucket/*"
            ]
        }
    ]

EOF

aws iam put-role-policy --role-name bops-objectlock --policy-name legal-hold-permissions --policy-document "${legal_hold_permissions}"
```

The following example turns off legal hold\.

```
export AWS_PROFILE='aws-user'
export AWS_DEFAULT_REGION='us-west-2'
export ACCOUNT_ID=123456789012
export ROLE_ARN='arn:aws:iam::123456789012:role/bops-objectlock'

read -d '' OPERATION <<EOF
{
  "S3PutObjectLegalHold": {
    "LegalHold": {
      "Status":"OFF"
    }
  }
}
EOF

read -d '' MANIFEST <<EOF
{
  "Spec": {
    "Format": "S3BatchOperations_CSV_20180820",
    "Fields": [
      "Bucket",
      "Key"
    ]
  },
  "Location": {
    "ObjectArn": "arn:aws:s3:::ManifestBucket/legalhold-object-manifest.csv",
    "ETag": "Your-manifest-ETag"
  }
}
EOF

read -d '' REPORT <<EOF
{
  "Bucket": "arn:aws:s3:::ReportBucket",
  "Format": "Report_CSV_20180820",
  "Enabled": true,
  "Prefix": "reports/legalhold-objects-bops",
  "ReportScope": "AllTasks"
}
EOF

aws \
    s3control create-job \
    --account-id "${ACCOUNT_ID}" \
    --manifest "${MANIFEST//$'\n'}" \
    --operation "${OPERATION//$'\n'/}" \
    --report "${REPORT//$'\n'}" \
    --priority 10 \
    --role-arn "${ROLE_ARN}" \
    --client-request-token "$(uuidgen)" \
    --region "${AWS_DEFAULT_REGION}" \
    --description "Turn off legal hold";
```


# Restore an archived object using the AWS SDK for Java<a name="restoring-objects-java"></a>

**Example**  
The following example shows how to restore a copy of an object that has been archived using the AWS SDK for Java\. The example initiates a restoration request for the specified archived object and checks its restoration status\. For more information about restoring archived objects, see [Restoring archived objects](restoring-objects.md)\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.RestoreObjectRequest;

import java.io.IOException;

public class RestoreArchivedObject {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Object key ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Create and submit a request to restore an object from Glacier for two days.
            RestoreObjectRequest requestRestore = new RestoreObjectRequest(bucketName, keyName, 2);
            s3Client.restoreObjectV2(requestRestore);

            // Check the restoration status of the object.
            ObjectMetadata response = s3Client.getObjectMetadata(bucketName, keyName);
            Boolean restoreFlag = response.getOngoingRestore();
            System.out.format("Restoration status: %s.\n",
                    restoreFlag ? "in progress" : "not in progress (finished or failed)");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Using the AWS SDK for Python \(Boto\)<a name="UsingTheBotoAPI"></a>

Boto is a Python package that provides interfaces to AWS including Amazon S3\. For more information about Boto, go to the [AWS SDK for Python \(Boto\)](https://aws.amazon.com/sdk-for-python/)\. The getting started link on this page provides step\-by\-step instructions to get started\. 


# Put object copy<a name="batch-ops-copy-object"></a>

The Put object copy operation copies each object specified in the manifest\. You can copy objects to a different bucket in the same AWS Region or to a bucket in a different Region\. S3 Batch Operations supports most options available through Amazon S3 for copying objects\. These options include setting object metadata, setting permissions, and changing an object's storage class\. For more information about the functionality available through Amazon S3 for copying objects, see [Copying objects](CopyingObjectsExamples.md)\. 

## Restrictions and limitations<a name="batch-ops-copy-object-restrictions"></a>
+ All source objects must be in one bucket\.
+ All destination objects must be in one bucket\.
+ You must have read permissions for the source bucket and write permissions for the destination bucket\.
+ Objects to be copied can be up to 5 GB in size\.
+ Put object copy jobs must be created in the destination region, which is the region you intend to copy the objects to\.
+ All Put object copy options are supported except for conditional checks on ETags and server\-side encryption with customer\-provided encryption keys \(SSE\-C\)\.
+ If the buckets are unversioned, you will overwrite objects with the same key names\.
+ Objects are not necessarily copied in the same order as they are listed in the manifest\. For versioned buckets, if preserving current/non\-current version order is important, you should copy all non\-current versions first\. Then, after the first job is complete, copy the current versions in a subsequent job\. 
+ Copying objects to the Reduced Redundancy Storage \(RRS\) class is not supported\.


# Using the AWS SDK for \.NET for multipart upload \(low\-level API\)<a name="usingLLmpuDotNet"></a>

The AWS SDK for \.NET exposes a low\-level API that closely resembles the Amazon S3 REST API for multipart upload \(see [Using the REST API for multipart upload](UsingRESTAPImpUpload.md) \)\. Use the low\-level API when you need to pause and resume multipart uploads, vary part sizes during the upload, or when you do not know the size of the data in advance\. Use the high\-level API \(see [Using the AWS SDK for \.NET for multipart upload \(high\-level API\)](usingHLmpuDotNet.md)\), whenever you don't have these requirements\.

**Topics**
+ [Upload a file to an S3 Bucket using the AWS SDK for \.NET \(low\-level API\)](LLuploadFileDotNet.md)
+ [List multipart uploads to an S3 Bucket using the AWS SDK for \.NET \(low\-level\)](LLlistMPuploadsDotNet.md)
+ [Track the progress of a multipart upload to an S3 Bucket using the AWS SDK for \.NET \(low\-level\)](LLTrackProgressMPUNet.md)
+ [Stop multipart uploads to an S3 Bucket using the AWS SDK for \.NET \(low\-level\)](LLAbortMPUnet.md)


# Billing and usage reporting for S3 buckets<a name="BucketBilling"></a>

When using Amazon Simple Storage Service \(Amazon S3\), you don't have to pay any upfront fees or commit to how much content you'll store\. As with the other Amazon Web Services \(AWS\) services, you pay as you go and pay only for what you use\.

AWS provides the following reports for Amazon S3:
+ **Billing reports** – Multiple reports that provide high\-level views of all of the activity for the AWS services that you're using, including Amazon S3\. AWS always bills the owner of the S3 bucket for Amazon S3 fees, unless the bucket was created as a Requester Pays bucket\. For more information about Requester Pays, see [Requester Pays buckets](RequesterPaysBuckets.md)\. For more information about billing reports, see [AWS Billing reports for Amazon S3](aws-billing-reports.md)\.
+ **Usage report** – A summary of activity for a specific service, aggregated by hour, day, or month\. You can choose which usage type and operation to include\. You can also choose how the data is aggregated\. For more information, see [AWS usage report for Amazon S3](aws-usage-report.md)\.

The following topics provide information about billing and usage reporting for Amazon S3\.

**Topics**
+ [AWS Billing reports for Amazon S3](aws-billing-reports.md)
+ [AWS usage report for Amazon S3](aws-usage-report.md)
+ [Understanding your AWS billing and usage reports for Amazon S3](aws-usage-report-understand.md)
+ [Using cost allocation S3 bucket tags](CostAllocTagging.md)


# Using the AWS Java SDK for multipart upload \(high\-level API\)<a name="usingHLmpuJava"></a>

**Topics**
+ [Upload a file](HLuploadFileJava.md)
+ [Stop multipart uploads](HLAbortMPUploadsJava.md)
+ [Track multipart upload progress](HLTrackProgressMPUJava.md)

The AWS SDK for Java exposes a high\-level API, called `TransferManager`, that simplifies multipart uploads \(see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\)\. You can upload data from a file or a stream\. You can also set advanced options, such as the part size you want to use for the multipart upload, or the number of concurrent threads you want to use when uploading the parts\. You can also set optional object properties, the storage class, or the ACL\. You use the `PutObjectRequest` and the `TransferManagerConfiguration` classes to set these advanced options\. 

When possible, `TransferManager` attempts to use multiple threads to upload multiple parts of a single upload at once\. When dealing with large content sizes and high bandwidth, this can increase throughput significantly\.

In addition to file\-upload functionality, the `TransferManager` class enables you to stop an in\-progress multipart upload\. An upload is considered to be in progress after you initiate it and until you complete or stop it\. The `TransferManager` stops all in\-progress multipart uploads on a specified bucket that were initiated before a specified date and time\. 

For more information about multipart uploads, including additional functionality offered by the low\-level API methods, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\. 


# Adding objects to versioning\-enabled buckets<a name="AddingObjectstoVersioningEnabledBuckets"></a>

Once you enable versioning on a bucket, Amazon S3 automatically adds a unique version ID to every object stored \(using `PUT`, `POST`, or `COPY`\) in the bucket\. 

The following figure shows that Amazon S3 adds a unique version ID to an object when it is added to a versioning\-enabled bucket\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_PUT_versionEnabled.png)

**Topics**
+ [Using the console](#add-obj-versioning-enabled-bucket-console)
+ [Using the AWS SDKs](#add-obj-versioning-enabled-bucket-sdk)
+ [Using the REST API](#add-obj-versioning-enabled-bucket-rest)

## Using the console<a name="add-obj-versioning-enabled-bucket-console"></a>

 For instructions, see [How Do I Upload an Object to an S3 Bucket? ](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html) in the *Amazon Simple Storage Service Console User Guide*\. 

## Using the AWS SDKs<a name="add-obj-versioning-enabled-bucket-sdk"></a>

For examples of uploading objects using the AWS SDKs for Java, \.NET, and PHP, see [Uploading objects](UploadingObjects.md)\. The examples for uploading objects in nonversioned and versioning\-enabled buckets are the same, although in the case of versioning\-enabled buckets, Amazon S3 assigns a version number\. Otherwise, the version number is null\. 

For information about using other AWS SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code/)\. 

## Using the REST API<a name="add-obj-versioning-enabled-bucket-rest"></a>


**Adding objects to versioning\-enabled buckets**  

|  |  | 
| --- |--- |
| 1 | Enable versioning on a bucket using a PUT Bucket versioning request\. For more information, see [PUT Bucket versioning](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTVersioningStatus.html)\. | 
| 2 | Send a PUT, POST, or COPY request to store an object in the bucket\. | 

When you add an object to a versioning\-enabled bucket, Amazon S3 returns the version ID of the object in the `x-amz-version-id` response header, for example:

```
1. x-amz-version-id: 3/L4kqtJlcpXroDTDmJ+rmSpXd3dIbrHY
```

**Note**  
Normal Amazon S3 rates apply for every version of an object stored and transferred\. Each version of an object is the entire object; it is not just a diff from the previous version\. Thus, if you have three versions of an object stored, you are charged for three objects\. 

**Note**  
The version ID values that Amazon S3 assigns are URL safe \(can be included as part of a URI\)\.


# Enabling website hosting<a name="EnableWebsiteHosting"></a>

When you configure a bucket as a static website, you must enable static website hosting, configure an index document, and set permissions:

Follow these steps to enable website hosting for your Amazon S3 bucket using the [Amazon S3 console](https://console.aws.amazon.com/s3/home)\. You can also enable static website hosting using the REST API, the AWS SDKs, the AWS CLI, or AWS CloudFormation\. For more information, see the following:
+ [Configuring a static website using the REST API and AWS SDKs](https://docs.aws.amazon.com/AmazonS3/latest/dev/ManagingBucketWebsiteConfig.html)
+ [AWS::S3::Bucket WebsiteConfiguration](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-websiteconfiguration.html) in the AWS CloudFormation User Guide
+ [put\-bucket\-website](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/put-bucket-website.html) in the AWS CLI Command Reference

To configure your website with a custom domain, see [Example walkthroughs \- Hosting websites on Amazon S3](hosting-websites-on-s3-examples.md)\.

**To enable static website hosting**

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose the name of the bucket that you want to enable static website hosting for\.

1. Choose **Properties**\.

1. Under **Static website hosting**, choose **Edit**\.

1. Choose **Use this bucket to host a website**\. 

1. Under **Static website hosting**, choose **Enable**\.

1. In **Index document**, enter the file name of the index document, typically `index.html`\. 

   The index document name is case sensitive and must exactly match the file name of the HTML index document that you plan to upload to your S3 bucket\. When you configure a bucket for website hosting, you must specify an index document\. Amazon S3 returns this index document when requests are made to the root domain or any of the subfolders\. For more information, see [Configuring an index document](https://docs.aws.amazon.com/AmazonS3/latest/dev/IndexDocumentSupport.html)\.

1. \(Optional\) If you want to provide your own custom error document for 4XX class errors, in **Error document**, enter the custom error document file name\. 

   The error document name is case sensitive and must exactly match the file name of the HTML error document that you plan to upload to your S3 bucket\. If you don't specify a custom error document and an error occurs, Amazon S3 returns a default HTML error document\. For more information, see [Configuring a custom error document](https://docs.aws.amazon.com/AmazonS3/latest/dev/CustomErrorDocSupport.html)\.

1. \(Optional\) If you want to specify advanced redirection rules, in **Redirection rules**, enter XML to describe the rules\.

   For example, you can conditionally route requests according to specific object key names or prefixes in the request\. For more information, see [Configuring advanced conditional redirects](https://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html#advanced-conditional-redirects)\.

1. Choose **Save changes**\.

   Amazon S3 enables static website hosting for your bucket\. At the bottom of the page, under **Static website hosting**, you see the website endpoint for your bucket\.

1. Under **Static website hosting**, note the **Endpoint**\.

   The **Endpoint** is the Amazon S3 website endpoint for your bucket\. After you finish configuring your bucket as a static website, you can use this endpoint to test your website\.

Next, you must configure your index document and set permissions\. For more information, see [Configuring an index document](IndexDocumentSupport.md) and [Setting permissions for website access](WebsiteAccessPermissionsReqd.md)\. You can also optionally configure an [error document](CustomErrorDocSupport.md), [web traffic logging](LoggingWebsiteTraffic.md), or a [redirect](how-to-page-redirect.md)\.


# Amazon S3 Storage Lens metrics glossary<a name="storage_lens_metrics_glossary"></a>

By default, all dashboards are configured with *free metrics*, which include *usage metrics* aggregated down to the bucket level with a 14\-day data retention\. This means that you can see all the usage metrics that S3 Storage Lens aggregates, and your metrics are available 14 days from the day the data was aggregated\. 

*Advanced metrics and recommendations* include *usage* and *activity* metrics that can be aggregated by prefix\. Activity metrics can be aggregated by bucket with a 15\-month data retention policy\. There are additional charges when you use S3 Storage Lens with advanced metrics and recommendations\. For more information, see [ Amazon S3 pricing](http://aws.amazon.com/s3/pricing/)\.


****  

| Metric name | Description | Free | Type | Category | Derived? | Derived metric formula | 
| --- | --- | --- | --- | --- | --- | --- | 
|  Total Storage  | The total storage |  Y  |  Usage  |  Summary |  N  |   | 
|  Object Count  | The total object count |  Y  |  Usage  |  Summary |  N  |   | 
|  \# Avg Object Size  | The average object size |  Y  |  Usage  |  Summary |  Y  | Sum\(StorageBytes\)/ sum\(ObjectCount\)  | 
| \# Buckets  | The total number of active buckets |  Y  |  Usage  |  Summary |  Y  | DistinctCount\[Bucketname\]  | 
| \# Accounts  | The number of accounts whose storage is in scope |  Y  |  Usage  |  Summary |  Y  | DistinctCount\[AccountID\]  | 
| Current Version Storage Bytes  | The number of bytes that are a current version  |  Y  |  Usage  | Data Protection, Cost Efficiency  |  N  |   | 
| % Current Version Bytes  | The percentage of bytes in scope that are current version |  Y  |  Usage  | Data Protection, Cost Efficiency  |  Y  | Sum\(CurrentVersion Bytes\) /sum\(Storage Bytes\)  | 
| Current Version Object Count  | The number of bytes that are noncurrent version  |  Y  |  Usage  | Data Protection, Cost Efficiency  |  N  |   | 
| % Current Version Objects  | The percentage of objects in scope that are a noncurrent version  |  Y  |  Usage  | Data Protection, Cost Efficiency  |  Y  | Sum\(CurrentVersion Objects\)/sum\(ObjectCount\)  | 
| Non\-Current Version Storage Bytes  | The number of noncurrent versioned bytes  |  Y  |  Usage  | Data Protection, Cost Efficiency  |  N  |   | 
| % Non\-Current Version Bytes  | The percentage of bytes in scope that are noncurrent version |  Y  |  Usage  | Data Protection, Cost Efficiency  |  Y  | Sum\(NonCurrentVersionSto rageBytes\)/ Sum\(StorageBytes\)  | 
| Non\-Current Version Object Count  | The count of the noncurrent version objects |  Y  |  Usage  | Data Protection, Cost Efficiency  |  N  |   | 
| % Non\-Current Version Objects  | The percentage of objects in scope that are a noncurrent version |  Y  |  Usage  | Data Protection, Cost Efficiency  |  Y  | Sum\(NonCurrentVersionObjectCount\)/Sum\(ObjectCount\)  | 
| Delete Marker Object Count  | The total number of objects with a delete marker |  Y  |  Usage  |  Cost Efficiency  |  N  |   | 
| % Delete Marker Objects  | The percentage of objects in scope with a delete marker  |  Y  |  Usage  |  Cost Efficiency  |  Y  |   | 
| Encrypted Storage Bytes  | The total number of encrypted bytes using [Amazon S3 server\-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html ) |  Y  |  Usage  | Data Protection  |  N  |   | 
| % Encrypted Bytes | The percentage of total bytes in scope that are encrypted using [Amazon S3 server\-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html ) |  Y  |  Usage  | Data Protection  |  Y  | Sum\(EncryptedStorageBytes\)/ Sum\(StorageBytes\)  | 
| Encrypted Object Count  | The total object counts that are encrypted using [Amazon S3 server\-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html ) |  Y  |  Usage  | Data Protection  |  N  |   | 
| % Encrypted Objects  | The percentage of objects in scope that are encrypted using [Amazon S3 server\-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html ) |  Y  |  Usage  | Data Protection  |  Y  | Sum\(EncryptedStorageBytes\)/Sum\(ObjectCount\)  | 
| Unencrypted Storage Bytes  | The number of bytes in scope that are unencrypted  |  Y  |  Usage  | Data Protection  |  Y  | Sum\(StorageBytes\) \- sum\(EncryptedStorageBytes\)  | 
| % Unencrypted Bytes  | The percentage of bytes in scope that are unencrypted  |  Y  |  Usage  | Data Protection  |  Y  | Sum\(UnencryptedStorageBytes\)/ Sum\(StorageBytes\)  | 
| Unencrypted Object Count  | The count of the objects that are unencrypted  |  Y  |  Usage  | Data Protection  |  Y  | Sum\(ObjectCounts\) \- sum\(EncryptedObjectCounts\)  | 
| % Unencrypted Objects  | The percentage of unencrypted objects  |  Y  |  Usage  | Data Protection  |  Y  | Sum\(UnencryptedStorageBytes\)/ Sum\(ObjectCount\)  | 
| Replicated Storage Bytes  | The total number of bytes in scope that are replicated  |  Y  |  Usage  | Data Protection  |  N  |   | 
| % Replicated Bytes  | The percentage of total bytes in scope that are replicated  |  Y  |  Usage  | Data Protection  |  Y  | Sum\(ReplicatedStorageBytes\)/ Sum\(StorageBytes\)  | 
| Replicated Object Count  | The count of replicated objects  |  Y  |  Usage  | Data Protection  |  N  |   | 
| % Replicated Objects  | The percentage of total objects that are replicated  |  Y  |  Usage  | Data Protection  |  Y  | Sum\(ReplicatedObjects\)/ Sum\(ObjectCount\)  | 
| Object Lock Enabled Storage Bytes  | The total number of bytes in scope that have Object Lock enabled |  Y  |  Usage  | Data Protection  |  N  |   | 
| % Object Lock Bytes  | The percentage of total bytes in scope that have Object Lock enabled  |  Y  |  Usage  | Data Protection  |  Y  | Sum\(ObjectLockBytes\)/ Sum\(StorageBytes\)  | 
| Object Lock Enabled Object Count  | The total number of objects in scope that have Object Lock enabled |  Y  |  Usage  | Data Protection  |  N  |   | 
| % Object Lock Objects  | The percentage of objects in scope that have Object Lock enabled |  Y  |  Usage  | Data Protection  |  Y  | Sum\(ObjectLockObjects\)/ Sum\(ObjectCount\)  | 
| Incomplete Multipart Upload Storage Bytes  | The total bytes in scope with incomplete multipart uploads  |  Y  |  Usage  |  Cost Efficiency  |  N  |   | 
| % Incomplete MPU Bytes  | The percentage of bytes in scope that are results of incomplete multipart uploads  |  Y  |  Usage  |  Cost Efficiency  |  Y  | Sum\(IncompleteMPUbytes\)/ Sum\(StorageBytes\)  | 
| Incomplete Multipart Upload Object Count  | The number of objects in scope that are incomplete multipart uploads  |  Y  |  Usage  |  Cost Efficiency  |  N  |   | 
| % Incomplete MPU Objects  | The percentage of objects in scope that are incomplete multipart uploads  |  Y  |  Usage  |  Cost Efficiency  |  Y  | Sum\(IncompleteMPUobjects \)/Sum\( ObjectCount\)  | 
| All Requests  | The total number of requests made  |  N  |  Activity |  Summary, Activity |  N  |   | 
| Get Requests  | The total number of GET requests made  |  N  |  Activity |  Activity |  N  |   | 
| Put Requests  | The total number of PUT requests made  |  N  |  Activity |  Activity |  N  |   | 
| Head Requests | The total number of head requests made  |  N  |  Activity |  Activity |  N  |   | 
| Delete Requests  | The total number of delete requests made  |  N  |  Activity |  Activity |  N  |   | 
| List Requests | The total number of list requests made  |  N  |  Activity |  Activity |  N  |   | 
| Post Requests  | The total number of post requests made  |  N  |  Activity |  Activity |  N  |   | 
| Select Requests | The total number of select requests |  N  |  Activity |  Activity |  N  |   | 
| Select Scanned Bytes | The number of select bytes scanned |  N  |  Activity |  Activity |  N  |   | 
| Select Returned Bytes  | The number of select bytes returned  |  N  |  Activity |  Activity |  N  |   | 
| Bytes Downloaded  | The number of bytes in scope that were downloaded  |  N  |  Activity |  Activity |  N  |   | 
| % Retrieval Rate  | The percentage of retrieval rate  |  N  |  Activity |  Activity, Cost Efficiency  |  Y  | Sum\(BytesDownloaded\)/Sum\(StorageBytes\)  | 
| Bytes Uploaded  | The number of bytes uploaded  |  N  |  Activity |  Activity |  N  |   | 
| % Ingest Ratio  | The number of bytes loaded as a percentage of total storage bytes in scope  |  N  |  Activity |  Activity, Cost Efficiency  |  Y  | Sum\(BytesUploaded\) /Sum\(Storage Bytes\)  | 
| 4xx Errors  | The total 4xx errors in scope  |  N  |  Activity |  Activity |  N  |   | 
| 5xx Errors  | The total 5xx errors in scope  |  N  |  Activity |  Activity |  N  |   | 
| Total Errors | The sum of all the \(4xx\) and \(5xx\) errors  |  N  |  Activity |  Activity |  Y  |  Sum\(4xxErrors\) \+ Sum\(5xxErrors\) | 
| % Error Rate | The total errors as a percent of total requests |  N  |  Activity |  Activity |  Y  |  Sum\(TotalErrors\)/ Sum\(TotalRequests\)  | 


# Logging and monitoring in Amazon S3<a name="s3-incident-response"></a>

Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 and your AWS solutions\. You should collect monitoring data from all of the parts of your AWS solution so that you can more easily debug a multi\-point failure if one occurs\. AWS provides several tools for monitoring your Amazon S3 resources and responding to potential incidents:

**Amazon CloudWatch Alarms**  
Using Amazon CloudWatch alarms, you watch a single metric over a time period that you specify\. If the metric exceeds a given threshold, a notification is sent to an Amazon SNS topic or AWS Auto Scaling policy\. CloudWatch alarms do not invoke actions because they are in a particular state\. Rather the state must have changed and been maintained for a specified number of periods\. For more information, see [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)\.

**AWS CloudTrail Logs**  
CloudTrail provides a record of actions taken by a user, role, or an AWS service in Amazon S3\. Using the information collected by CloudTrail, you can determine the request that was made to Amazon S3, the IP address from which the request was made, who made the request, when it was made, and additional details\. For more information, see [Logging Amazon S3 API calls using AWS CloudTrail](cloudtrail-logging.md)\.

**Amazon S3 Access Logs**  
Server access logs provide detailed records about requests that are made to a bucket\. Server access logs are useful for many applications\. For example, access log information can be useful in security and access audits\. For more information, see [Amazon S3 server access logging](ServerLogs.md)\.

**AWS Trusted Advisor**  
Trusted Advisor draws upon best practices learned from serving hundreds of thousands of AWS customers\. Trusted Advisor inspects your AWS environment and then makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps\. All AWS customers have access to five Trusted Advisor checks\. Customers with a Business or Enterprise support plan can view all Trusted Advisor checks\.   
Trusted Advisor has the following Amazon S3\-related checks:  
+ Logging configuration of Amazon S3 buckets\.
+ Security checks for Amazon S3 buckets that have open access permissions\.
+ Fault tolerance checks for Amazon S3 buckets that don't have versioning enabled, or have versioning suspended\.
For more information, see [AWS Trusted Advisor](https://docs.aws.amazon.com/awssupport/latest/user/getting-started.html#trusted-advisor) in the *AWS Support User Guide*\.

The following security best practices also address logging and monitoring:
+ [Identify and audit all your Amazon S3 buckets](security-best-practices.md#audit)
+ [Implement monitoring using AWS monitoring tools](security-best-practices.md#tools)
+ [Enable AWS Config](security-best-practices.md#config)
+ [Enable Amazon S3 server access logging](security-best-practices.md#serverlog)
+ [Use AWS CloudTrail](security-best-practices.md#objectlog)
+ [Monitor AWS security advisories](security-best-practices.md#advisories)


# Replication<a name="replication"></a>

Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets\. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts\. Object may be replicated to a single destination bucket or multiple destination buckets\. Destination buckets can be in different AWS Regions or within the same Region as the source bucket\. 

To enable object replication, you add a replication configuration to your source bucket\. The minimum configuration must provide the following:
+ The destination bucket or buckets where you want Amazon S3 to replicate objects 
+ An AWS Identity and Access Management \(IAM\) role that Amazon S3 can assume to replicate objects on your behalf

Additional configuration options are available\. For more information, see [Additional replication configurations](replication-additional-configs.md)\.

## Why use replication<a name="replication-scenario"></a>

Replication can help you do the following:
+ **Replicate objects while retaining metadata** — You can use replication to make copies of your objects that retain all metadata, such as the original object creation time and version IDs\. This capability is important if you need to ensure that your replica is identical to the source object\.

   
+ **Replicate objects into different storage classes** — You can use replication to directly put objects into S3 Glacier, S3 Glacier Deep Archive, or another storage class in the destination buckets\. You can also replicate your data to the same storage class and use lifecycle policies on the destination buckets to move your objects to a colder storage class as it ages\.

   
+ **Maintain object copies under different ownership** — Regardless of who owns the source object, you can tell Amazon S3 to change replica ownership to the AWS account that owns the destination bucket\. This is referred to as the *owner override* option\. You can use this option to restrict access to object replicas\.

   
+ **Keep objects stored over multiple AWS Regions** — You can set multiple destination buckets across different AWS Regions to ensure geographic differences in where your data is kept\. This could be useful in meeting certain compliance requirements\. 

   
+ **Replicate objects within 15 minutes** — You can use S3 Replication Time Control \(S3 RTC\) to replicate your data in the same AWS Region or across different Regions in a predictable time frame\. S3 RTC replicates 99\.99 percent of new objects stored in Amazon S3 within 15 minutes \(backed by a service level agreement\)\. For more information, see [Meeting compliance requirements using S3 Replication Time Control \(S3 RTC\)](replication-time-control.md)\.

## When to use Cross\-Region Replication<a name="crr-scenario"></a>

S3 Cross\-Region Replication \(CRR\) is used to copy objects across Amazon S3 buckets in different AWS Regions\. CRR can help you do the following:
+ **Meet compliance requirements** — Although Amazon S3 stores your data across multiple geographically distant Availability Zones by default, compliance requirements might dictate that you store data at even greater distances\. Cross\-Region Replication allows you to replicate data between distant AWS Regions to satisfy these requirements\.

    
+ **Minimize latency** — If your customers are in two geographic locations, you can minimize latency in accessing objects by maintaining object copies in AWS Regions that are geographically closer to your users\.

   
+ **Increase operational efficiency** — If you have compute clusters in two different AWS Regions that analyze the same set of objects, you might choose to maintain object copies in those Regions\.

## When to use Same\-Region Replication<a name="srr-scenario"></a>

Same\-Region Replication \(SRR\) is used to copy objects across Amazon S3 buckets in the same AWS Region\. SRR can help you do the following:
+ **Aggregate logs into a single bucket** — If you store logs in multiple buckets or across multiple accounts, you can easily replicate logs into a single, in\-Region bucket\. This allows for simpler processing of logs in a single location\.

   
+ **Configure live replication between production and test accounts** — If you or your customers have production and test accounts that use the same data, you can replicate objects between those multiple accounts, while maintaining  object metadata\.

   
+ **Abide by data sovereignty laws** — You might be required to store multiple copies of your data in separate AWS accounts within a certain Region\. Same\-Region Replication can help you automatically replicate critical data when compliance regulations don't allow the data to leave your country\.

## Requirements for replication<a name="replication-requirements"></a>

Replication requires the following:
+ The source bucket owner must have the source and destination AWS Regions enabled for their account\. The destination bucket owner must have the destination Region\-enabled for their account\. For more information about enabling or disabling an AWS Region, see [AWS Service Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html) in the *AWS General Reference*\.

   
+ Both source and destination buckets must have versioning enabled\. For more information about versioning, see [Using versioning](Versioning.md)\.

   
+ Amazon S3 must have permissions to replicate objects from the source bucket to the destination bucket or buckets on your behalf\. 

   
+ If the owner of the source bucket doesn't own the object in the bucket, the object owner must grant the bucket owner `READ` and `READ_ACP` permissions with the object access control list \(ACL\)\. For more information, see [Managing Access with ACLs](S3_ACLs_UsingACLs.md)\.

   
+ If the source bucket has S3 Object Lock enabled, the destination buckets must also have S3 Object Lock enabled\. For more information, see [Locking objects using S3 Object Lock](object-lock.md)\.

  To enable replication on a bucket that has Object Lock enabled, contact [AWS Support](https://console.aws.amazon.com/support/home)\.

   

If you are setting the replication configuration in a *cross\-account scenario*, where source and destination buckets are owned by different AWS accounts, the following additional requirement applies:
+ The owner of the destination buckets must grant the owner of the source bucket permissions to replicate objects with a bucket policy\. For more information, see [Granting permissions when source and destination buckets are owned by different AWS accounts](setting-repl-config-perm-overview.md#setting-repl-config-crossacct)\.
+ The destination buckets cannot be configured as Requester Pays buckets\. For more information, see [Requester Pays buckets](RequesterPaysBuckets.md)\.

   


# Data protection in Amazon S3<a name="DataDurability"></a>

Amazon S3 provides a highly durable storage infrastructure designed for mission\-critical and primary data storage\. Objects are redundantly stored on multiple devices across multiple facilities in an Amazon S3 Region\. To help better ensure data durability, Amazon S3 `PUT` and `PUT Object copy` operations synchronously store your data across multiple facilities\. After the objects are stored, Amazon S3 maintains their durability by quickly detecting and repairing any lost redundancy\. 

Amazon S3 standard storage offers the following features: 
+ Backed with the [Amazon S3 Service Level Agreement](https://aws.amazon.com/s3/sla/)
+ Designed to provide 99\.999999999% durability and 99\.99% availability of objects over a given year
+ Designed to sustain the concurrent loss of data in two facilities 

Amazon S3 further protects your data using versioning\. You can use versioning to preserve, retrieve, and restore every version of every object that is stored in your Amazon S3 bucket\. With versioning, you can easily recover from both unintended user actions and application failures\. By default, requests retrieve the most recently written version\. You can retrieve older versions of an object by specifying a version of the object in a request\. 

For data protection purposes, we recommend that you protect AWS account credentials and set up individual user accounts with AWS Identity and Access Management \(IAM\), so that each user is given only the permissions necessary to fulfill their job duties\.

If you require FIPS 140\-2 validated cryptographic modules when accessing AWS through a command line interface or an API, use a FIPS endpoint\. For more information about the available FIPS endpoints, see [Federal Information Processing Standard \(FIPS\) 140\-2](http://aws.amazon.com/compliance/fips/)\.

The following security best practices also address data protection in Amazon S3:
+ [Implement server-side encryption](security-best-practices.md#server-side)
+ [Enforce encryption of data in transit](security-best-practices.md#transit)
+ [Consider using Amazon Macie with Amazon S3](security-best-practices.md#macie)
+ [Identify and audit all your Amazon S3 buckets](security-best-practices.md#audit)
+ [Monitor AWS security advisories](security-best-practices.md#advisories)


# Managing S3 Object Lock retention dates<a name="batch-ops-retention-date"></a>

You can use S3 Object Lock with retention dates for your object using two modes: *governance* mode and *compliance* mode\. These retention modes apply different levels of protection to your objects\. You can apply either retention mode to any object version\. Retention dates, like legal holds, prevent an object from being overwritten or deleted\. Amazon S3 stores the *retain until date* specified in the object’s metadata and protects the specified version of the object version until the retention period expires\.

You can use S3 Batch Operations with Object Lock to manage retention dates of many Amazon S3 objects at once\. You specify the list of target objects in your manifest and submit it to Batch Operations for completion\. For more information, see S3 Object Lock [Retention periods](object-lock-overview.md#object-lock-retention-periods)\. 

Your S3 Batch Operations job with retention dates runs *until completion, until cancellation, or until a failure state* is reached\. You should use S3 Batch Operations and S3 Object Lock retention when you want to add, change, or remove the retention date for many objects with a single request\. 

Batch Operations verifies that Object Lock is enabled on your bucket before processing any keys in the manifest\. To perform the operations and validation, Batch Operations needs `s3:GetBucketObjectLockConfiguration` and `s3:PutObjectRetention` permissions in an IAM role to allow Batch Operations to call Object Lock on your behalf\. 

For information about using this operation with the REST API, see `S3PutObjectRetention` in the [CreateJob](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_CreateJob.html) operation in the *Amazon Simple Storage Service API Reference*\. 

For an AWS Command Line Interface example of using this operation, see [Use S3 Batch Operations with S3 Object Lock retention](batch-ops-examples-java.md#batch-ops-examples-java-object-lock-retention)\. For an AWS SDK for Java example, see [Use S3 Batch Operations to set S3 Object Lock retention](batch-ops-examples-cli.md#batch-ops-cli-object-lock-retention-example)\. 

## Restrictions and limitations<a name="batch-ops-retention-date-restrictions"></a>
+ S3 Batch Operations does not make any bucket level changes\.
+ Versioning and S3 Object Lock must be configured on the bucket where the job is performed\.
+ All objects listed in the manifest must be in the same bucket\.
+ The operation works on the latest version of the object unless a version is explicitly specified in the manifest\.
+ You need `s3:PutObjectRetention` permission in your IAM role to use this\.
+ `s3:GetBucketObjectLockConfiguration` IAM permission is required to confirm that Object Lock is enabled for the S3 bucket\. 
+ You can only extend the retention period of objects with `COMPLIANCE` mode retention dates applied, and it cannot be shortened\.


# Handling REST and SOAP errors<a name="HandlingErrors"></a>

**Topics**
+ [The REST error response](UsingRESTError.md)
+ [The SOAP error response](UsingSOAPError.md)
+ [Amazon S3 error best practices](ErrorBestPractices.md)

This section describes REST and SOAP errors and how to handle them\.

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 


# Managing ACLs Using the AWS SDK for \.NET<a name="acl-using-dot-net-sdk"></a>

This section provides examples of configuring ACL grants on Amazon S3 buckets and objects\.

## Example 1: Creating a Bucket and Using a Canned ACL to Set Permissions<a name="set-acl-dot-net-create-resource-example1"></a>

This C\# example creates a bucket\. In the request, the code also specifies a canned ACL that grants the Log Delivery group permissions to write the logs to the bucket\.

 For instructions on creating and testing a working example, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ManagingBucketACLTest
    {
        private const string newBucketName = "*** bucket name ***"; 
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            CreateBucketUseCannedACLAsync().Wait();
        }

        private static async Task CreateBucketUseCannedACLAsync()
        {
            try
            {
                // Add bucket (specify canned ACL).
                PutBucketRequest putBucketRequest = new PutBucketRequest()
                {
                    BucketName = newBucketName,
                    BucketRegion = S3Region.EUW1, // S3Region.US,
                                                  // Add canned ACL.
                    CannedACL = S3CannedACL.LogDeliveryWrite
                };
                PutBucketResponse putBucketResponse = await client.PutBucketAsync(putBucketRequest);

                // Retrieve bucket ACL.
                GetACLResponse getACLResponse = await client.GetACLAsync(new GetACLRequest
                {
                    BucketName = newBucketName
                });
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("S3 error occurred. Exception: " + amazonS3Exception.ToString());
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
            }
        }
    }
}
```

## Example 2: Configure ACL Grants on an Existing Object<a name="set-acl-dot-net-create-resource-example"></a>

This C\# example updates the ACL on an existing object\. The example performs the following tasks:
+ Retrieves an object's ACL\.
+ Clears the ACL by removing all existing permissions\.
+ Adds two permissions: full access to the owner, and WRITE\_ACP to a user identified by email address\.
+ Saves the ACL by sending a `PutAcl` request\.

 For instructions on creating and testing a working example, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ManagingObjectACLTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string keyName = "*** object key name ***"; 
        private const string emailAddress = "*** email address ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;
        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            TestObjectACLTestAsync().Wait();
        }
        private static async Task TestObjectACLTestAsync()
        {
            try
            {
                    // Retrieve the ACL for the object.
                    GetACLResponse aclResponse = await client.GetACLAsync(new GetACLRequest
                    {
                        BucketName = bucketName,
                        Key = keyName
                    });

                    S3AccessControlList acl = aclResponse.AccessControlList;

                    // Retrieve the owner (we use this to re-add permissions after we clear the ACL).
                    Owner owner = acl.Owner;

                    // Clear existing grants.
                    acl.Grants.Clear();

                    // Add a grant to reset the owner's full permission (the previous clear statement removed all permissions).
                    S3Grant fullControlGrant = new S3Grant
                    {
                        Grantee = new S3Grantee { CanonicalUser = owner.Id },
                        Permission = S3Permission.FULL_CONTROL
                        
                    };

                    // Describe the grant for the permission using an email address.
                    S3Grant grantUsingEmail = new S3Grant
                    {
                        Grantee = new S3Grantee { EmailAddress = emailAddress },
                        Permission = S3Permission.WRITE_ACP
                    };
                    acl.Grants.AddRange(new List<S3Grant> { fullControlGrant, grantUsingEmail });
 
                    // Set a new ACL.
                    PutACLResponse response = await client.PutACLAsync(new PutACLRequest
                    {
                        BucketName = bucketName,
                        Key = keyName,
                        AccessControlList = acl
                    });
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("An AmazonS3Exception was thrown. Exception: " + amazonS3Exception.ToString());
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
            }
        }
    }
}
```


# Configuring a static website<a name="HostingWebsiteOnS3Setup"></a>

You can configure an Amazon S3 bucket to function like a website\. This example walks you through the steps of hosting a website on Amazon S3\.

**Note**  
Amazon S3 does not support HTTPS access to the website\. If you want to use HTTPS, you can use Amazon CloudFront to serve a static website hosted on Amazon S3\.  
For more information, see [How do I use CloudFront to serve a static website hosted on Amazon S3?](http://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/) and [ Requiring HTTPS for communication between viewers and CloudFront](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html)\.

**Topics**
+ [Step 1: Create a bucket](#step1-create-bucket-config-as-website)
+ [Step 2: Enable static website hosting](#step2-create-bucket-config-as-website)
+ [Step 3: Edit S3 Block Public Access settings](#step2-edit-block-public-access)
+ [Step 4: Add a bucket policy that makes your bucket content publicly available](#step3-add-bucket-policy-make-content-public)
+ [Step 5: Configure an index document](#step3-upload-index-doc)
+ [Step 6: Test your website endpoint](#step4-test-web-site)
+ [Step 7: Clean up](#getting-started-cleanup-s3-website-overview)

## Step 1: Create a bucket<a name="step1-create-bucket-config-as-website"></a>

The following instructions provide an overview of how to create your buckets for website hosting\. For detailed, step\-by\-step instructions on creating a bucket, see [How Do I Create an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\.

**To create a bucket**

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Choose **Create bucket**\.

1. Enter the **Bucket name** \(for example, `example.com`\)\.

1. Choose the Region where you want to create the bucket\. 

   Choose a Region close to you to minimize latency and costs, or to address regulatory requirements\. The Region that you choose determines your Amazon S3 website endpoint\. For more information, see [Website endpoints](WebsiteEndpoints.md)\.

1. To accept the default settings and create the bucket, choose **Create**\.

## Step 2: Enable static website hosting<a name="step2-create-bucket-config-as-website"></a>

After you create a bucket, you can enable static website hosting for your bucket\. You can create a new bucket or use an existing bucket\.

**To enable static website hosting**

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose the name of the bucket that you want to enable static website hosting for\.

1. Choose **Properties**\.

1. Under **Static website hosting**, choose **Edit**\.

1. Choose **Use this bucket to host a website**\. 

1. Under **Static website hosting**, choose **Enable**\.

1. In **Index document**, enter the file name of the index document, typically `index.html`\. 

   The index document name is case sensitive and must exactly match the file name of the HTML index document that you plan to upload to your S3 bucket\. When you configure a bucket for website hosting, you must specify an index document\. Amazon S3 returns this index document when requests are made to the root domain or any of the subfolders\. For more information, see [Configuring an index document](https://docs.aws.amazon.com/AmazonS3/latest/dev/IndexDocumentSupport.html)\.

1. \(Optional\) If you want to provide your own custom error document for 4XX class errors, in **Error document**, enter the custom error document file name\. 

   The error document name is case sensitive and must exactly match the file name of the HTML error document that you plan to upload to your S3 bucket\. If you don't specify a custom error document and an error occurs, Amazon S3 returns a default HTML error document\. For more information, see [Configuring a custom error document](https://docs.aws.amazon.com/AmazonS3/latest/dev/CustomErrorDocSupport.html)\.

1. \(Optional\) If you want to specify advanced redirection rules, in **Redirection rules**, enter XML to describe the rules\.

   For example, you can conditionally route requests according to specific object key names or prefixes in the request\. For more information, see [Configuring advanced conditional redirects](https://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html#advanced-conditional-redirects)\.

1. Choose **Save changes**\.

   Amazon S3 enables static website hosting for your bucket\. At the bottom of the page, under **Static website hosting**, you see the website endpoint for your bucket\.

1. Under **Static website hosting**, note the **Endpoint**\.

   The **Endpoint** is the Amazon S3 website endpoint for your bucket\. After you finish configuring your bucket as a static website, you can use this endpoint to test your website\.

## Step 3: Edit S3 Block Public Access settings<a name="step2-edit-block-public-access"></a>

By default, Amazon S3 blocks public access to your account and buckets\. If you want to use a bucket to host a static website, you can use these steps to edit your block public access settings\. 

**Warning**  
Before you complete this step, review [Using Amazon S3 Block Public Access](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) to ensure that you understand and accept the risks involved with allowing public access\. When you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket\. We recommend that you block all public access to your buckets\.

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Choose the name of the bucket that you have configured as a static website\.

1. Choose **Permissions**\.

1. Under **Block public access \(bucket settings\)**, choose **Edit**\.

1. Clear **Block *all* public access**, and choose **Save changes**\.
**Warning**  
Before you complete this step, review [Using Amazon S3 Block Public Access](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) to ensure you understand and accept the risks involved with allowing public access\. When you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket\. We recommend that you block all public access to your buckets\.  
![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/edit-public-access-clear.png)

   Amazon S3 turns off Block Public Access settings for your bucket\. To create a public, static website, you might also have to [edit the Block Public Access settings](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access-account.html) for your account before adding a bucket policy\. If account settings for Block Public Access are currently turned on, you see a note under **Block public access \(bucket settings\)**\.

## Step 4: Add a bucket policy that makes your bucket content publicly available<a name="step3-add-bucket-policy-make-content-public"></a>

After you edit S3 Block Public Access settings, you can add a bucket policy to grant public read access to your bucket\. When you grant public read access, anyone on the internet can access your bucket\.

**Important**  
The following policy is an example only and allows full access to the contents of your bucket\. Before you proceed with this step, review [How can I secure the files in my Amazon S3 bucket?](https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/) to ensure that you understand the best practices for securing the files in your S3 bucket and risks involved in granting public access\.

1. Under **Buckets**, choose the name of your bucket\.

1. Choose **Permissions**\.

1. Under **Bucket Policy**, choose **Edit**\.

1. To grant public read access for your website, copy the following bucket policy, and paste it in the **Bucket policy editor**\.

   ```
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Sid": "PublicReadGetObject",
               "Effect": "Allow",
               "Principal": "*",
               "Action": [
                   "s3:GetObject"
               ],
               "Resource": [
                   "arn:aws:s3:::example.com/*"
               ]
           }
       ]
   }
   ```

1. Update the `Resource` to your bucket name\.

   In the preceding example bucket policy, *example\.com* is the bucket name\. To use this bucket policy with your own bucket, you must update this name to match your bucket name\.

1. Choose **Save changes**\.

   A message appears indicating that the bucket policy has been successfully added\.

   If you see an error that says `Policy has invalid resource`, confirm that the bucket name in the bucket policy matches your bucket name\. For information about adding a bucket policy, see [How do I add an S3 bucket policy?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html)

   If you get an error message and cannot save the bucket policy, check your account and bucket Block Public Access settings to confirm that you allow public access to the bucket\.

## Step 5: Configure an index document<a name="step3-upload-index-doc"></a>

When you enable static website hosting for your bucket, you enter the name of the index document \(for example, **index\.html**\)\. After you enable static website hosting for the bucket, you upload an HTML file with this index document name to your bucket\.

**To configure the index document**

1. Create an `index.html` file\.

   If you don't have an `index.html` file, you can use the following HTML to create one:

   ```
   <html xmlns="http://www.w3.org/1999/xhtml" >
   <head>
       <title>My Website Home Page</title>
   </head>
   <body>
     <h1>Welcome to my website</h1>
     <p>Now hosted on Amazon S3!</p>
   </body>
   </html>
   ```

1. Save the index file locally with the *exact* index document name that you entered when you enabled static website hosting for your bucket \(for example, `index.html`\)\.

   The index document file name must exactly match the index document name that you enter in the **Static website hosting** dialog box\. The index document name is case sensitive\. For example, if you enter `index.html` for the **Index document** name in the **Static website hosting** dialog box, your index document file name must also be `index.html` and not `Index.html`\.

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose the name of the bucket that you want to use to host a static website\.

1. Enable static website hosting for your bucket, and enter the exact name of your index document \(for example, `index.html`\)\. For more information, see [Enabling website hosting](EnableWebsiteHosting.md)\.

   After enabling static website hosting, proceed to step 6\. 

1. To upload the index document to your bucket, do one of the following:
   + Drag and drop the index file into the console bucket listing\.
   + Choose **Upload**, and follow the prompts to choose and upload the index file\.

   For step\-by\-step instructions, see [How Do I Upload Files and Folders to an Amazon S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html) in the *Amazon Simple Storage Service Console User Guide*\.

1. \(Optional\) Upload other website content to your bucket\.

## Step 6: Test your website endpoint<a name="step4-test-web-site"></a>

After you configure static website hosting for your bucket, you can test your website endpoint\.

**Note**  
Amazon S3 does not support HTTPS access to the website\. If you want to use HTTPS, you can use Amazon CloudFront to serve a static website hosted on Amazon S3\.  
For more information, see [How do I use CloudFront to serve a static website hosted on Amazon S3?](http://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/) and [Requiring HTTPS for communication between viewers and CloudFront](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html)\.

1. Under **Buckets**, choose the name of your bucket\.

1. Choose **Properties**\.

1. At the bottom of the page, under **Static website hosting**, choose your **Bucket website endpoint**\.

   Your index document opens in a separate browser window\.

You now have a website hosted on Amazon S3\. This website is available at the Amazon S3 website endpoint\. However, you might have a domain, such as `example.com`, that you want to use to serve the content from the website you created\. You might also want to use Amazon S3 root domain support to serve requests for both `http://www.example.com` and `http://example.com`\. This requires additional steps\. For an example, see [Configuring a static website using a custom domain registered with Route 53](website-hosting-custom-domain-walkthrough.md)\. 

## Step 7: Clean up<a name="getting-started-cleanup-s3-website-overview"></a>

If you created your static website only as a learning exercise, delete the AWS resources that you allocated so that you no longer accrue charges\. After you delete your AWS resources, your website is no longer available\. For more information, see [How Do I Delete an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/delete-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\.


# Date Functions<a name="s3-glacier-select-sql-reference-date"></a>

Amazon S3 Select and S3 Glacier Select support the following date functions\.

**Topics**
+ [DATE\_ADD](#s3-glacier-select-sql-reference-date-add)
+ [DATE\_DIFF](#s3-glacier-select-sql-reference-date-diff)
+ [EXTRACT](#s3-glacier-select-sql-reference-extract)
+ [TO\_STRING](#s3-glacier-select-sql-reference-to-string)
+ [TO\_TIMESTAMP](#s3-glacier-select-sql-reference-to-timestamp)
+ [UTCNOW](#s3-glacier-select-sql-reference-utcnow)

## DATE\_ADD<a name="s3-glacier-select-sql-reference-date-add"></a>

Given a date part, a quantity, and a time stamp, returns an updated time stamp by altering the date part by the quantity\.

### Syntax<a name="s3-glacier-select-sql-reference-date-add-syntax"></a>

```
DATE_ADD( date_part, quantity, timestamp )
```

### Parameters<a name="s3-glacier-select-sql-reference-date-add-parameters"></a>

*date\_part*   
Specifies which part of the date to modify\. This can be one of the following:  
+ year
+ month
+ day
+ hour
+ minute
+ second

 *quantity*   
The value to apply to the updated time stamp\. Positive values for quantity add to the time stamp's date\_part, and negative values subtract\.

 *timestamp*   
The target time stamp that the function operates on\.

### Examples<a name="s3-glacier-select-sql-reference-date-add-examples"></a>

```
DATE_ADD(year, 5, `2010-01-01T`)                -- 2015-01-01 (equivalent to 2015-01-01T)
DATE_ADD(month, 1, `2010T`)                     -- 2010-02T (result will add precision as necessary)
DATE_ADD(month, 13, `2010T`)                    -- 2011-02T
DATE_ADD(day, -1, `2017-01-10T`)                -- 2017-01-09 (equivalent to 2017-01-09T)
DATE_ADD(hour, 1, `2017T`)                      -- 2017-01-01T01:00-00:00
DATE_ADD(hour, 1, `2017-01-02T03:04Z`)          -- 2017-01-02T04:04Z
DATE_ADD(minute, 1, `2017-01-02T03:04:05.006Z`) -- 2017-01-02T03:05:05.006Z
DATE_ADD(second, 1, `2017-01-02T03:04:05.006Z`) -- 2017-01-02T03:04:06.006Z
```

## DATE\_DIFF<a name="s3-glacier-select-sql-reference-date-diff"></a>

Given a date part and two valid time stamps, returns the difference in date parts\. The return value is a negative integer when the `date_part` value of `timestamp1` is greater than the `date_part` value of `timestamp2`\. The return value is a positive integer when the `date_part` value of `timestamp1` is less than the `date_part` value of `timestamp2`\.

### Syntax<a name="s3-glacier-select-sql-reference-date-diff-syntax"></a>

```
DATE_DIFF( date_part, timestamp1, timestamp2 )
```

### Parameters<a name="s3-glacier-select-sql-reference-date-diff-parameters"></a>

 *date\_part*   
Specifies which part of the time stamps to compare\. For the definition of `date_part`, see [DATE\_ADD](#s3-glacier-select-sql-reference-date-add)\.

 *timestamp1*   
The first time stamp to compare\.

 *timestamp2*   
The second time stamp to compare\.

### Examples<a name="s3-glacier-select-sql-reference-date-diff-examples"></a>

```
DATE_DIFF(year, `2010-01-01T`, `2011-01-01T`)            -- 1
DATE_DIFF(year, `2010T`, `2010-05T`)                     -- 4 (2010T is equivalent to 2010-01-01T00:00:00.000Z)
DATE_DIFF(month, `2010T`, `2011T`)                       -- 12
DATE_DIFF(month, `2011T`, `2010T`)                       -- -12
DATE_DIFF(day, `2010-01-01T23:00`, `2010-01-02T01:00`) -- 0 (need to be at least 24h apart to be 1 day apart)
```

## EXTRACT<a name="s3-glacier-select-sql-reference-extract"></a>

Given a date part and a time stamp, returns the time stamp's date part value\.

### Syntax<a name="s3-glacier-select-sql-reference-extract-syntax"></a>

```
EXTRACT( date_part FROM timestamp )
```

### Parameters<a name="s3-glacier-select-sql-reference-extract-parameters"></a>

 *date\_part*   
Specifies which part of the time stamps to extract\. This can be one of the following:  
+ year
+ month
+ day
+ hour
+ minute
+ second
+ timezone\_hour
+ timezone\_minute

 *timestamp*   
The target time stamp that the function operates on\.

### Examples<a name="s3-glacier-select-sql-reference-extract-examples"></a>

```
EXTRACT(YEAR FROM `2010-01-01T`)                           -- 2010
EXTRACT(MONTH FROM `2010T`)                                -- 1 (equivalent to 2010-01-01T00:00:00.000Z)
EXTRACT(MONTH FROM `2010-10T`)                             -- 10
EXTRACT(HOUR FROM `2017-01-02T03:04:05+07:08`)             -- 3
EXTRACT(MINUTE FROM `2017-01-02T03:04:05+07:08`)           -- 4
EXTRACT(TIMEZONE_HOUR FROM `2017-01-02T03:04:05+07:08`)    -- 7
EXTRACT(TIMEZONE_MINUTE FROM `2017-01-02T03:04:05+07:08`)  -- 8
```

## TO\_STRING<a name="s3-glacier-select-sql-reference-to-string"></a>

Given a time stamp and a format pattern, returns a string representation of the time stamp in the given format\.

### Syntax<a name="s3-glacier-select-sql-reference-size-syntax"></a>

```
TO_STRING ( timestamp time_format_pattern )
```

### Parameters<a name="s3-glacier-select-sql-reference-size-parameters"></a>

 *timestamp*   
The target time stamp that the function operates on\.

 *time\_format\_pattern*   
A string that has the following special character interpretations\.      
[\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/s3-glacier-select-sql-reference-date.html)

### Examples<a name="s3-glacier-select-sql-reference-size-examples"></a>

```
TO_STRING(`1969-07-20T20:18Z`,  'MMMM d, y')                    -- "July 20, 1969"
TO_STRING(`1969-07-20T20:18Z`, 'MMM d, yyyy')                   -- "Jul 20, 1969"
TO_STRING(`1969-07-20T20:18Z`, 'M-d-yy')                        -- "7-20-69"
TO_STRING(`1969-07-20T20:18Z`, 'MM-d-y')                        -- "07-20-1969"
TO_STRING(`1969-07-20T20:18Z`, 'MMMM d, y h:m a')               -- "July 20, 1969 8:18 PM"
TO_STRING(`1969-07-20T20:18Z`, 'y-MM-dd''T''H:m:ssX')           -- "1969-07-20T20:18:00Z"
TO_STRING(`1969-07-20T20:18+08:00Z`, 'y-MM-dd''T''H:m:ssX')     -- "1969-07-20T20:18:00Z"
TO_STRING(`1969-07-20T20:18+08:00`, 'y-MM-dd''T''H:m:ssXXXX')   -- "1969-07-20T20:18:00+0800"
TO_STRING(`1969-07-20T20:18+08:00`, 'y-MM-dd''T''H:m:ssXXXXX')  -- "1969-07-20T20:18:00+08:00"
```

## TO\_TIMESTAMP<a name="s3-glacier-select-sql-reference-to-timestamp"></a>

Given a string, converts it to a time stamp\. This is the inverse operation of TO\_STRING\.

### Syntax<a name="s3-glacier-select-sql-reference-to-timestamp-syntax"></a>

```
TO_TIMESTAMP ( string )
```

### Parameters<a name="s3-glacier-select-sql-reference-to-timestamp-parameters"></a>

 *string*   
The target string that the function operates on\.

### Examples<a name="s3-glacier-select-sql-reference-to-timestamp-examples"></a>

```
TO_TIMESTAMP('2007T')                         -- `2007T`
TO_TIMESTAMP('2007-02-23T12:14:33.079-08:00') -- `2007-02-23T12:14:33.079-08:00`
```

## UTCNOW<a name="s3-glacier-select-sql-reference-utcnow"></a>

Returns the current time in UTC as a time stamp\.

### Syntax<a name="s3-glacier-select-sql-reference-utcnow-syntax"></a>

```
UTCNOW()
```

### Parameters<a name="s3-glacier-select-sql-reference-utcnow-parameters"></a>

*none*

### Examples<a name="s3-glacier-select-sql-reference-utcnow-examples"></a>

```
UTCNOW() -- 2017-10-13T16:02:11.123Z
```


# Amazon S3 server access logging<a name="ServerLogs"></a>

Server access logging provides detailed records for the requests that are made to a bucket\. Server access logs are useful for many applications\. For example, access log information can be useful in security and access audits\. It can also help you learn about your customer base and understand your Amazon S3 bill\.

**Note**  
Server access logs don't record information about wrong\-region redirect errors for Regions that launched after March 20, 2019\. Wrong\-region redirect errors occur when a request for an object or bucket is made outside the Region in which the bucket exists\. 

**Topics**
+ [How to enable server access logging](#server-access-logging-overview)
+ [Log object key format](#server-log-keyname-format)
+ [How are logs delivered?](#how-logs-delivered)
+ [Best effort server log delivery](#LogDeliveryBestEffort)
+ [Bucket logging status changes take effect over time](#BucketLoggingStatusChanges)
+ [Enabling logging using the console](enable-logging-console.md)
+ [Enabling logging programmatically](enable-logging-programming.md)
+ [Amazon S3 Server Access Log Format](LogFormat.md)
+ [Deleting Amazon S3 log files](deleting-log-files-lifecycle.md)
+ [Using Amazon S3 access logs to identify requests](using-s3-access-logs-to-identify-requests.md)

## How to enable server access logging<a name="server-access-logging-overview"></a>

To track requests for access to your bucket, you can enable server access logging\. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant\. 

There is no extra charge for enabling server access logging on an Amazon S3 bucket, and you are not charged when the logs are PUT to your bucket\. However, any log files that the system delivers to your bucket accrue the usual charges for storage\. You can delete these log files at any time\. Subsequent reads and other requests to these log files are charged normally, as for any other object, including data transfer charges\.

By default, logging is disabled\. When logging is enabled, logs are saved to a bucket in the same AWS Region as the source bucket\. 

To enable logging: 

1. Turn on logging on the Amazon S3 bucket that you want to monitor\. We refer to this bucket as the *source bucket*\. 

1. Grant the Amazon S3 Log Delivery group write permission on the bucket where you want the access logs saved\. We refer to this bucket as the *target bucket*\. 

**Note**  
In Amazon S3 you can grant permission to deliver access logs through bucket access control lists \(ACLs\), but not through bucket policy\.
Adding *deny* conditions to a bucket policy might prevent Amazon S3 from delivering access logs\.
[Default bucket encryption](bucket-encryption.html) on the target bucket *can only be used* if **AES256 \(SSE\-S3\)** is selected\. SSE\-KMS encryption is not supported\. 
S3 Object Lock cannot be enabled on the target bucket\.

To enable log delivery:

1. Provide the name of the target bucket where you want Amazon S3 to save the access logs as objects\. Both the source and target buckets must be in the same AWS Region and owned by the same account\. 

   You can have logs delivered to any bucket that you own that is in the same Region as the source bucket, including the source bucket itself\. But for simpler log management, we recommend that you save access logs in a different bucket\. 

   When your source bucket and target bucket are the same bucket, additional logs are created for the logs that are written to the bucket\. This might not be ideal because it could result in a small increase in your storage billing\. In addition, the extra logs about logs might make it harder to find the log that you are looking for\. If you choose to save access logs in the source bucket, we recommend that you specify a prefix for all log object keys so that the object names begin with a common string and the log objects are easier to identify\. 

   [Key prefixes](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix) are also useful to distinguish between source buckets when multiple buckets log to the same target bucket\.

1. \(Optional\) Assign a prefix to all Amazon S3 log object keys\. The prefix makes it simpler for you to locate the log objects\. For example, if you specify the prefix value `logs/`, each log object that Amazon S3 creates begins with the `logs/` prefix in its key\.

   ```
   logs/2013-11-01-21-32-16-E568B2907131C0C0
   ```

   The key prefix can also help when you delete the logs\. For example, you can set a lifecycle configuration rule for Amazon S3 to delete objects with a specific key prefix\. For more information, see [Deleting Amazon S3 log files](deleting-log-files-lifecycle.md)\.

1. \(Optional\) Set permissions so that others can access the generated logs\. By default, only the bucket owner always has full access to the log objects\. 

For more information about enabling server access logging, see [Enabling logging using the console](enable-logging-console.md) and [Enabling logging programmatically](enable-logging-programming.md)\. 

### <a name="additional-logging-considerations"></a>

## Log object key format<a name="server-log-keyname-format"></a>

Amazon S3 uses the following object key format for the log objects it uploads in the target bucket:

```
TargetPrefixYYYY-mm-DD-HH-MM-SS-UniqueString/
```

In the key, `YYYY`, `mm`, `DD`, `HH`, `MM`, and `SS` are the digits of the year, month, day, hour, minute, and seconds \(respectively\) when the log file was delivered\. These dates and times are in Coordinated Universal Time \(UTC\)\. 

A log file delivered at a specific time can contain records written at any point before that time\. There is no way to know whether all log records for a certain time interval have been delivered or not\. 

The `UniqueString` component of the key is there to prevent overwriting of files\. It has no meaning, and log processing software should ignore it\. 

The trailing slash */* is required to denote the end of the prefix\.

## How are logs delivered?<a name="how-logs-delivered"></a>

Amazon S3 periodically collects access log records, consolidates the records in log files, and then uploads log files to your target bucket as log objects\. If you enable logging on multiple source buckets that identify the same target bucket, the target bucket will have access logs for all those source buckets\. However, each log object reports access log records for a specific source bucket\. 

Amazon S3 uses a special log delivery account, called the *Log Delivery* group, to write access logs\. These writes are subject to the usual access control restrictions\. You must grant the Log Delivery group write permission on the target bucket by adding a grant entry in the bucket's access control list \(ACL\)\. If you use the Amazon S3 console to enable logging on a bucket, the console both enables logging on the source bucket and updates the ACL on the target bucket to grant write permission to the Log Delivery group\.

## Best effort server log delivery<a name="LogDeliveryBestEffort"></a>

Server access log records are delivered on a best effort basis\. Most requests for a bucket that is properly configured for logging result in a delivered log record\. Most log records are delivered within a few hours of the time that they are recorded, but they can be delivered more frequently\. 

The completeness and timeliness of server logging is not guaranteed\. The log record for a particular request might be delivered long after the request was actually processed, or *it might not be delivered at all*\. The purpose of server logs is to give you an idea of the nature of traffic against your bucket\. It is rare to lose log records, but server logging is not meant to be a complete accounting of all requests\. 

It follows from the best\-effort nature of the server logging feature that the usage reports available at the AWS portal \(Billing and Cost Management reports on the [AWS Management Console](https://console.aws.amazon.com/)\) might include one or more access requests that do not appear in a delivered server log\. 

## Bucket logging status changes take effect over time<a name="BucketLoggingStatusChanges"></a>

Changes to the logging status of a bucket take time to actually affect the delivery of log files\. For example, if you enable logging for a bucket, some requests made in the following hour might be logged, while others might not\. If you change the target bucket for logging from bucket A to bucket B, some logs for the next hour might continue to be delivered to bucket A, while others might be delivered to the new target bucket B\. In all cases, the new settings eventually take effect without any further action on your part\. 


# Deleting multiple objects using the AWS SDK for PHP<a name="DeletingMultipleObjectsUsingPHPSDK"></a>

This topic shows how to use classes from version 3 of the AWS SDK for PHP to delete multiple objects from versioned and non\-versioned Amazon S3 buckets\. For more information about versioning, see [Using versioning](Versioning.md)\.

 This topic assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\.

**Example Deleting multiple objects from a non\-versioned bucket**  
The following PHP example uses the `deleteObjects()` method to delete multiple objects from a bucket that is not version\-enabled\.  
 For information about running the PHP examples in this guide, see [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\.   

```
<?php

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// 1. Create a few objects.
for ($i = 1; $i <= 3; $i++) {
    $s3->putObject([
        'Bucket' => $bucket,
        'Key'    => "key{$i}",
        'Body'   => "content {$i}",
    ]);
}

// 2. List the objects and get the keys.
$keys = $s3->listObjects([
    'Bucket' => $bucket
]); 

// 3. Delete the objects.
foreach ($keys['Contents'] as $key)
{
    $s3->deleteObjects([
        'Bucket'  => $bucket,
        'Delete' => [
            'Objects' => [
                [
                    'Key' => $key['Key']
                ]
            ]
        ]
    ]);
}
```

**Example Deleting multiple objects from a version\-enabled bucket**  
The following PHP example uses the `deleteObjects()` method to delete multiple objects from a version\-enabled bucket\.  
 For information about running the PHP examples in this guide, see [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\.   

```
<?php

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// 1. Enable object versioning for the bucket.
$s3->putBucketVersioning([
    'Bucket' => $bucket,
    'VersioningConfiguration' => [ 
        'Status' => 'Enabled'
    ]
]);

// 2. Create a few versions of an object.
for ($i = 1; $i <= 3; $i++) {
    $s3->putObject([
        'Bucket' => $bucket,
        'Key'    => $keyname,
        'Body'   => "content {$i}",
    ]);
}

// 3. List the objects versions and get the keys and version IDs.
$versions = $s3->listObjectVersions(['Bucket' => $bucket]);

// 4. Delete the object versions.
$deletedResults = 'The following objects were deleted successfully:' . PHP_EOL;
$deleted = false;
$errorResults = 'The following objects could not be deleted:' . PHP_EOL;
$errors = false;

foreach ($versions['Versions'] as $version)
{
    $result = $s3->deleteObjects([
        'Bucket'  => $bucket,
        'Delete' => [
            'Objects' => [
                [
                    'Key' => $version['Key'],
                    'VersionId' => $version['VersionId']
                ]
            ]
        ]
    ]);

    if (isset($result['Deleted']))
    {
        $deleted = true;

        $deletedResults .= "Key: {$result['Deleted'][0]['Key']}, " . 
            "VersionId: {$result['Deleted'][0]['VersionId']}" . PHP_EOL;
    }

    if (isset($result['Errors']))
    {
        $errors = true;

        $errorResults .= "Key: {$result['Errors'][0]['Key']}, " . 
            "VersionId: {$result['Errors'][0]['VersionId']}, " .
            "Message: {$result['Errors'][0]['Message']}" . PHP_EOL;
    }
}

if ($deleted)
{
    echo $deletedResults;
}

if ($errors)
{
    echo $errorResults;
}

// 5. Suspend object versioning for the bucket.
$s3->putBucketVersioning([
    'Bucket' => $bucket,
    'VersioningConfiguration' => [ 
        'Status' => 'Suspended'
    ]
]);
```

## Related resources<a name="RelatedResources-DeletingMultipleObjectsUsingPHPSDK"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Managing websites with the REST API<a name="ConfigWebSiteREST"></a>

**Topics**

You can use the AWS Management Console or the AWS SDK to configure a bucket as a website\. However, if your application requires it, you can send REST requests directly\. For more information, see the following sections in the Amazon Simple Storage Service API Reference\.
+ [PUT Bucket website](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTwebsite.html)
+ [GET Bucket website](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETwebsite.html)
+ [DELETE Bucket website](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEwebsite.html)


# Making requests using AWS account or IAM user credentials \- AWS SDK for PHP<a name="AuthUsingAcctOrUserCredPHP3"></a>

This section explains how to use a class from version 3 of the AWS SDK for PHP to send authenticated requests using your AWS account or IAM user credentials\. It assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\. 

The following PHP example shows how the client makes a request using your security credentials to list all of the buckets for your account\. 

**Example**  

```
 
require 'vendor/autoload.php';

use Aws\Sts\StsClient;
use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';

$s3 = new S3Client([
    'region' => 'us-east-1',
    'version' => 'latest',
]);

// Retrieve the list of buckets.
$result = $s3->listBuckets();

try {
    // Retrieve a paginator for listing objects.
    $objects = $s3->getPaginator('ListObjects', [
        'Bucket' => $bucket
    ]);

    echo "Keys retrieved!" . PHP_EOL;
    
    // Print the list of objects to the page.
    foreach ($objects as $object) {
        echo $object['Key'] . PHP_EOL;
    }
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}
```

**Note**  
You can create the `S3Client` client without providing your security credentials\. Requests sent using this client are anonymous requests, without a signature\. Amazon S3 returns an error if you send anonymous requests for a resource that is not publicly available\. For more information, see [Creating Anonymous Clients](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_credentials_anonymous.html) in the [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)\.

For working examples, see [Operations on objects](ObjectOperations.md)\. You can test these examples using your AWS account or IAM user credentials\. 

For an example of listing object keys in a bucket, see [Listing Keys Using the AWS SDK for PHP](ListingObjectKeysUsingPHP.md)\. 

## Related resources<a name="RelatedResources-AuthUsingAcctOrUserCredPHP3-related-resources"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Using the AWS PHP SDK for multipart upload \(low\-level API\)<a name="usingLLmpuPHP"></a>

**Topics**
+ [Upload a file in multiple parts using the PHP SDK low\-level API](LLuploadFilePHP.md)
+ [List multipart uploads using the low\-level AWS SDK for PHP API](LLlistMPuploadsPHP.md)
+ [Abort a multipart upload](LLAbortMPUphp.md)

The AWS SDK for PHP exposes a low\-level API that closely resembles the Amazon S3 REST API for multipart upload \(see [Using the REST API for multipart upload](UsingRESTAPImpUpload.md) \)\. Use the low\-level API when you need to pause and resume multipart uploads, vary part sizes during the upload, or if you do not know the size of the data in advance\. Use the AWS SDK for PHP high\-level abstractions \(see [Using the AWS PHP SDK for multipart upload](usingHLmpuPHP.md)\) whenever you don't have these requirements\.


# Selecting content from objects using other SDKs<a name="SelectObjectContentUsingOtherSDKs"></a>

You can select the contents of an object using Amazon S3 Select using other SDKs\. For more information, see the following:
+ Python: [Using the AWS SDK for Python \(Boto\)](UsingTheBotoAPI.md)\.
+ Ruby: [Using the AWS SDK for Ruby \- Version 3](UsingTheMPRubyAPI.md)\.


# Making requests using federated user temporary credentials \- AWS SDK for \.NET<a name="AuthUsingTempFederationTokenDotNet"></a>

You can provide temporary security credentials for your federated users and applications so that they can send authenticated requests to access your AWS resources\. When requesting these temporary credentials, you must provide a user name and an IAM policy that describes the resource permissions that you want to grant\. By default, the duration of a session is one hour\. You can explicitly set a different duration value when requesting the temporary security credentials for federated users and applications\. For information about sending authenticated requests, see [Making requests](MakingRequests.md)\.

**Note**  
When requesting temporary security credentials for federated users and applications, for added security, we suggest that you use a dedicated IAM user with only the necessary access permissions\. The temporary user you create can never get more permissions than the IAM user who requested the temporary security credentials\. For more information, see [ AWS Identity and Access Management FAQs ](https://aws.amazon.com/iam/faqs/#What_are_the_best_practices_for_using_temporary_security_credentials)\.

You do the following:
+ Create an instance of the AWS Security Token Service client, `AmazonSecurityTokenServiceClient` class\. For information about providing credentials, see [Using the AWS SDK for \.NET](UsingTheMPDotNetAPI.md)\.
+ Start a session by calling the `GetFederationToken` method of the STS client\. You need to provide session information, including the user name and an IAM policy that you want to attach to the temporary credentials\. Optionally, you can provide a session duration\. This method returns your temporary security credentials\.
+ Package the temporary security credentials in an instance of the `SessionAWSCredentials` object\. You use this object to provide the temporary security credentials to your Amazon S3 client\.
+ Create an instance of the `AmazonS3Client` class by passing the temporary security credentials\. You use this client to send requests to Amazon S3\. If you send requests using expired credentials, Amazon S3 returns an error\. 

**Example**  
The following C\# example lists the keys in the specified bucket\. In the example, you obtain temporary security credentials for a two\-hour session for your federated user \(User1\), and use the credentials to send authenticated requests to Amazon S3\.   
+ For this exercise, you create an IAM user with minimal permissions\. Using the credentials of this IAM user, you request temporary credentials for others\. This example lists only the objects in a specific bucket\. Create an IAM user with the following policy attached: 

  ```
   1. {
   2.   "Statement":[{
   3.       "Action":["s3:ListBucket",
   4.         "sts:GetFederationToken*"
   5.       ],
   6.       "Effect":"Allow",
   7.       "Resource":"*"
   8.     }
   9.   ]
  10. }
  ```

  The policy allows the IAM user to request temporary security credentials and access permission only to list your AWS resources\. For more information about how to create an IAM user, see [Creating Your First IAM User and Administrators Group](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html) in the *IAM User Guide*\. 
+ Use the IAM user security credentials to test the following example\. The example sends authenticated request to Amazon S3 using temporary security credentials\. The example specifies the following policy when requesting temporary security credentials for the federated user \(User1\), which restricts access to listing objects in a specific bucket \(`YourBucketName`\)\. You must update the policy and provide your own existing bucket name\.

  ```
   1. {
   2.   "Statement":[
   3.     {
   4.       "Sid":"1",
   5.       "Action":["s3:ListBucket"],
   6.       "Effect":"Allow", 
   7.       "Resource":"arn:aws:s3:::YourBucketName"
   8.     }
   9.   ]
  10. }
  ```
+   
**Example**  

  Update the following sample and provide the bucket name that you specified in the preceding federated user access policy\. For instructions on how to create and test a working example, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

  ```
  using Amazon;
  using Amazon.Runtime;
  using Amazon.S3;
  using Amazon.S3.Model;
  using Amazon.SecurityToken;
  using Amazon.SecurityToken.Model;
  using System;
  using System.Collections.Generic;
  using System.Threading.Tasks;
  
  namespace Amazon.DocSamples.S3
  {
      class TempFederatedCredentialsTest
      {
          private const string bucketName = "*** bucket name ***";
          // Specify your bucket region (an example region is shown).
          private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
          private static IAmazonS3 client;
  
          public static void Main()
          {
              ListObjectsAsync().Wait();
          }
  
          private static async Task ListObjectsAsync()
          {
              try
              {
                  Console.WriteLine("Listing objects stored in a bucket");
                  // Credentials use the default AWS SDK for .NET credential search chain. 
                  // On local development machines, this is your default profile.
                  SessionAWSCredentials tempCredentials =
                      await GetTemporaryFederatedCredentialsAsync();
  
                  // Create a client by providing temporary security credentials.
                  using (client = new AmazonS3Client(bucketRegion))
                  {
                      ListObjectsRequest listObjectRequest = new ListObjectsRequest();
                      listObjectRequest.BucketName = bucketName;
  
                      ListObjectsResponse response = await client.ListObjectsAsync(listObjectRequest);
                      List<S3Object> objects = response.S3Objects;
                      Console.WriteLine("Object count = {0}", objects.Count);
  
                      Console.WriteLine("Press any key to continue...");
                      Console.ReadKey();
                  }
              }
              catch (AmazonS3Exception e)
              {
                  Console.WriteLine("Error encountered ***. Message:'{0}' when writing an object", e.Message);
              }
              catch (Exception e)
              {
                  Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
              }
          }
  
          private static async Task<SessionAWSCredentials> GetTemporaryFederatedCredentialsAsync()
          {
              AmazonSecurityTokenServiceConfig config = new AmazonSecurityTokenServiceConfig();
              AmazonSecurityTokenServiceClient stsClient =
                  new AmazonSecurityTokenServiceClient(
                                               config);
  
              GetFederationTokenRequest federationTokenRequest =
                                       new GetFederationTokenRequest();
              federationTokenRequest.DurationSeconds = 7200;
              federationTokenRequest.Name = "User1";
              federationTokenRequest.Policy = @"{
                 ""Statement"":
                 [
                   {
                     ""Sid"":""Stmt1311212314284"",
                     ""Action"":[""s3:ListBucket""],
                     ""Effect"":""Allow"",
                     ""Resource"":""arn:aws:s3:::" + bucketName + @"""
                    }
                 ]
               }
              ";
  
              GetFederationTokenResponse federationTokenResponse =
                          await stsClient.GetFederationTokenAsync(federationTokenRequest);
              Credentials credentials = federationTokenResponse.Credentials;
  
              SessionAWSCredentials sessionCredentials =
                  new SessionAWSCredentials(credentials.AccessKeyId,
                                            credentials.SecretAccessKey,
                                            credentials.SessionToken);
              return sessionCredentials;
          }
      }
  }
  ```

## Related resources<a name="RelatedResources006"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Managing object tags using the console<a name="tagging-manage-console"></a>

You can use the Amazon S3 console to add tags to new objects when you upload them or you can add them to existing objects\. For instructions on how to add tags to objects using the Amazon S3 console, see [Adding Object Tags](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-object-tags.html) in the Amazon Simple Storage Service Console User Guide\. 


# Creating and managing Amazon S3 on Outposts bucket<a name="S3OutpostsBucketJava"></a>

You can use the SDK for Java to create and manage your S3 on Outposts buckets\. From these examples, you can create and get an Outposts bucket, list buckets for an Outpost, create and manage access points, lifecycleconfiguration, and policy for the Outpost bucket\. 

**Topics**
+ [Configuring S3 control client for an Amazon S3 on Outposts](#S3OutpostsCongfigureS3ControlClientJava)
+ [Create an Amazon S3 on Outposts bucket](#S3OutpostsCreateBucketJava)
+ [Get the Amazon S3 on Outposts bucket](#S3OutpostsGetBucketJava)
+ [Get a list of buckets in an Outpost](#S3OutpostsListRegionalBucketJava)
+ [Create an access point for an Amazon S3 on Outposts bucket](#S3OutpostsCreateAccessPointJava)
+ [Gets an access point for an Amazon S3 on Outposts bucket](#S3OutpostsGetAccessPointJava)
+ [:List access points for an AWS Outpost](#S3OutpostsListAccessPointJava)
+ [Add a lifecycle configuration for your Outposts bucket](#S3OutpostsPutBucketLifecycleConfigurationJava)
+ [Gets a lifecycle configuration for an Amazon S3 on Outposts bucket](#S3OutpostsGetBucketLifecycleConfigurationJava)
+ [Put a policy on your Outposts bucket](#S3OutpostsPutBucketPolicyJava)
+ [Gets a policy for an Amazon S3 on Outposts bucket](#S3OutpostsGetBucketPolicyJava)
+ [Put a policy on your Outposts access point](#S3OutpostsPutAccessPointPolicyJava)
+ [Gets a policy for an Amazon S3 on Outposts access point](#S3OutpostsGetAccessPointPolicyJava)
+ [Create an endpoint on an Outpost](#S3OutpostsCreateEndpointJava)
+ [Delete an endpoint on an Outpost](#S3OutpostsDeleteEndpointJava)
+ [List endpoints for Amazon S3 on Outposts Outpost](#S3OutpostsListEndpointsJava)

## Configuring S3 control client for an Amazon S3 on Outposts<a name="S3OutpostsCongfigureS3ControlClientJava"></a>

The following example configures the S3 control client for S3 on Outposts using the SDK for Java\. 

```
import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;

public AWSS3Control createS3ControlClient() {

    String accessKey = AWSAccessKey;
    String secretKey = SecretAccessKey;
    BasicAWSCredentials awsCreds = new BasicAWSCredentials(accessKey, secretKey);

    return AWSS3ControlClient.builder().enableUseArnRegion()
            .withCredentials(new AWSStaticCredentialsProvider(awsCreds))
            .build();

}
```

## Create an Amazon S3 on Outposts bucket<a name="S3OutpostsCreateBucketJava"></a>

The following example creates an S3 on Outposts `s3-outposts:CreateBucket` using the SDK for Java\. 

```
import com.amazonaws.services.s3control.model.*;

public String createBucket(String bucketName) {

    CreateBucketRequest reqCreateBucket = new CreateBucketRequest()
            .withBucket(bucketName)
            .withOutpostId(OutpostId)
            .withCreateBucketConfiguration(new CreateBucketConfiguration());

    CreateBucketResult respCreateBucket = s3ControlClient.createBucket(reqCreateBucket);
    System.out.printf("CreateBucket Response: %s%n", respCreateBucket.toString());

    return respCreateBucket.getBucketArn();

}
```

## Get the Amazon S3 on Outposts bucket<a name="S3OutpostsGetBucketJava"></a>

The following S3 on Outposts example gets a bucket using the SDK for Java\. 

```
import com.amazonaws.services.s3control.model.*;

public void getBucket(String bucketArn) {

    GetBucketRequest reqGetBucket = new GetBucketRequest()
            .withBucket(bucketArn)
            .withAccountId(AccountId);

    GetBucketResult respGetBucket = s3ControlClient.getBucket(reqGetBucket);
    System.out.printf("GetBucket Response: %s%n", respGetBucket.toString());

}
```



## Get a list of buckets in an Outpost<a name="S3OutpostsListRegionalBucketJava"></a>

The following SDK for Java example gets a list of buckets in an Outpost\. 

```
import com.amazonaws.services.s3control.model.*;

public void listRegionalBuckets() {

    ListRegionalBucketsRequest reqListBuckets = new ListRegionalBucketsRequest()
            .withAccountId(AccountId)
            .withOutpostId(OutpostId);

    ListRegionalBucketsResult respListBuckets = s3ControlClient.listRegionalBuckets(reqListBuckets);
    System.out.printf("ListRegionalBuckets Response: %s%n", respListBuckets.toString());

}
```



## Create an access point for an Amazon S3 on Outposts bucket<a name="S3OutpostsCreateAccessPointJava"></a>

The following SDK for Java example creates an access point for an Outposts bucket\.

```
import com.amazonaws.services.s3control.model.*;

public String createAccessPoint(String bucketArn, String accessPointName) {

    CreateAccessPointRequest reqCreateAP = new CreateAccessPointRequest()
            .withAccountId(AccountId)
            .withBucket(bucketArn)
            .withName(accessPointName)
            .withVpcConfiguration(new VpcConfiguration().withVpcId("vpc-12345"));

    CreateAccessPointResult respCreateAP = s3ControlClient.createAccessPoint(reqCreateAP);
    System.out.printf("CreateAccessPoint Response: %s%n", respCreateAP.toString());

    return respCreateAP.getAccessPointArn();

}
```



## Gets an access point for an Amazon S3 on Outposts bucket<a name="S3OutpostsGetAccessPointJava"></a>

The following SDK for Java example gets an access point for an Outposts bucket\.

```
import com.amazonaws.services.s3control.model.*;

public void getAccessPoint(String accessPointArn) {

    GetAccessPointRequest reqGetAP = new GetAccessPointRequest()
            .withAccountId(AccountId)
            .withName(accessPointArn);

    GetAccessPointResult respGetAP = s3ControlClient.getAccessPoint(reqGetAP);
    System.out.printf("GetAccessPoint Response: %s%n", respGetAP.toString());

}
```



## :List access points for an AWS Outpost<a name="S3OutpostsListAccessPointJava"></a>

The following SDK for Java example List access points for an Outposts bucket\.

```
import com.amazonaws.services.s3control.model.*;

public void listAccessPoints(String bucketArn) {

    ListAccessPointsRequest reqListAPs = new ListAccessPointsRequest()
            .withAccountId(AccountId)
            .withBucket(bucketArn);

    ListAccessPointsResult respListAPs = s3ControlClient.listAccessPoints(reqListAPs);
    System.out.printf("ListAccessPoints Response: %s%n", respListAPs.toString());

}
```



## Add a lifecycle configuration for your Outposts bucket<a name="S3OutpostsPutBucketLifecycleConfigurationJava"></a>

The following SDK for Java example puts an lifecycle configruations for an Outposts bucket where all objects with the flagged prefix and tags expire after 10 days\.

```
import com.amazonaws.services.s3control.model.*;

public void putBucketLifecycleConfiguration(String bucketArn) {

    S3Tag tag1 = new S3Tag().withKey("mytagkey1").withValue("mytagvalue1");
    S3Tag tag2 = new S3Tag().withKey("mytagkey2").withValue("mytagvalue2");

    LifecycleRuleFilter lifecycleRuleFilter = new LifecycleRuleFilter()
            .withAnd(new LifecycleRuleAndOperator()
                    .withPrefix("myprefix")
                    .withTags(tag1, tag2));

    LifecycleExpiration lifecycleExpiration = new LifecycleExpiration()
            .withExpiredObjectDeleteMarker(false)
            .withDays(1);

    LifecycleRule lifecycleRule = new LifecycleRule()
            .withStatus("Enabled")
            .withFilter(lifecycleRuleFilter)
            .withExpiration(lifecycleExpiration)
            .withID("id-1");


    LifecycleConfiguration lifecycleConfiguration = new LifecycleConfiguration()
            .withRules(lifecycleRule);

    PutBucketLifecycleConfigurationRequest reqPutBucketLifecycle = new PutBucketLifecycleConfigurationRequest()
            .withAccountId(AccountId)
            .withBucket(bucketArn)
            .withLifecycleConfiguration(lifecycleConfiguration);

    PutBucketLifecycleConfigurationResult respPutBucketLifecycle = s3ControlClient.putBucketLifecycleConfiguration(reqPutBucketLifecycle);
    System.out.printf("PutBucketLifecycleConfiguration Response: %s%n", respPutBucketLifecycle.toString());


}
```



## Gets a lifecycle configuration for an Amazon S3 on Outposts bucket<a name="S3OutpostsGetBucketLifecycleConfigurationJava"></a>

The following SDK for Java example gets an access point for an Outposts bucket\.

```
import com.amazonaws.services.s3control.model.*;

public void getBucketLifecycleConfiguration(String bucketArn) {

    GetBucketLifecycleConfigurationRequest reqGetBucketLifecycle = new GetBucketLifecycleConfigurationRequest()
            .withAccountId(AccountId)
            .withBucket(bucketArn);

    GetBucketLifecycleConfigurationResult respGetBucketLifecycle = s3ControlClient.getBucketLifecycleConfiguration(reqGetBucketLifecycle);
    System.out.printf("GetBucketLifecycleConfiguration Response: %s%n", respGetBucketLifecycle.toString());

}
```



## Put a policy on your Outposts bucket<a name="S3OutpostsPutBucketPolicyJava"></a>

The following SDK for Java example puts policy for an Outposts bucket\.

```
import com.amazonaws.services.s3control.model.*;

public void putBucketPolicy(String bucketArn) {

    String policy = "{\"Version\":\"2012-10-17\",\"Id\":\"testBucketPolicy\",\"Statement\":[{\"Sid\":\"st1\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"" + AccountId+ "\"},\"Action\":\"s3-outposts:*\",\"Resource\":\"" + bucketArn + "\"}]}";

    PutBucketPolicyRequest reqPutBucketPolicy = new PutBucketPolicyRequest()
            .withAccountId(AccountId)
            .withBucket(bucketArn)
            .withPolicy(policy);

    PutBucketPolicyResult respPutBucketPolicy = s3ControlClient.putBucketPolicy(reqPutBucketPolicy);
    System.out.printf("PutBucketPolicy Response: %s%n", respPutBucketPolicy.toString());

}
```



## Gets a policy for an Amazon S3 on Outposts bucket<a name="S3OutpostsGetBucketPolicyJava"></a>

The following SDK for Java example gets a policy for an Outposts bucket\.



```
import com.amazonaws.services.s3control.model.*;

public void getBucketPolicy(String bucketArn) {

    GetBucketPolicyRequest reqGetBucketPolicy = new GetBucketPolicyRequest()
            .withAccountId(AccountId)
            .withBucket(bucketArn);

    GetBucketPolicyResult respGetBucketPolicy = s3ControlClient.getBucketPolicy(reqGetBucketPolicy);
    System.out.printf("GetBucketPolicy Response: %s%n", respGetBucketPolicy.toString());

}
```

## Put a policy on your Outposts access point<a name="S3OutpostsPutAccessPointPolicyJava"></a>

The following SDK for Java example puts policy for an Outposts bucket\.

```
import com.amazonaws.services.s3control.model.*;

public void putAccessPointPolicy(String accessPointArn) {

    String policy = "{\"Version\":\"2012-10-17\",\"Id\":\"testAccessPointPolicy\",\"Statement\":[{\"Sid\":\"st1\",\"Effect\":\"Allow\",\"Principal\":{\"AWS\":\"" + AccountId + "\"},\"Action\":\"s3-outposts:*\",\"Resource\":\"" + accessPointArn + "\"}]}";

    PutAccessPointPolicyRequest reqPutAccessPointPolicy = new PutAccessPointPolicyRequest()
            .withAccountId(AccountId)
            .withName(accessPointArn)
            .withPolicy(policy);

    PutAccessPointPolicyResult respPutAccessPointPolicy = s3ControlClient.putAccessPointPolicy(reqPutAccessPointPolicy);
    System.out.printf("PutAccessPointPolicy Response: %s%n", respPutAccessPointPolicy.toString());
    printWriter.printf("PutAccessPointPolicy Response: %s%n", respPutAccessPointPolicy.toString());

}
```



## Gets a policy for an Amazon S3 on Outposts access point<a name="S3OutpostsGetAccessPointPolicyJava"></a>

The following SDK for Java example gets a policy for an Outposts bucket\.



```
import com.amazonaws.services.s3control.model.*;

public void getAccessPointPolicy(String accessPointArn) {

    GetAccessPointPolicyRequest reqGetAccessPointPolicy = new GetAccessPointPolicyRequest()
            .withAccountId(AccountId)
            .withName(accessPointArn);

    GetAccessPointPolicyResult respGetAccessPointPolicy = s3ControlClient.getAccessPointPolicy(reqGetAccessPointPolicy);
    System.out.printf("GetAccessPointPolicy Response: %s%n", respGetAccessPointPolicy.toString());
    printWriter.printf("GetAccessPointPolicy Response: %s%n", respGetAccessPointPolicy.toString());

}
```

## Create an endpoint on an Outpost<a name="S3OutpostsCreateEndpointJava"></a>

The following SDK for Java example creates an endpoint for an Outpost\.

```
import com.amazonaws.services.s3outposts.AmazonS3Outposts;
import com.amazonaws.services.s3outposts.AmazonS3OutpostsClientBuilder;
import com.amazonaws.services.s3outposts.model.CreateEndpointRequest;
import com.amazonaws.services.s3outposts.model.CreateEndpointResult;

public void createEndpoint() {
    AmazonS3Outposts s3OutpostsClient = AmazonS3OutpostsClientBuilder
                .standard().build();
                
    CreateEndpointRequest createEndpointRequest = new CreateEndpointRequest()
                .withOutpostId("op-0d79779cef3c30a40")
                .withSubnetId("subnet-8c7a57c5")
                .withSecurityGroupId("sg-ab19e0d1");
    CreateEndpointResult createEndpointResult = s3OutpostsClient.createEndpoint(createEndpointRequest);
    System.out.println("Endpoint is created and its arn is " + createEndpointResult.getEndpointArn());
}
```

## Delete an endpoint on an Outpost<a name="S3OutpostsDeleteEndpointJava"></a>

The following SDK for Java example delete an endpoint for an Outpost\.

```
import com.amazonaws.arn.Arn;
import com.amazonaws.services.s3outposts.AmazonS3Outposts;
import com.amazonaws.services.s3outposts.AmazonS3OutpostsClientBuilder;
import com.amazonaws.services.s3outposts.model.DeleteEndpointRequest;

public void deleteEndpoint(String endpointArnInput) {
    String outpostId = "op-0d79779cef3c30a40";
    AmazonS3Outposts s3OutpostsClient = AmazonS3OutpostsClientBuilder
                .standard().build();
                
    Arn endpointArn = Arn.fromString(endpointArnInput);
    String[] resourceParts = endpointArn.getResource().getResource().split("/");
    String endpointId = resourceParts[resourceParts.length - 1];
    DeleteEndpointRequest deleteEndpointRequest = new DeleteEndpointRequest()
                .withEndpointId(endpointId)
                .withOutpostId(outpostId);
    s3OutpostsClient.deleteEndpoint(deleteEndpointRequest);
    System.out.println("Endpoint with id " + endpointId + " is deleted.");
}
```

## List endpoints for Amazon S3 on Outposts Outpost<a name="S3OutpostsListEndpointsJava"></a>

The following SDK for Java example lists endpoints for an Outpost\.

```
import com.amazonaws.services.s3outposts.AmazonS3Outposts;
import com.amazonaws.services.s3outposts.AmazonS3OutpostsClientBuilder;
import com.amazonaws.services.s3outposts.model.ListEndpointsRequest;
import com.amazonaws.services.s3outposts.model.ListEndpointsResult;

public void listEndpoints() {
    AmazonS3Outposts s3OutpostsClient = AmazonS3OutpostsClientBuilder
                .standard().build();
                
    ListEndpointsRequest listEndpointsRequest = new ListEndpointsRequest();
    ListEndpointsResult listEndpointsResult = s3OutpostsClient.listEndpoints(listEndpointsRequest);
    System.out.println("List endpoints result is " + listEndpointsResult);
}
```




# Aggregate Functions \(Amazon S3 Select only\)<a name="s3-glacier-select-sql-reference-aggregate"></a>

Amazon S3 Select supports the following aggregate functions\.

**Note**  
S3 Glacier Select does not support aggregate functions\.


| Function | Argument Type | Return Type | 
| --- | --- | --- | 
| AVG\(expression\) | INT, FLOAT, DECIMAL | DECIMAL for an INT argument, FLOAT for a floating\-point argument; otherwise the same as the argument data type\. | 
| COUNT |  \-  | INT | 
| MAX\(expression\) | INT, DECIMAL | Same as the argument type\. | 
| MIN\(expression\) | INT, DECIMAL | Same as the argument type\. | 
| SUM\(expression\) | INT, FLOAT, DOUBLE, DECIMAL | INT for INT argument, FLOAT for a floating\-point argument; otherwise, the same as the argument data type\. | 


# Using BitTorrent with Amazon S3<a name="S3Torrent"></a>

**Topics**
+ [How you are charged for BitTorrent delivery](S3TorrentCharge.md)
+ [Using BitTorrent to retrieve objects stored in Amazon S3](S3TorrentRetrieve.md)
+ [Publishing content using Amazon S3 and BitTorrent](S3TorrentPublish.md)

BitTorrent is an open, peer\-to\-peer protocol for distributing files\. You can use the BitTorrent protocol to retrieve any publicly\-accessible object in Amazon S3\. This section describes why you might want to use BitTorrent to distribute your data out of Amazon S3 and how to do so\.

Amazon S3 supports the BitTorrent protocol so that developers can save costs when distributing content at high scale\. Amazon S3 is useful for simple, reliable storage of any data\. The default distribution mechanism for Amazon S3 data is via client/server download\. In client/server distribution, the entire object is transferred point\-to\-point from Amazon S3 to every authorized user who requests that object\. While client/server delivery is appropriate for a wide variety of use cases, it is not optimal for everybody\. Specifically, the costs of client/server distribution increase linearly as the number of users downloading objects increases\. This can make it expensive to distribute popular objects\. 

BitTorrent addresses this problem by recruiting the very clients that are downloading the object as distributors themselves: Each client downloads some pieces of the object from Amazon S3 and some from other clients, while simultaneously uploading pieces of the same object to other interested "peers\." The benefit for publishers is that for large, popular files the amount of data actually supplied by Amazon S3 can be substantially lower than what it would have been serving the same clients via client/server download\. Less data transferred means lower costs for the publisher of the object\.

**Note**  
 Amazon S3 does not support the BitTorrent protocol in AWS Regions launched after May 30, 2016\.
You can only get a torrent file for objects that are less than 5 GBs in size\.


# Deleting an object using the REST API<a name="DeletingAnObjectsUsingREST"></a>

You can use the AWS SDKs to delete an object\. However, if your application requires it, you can send REST requests directly\. For more information, go to [DELETE Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html) in the *Amazon Simple Storage Service API Reference*\. 


# Identity and access management in Amazon S3<a name="s3-access-control"></a>

By default, all Amazon S3 resources—buckets, objects, and related subresources \(for example, `lifecycle` configuration and `website` configuration\)—are private: only the resource owner, an AWS account that created it, can access the resource\. The resource owner can optionally grant access permissions to others by writing an access policy\. 

Amazon S3 offers access policy options broadly categorized as resource\-based policies and user policies\. Access policies you attach to your resources \(buckets and objects\) are referred to as resource\-based policies\. For example, bucket policies and access control lists \(ACLs\) are resource\-based policies\. You can also attach access policies to users in your account\. These are called user policies\. You may choose to use resource\-based policies, user policies, or some combination of these to manage permissions to your Amazon S3 resources\. The introductory topics provide general guidelines for managing permissions\.

## Introduction to managing access to Amazon S3 resources<a name="intro-managing-access-s3-resources"></a>

We recommend you first review the introductory topics that explain the options for managing access to your Amazon S3 resources:
+ [Overview of managing access](access-control-overview.md)
+ [How Amazon S3 Authorizes a Request](how-s3-evaluates-access-control.md)
+ [Guidelines for using the available access policy options](access-policy-alternatives-guidelines.md)
+ [Example walkthroughs: Managing access to your Amazon S3 resources ](example-walkthroughs-managing-access.md)

Several security best practices also address access control, including:
+ [Ensure Amazon S3 buckets are not publicly accessible](security-best-practices.md#public)
+ [Implement least privilege access](security-best-practices.md#least)
+ [Use IAM roles](security-best-practices.md#roles)
+ [Enable MFA (Multi-Factor Authentication) Delete](security-best-practices.md#mfa)
+ [Identify and audit all your Amazon S3 buckets](security-best-practices.md#audit)
+ [Monitor AWS security advisories](security-best-practices.md#advisories)

## Amazon S3 resource access options<a name="s3-resource-access-options"></a>

After you've reviewed introductory topics about managing access to Amazon S3 resources, you can then use the following topics to get more information about specific access policy options:
+ [Using Bucket Policies and User Policies](using-iam-policies.md)
+ [Managing Access with ACLs](S3_ACLs_UsingACLs.md)
+ [Controlling ownership of uploaded objects using S3 Object Ownership](about-object-ownership.md)
+ [Using Amazon S3 block public access](access-control-block-public-access.md)
+ [Using Service\-Linked Roles for Amazon S3 Storage Lens](using-service-linked-roles.md)


# Setting default server\-side encryption behavior for Amazon S3 buckets<a name="bucket-encryption"></a>

Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket\. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket\. The objects are encrypted using server\-side encryption with either Amazon S3\-managed keys \(SSE\-S3\) or customer master keys \(CMKs\) stored in AWS Key Management Service \(AWS KMS\)\. 

When you configure your bucket to use default encryption with SSE\-KMS, you can also enable an S3 Bucket Key to decrease request traffic from Amazon S3 to AWS Key Management Service \(AWS KMS\) and reduce the cost of encryption\. For more information, see [Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys](bucket-key.md)\.

When you use server\-side encryption, Amazon S3 encrypts an object before saving it to disk and decrypts it when you download the objects\. For more information about protecting data using server\-side encryption and encryption key management, see [Protecting data using server\-side encryption](serv-side-encryption.md)\.

For more information about permissions required for default encryption, see [PutBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketEncryption.html) in the *Amazon Simple Storage Service API Reference*\.

**Topics**
+ [How do I set up Amazon S3 default encryption for an S3 bucket?](#bucket-encryption-how-to-set-up)
+ [Using an S3 Bucket Key with default encryption](#bucket-key-default-encryption)
+ [Using encryption for cross\-account operations](#bucket-encryption-cross-account-operations)
+ [Using default encryption with replication](#bucket-encryption-update-bucket-policy)
+ [Monitoring default encryption with CloudTrail and CloudWatch](#bucket-encryption-tracking)
+ [More info](#bucket-encryption-related-resources)

## How do I set up Amazon S3 default encryption for an S3 bucket?<a name="bucket-encryption-how-to-set-up"></a>

This section describes how to set up Amazon S3 default encryption\. You can use the AWS SDKs, the Amazon S3 REST API, the AWS Command Line Interface \(AWS CLI\), or the Amazon S3 console to enable the default encryption\. The easiest way to set up default encryption for an S3 bucket is by using the AWS Management Console\.

To set up default encryption on a bucket, you can use any of these methods:
+ Use the Amazon S3 console\. For more information, see [How Do I Enable Default Encryption for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html) in the *Amazon Simple Storage Service Console User Guide*\.
+ Use the REST API [PUT Bucket encryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTencryption.html) operation to enable default encryption and set the type of server\-side encryption to use—SSE\-S3 or SSE\-KMS\.
+ Use the AWS CLI and AWS SDKs\. For more information, see [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)\. 

After you enable default encryption for a bucket, the following encryption behavior applies:
+ There is no change to the encryption of the objects that existed in the bucket before default encryption was enabled\. 
+ When you upload objects after enabling default encryption:
  + If your `PUT` request headers don't include encryption information, Amazon S3 uses the bucket’s default encryption settings to encrypt the objects\. 
  + If your `PUT` request headers include encryption information, Amazon S3 uses the encryption information from the `PUT` request to encrypt objects before storing them in Amazon S3\.
+ If you use the SSE\-KMS option for your default encryption configuration, you are subject to the RPS \(requests per second\) limits of AWS KMS\. For more information about AWS KMS limits and how to request a limit increase, see [AWS KMS limits](https://docs.aws.amazon.com/kms/latest/developerguide/limits.html)\. 

To encrypt your existing Amazon S3 objects with a single request, you can use Amazon S3 batch operations\. You provide S3 Batch Operations with a list of objects to operate on, and Batch Operations calls the respective API to perform the specified operation\. You can use the copy operation to copy the existing unencrypted objects and write the new encrypted objects to the same bucket\. A single Batch Operations job can perform the specified operation on billions of objects containing exabytes of data\. For more information, see [Performing S3 Batch Operations](batch-ops.md)\.

You can also encrypt existing objects using the Copy Object API\. For more information, see [Encrypting existing Amazon S3 objects with the AWS CLI](http://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-objects-with-the-aws-cli/)\.

**Note**  
Amazon S3 buckets with default bucket encryption using SSE\-KMS cannot be used as destination buckets for [Amazon S3 server access logging](ServerLogs.md)\. Only SSE\-S3 default encryption is supported for server access log destination buckets\.

## Using an S3 Bucket Key with default encryption<a name="bucket-key-default-encryption"></a>

When you configure your bucket to use default encryption for SSE\-KMS on new objects, you can also configure an S3 Bucket Key\. S3 Bucket Keys decrease the number of transactions from Amazon S3 to AWS KMS to reduce the cost of server\-side encryption using AWS Key Management Service \(SSE\-KMS\)\. 

When you configure your bucket to use an S3 Bucket Key for SSE\-KMS on new objects, AWS KMS generates a bucket\-level key that is used to create a unique [data key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys) for objects in the bucket\. This bucket key is used for a time\-limited period within Amazon S3, reducing the need for Amazon S3 to make requests to AWS KMS to complete encryption operations\. For more information about using an S3 Bucket Key, see [Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys](bucket-key.md)\.

## Using encryption for cross\-account operations<a name="bucket-encryption-cross-account-operations"></a>

Be aware of the following when using encryption for cross\-account operations:
+ The AWS managed CMK \(`aws/s3`\) is used when a CMK ARN or alias is not provided at request\-time, nor via the bucket's default encryption configuration\.
+ If you're uploading or accessing S3 objects using AWS Identity and Access Management \(IAM\) principals that are in the same AWS account as your CMK, you can use the AWS managed CMK \(`aws/s3`\)\. 
+ Use a customer managed CMK if you want to grant cross\-account access to your S3 objects\. You can configure the policy of a customer managed CMK to allow access from another account\.
+ If you specify your own CMK, you should use a fully qualified CMK key ARN\. When using a CMK alias, be aware that AWS KMS will resolve the key within the requester’s account\. This might result in data encrypted with a CMK that belongs to the requester, and not the bucket administrator\.
+ You must specify a key that you \(the requester\) have been granted `Encrypt` permission to\. For more information, see [Allows key users to use a CMK for cryptographic operations](https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html#key-policy-users-crypto) in the *AWS Key Management Service Developer Guide*\.

For more information about when to use customer managed CMKs and the AWS managed CMK, see [Should I use an AWS AWS KMS\-managed key or a custom AWS AWS KMS key to encrypt my objects on Amazon S3?](http://aws.amazon.com/premiumsupport/knowledge-center/s3-object-encrpytion-keys/)

## Using default encryption with replication<a name="bucket-encryption-update-bucket-policy"></a>

After you enable default encryption for a replication destination bucket, the following encryption behavior applies: 
+ If objects in the source bucket are not encrypted, the replica objects in the destination bucket are encrypted using the default encryption settings of the destination bucket\. This results in the `ETag` of the source object being different from the `ETag` of the replica object\. You must update applications that use the `ETag` to accommodate for this difference\.
+ If objects in the source bucket are encrypted using SSE\-S3 or SSE\-KMS, the replica objects in the destination bucket use the same encryption as the source object encryption\. The default encryption settings of the destination bucket are not used\.

For more information about using default encryption with SSE\-KMS, see [Replicating encrypted objects](replication-config-for-kms-objects.md)\.

## Monitoring default encryption with CloudTrail and CloudWatch<a name="bucket-encryption-tracking"></a>

You can track default encryption configuration requests through AWS CloudTrail events\. The API event names used in CloudTrail logs are `PutBucketEncryption`, `GetBucketEncryption`, and `DeleteBucketEncryption`\. You can also create Amazon CloudWatch Events with S3 bucket\-level operations as the event type\. For more information about CloudTrail events, see [How Do I Enable Object\-Level Logging for an S3 Bucket with CloudTrail Data Events?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html)

You can use CloudTrail logs for object\-level Amazon S3 actions to track `PUT` and `POST` requests to Amazon S3 to verify whether default encryption is being used to encrypt objects when incoming `PUT` requests don't have encryption headers\. 

When Amazon S3 encrypts an object using the default encryption settings, the log includes the following field as the name/value pair: `"SSEApplied":"Default_SSE_S3" or "SSEApplied":"Default_SSE_KMS"`\. 

When Amazon S3 encrypts an object using the `PUT` encryption headers, the log includes the following field as the name/value pair: `"SSEApplied":"SSE_S3", "SSEApplied":"SSE_KMS`, or `"SSEApplied":"SSE_C"`\. For multipart uploads, this information is included in the `InitiateMultipartUpload` API requests\. For more information about using CloudTrail and CloudWatch, see [Monitoring Amazon S3](monitoring-overview.md)\.

## More info<a name="bucket-encryption-related-resources"></a>
+  [PUT Bucket encryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTencryption.html) 
+  [DELETE Bucket encryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEencryption.html) 
+  [GET Bucket encryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETencryption.html) 


# Amazon S3 analytics – Storage Class Analysis<a name="analytics-storage-class"></a>

By using Amazon S3 analytics *Storage Class Analysis* you can analyze storage access patterns to help you decide when to transition the right data to the right storage class\. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD\_IA \(IA, for infrequent access\) storage class\. For more information about storage classes, see [Amazon S3 storage classes](storage-class-intro.md)\. 

After storage class analysis observes the infrequent access patterns of a filtered set of data over a period of time, you can use the analysis results to help you improve your lifecycle policies\. You can configure storage class analysis to analyze all the objects in a bucket\. Or, you can configure filters to group objects together for analysis by common prefix \(that is, objects that have names that begin with a common string\), by object tags, or by both prefix and tags\. You'll most likely find that filtering by object groups is the best way to benefit from storage class analysis\. 

**Important**  
Storage class analysis does not give recommendations for transitions to the ONEZONE\_IA or S3 Glacier storage classes\.

You can have multiple storage class analysis filters per bucket, up to 1,000, and will receive a separate analysis for each filter\. Multiple filter configurations allow you analyze specific groups of objects to improve your lifecycle policies that transition objects to STANDARD\_IA\. 

Storage class analysis provides storage usage visualizations in the Amazon S3 console that are updated daily\. You can also export this daily usage data to an S3 bucket and view them in a spreadsheet application, or with business intelligence tools, like Amazon QuickSight\.

There are costs associated with the storage class analysis\. For pricing information, see *Management and replication* [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

**Topics**
+ [How do I set up storage class analysis?](#analytics-storage-class-how-to-set-up)
+ [How do I use storage class analysis?](#analytics-storage-class-contents)
+ [How can I export storage class analysis data?](#analytics-storage-class-export-to-file)
+ [Amazon S3 analytics REST APIs](#analytics-storage-class-related-resources)

## How do I set up storage class analysis?<a name="analytics-storage-class-how-to-set-up"></a>

You set up storage class analysis by configuring what object data you want to analyze\. You can configure storage class analysis to do the following:
+ **Analyze the entire contents of a bucket\.**

  You'll receive an analysis for all the objects in the bucket\.
+ **Analyze objects grouped together by prefix and tags\.**

  You can configure filters that group objects together for analysis by prefix, or by object tags, or by a combination of prefix and tags\. You receive a separate analysis for each filter you configure\. You can have multiple filter configurations per bucket, up to 1,000\. 
+ **Export analysis data\.** 

  When you configure storage class analysis for a bucket or filter, you can choose to have the analysis data exported to a file each day\. The analysis for the day is added to the file to form a historic analysis log for the configured filter\. The file is updated daily at the destination of your choice\. When selecting data to export, you specify a destination bucket and optional destination prefix where the file is written\.

You can use the Amazon S3 console, the REST API, or the AWS CLI or AWS SDKs to configure storage class analysis\.
+ For information about how to configure storage class analysis in the Amazon S3 console, see [ How Do I Configure Storage Class Analysis?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-analytics-storage-class.html)\.
+ To use the Amazon S3 API, use the [PutBucketAnalyticsConfiguration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTAnalyticsConfig.html) REST API, or the equivalent, from the AWS CLI or AWS SDKs\. 

## How do I use storage class analysis?<a name="analytics-storage-class-contents"></a>

You use storage class analysis to observe your data access patterns over time to gather information to help you improve the lifecycle management of your STANDARD\_IA storage\. After you configure a filter, you'll start seeing data analysis based on the filter in the Amazon S3 console in 24 to 48 hours\. However, storage class analysis observes the access patterns of a filtered data set for 30 days or longer to gather information for analysis before giving a result\. The analysis continues to run after the initial result and updates the result as the access patterns change

When you first configure a filter, the Amazon S3 console may take a moment to analyze the your data\.

Storage class analysis observes the access patterns of a filtered object data set for 30 days or longer to gather enough information for the analysis\. After storage class analysis has gathered sufficient information, you'll see a message in the Amazon S3 console that analysis is complete\.

When performing the analysis for infrequently accessed objects storage class analysis looks at the filtered set of objects grouped together based on age since they were uploaded to Amazon S3\. Storage class analysis determines if the age group is infrequently accessed by looking at the following factors for the filtered data set:
+ Objects in the STANDARD storage class that are larger than 128 KB\.
+ How much average total storage you have per age group\.
+ Average number of bytes transferred out \(not frequency\) per age group\.
+ Analytics export data only includes requests with data relevant to storage class analysis\. This might cause differences in the number of requests, and the total upload and request bytes compared to what are shown in storage metrics or tracked by your own internal systems\.
+ Failed GET and PUT requests are not counted for the analysis\. However, you will see failed requests in storage metrics\. 

**How Much of My Storage did I Retrieve?**

The Amazon S3 console graphs how much of the storage in the filtered data set has been retrieved for the observation period\.

**What Percentage of My Storage did I Retrieve?**

The Amazon S3 console also graphs what percentage of the storage in the filtered data set has been retrieved for the observation period\.

As stated earlier in this topic, when you are performing the analysis for infrequently accessed objects, storage class analysis looks at the filtered set of objects grouped together based on the age since they were uploaded to Amazon S3\. The storage class analysis uses the following predefined object age groups: 
+ Amazon S3 Objects less than 15 days old
+ Amazon S3 Objects 15\-29 days old
+ Amazon S3 Objects 30\-44 days old
+ Amazon S3 Objects 45\-59 days old
+ Amazon S3 Objects 60\-74 days old
+ Amazon S3 Objects 75\-89 days old
+ Amazon S3 Objects 90\-119 days old
+ Amazon S3 Objects 120\-149 days old
+ Amazon S3 Objects 150\-179 days old
+ Amazon S3 Objects 180\-364 days old
+ Amazon S3 Objects 365\-729 days old
+ Amazon S3 Objects 730 days and older

Usually it takes about 30 days of observing access patterns to gather enough information for an analysis result\. It might take longer than 30 days, depending on the unique access pattern of your data\. However, after you configure a filter you'll start seeing data analysis based on the filter in the Amazon S3 console in 24 to 48 hours\. You can see analysis on a daily basis of object access broken down by object age group in the Amazon S3 console\. 

**How Much of My Storage is Infrequently Accessed?**

The Amazon S3 console shows the access patterns grouped by the predefined object age groups\. The **Frequently accessed** or **Infrequently accessed** text shown is meant as a visual aid to help you in the lifecycle creation process\.

## How can I export storage class analysis data?<a name="analytics-storage-class-export-to-file"></a>

You can choose to have storage class analysis export analysis reports to a comma\-separated values \(CSV\) flat file\. Reports are updated daily and are based on the object age group filters you configure\. When using the Amazon S3 console you can choose the export report option when you create a filter\. When selecting data export you specify a destination bucket and optional destination prefix where the file is written\. You can export the data to a destination bucket in a different account\. The destination bucket must be in the same region as the bucket that you configure to be analyzed\.

You must create a bucket policy on the destination bucket to grant permissions to Amazon S3 to verify what AWS account owns the bucket and to write objects to the bucket in the defined location\. For an example policy, see [Granting Permissions for Amazon S3 Inventory and Amazon S3 Analytics](example-bucket-policies.md#example-bucket-policies-use-case-9)\.

After you configure storage class analysis reports, you start getting the exported report daily after 24 hours\. After that, Amazon S3 continues monitoring and providing daily exports\. 

You can open the CSV file in a spreadsheet application or import the file into other applications like [Amazon QuickSight](https://docs.aws.amazon.com/quicksight/latest/user/welcome.html)\. For information on using Amazon S3 files with Amazon QuickSight, see [ Create a Data Set Using Amazon S3 Files](https://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-s3.html) in the *Amazon QuickSight User Guide*\. 

Data in the exported file is sorted by date within object age group as shown in following examples\. If the storage class is STANDARD the row also contains data for the columns `ObjectAgeForSIATransition` and `RecommendedObjectAgeForSIATransition`\.

![\[Screen shot.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/storage-class-analysis-export-file1.png)![\[Screen shot.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)![\[Screen shot.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)

![\[Screen shot.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/storage-class-analysis-export-file2.png)![\[Screen shot.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)![\[Screen shot.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)

At the end of the report the object age group is given as ALL\. The ALL rows contain cumulative totals, including objects smaller than 128 KB, for all the age groups for that day\.

![\[Screen shot.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/storage-class-analysis-export-file3.png)![\[Screen shot.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)![\[Screen shot.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)

The next section describes the columns used in the report\.

### Exported file layout<a name="analytics-storage-class-export-file-layout"></a>

The following table describe the layout of the exported file\.

Use the scroll bars to see the rest of the table\.


**Amazon S3 storage class analysis export file layout**  

| Column name | Dimension/Metric | DataType | Description | 
| --- | --- | --- | --- | 
| Date  | Dimension | String  | Date when the record was processed\. Format is MM\-DD\-YYYY\. | 
| ConfigId  | Dimension | String  | Value entered as the filter name when adding the filter configuration\.  | 
| Filter | Dimension | String  | Full filter values as configured when adding the filter configuration\. | 
| StorageClass | Dimension | String  | Storage class of the data\. | 
| ObjectAge | Dimension | String  | Age group for the objects in the filter\. In addition to the 12 different age groups \(0\-14 days, 15\-29 days, 30\-44 days, 45\-59 days, 60\-74 days, 75\-89 days, 90\-119 days, 120\-149 days, 150\-179 days, 180\-364 days, 365\-729 days, 730 days\+\) for 128KB\+ objects, there is one extra value='ALL', which represents all age groups\. | 
| ObjectCount  | Metric  |  Integer  | Total number of objects counted per storage class for the day in the age group\. For the `AgeGroup='ALL'`, the value is the total object count for all the age groups for the day\. | 
| DataUploaded\_MB  | Metric | Number | Total data in MB uploaded per storage class for the day in the age group\. For the `AgeGroup='ALL'`, the value is the total upload count in MB for all the age groups for the day\. \(Note that you will not see multipart object upload activity in your export data because multipart upload requests do not currently have storage class information\.\) | 
| Storage\_MB  | Metric | Number  | Total storage in MB per storage class for the day in the age group\. For the `AgeGroup='ALL'`, the value is the overall storage count in MB for all the age groups for the day\. | 
| DataRetrieved\_MB | Metric | Number | Data transferred out in MBs per storage class with GET requests for the day in the age group\. For `AgeGroup='ALL'`, the value is the overall data transferred out in MB with GET requests for all the age groups for the day\. | 
| GetRequestCount | Metric | Integer | Number of GET requests made per storage class for the day in the age group\. For AgeGroup='ALL', the value represents the overall GET request count for all the age groups for the day\.  | 
| CumulativeAccessRatio | Metric | Number | Cumulative access ratio\. This ratio is used to represent the usage/byte heat on any given age group to help determine if an age group is eligible for transition to STANDARD\_IA\.  | 
| ObjectAgeForSIATransition | Metric | Integer In Days  | This value exists only where the `AgeGroup=’ALL’` and storage class = STANDARD\. It represents the observed age for transition to STANDARD\_IA\. | 
| RecommendedObjectAgeForSIATransition  | Metric | Integer In Days  | This value exists only where the `AgeGroup=’ALL’` and storage class = STANDARD\. It represents the object age in days to consider for transition to STANDARD\_IA after the `ObjectAgeForSIATransition` stabilizes\. | 

## Amazon S3 analytics REST APIs<a name="analytics-storage-class-related-resources"></a>

The following are the REST operations used for storage inventory\.
+  [ DELETE Bucket analytics configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEAnalyticsConfiguration.html) 
+  [ GET Bucket analytics configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETAnalyticsConfig.html) 
+  [ List Bucket Analytics Configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketListAnalyticsConfigs.html) 
+  [ PUT Bucket analytics configuration](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTAnalyticsConfig.html) 


# POST with adobe flash<a name="HTTPPOSTFlash"></a>

This section describes how to use `POST` with Adobe Flash\.

## Adobe flash player security<a name="AdobeFlashPlayerSecurity"></a>

By default, the Adobe Flash Player security model prohibits Adobe Flash Players from making network connections to servers outside the domain that serves the SWF file\.

To override the default, you must upload a publicly readable crossdomain\.xml file to the bucket that will accept POST uploads\. The following is a sample crossdomain\.xml file\.

```
1. <?xml version="1.0"?>
2. <!DOCTYPE cross-domain-policy SYSTEM
3. "http://www.macromedia.com/xml/dtds/cross-domain-policy.dtd">
4. <cross-domain-policy>
5. <allow-access-from domain="*" secure="false" />
6. </cross-domain-policy>
```

**Note**  
For more information about the Adobe Flash security model, go to the Adobe website\.  
Adding the crossdomain\.xml file to your bucket allows any Adobe Flash Player to connect to the crossdomain\.xml file within your bucket; however, it does not grant access to the actual Amazon S3 bucket\.

## Adobe flash considerations<a name="HTTPPOSTAdobeFlashConsiderations"></a>

 The FileReference API in Adobe Flash adds the `Filename` form field to the POST request\. When you build Adobe Flash applications that upload to Amazon S3 by using the FileReference API action, include the following condition in your policy: 

```
1. ['starts-with', '$Filename', '']
```

Some versions of the Adobe Flash Player do not properly handle HTTP responses that have an empty body\. To configure POST to return a response that does not have an empty body, set `success_action_status` to 201\. Amazon S3 will then return an XML document with a 201 status code\. For information about the content of the XML document, see [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)\. For information about form fields, see [HTML form fields](HTTPPOSTForms.md#HTTPPOSTFormFields)\. 


# Viewing S3 Storage Lens metrics on the dashboards<a name="storage_lens_view_metrics_dashboard"></a>

S3 Storage Lens provides you with a dashboard containing usage metrics at no additional cost\. If you want to receive advanced metrics and recommendations, including usage and activity metrics, prefix aggregations, and contextual recommendations in the dashboard, you must select it from the dashboard configuration page on the Amazon S3 console\.

The dashboard provides an interactive visualization for your storage usage and activity metrics\. You can view organization\-wide trends, or see more granular trends by AWS account, AWS Region, storage class, S3 bucket, or prefix\. 

If your account is a member of AWS Organizations, you can also see your storage usage and activity for your entire organization across member accounts\. This information is available to you provided that S3 Storage Lens has been given trusted access to your organization, and you are an authorized management or delegated administrator account\.

Use the interactive dashboard to explore your storage usage and activity trends and insights, and get contextual recommendations for best practices to optimize your storage\. For more information, see [ Understanding Amazon S3 Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_basics_metrics_recommendations.html)\.

Amazon S3 preconfigures the S3 Storage Lens *default dashboard* to help you visualize summarized insights and trends of your entire account’s aggregated storage usage and activity metrics\(optional upgrade\)\. You can't modify the default dashboard configuration scope, but the metrics selection can be upgraded from *free metrics* to the paid *advanced metrics and recommendations*\. You can configure the optional metrics export, or even disable the dashboard\. However, the default dashboard cannot be deleted\.

In addition to the default dashboard that Amazon S3 creates, you can also create custom dashboards scoped to your own organization’s accounts, Regions, buckets, and prefixes\(account\-level only\)\. These custom dashboards can be edited, deleted, and disabled\.

**Note**  
The Amazon S3 Storage Lens dashboard is only available from the Amazon S3 console\. For more information, see [Viewing your Amazon S3 Storage Lens dashboards in the Amazon S3 console](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/storage_lens_console_viewing.html)\.


# Deleting one object per request<a name="DeletingOneObject"></a>

**Topics**
+ [Deleting an object using the AWS SDK for Java](DeletingOneObjectUsingJava.md)
+ [Deleting an object using the AWS SDK for \.NET](DeletingOneObjectUsingNetSDK.md)
+ [Deleting an object using the AWS SDK for PHP](DeletingOneObjectUsingPHPSDK.md)
+ [Deleting an object using the REST API](DeletingAnObjectsUsingREST.md)
+ [Deleting an Object Using the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html)

To delete one object per request, use the `DELETE` API \(see [DELETE Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html)\)\. To learn more about object deletion, see [Deleting objects](DeletingObjects.md)\. 

You can use either the REST API directly or the wrapper libraries provided by the AWS SDKs that simplify application development\. 


# Retrieving object versions<a name="RetrievingObjectVersions"></a>

A simple `GET` request retrieves the current version of an object\. The following figure shows how `GET` returns the current version of the object, `photo.gif`\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_GET_NoVersionID.png)

To retrieve a specific version, you have to specify its version ID\. The following figure shows that a `GET versionId` request retrieves the specified version of the object \(not necessarily the current one\)\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_GET_Versioned.png)

## Using the console<a name="retrieve-obj-versioning-enabled-console"></a>

For instructions see, [How Do I See the Versions of an S3 Object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/view-object-versions.html) in the Amazon Simple Storage Service Console User Guide\.

## Using the AWS SDKs<a name="retrieve-obj-version-sdks"></a>

For examples of uploading objects using AWS SDKs for Java, \.NET, and PHP, see [Getting objects](GettingObjectsUsingAPIs.md)\. The examples for uploading objects in a nonversioned and versioning\-enabled buckets are the same, although in the case of versioning\-enabled buckets, Amazon S3 assigns a version number\. Otherwise, the version number is null\. 

For information about using other AWS SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code/)\. 

## Using REST<a name="retrieve-obj-version-rest"></a>

**To retrieve a specific object version**

1. Set `versionId` to the ID of the version of the object you want to retrieve\.

1. Send a `GET Object versionId `request\.

**Example Retrieving a versioned object**  
The following request retrieves version L4kqtJlcpXroDTDmpUMLUo of `my-image.jpg`\.  

```
1. GET /my-image.jpg?versionId=L4kqtJlcpXroDTDmpUMLUo HTTP/1.1
2. Host: bucket.s3.amazonaws.com
3. Date: Wed, 28 Oct 2009 22:32:00 GMT
4. Authorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=
```

## Related topics<a name="retrieve-obj-versioning-enabled-related-topics"></a>

 [Retrieving the metadata of an object version](RetMetaOfObjVersion.md) 


# Logging with Amazon S3<a name="logging-with-S3"></a>

You can record the actions that are taken by users, roles, or AWS services on Amazon S3 resources and maintain log records for auditing and compliance purposes\. To do this, you can use [Amazon S3 server access logging](ServerLogs.md), [AWS CloudTrail logs](https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html), or a combination of both\. We recommend that you use AWS CloudTrail for logging bucket and object\-level actions for your Amazon S3 resources\. 

The following table lists the key properties of AWS CloudTrail logs and Amazon S3 server access logs\. 


| Log properties | AWS CloudTrail | Amazon S3 server logs | 
| --- |--- |--- |
|  Can be forwarded to other systems \(CloudWatch Logs, CloudWatch Events\)  |  Yes  |  | 
|  Deliver logs to more than one destination \(for example, send the same logs to two different buckets\)  |  Yes  |  | 
|  Turn on logs for a subset of objects \(prefix\)  |  Yes  |  | 
|  Cross\-account log delivery \(target and source bucket owned by different accounts\)  |  Yes  |  | 
|  Integrity validation of log file using digital signature/hashing  |  Yes  |  | 
|  Default/choice of encryption for log files  |  Yes  |  | 
|  Object operations \(using Amazon S3 APIs\)  |  Yes  |  Yes  | 
|  Bucket operations \(using Amazon S3 APIs\)  |  Yes  |  Yes  | 
|  Searchable UI for logs  |  Yes  |  | 
|  Fields for Object Lock parameters, Amazon S3 select properties for log records  |  Yes  |  | 
|  Fields for `Object Size`, `Total Time`, `Turn-Around Time`, and `HTTP Referer` for log records  |  |  Yes  | 
|  Lifecycle transitions, expirations, restores  |  |  [Yes](https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-and-other-bucket-config.html#lifecycle-general-considerations-logging)  | 
|  Logging of keys in a batch delete operation  |  |  Yes  | 
|  Authentication failures1  |  |  Yes  | 
|  Accounts where logs get delivered  |  Bucket owner2, and requester  |  Bucket owner only  | 
| Performance and Cost | AWS CloudTrail | Amazon S3 Server Logs | 
| --- |--- |--- |
|  Price  |  Management events \(first delivery\) are free; data events incur a fee, in addition to storage of logs  |  No additional cost in addition to storage of logs  | 
|  Speed of log delivery  |  Data events every 5 mins; management events every 15 mins  |  Within a few hours  | 
|  Log format  |  JSON  |  Log file with space\-separated, newline\-delimited records  | 

**Notes:**

1. CloudTrail does not deliver logs for requests that fail authentication \(in which the provided credentials are not valid\)\. However, it does include logs for requests in which authorization fails \(`AccessDenied`\) and requests that are made by anonymous users\.

1. The S3 bucket owner receives CloudTrail logs only if the account also owns or has full access to the object in the request\. For more information, see [Object\-level actions in cross\-account scenarios](cloudtrail-logging.md#cloudtrail-object-level-crossaccount)\. 


# Operations on objects<a name="ObjectOperations"></a>

Amazon S3 enables you to store, retrieve, and delete objects\. You can retrieve an entire object or a portion of an object\. If you enabled S3 Versioning on your bucket, you can retrieve a specific version of the object\. You can also retrieve a subresource associated with your object and update it where applicable\. You can make a copy of your existing object\. Depending on the object size, the following upload and copy related considerations apply: 
+ **Uploading objects—**You can upload objects of up to 5 GB in size in a single operation\. For objects greater than 5 GB you must use the multipart upload API\. 

  Using the multipart upload API you can upload objects up to 5 TB each\. For more information, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\.
+ **Copying objects—**The copy operation creates a copy of an object that is already stored in Amazon S3\. 

  You can create a copy of your object up to 5 GB in size in a single atomic operation\. However, for copying an object greater than 5 GB, you must use the multipart upload API\. For more information, see [Copying objects](CopyingObjectsExamples.md)\.

You can use the REST API \(see [Making requests using the REST API](RESTAPI.md)\) to work with objects or use one of the following AWS SDK libraries:
+ [AWS SDK for Java](https://aws.amazon.com/sdk-for-java/)
+ [AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)
+ [AWS SDK for PHP](https://aws.amazon.com/sdk-for-php/)

These libraries provide a high\-level abstraction that makes working with objects easy\. However, if your application requires, you can use the REST API directly\.


# Understanding the Amazon S3 Storage Lens export schema<a name="storage_lens_understanding_metrics_export_schema"></a>

The following table contains the schema of your S3 Storage Lens metrics export\.


| Attribute Name  | Data Type | Column Name | Description | 
| --- | --- | --- | --- | 
| VersionNumber | String | version\_number | The version of the S3 Storage Lens metrics being used\. | 
| ConfigurationId | String | configuration\_id | The name of the configuration\_id of your S3 Storage Lens configuration\. | 
| ReportDate  | String  | report\_date  | The date the metrics were tracked\.  | 
|  AwsAccountNumber  |  String  |  aws\_account\_number  |  Your AWS account number\.  | 
|  AwsRegion  |  String  |  aws\_region  |  The AWS Region for which the metrics are being tracked\.  | 
|  StorageClass  |  String  |  storage\_class  |  The storage class of the bucket in question\.  | 
|  RecordType  |  ENUM  |  record\_type  |  The type of artifact that is being reported \(ACCOUNT, BUCKET, or PREFIX\)\.  | 
|  RecordValue  |  String  |  record\_value  |  The record value\. This field is populated when the record\_type is PREFIX\. The record value is only URL\-encoded in the CSV format  | 
|  BucketName  |  String  |  bucket\_name  |  The name of the bucket that is being reported\.  | 
|  MetricName  |  String  |  metric\_name  |  The name of the metric that is being reported\.  | 
|  MetricValue  |  Long  |  metric\_value  |  The value of the metric that is being reported\.  | 

## Example of an S3 Storage Lens metrics export<a name="storage_lens_sample_metrics_export"></a>

The following is an example of an S3 Storage Lens metrics export based on this schema\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/sample_storage_lens_export.png)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)


# Principals<a name="s3-bucket-user-policy-specifying-principal-intro"></a>

The `Principal` element specifies the user, account, service, or other entity that is allowed or denied access to a resource\. The following are examples of specifying `Principal`\. For more information, see [Principal](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html) in the *IAM User Guide*\.

## Grant Permissions to an AWS Account<a name="s3-aws-account-permissions"></a>

To grant permissions to an AWS account, identify the account using the following format\.

```
"AWS":"account-ARN"
```

The following are examples\.

```
"Principal":{"AWS":"arn:aws:iam::AccountNumber-WithoutHyphens:root"}
```

```
"Principal":{"AWS":["arn:aws:iam::AccountNumber1-WithoutHyphens:root","arn:aws:iam::AccountNumber2-WithoutHyphens:root"]}
```

Amazon S3 also supports a canonical user ID, which is an obfuscated form of the AWS account ID\. You can specify this ID using the following format\.

```
"CanonicalUser":"64-digit-alphanumeric-value"
```

The following is an example\.

```
"Principal":{"CanonicalUser":"64-digit-alphanumeric-value"}
```

For information about how to find the canonical user ID for your account, see [Finding Your Account Canonical User ID](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingCanonicalId)\.

**Important**  
When you use a canonical user ID in a policy, Amazon S3 might change the canonical ID to the corresponding AWS account ID\. This does not impact the policy because both of these IDs identify the same account\.   
 

## Grant Permissions to an IAM User<a name="s3-aws-user-permissions"></a>

To grant permission to an IAM user within your account, you must provide an `"AWS":"user-ARN"` name\-value pair\.

```
"Principal":{"AWS":"arn:aws:iam::account-number-without-hyphens:user/username"}
```

## Grant Anonymous Permissions<a name="s3-anonymous-permissions"></a>

To grant permission to everyone, also referred as anonymous access, you set the wildcard \(`"*"`\) as the `Principal` value\. For example, if you configure your bucket as a website, you want all the objects in the bucket to be publicly accessible\. The following are equivalent\.

```
"Principal":"*"
```

```
"Principal":{"AWS":"*"}
```

**Warning**  
Use caution when granting anonymous access to your S3 bucket\. When you grant anonymous access, anyone in the world can access your bucket\. We highly recommend that you never grant any kind of anonymous write access to your S3 bucket\.

## Require Access Through CloudFront URLs<a name="require-cloudfront-urls"></a>

You can require that your users access your Amazon S3 content by using Amazon CloudFront URLs instead of Amazon S3 URLs\. To do this, create a CloudFront origin access identity \(OAI\)\. Then, change the permissions either on your bucket or on the objects in your bucket\. The format for specifying the OAI in a `Principal` statement is as follows\.

```
"Principal":{"CanonicalUser":"Amazon S3 Canonical User ID assigned to origin access identity"}
```

For more information, see [ Using an Origin Access Identity to Restrict Access to Your Amazon S3 Content](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html) in the *Amazon CloudFront Developer Guide*\. 


# Abort a multipart upload<a name="LLAbortMPUJava"></a>

You can stop an in\-progress multipart upload by calling the `AmazonS3Client.abortMultipartUpload()` method\. This method deletes all parts that were uploaded to Amazon S3 and frees the resources\. You provide the upload ID, bucket name, and key name\.

**Example**  
The following example shows how to stop multipart uploads using the low\-level Java API\.  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.AbortMultipartUploadRequest;
import com.amazonaws.services.s3.model.ListMultipartUploadsRequest;
import com.amazonaws.services.s3.model.MultipartUpload;
import com.amazonaws.services.s3.model.MultipartUploadListing;

import java.util.List;

public class LowLevelAbortMultipartUpload {

    public static void main(String[] args) {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

            // Find all in-progress multipart uploads.
            ListMultipartUploadsRequest allMultipartUploadsRequest = new ListMultipartUploadsRequest(bucketName);
            MultipartUploadListing multipartUploadListing = s3Client.listMultipartUploads(allMultipartUploadsRequest);

            List<MultipartUpload> uploads = multipartUploadListing.getMultipartUploads();
            System.out.println("Before deletions, " + uploads.size() + " multipart uploads in progress.");

            // Abort each upload.
            for (MultipartUpload u : uploads) {
                System.out.println("Upload in progress: Key = \"" + u.getKey() + "\", id = " + u.getUploadId());
                s3Client.abortMultipartUpload(new AbortMultipartUploadRequest(bucketName, u.getKey(), u.getUploadId()));
                System.out.println("Upload deleted: Key = \"" + u.getKey() + "\", id = " + u.getUploadId());
            }

            // Verify that all in-progress multipart uploads have been aborted.
            multipartUploadListing = s3Client.listMultipartUploads(allMultipartUploadsRequest);
            uploads = multipartUploadListing.getMultipartUploads();
            System.out.println("After aborting uploads, " + uploads.size() + " multipart uploads in progress.");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

**Note**  
Instead of stopping multipart uploads individually, you can stop all of your in\-progress multipart uploads that were initiated before a specific time\. This clean\-up operation is useful for aborting multipart uploads that you initiated but that didn't complete or were aborted\. For more information, see [Stop multipart uploads](HLAbortMPUploadsJava.md)\.


# Protecting data using client\-side encryption<a name="UsingClientSideEncryption"></a>

*Client\-side encryption* is the act of encrypting data before sending it to Amazon S3\. To enable client\-side encryption, you have the following options:
+ Use a customer master key \(CMK\) stored in AWS Key Management Service \(AWS KMS\)\.
+ Use a master key that you store within your application\.

The following AWS SDKs support client\-side encryption:
+ [AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)
+ [AWS SDK for Go](https://aws.amazon.com/sdk-for-go/)
+ [AWS SDK for Java](https://aws.amazon.com/sdk-for-java/)
+ [AWS SDK for PHP](https://aws.amazon.com/sdk-for-php/)
+ [AWS SDK for Ruby](https://aws.amazon.com/sdk-for-ruby/)
+ [AWS SDK for C\+\+](https://aws.amazon.com/sdk-for-cpp/)

## Option 1: Using a CMK stored in AWS KMS<a name="client-side-encryption-kms-managed-master-key-intro"></a>

With this option, you use an AWS KMS CMK for client\-side encryption when uploading or downloading data in Amazon S3\.
+ **When uploading an object** — Using the CMK ID, the client first sends a request to AWS KMS for a CMK that it can use to encrypt your object data\. AWS KMS returns two versions of a randomly generated data key:
  + A plaintext version of the data key that the client uses to encrypt the object data\.
  + A cipher blob of the same data key that the client uploads to Amazon S3 as object metadata\.
**Note**  
The client obtains a unique data key for each object that it uploads\.
+  **When downloading an object** — The client downloads the encrypted object from Amazon S3 along with the cipher blob version of the data key stored as object metadata\. The client then sends the cipher blob to AWS KMS to get the plaintext version of the data key so that it can decrypt the object data\.

For more information about AWS KMS, see [What is AWS Key Management Service?](https://docs.aws.amazon.com/kms/latest/developerguide/overview.html) in the *AWS Key Management Service Developer Guide*\.

**Example**  
The following code example demonstrates how to upload an object to Amazon S3 using AWS KMS with the AWS SDK for Java\. The example uses an AWS managed CMK to encrypt data on the client side before uploading it to Amazon S3\. If you already have a CMK, you can use that by specifying the value of the `keyId` variable in the example code\. If you don't have a CMK, or you need another one, you can generate one through the Java API\. The example code automatically generates a CMK to use\.  
For instructions on creating and testing a working example, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
        AWSKMS kmsClient = AWSKMSClientBuilder.standard()
                .withRegion(Regions.DEFAULT_REGION)
                .build();

        // create CMK for for testing this example
        CreateKeyRequest createKeyRequest = new CreateKeyRequest();
        CreateKeyResult createKeyResult = kmsClient.createKey(createKeyRequest);

// --
        // specify an Amazon KMS customer master key (CMK) ID
        String keyId = createKeyResult.getKeyMetadata().getKeyId();

        String s3ObjectKey = "EncryptedContent1.txt";
        String s3ObjectContent = "This is the 1st content to encrypt";
// --

        AmazonS3EncryptionV2 s3Encryption = AmazonS3EncryptionClientV2Builder.standard()
                .withRegion(Regions.US_WEST_2)
                .withCryptoConfiguration(new CryptoConfigurationV2().withCryptoMode(CryptoMode.StrictAuthenticatedEncryption))
                .withEncryptionMaterialsProvider(new KMSEncryptionMaterialsProvider(keyId))
                .build();

        s3Encryption.putObject(bucket_name, s3ObjectKey, s3ObjectContent);
        System.out.println(s3Encryption.getObjectAsString(bucket_name, s3ObjectKey));

        // schedule deletion of CMK generated for testing
        ScheduleKeyDeletionRequest scheduleKeyDeletionRequest =
                new ScheduleKeyDeletionRequest().withKeyId(keyId).withPendingWindowInDays(7);
        kmsClient.scheduleKeyDeletion(scheduleKeyDeletionRequest);

        s3Encryption.shutdown();
        kmsClient.shutdown();
```

## Option 2: Using a master key stored within your application<a name="client-side-encryption-client-side-master-key-intro"></a>

With this option, you use a master key that is stored within your application for client\-side data encryption\. 

**Important**  
Your client\-side master keys and your unencrypted data are never sent to AWS\. It's important that you safely manage your encryption keys\. If you lose them, you can't decrypt your data\.

This is how it works:
+ **When uploading an object** — You provide a client\-side master key to the Amazon S3 encryption client\. The client uses the master key only to encrypt the data encryption key that it generates randomly\. 

  The following steps describe the process:

  1. The Amazon S3 encryption client generates a one\-time\-use symmetric key \(also known as a *data encryption key* or *data key*\) locally\. It uses the data key to encrypt the data of a single Amazon S3 object\. The client generates a separate data key for each object\.

  1. The client encrypts the data encryption key using the master key that you provide\. The client uploads the encrypted data key and its material description as part of the object metadata\. The client uses the material description to determine which client\-side master key to use for decryption\.

  1. The client uploads the encrypted data to Amazon S3 and saves the encrypted data key as object metadata \(`x-amz-meta-x-amz-key`\) in Amazon S3\.
+ **When downloading an object** — The client downloads the encrypted object from Amazon S3\. Using the material description from the object's metadata, the client determines which master key to use to decrypt the data key\. The client uses that master key to decrypt the data key and then uses the data key to decrypt the object\. 

The client\-side master key that you provide can be either a symmetric key or a public/private key pair\. The following code examples show how to use each type of key\.

For more information, see [Client\-Side Data Encryption with the AWS SDK for Java and Amazon S3](https://aws.amazon.com/articles/2850096021478074)\.

**Note**  
If you get a cipher\-encryption error message when you use the encryption API for the first time, your version of the JDK might have a Java Cryptography Extension \(JCE\) jurisdiction policy file that limits the maximum key length for encryption and decryption transformations to 128 bits\. The AWS SDK requires a maximum key length of 256 bits\.   
To check your maximum key length, use the `getMaxAllowedKeyLength()` method of the `javax.crypto.Cipher` class\. To remove the key\-length restriction, install the [Java Cryptography Extension \(JCE\) Unlimited Strength Jurisdiction Policy Files](https://www.oracle.com/java/technologies/javase-jce8-downloads.html      )\.

**Example**  
The following code example shows how to do these tasks:  
+ Generate a 256\-bit AES key\.
+ Use the AES key to encrypt data on the client side before sending it to Amazon S3\.
+ Use the AES key to decrypt data received from Amazon S3\.
+ Print out a string representation of the decrypted object\.
For instructions on creating and testing a working example, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
        KeyGenerator keyGenerator = KeyGenerator.getInstance("AES");
        keyGenerator.init(256);

// --
        // generate a symmetric encryption key for testing
        SecretKey secretKey = keyGenerator.generateKey();

        String s3ObjectKey = "EncryptedContent2.txt";
        String s3ObjectContent = "This is the 2nd content to encrypt";
// --

        AmazonS3EncryptionV2 s3Encryption = AmazonS3EncryptionClientV2Builder.standard()
                .withRegion(Regions.DEFAULT_REGION)
                .withClientConfiguration(new ClientConfiguration())
                .withCryptoConfiguration(new CryptoConfigurationV2().withCryptoMode(CryptoMode.AuthenticatedEncryption))
                .withEncryptionMaterialsProvider(new StaticEncryptionMaterialsProvider(new EncryptionMaterials(secretKey)))
                .build();

        s3Encryption.putObject(bucket_name, s3ObjectKey, s3ObjectContent);
        System.out.println(s3Encryption.getObjectAsString(bucket_name, s3ObjectKey));
        s3Encryption.shutdown();
```


**Example**  
The following code example shows how to do these tasks:  
+ Generate a 2048\-bit RSA key pair for testing purposes\.
+ Use the RSA keys to encrypt data on the client side before sending it to Amazon S3\.
+ Use the RSA keys to decrypt data received from Amazon S3\.
+ Print out a string representation of the decrypted object\.
For instructions on creating and testing a working example, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
	        KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance("RSA");
	        keyPairGenerator.initialize(2048);
	
	// --
	        // generate an asymmetric key pair for testing
	        KeyPair keyPair = keyPairGenerator.generateKeyPair();
	
	        String s3ObjectKey = "EncryptedContent3.txt";
	        String s3ObjectContent = "This is the 3rd content to encrypt";
	// --
	
	        AmazonS3EncryptionV2 s3Encryption = AmazonS3EncryptionClientV2Builder.standard()
	                .withRegion(Regions.US_WEST_2)
	                .withCryptoConfiguration(new CryptoConfigurationV2().withCryptoMode(CryptoMode.StrictAuthenticatedEncryption))
	                .withEncryptionMaterialsProvider(new StaticEncryptionMaterialsProvider(new EncryptionMaterials(keyPair)))
	                .build();
	
	        s3Encryption.putObject(bucket_name, s3ObjectKey, s3ObjectContent);
	        System.out.println(s3Encryption.getObjectAsString(bucket_name, s3ObjectKey));
	        s3Encryption.shutdown();
```


# Protecting data using server\-side encryption<a name="serv-side-encryption"></a>

Server\-side encryption is the encryption of data at its destination by the application or service that receives it\. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it\. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects\. For example, if you share your objects using a presigned URL, that URL works the same way for both encrypted and unencrypted objects\. Additionally, when you list objects in your bucket, the list API returns a list of all objects, regardless of whether they are encrypted\.

**Note**  
You can't apply different types of server\-side encryption to the same object simultaneously\.

You have three mutually exclusive options, depending on how you choose to manage the encryption keys\.

**Server\-Side Encryption with Amazon S3\-Managed Keys \(SSE\-S3\)**  
When you use Server\-Side Encryption with Amazon S3\-Managed Keys \(SSE\-S3\), each object is encrypted with a unique key\. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates\. Amazon S3 server\-side encryption uses one of the strongest block ciphers available, 256\-bit Advanced Encryption Standard \(AES\-256\), to encrypt your data\. For more information, see [Protecting data using server\-side encryption with Amazon S3\-managed encryption keys \(SSE\-S3\)](UsingServerSideEncryption.md)\.

**Server\-Side Encryption with Customer Master Keys \(CMKs\) Stored in AWS Key Management Service \(SSE\-KMS\)**  
Server\-Side Encryption with Customer Master Keys \(CMKs\) Stored in AWS Key Management Service \(SSE\-KMS\) is similar to SSE\-S3, but with some additional benefits and charges for using this service\. There are separate permissions for the use of a CMK that provides added protection against unauthorized access of your objects in Amazon S3\. SSE\-KMS also provides you with an audit trail that shows when your CMK was used and by whom\. Additionally, you can create and manage customer managed CMKs or use AWS managed CMKs that are unique to you, your service, and your Region\. For more information, see [Protecting data with server\-side encryption using AWS KMS CMKs \(SSE\-KMS\)](UsingKMSEncryption.md)\.

**Server\-Side Encryption with Customer\-Provided Keys \(SSE\-C\)**  
With Server\-Side Encryption with Customer\-Provided Keys \(SSE\-C\), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects\. For more information, see [Protecting data using server\-side encryption with customer\-provided encryption keys \(SSE\-C\)](ServerSideEncryptionCustomerKeys.md)\.


# Making requests using AWS account or IAM user credentials \- AWS SDK for Java<a name="AuthUsingAcctOrUserCredJava"></a>

To send authenticated requests to Amazon S3 using your AWS account or IAM user credentials, do the following: 
+ Use the `AmazonS3ClientBuilder` class to create an `AmazonS3Client` instance\.
+ Run one of the `AmazonS3Client` methods to send requests to Amazon S3\. The client generates the necessary signature from the credentials that you provide and includes it in the request\.

 The following example performs the preceding tasks\. For information on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\. 

**Example**  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListObjectsRequest;
import com.amazonaws.services.s3.model.ObjectListing;
import com.amazonaws.services.s3.model.S3ObjectSummary;

import java.io.IOException;
import java.util.List;

public class MakingRequests {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Get a list of objects in the bucket, two at a time, and 
            // print the name and size of each object.
            ListObjectsRequest listRequest = new ListObjectsRequest().withBucketName(bucketName).withMaxKeys(2);
            ObjectListing objects = s3Client.listObjects(listRequest);
            while (true) {
                List<S3ObjectSummary> summaries = objects.getObjectSummaries();
                for (S3ObjectSummary summary : summaries) {
                    System.out.printf("Object \"%s\" retrieved with size %d\n", summary.getKey(), summary.getSize());
                }
                if (objects.isTruncated()) {
                    objects = s3Client.listNextBatchOfObjects(objects);
                } else {
                    break;
                }
            }
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Related resources<a name="RelatedResources002"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Enabling cross\-origin resource sharing \(CORS\) using the AWS SDK for \.NET<a name="ManageCorsUsingDotNet"></a>

To manage cross\-origin resource sharing \(CORS\) for a bucket, you can use the AWS SDK for \.NET\. For more information about CORS, see [Cross\-origin resource sharing \(CORS\)](cors.md)\.

**Example**  
The following C\# code:  
+ Creates a CORS configuration and sets the configuration on a bucket
+ Retrieves the configuration and modifies it by adding a rule
+ Adds the modified configuration to the bucket
+ Deletes the configuration
For information about creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.   

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CORSTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2; 
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            CORSConfigTestAsync().Wait();
        }
        private static async Task CORSConfigTestAsync()
        {
            try
            {
                // Create a new configuration request and add two rules    
                CORSConfiguration configuration = new CORSConfiguration
                {
                    Rules = new System.Collections.Generic.List<CORSRule>
                        {
                          new CORSRule
                          {
                            Id = "CORSRule1",
                            AllowedMethods = new List<string> {"PUT", "POST", "DELETE"},
                            AllowedOrigins = new List<string> {"http://*.example.com"}
                          },
                          new CORSRule
                          {
                            Id = "CORSRule2",
                            AllowedMethods = new List<string> {"GET"},
                            AllowedOrigins = new List<string> {"*"},
                            MaxAgeSeconds = 3000,
                            ExposeHeaders = new List<string> {"x-amz-server-side-encryption"}
                          }
                        }
                };

                // Add the configuration to the bucket. 
                await PutCORSConfigurationAsync(configuration);

                // Retrieve an existing configuration. 
                configuration = await RetrieveCORSConfigurationAsync();

                // Add a new rule.
                configuration.Rules.Add(new CORSRule
                {
                    Id = "CORSRule3",
                    AllowedMethods = new List<string> { "HEAD" },
                    AllowedOrigins = new List<string> { "http://www.example.com" }
                });

                // Add the configuration to the bucket. 
                await PutCORSConfigurationAsync(configuration);

                // Verify that there are now three rules.
                configuration = await RetrieveCORSConfigurationAsync();
                Console.WriteLine();
                Console.WriteLine("Expected # of rulest=3; found:{0}", configuration.Rules.Count);
                Console.WriteLine();
                Console.WriteLine("Pause before configuration delete. To continue, click Enter...");
                Console.ReadKey();

                // Delete the configuration.
                await DeleteCORSConfigurationAsync();

                // Retrieve a nonexistent configuration.
                configuration = await RetrieveCORSConfigurationAsync();
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        static async Task PutCORSConfigurationAsync(CORSConfiguration configuration)
        {

            PutCORSConfigurationRequest request = new PutCORSConfigurationRequest
            {
                BucketName = bucketName,
                Configuration = configuration
            };

            var response = await s3Client.PutCORSConfigurationAsync(request);
        }

        static async Task<CORSConfiguration> RetrieveCORSConfigurationAsync()
        {
            GetCORSConfigurationRequest request = new GetCORSConfigurationRequest
            {
                BucketName = bucketName

            };
            var response = await s3Client.GetCORSConfigurationAsync(request);
            var configuration = response.Configuration;
            PrintCORSRules(configuration);
            return configuration;
        }

        static async Task DeleteCORSConfigurationAsync()
        {
            DeleteCORSConfigurationRequest request = new DeleteCORSConfigurationRequest
            {
                BucketName = bucketName
            };
            await s3Client.DeleteCORSConfigurationAsync(request);
        }

        static void PrintCORSRules(CORSConfiguration configuration)
        {
            Console.WriteLine();

            if (configuration == null)
            {
                Console.WriteLine("\nConfiguration is null");
                return;
            }

            Console.WriteLine("Configuration has {0} rules:", configuration.Rules.Count);
            foreach (CORSRule rule in configuration.Rules)
            {
                Console.WriteLine("Rule ID: {0}", rule.Id);
                Console.WriteLine("MaxAgeSeconds: {0}", rule.MaxAgeSeconds);
                Console.WriteLine("AllowedMethod: {0}", string.Join(", ", rule.AllowedMethods.ToArray()));
                Console.WriteLine("AllowedOrigins: {0}", string.Join(", ", rule.AllowedOrigins.ToArray()));
                Console.WriteLine("AllowedHeaders: {0}", string.Join(", ", rule.AllowedHeaders.ToArray()));
                Console.WriteLine("ExposeHeader: {0}", string.Join(", ", rule.ExposeHeaders.ToArray()));
            }
        }
    }
}
```


# Multipart upload overview<a name="mpuoverview"></a>

**Topics**
+ [Concurrent multipart upload operations](#distributedmpupload)
+ [Multipart upload and pricing](#mpuploadpricing)
+ [Stopping incomplete multipart uploads using a bucket lifecycle policy](#mpu-stop-incomplete-mpu-lifecycle-config)
+ [Amazon S3 multipart upload limits](qfacts.md)
+ [API support for multipart upload](sdksupportformpu.md)
+ [Multipart upload API and permissions](mpuAndPermissions.md)

The multipart upload API enables you to upload large objects in parts\. You can use this API to upload new large objects or make a copy of an existing object \(see [Operations on objects](ObjectOperations.md)\)\. 

Multipart upload is a three\-step process: You initiate the upload, you upload the object parts, and after you have uploaded all the parts, you complete the multipart upload\. Upon receiving the complete multipart upload request, Amazon S3 constructs the object from the uploaded parts, and you can then access the object just as you would any other object in your bucket\. 

You can list all of your in\-progress multipart uploads or get a list of the parts that you have uploaded for a specific multipart upload\. Each of these operations is explained in this section\.

**Multipart upload initiation**  
When you send a request to initiate a multipart upload, Amazon S3 returns a response with an upload ID, which is a unique identifier for your multipart upload\. You must include this upload ID whenever you upload parts, list the parts, complete an upload, or stop an upload\. If you want to provide any metadata describing the object being uploaded, you must provide it in the request to initiate multipart upload\.

**Parts upload**  
 When uploading a part, in addition to the upload ID, you must specify a part number\. You can choose any part number between 1 and 10,000\. A part number uniquely identifies a part and its position in the object you are uploading\. The part number that you choose doesn’t need to be in a consecutive sequence \(for example, it can be 1, 5, and 14\)\. If you upload a new part using the same part number as a previously uploaded part, the previously uploaded part is overwritten\. Whenever you upload a part, Amazon S3 returns an *ETag* header in its response\. For each part upload, you must record the part number and the ETag value\. You need to include these values in the subsequent request to complete the multipart upload\.

**Note**  
After you initiate a multipart upload and upload one or more parts, you must either complete or stop the multipart upload in order to stop getting charged for storage of the uploaded parts\. Only *after* you either complete or stop a multipart upload will Amazon S3 free up the parts storage and stop charging you for the parts storage\.

**Multipart upload completion**  
When you complete a multipart upload, Amazon S3 creates an object by concatenating the parts in ascending order based on the part number\. If any object metadata was provided in the *initiate multipart upload* request, Amazon S3 associates that metadata with the object\. After a successful *complete* request, the parts no longer exist\. Your *complete multipart upload* request must include the upload ID and a list of both part numbers and corresponding ETag values\. Amazon S3 response includes an ETag that uniquely identifies the combined object data\. This ETag will not necessarily be an MD5 hash of the object data\. You can optionally stop the multipart upload\. After stopping a multipart upload, you cannot upload any part using that upload ID again\. All storage from any part of the cancelled multipart upload is then freed\. If any part uploads were in\-progress, they can still succeed or fail even after you stop\. To free all storage consumed by all parts, you must stop a multipart upload only after all part uploads have completed\.

**Multipart upload listings**  
You can list the parts of a specific multipart upload or all in\-progress multipart uploads\. The list parts operation returns the parts information that you have uploaded for a specific multipart upload\. For each list parts request, Amazon S3 returns the parts information for the specified multipart upload, up to a maximum of 1,000 parts\. If there are more than 1,000 parts in the multipart upload, you must send a series of list part requests to retrieve all the parts\. Note that the returned list of parts doesn't include parts that haven't completed uploading\. Using the *list multipart uploads* operation, you can obtain a list of multipart uploads in progress\. An in\-progress multipart upload is an upload that you have initiated, but have not yet completed or stopped\. Each request returns at most 1000 multipart uploads\. If there are more than 1,000 multipart uploads in progress, you need to send additional requests to retrieve the remaining multipart uploads\. Only use the returned listing for verification\. You should not use the result of this listing when sending a *complete multipart upload* request\. Instead, maintain your own list of the part numbers you specified when uploading parts and the corresponding ETag values that Amazon S3 returns\.

## Concurrent multipart upload operations<a name="distributedmpupload"></a>

In a distributed development environment, it is possible for your application to initiate several updates on the same object at the same time\. Your application might initiate several multipart uploads using the same object key\. For each of these uploads, your application can then upload parts and send a complete upload request to Amazon S3 to create the object\. When the buckets have versioning enabled, completing a multipart upload always creates a new version\. For buckets that do not have versioning enabled, it is possible that some other request received between the time when a multipart upload is initiated and when it is completed might take precedence\. 

**Note**  
It is possible for some other request received between the time you initiated a multipart upload and completed it to take precedence\. For example, if another operation deletes a key after you initiate a multipart upload with that key, but before you complete it, the complete multipart upload response might indicate a successful object creation without you ever seeing the object\. 

## Multipart upload and pricing<a name="mpuploadpricing"></a>

Once you initiate a multipart upload, Amazon S3 retains all the parts until you either complete or stop the upload\. Throughout its lifetime, you are billed for all storage, bandwidth, and requests for this multipart upload and its associated parts\. If you stop the multipart upload, Amazon S3 deletes upload artifacts and any parts that you have uploaded, and you are no longer billed for them\. For more information about pricing, see [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\.

## Stopping incomplete multipart uploads using a bucket lifecycle policy<a name="mpu-stop-incomplete-mpu-lifecycle-config"></a>

After you initiate a multipart upload, you begin uploading parts\. Amazon S3 stores these parts, but it creates the object from the parts only after you upload all of them and send a `successful` request to complete the multipart upload \(you should verify that your request to complete multipart upload is successful\)\. Upon receiving the complete multipart upload request, Amazon S3 assembles the parts and creates an object\.

If you don't send the complete multipart upload request successfully, Amazon S3 will not assemble the parts and will not create any object\. Therefore, the parts remain in Amazon S3 and you pay for the parts that are stored in Amazon S3\. As a best practice, we recommend you configure a lifecycle rule \(using the `AbortIncompleteMultipartUpload` action\) to minimize your storage costs\.

Amazon S3 supports a bucket lifecycle rule that you can use to direct Amazon S3 to stop multipart uploads that don't complete within a specified number of days after being initiated\. When a multipart upload is not completed within the time frame, it becomes eligible for stoppage and Amazon S3 stops the multipart upload \(and deletes the parts associated with the multipart upload\)\.

 The following is an example lifecycle configuration that specifies a rule with the `AbortIncompleteMultipartUpload` action\.

```
<LifecycleConfiguration>
    <Rule>
        <ID>sample-rule</ID>
        <Prefix></Prefix>
        <Status>Enabled</Status>
        <AbortIncompleteMultipartUpload>
          <DaysAfterInitiation>7</DaysAfterInitiation>
        </AbortIncompleteMultipartUpload>
    </Rule>
</LifecycleConfiguration>
```

In the example, the rule does not specify a value for the `Prefix` element \([object key name prefix](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix)\)\. Therefore, it applies to all objects in the bucket for which you initiated multipart uploads\. Any multipart uploads that were initiated and did not complete within seven days become eligible for stoppage\. This action has no effect on completed multipart uploads\.



For more information about the bucket lifecycle configuration, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

**Note**  
If the multipart upload is completed within the number of days specified in the rule, the `AbortIncompleteMultipartUpload` lifecycle action does not apply \(that is, Amazon S3 will not take any action\)\. Also, this action does not apply to objects, no objects are deleted by this lifecycle action\.



The following `put-bucket-lifecycle-configuration` CLI command adds the lifecycle configuration for the specified bucket\. 

```
$ aws s3api put-bucket-lifecycle-configuration  \
        --bucket bucketname  \
        --lifecycle-configuration filename-containing-lifecycle-configuration
```

To test the CLI command, do the following:



1.  Set up the AWS CLI\. For instructions, see [Setting Up the AWS CLI](setup-aws-cli.md)\. 

1.  Save the following example lifecycle configuration in a file \(lifecycle\.json\)\. The example configuration specifies empty prefix and therefore it applies to all objects in the bucket\. You can specify a prefix to restrict the policy to a subset of objects\.

   ```
   {
       "Rules": [
           {
               "ID": "Test Rule",
               "Status": "Enabled",
               "Filter": {
                   "Prefix": ""
               },
               "AbortIncompleteMultipartUpload": {
                   "DaysAfterInitiation": 7
               }
           }
       ]
   }
   ```

1.  Run the following CLI command to set lifecycle configuration on your bucket\. 

   ```
   aws s3api put-bucket-lifecycle-configuration   \
   --bucket bucketname  \
   --lifecycle-configuration file://lifecycle.json
   ```

1.  To verify, retrieve the lifecycle configuration using the `get-bucket-lifecycle` CLI command\. 

   ```
   aws s3api get-bucket-lifecycle  \
   --bucket bucketname
   ```

1.  To delete the lifecycle configuration use the `delete-bucket-lifecycle` CLI command\. 

   ```
   aws s3api delete-bucket-lifecycle \
   --bucket bucketname
   ```






# Upload a file<a name="HLuploadFileJava"></a>

**Example**  
The following example shows how to upload an object using the high\-level multipart\-upload Java API \(the `TransferManager` class\)\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.transfer.TransferManager;
import com.amazonaws.services.s3.transfer.TransferManagerBuilder;
import com.amazonaws.services.s3.transfer.Upload;

import java.io.File;

public class HighLevelMultipartUpload {

    public static void main(String[] args) throws Exception {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Object key ***";
        String filePath = "*** Path for file to upload ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();
            TransferManager tm = TransferManagerBuilder.standard()
                    .withS3Client(s3Client)
                    .build();

            // TransferManager processes all transfers asynchronously,
            // so this call returns immediately.
            Upload upload = tm.upload(bucketName, keyName, new File(filePath));
            System.out.println("Object upload started");

            // Optionally, wait for the upload to finish before continuing.
            upload.waitForCompletion();
            System.out.println("Object upload complete");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Monitoring tools<a name="monitoring-automated-manual"></a>

AWS provides various tools that you can use to monitor Amazon S3\. You can configure some of these tools to do the monitoring for you, while some of the tools require manual intervention\. We recommend that you automate monitoring tasks as much as possible\.

## Automated monitoring tools<a name="monitoring-automated_tools"></a>

You can use the following automated monitoring tools to watch Amazon S3 and report when something is wrong:
+ **Amazon CloudWatch Alarms** – Watch a single metric over a time period that you specify, and perform one or more actions based on the value of the metric relative to a given threshold over a number of time periods\. The action is a notification sent to an Amazon Simple Notification Service \(Amazon SNS\) topic or Amazon EC2 Auto Scaling policy\. CloudWatch alarms do not invoke actions simply because they are in a particular state\. The state must have changed and been maintained for a specified number of periods\. For more information, see [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)\.
+ **AWS CloudTrail Log Monitoring** – Share log files between accounts, monitor CloudTrail log files in real time by sending them to CloudWatch Logs, write log processing applications in Java, and validate that your log files have not changed after delivery by CloudTrail\. For more information, see [Logging Amazon S3 API calls using AWS CloudTrail](cloudtrail-logging.md)\.

## Manual monitoring tools<a name="monitoring-manual-tools"></a>

Another important part of monitoring Amazon S3 involves manually monitoring those items that the CloudWatch alarms don't cover\. The Amazon S3, CloudWatch, Trusted Advisor, and other AWS Management Console dashboards provide an at\-a\-glance view of the state of your AWS environment\. You might want to enable server access logging, which tracks requests for access to your bucket\. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and error code, if any\. For more information, see [Amazon S3 server access logging](ServerLogs.md) in the *Amazon Simple Storage Service Developer Guide*\.
+ Amazon S3 dashboard shows:
  + Your buckets and the objects and properties they contain\.
+ CloudWatch home page shows:
  + Current alarms and status\.
  + Graphs of alarms and resources\.
  + Service health status\.

  In addition, you can use CloudWatch to do the following: 
  + Create [customized dashboards](https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/CloudWatch_Dashboards.html) to monitor the services you care about\.
  + Graph metric data to troubleshoot issues and discover trends\.
  + Search and browse all your AWS resource metrics\.
  + Create and edit alarms to be notified of problems\.
+ AWS Trusted Advisor can help you monitor your AWS resources to improve performance, reliability, security, and cost effectiveness\. Four Trusted Advisor checks are available to all users; more than 50 checks are available to users with a Business or Enterprise support plan\. For more information, see [AWS Trusted Advisor](https://aws.amazon.com/premiumsupport/trustedadvisor/)\.

  Trusted Advisor has these checks that relate to Amazon S3: 
  + Checks of the logging configuration of Amazon S3 buckets\.
  + Security checks for Amazon S3 buckets that have open access permissions\.
  + Fault tolerance checks for Amazon S3 buckets that do not have versioning enabled, or have versioning suspended\.


# Generate a presigned object URL using the AWS SDK for Java<a name="ShareObjectPreSignedURLJavaSDK"></a>

**Example**  
The following example generates a presigned URL that you can give to others so that they can retrieve an object from an S3 bucket\. For more information, see [Share an object with others](ShareObjectPreSignedURL.md)\.   
 For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.HttpMethod;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.GeneratePresignedUrlRequest;

import java.io.IOException;
import java.net.URL;

public class GeneratePresignedURL {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String objectKey = "*** Object key ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

            // Set the presigned URL to expire after one hour.
            java.util.Date expiration = new java.util.Date();
            long expTimeMillis = expiration.getTime();
            expTimeMillis += 1000 * 60 * 60;
            expiration.setTime(expTimeMillis);

            // Generate the presigned URL.
            System.out.println("Generating pre-signed URL.");
            GeneratePresignedUrlRequest generatePresignedUrlRequest =
                    new GeneratePresignedUrlRequest(bucketName, objectKey)
                            .withMethod(HttpMethod.GET)
                            .withExpiration(expiration);
            URL url = s3Client.generatePresignedUrl(generatePresignedUrlRequest);

            System.out.println("Pre-Signed URL: " + url.toString());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Replication configuration overview<a name="replication-add-config"></a>

Amazon S3 stores a replication configuration as XML\. In the replication configuration XML file, you specify an AWS Identity and Access Management \(IAM\) role and one or more rules\. 

```
<ReplicationConfiguration>
    <Role>IAM-role-ARN</Role>
    <Rule>
        ...
    </Rule>
    <Rule>
         ... 
    </Rule>
     ...
</ReplicationConfiguration>
```

Amazon S3 can't replicate objects without your permission\. You grant permissions with the IAM role that you specify in the replication configuration\. Amazon S3 assumes the IAM role to replicate objects on your behalf\. You must grant the required permissions to the IAM role first\. For more information about managing permissions, see [Setting up permissions for replication](setting-repl-config-perm-overview.md)\.

You add one rule in replication configuration in the following scenarios:
+ You want to replicate all objects\.
+ You want to replicate a subset of objects\. You identify the object subset by adding a filter in the rule\. In the filter, you specify an object key prefix, tags, or a combination of both, to identify the subset of objects that the rule applies to\. 

You add multiple rules in a replication configuration if you want to select a different subset of objects\. In each rule, you specify a filter that selects a different subset of objects\. For example, you might choose to replicate objects that have either `tax/` or` document/` key prefixes\. You would add two rules and specify the `tax/` key prefix filter in one rule and the `document/` key prefix in the other\.

The following sections provide additional information\.

**Topics**
+ [Basic rule configuration](#replication-config-min-rule-config)
+ [Optional: Specifying a filter](#replication-config-optional-filter)
+ [Additional destination configurations](#replication-config-optional-dest-config)
+ [Example replication configurations](#replication-config-example-configs)
+ [Backward compatibility](#replication-backward-compat-considerations)

## Basic rule configuration<a name="replication-config-min-rule-config"></a>

### <a name="replication-config-min-rule-config"></a>

Each rule must include the rule's status and priority, and indicate whether to replicate delete markers\. 
+ `Status` indicates whether the rule is enabled or disabled\. If a rule is disabled, Amazon S3 doesn't perform the actions specified in the rule\. 
+ `Priority` indicates which rule has precedence whenever two or more replication rules conflict\. Amazon S3 will attempt to replicate objects according to all replication rules\. However, if there are two or more rules with the same destination bucket, then objects will be replicated according to the rule with the highest priority\. The higher the number, the higher the priority\.

In the destination configuration, you must provide the name of the bucket or buckets where you want Amazon S3 to replicate objects\. 

The following code shows the minimum requirements for a rule\.

```
...
    <Rule>
        <ID>Rule-1</ID>
        <Status>rule-Enabled-or-Disabled</Status>
        <Priority>integer</Priority>
        <DeleteMarkerReplication>
           <Status>Disabled</Status>
        </DeleteMarkerReplication>
        <Destination>        
           <Bucket>arn:aws:s3:::bucket-name</Bucket> 
        </Destination>    
    </Rule>
    <Rule>
         ...
    </Rule>
     ...
...
```

You can also specify other configuration options\. For example, you might choose to use a storage class for object replicas that differs from the class for the source object\. 

## Optional: Specifying a filter<a name="replication-config-optional-filter"></a>

To choose a subset of objects that the rule applies to, add an optional filter\. You can filter by object key prefix, object tags, or combination of both\. If you filter on both a key prefix and object tags, Amazon S3 combines the filters using a logical AND operator\. In other words, the rule applies to a subset of objects with a specific key prefix and specific tags\. 

To specify a rule with a filter based on an object key prefix, use the following code\. You can specify only one prefix\.

```
<Rule>
    ...
    <Filter>
        <Prefix>key-prefix</Prefix>   
    </Filter>
    ...
</Rule>
...
```

To specify a rule with a filter based on object tags, use the following code\. You can specify one or more object tags\.

```
<Rule>
    ...
    <Filter>
        <And>
            <Tag>
                <Key>key1</Key>
                <Value>value1</Value>
            </Tag>
            <Tag>
                <Key>key2</Key>
                <Value>value2</Value>
            </Tag>
             ...
        </And>
    </Filter>
    ...
</Rule>
...
```

To specify a rule filter with a combination of a key prefix and object tags, use the following code\. You wrap these filters in an `AND` parent element\. Amazon S3 performs a logical `AND` operation to combine these filters\. In other words, the rule applies to a subset of objects with a specific key prefix and specific tags\. 

```
<Rule>
    ...
    <Filter>
        <And>
            <Prefix>key-prefix</Prefix>
            <Tag>
                <Key>key1</Key>
                <Value>value1</Value>
            </Tag>
            <Tag>
                <Key>key2</Key>
                <Value>value2</Value>
            </Tag>
             ...
    </Filter>
    ...
</Rule>
...
```

## Additional destination configurations<a name="replication-config-optional-dest-config"></a>

In the destination configuration, you specify the bucket or buckets where you want Amazon S3 to replicate objects\. You can set configurations to replicate objects from one source bucket to one or more destination buckets\. 

```
...
<Destination>        
    <Bucket>arn:aws:s3:::destination-bucket</Bucket>
</Destination>
...
```

You can add the following options in the `<Destination>` element:
+ You can specify the storage class for the object replicas\. By default, Amazon S3 uses the storage class of the source object to create object replicas, as in the following example\. 

  ```
  ...
  <Destination>
         <Bucket>arn:aws:s3:::destinationbucket</Bucket>
         <StorageClass>storage-class</StorageClass>
  </Destination>
  ...
  ```
+ You can add multiple destination buckets in a single replication configuration, as follows\.

  ```
  ...
  <Rule>
      <ID>Rule-1</ID>
      <Status>rule-Enabled-or-Disabled</Status>
      <Priority>integer</Priority>
      <DeleteMarkerReplication>
         <Status>Enabled-or-Disabled</Status>
      </DeleteMarkerReplication>
      <Destination>        
         <Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket> 
      </Destination>    
  </Rule>
  <Rule>
      <ID>Rule-2</ID>
      <Status>rule-Enabled-or-Disabled</Status>
      <Priority>integer</Priority>
      <DeleteMarkerReplication>
         <Status>Enabled-or-Disabled</Status>
      </DeleteMarkerReplication>
      <Destination>        
         <Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET2</Bucket> 
      </Destination>    
  </Rule>
  ...
  ```
+ When adding multiple destination buckets in a single replication configuration, you can specify different parameters for each replication rule, as follows\.

  ```
  ...
  <Rule>
      <ID>Rule-1</ID>
      <Status>rule-Enabled-or-Disabled</Status>
      <Priority>integer</Priority>
      <DeleteMarkerReplication>
         <Status>Disabled</Status>
      </DeleteMarkerReplication>
        <Metrics>
      <Status>Enabled</Status>
      <EventThreshold>
        <Minutes>15</Minutes> 
      </EventThreshold>
    </Metrics>
      <Destination>        
         <Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket> 
      </Destination>    
  </Rule>
  <Rule>
      <ID>Rule-2</ID>
      <Status>rule-Enabled-or-Disabled</Status>
      <Priority>integer</Priority>
      <DeleteMarkerReplication>
         <Status>Enabled</Status>
      </DeleteMarkerReplication>
        <Metrics>
      <Status>Enabled</Status>
      <EventThreshold>
        <Minutes>15</Minutes> 
      </EventThreshold>
    </Metrics>
    <ReplicationTime>
      <Status>Enabled</Status>
      <Time>
        <Minutes>15</Minutes>
      </Time>
    </ReplicationTime>
      <Destination>        
         <Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET2</Bucket> 
      </Destination>    
  </Rule>
  ...
  ```
+ When source and destination buckets aren't owned by the same accounts, you can change the ownership of the replica to the AWS account that owns the destination bucket by adding the `AccessControlTranslation` element\.

  ```
  ...
  <Destination>
     <Bucket>arn:aws:s3:::destinationbucket</Bucket>
     <Account>destination-bucket-owner-account-id</Account>
     <AccessControlTranslation>
         <Owner>Destination</Owner>
     </AccessControlTranslation>
  </Destination>
  ...
  ```

  If you don't add this element to the replication configuration, the replicas are owned by the same AWS account that owns the source object\. For more information, see [Changing the replica owner](replication-change-owner.md)\.
+ You can enable S3 Replication Time Control \(S3 RTC\) in your replication configuration\. S3 RTC replicates most objects in seconds and 99\.99 percent of objects within 15 minutes \(backed by a service level agreement\)\. 
**Note**  
Only a valid value of `<Minutes>15</Minutes>` is accepted for `EventThreshold` and `Time`\.

  ```
  ...
  <Destination>
    <Bucket>arn:aws:s3:::destinationbucket</Bucket>
    <Metrics>
      <Status>Enabled</Status>
      <EventThreshold>
        <Minutes>15</Minutes> 
      </EventThreshold>
    </Metrics>
    <ReplicationTime>
      <Status>Enabled</Status>
      <Time>
        <Minutes>15</Minutes>
      </Time>
    </ReplicationTime>
  </Destination>
  ...
  ```

  For more information, see [Meeting compliance requirements using S3 Replication Time Control \(S3 RTC\)](replication-time-control.md)\. For API examples, see [PutBucketReplication](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketReplication.html) in the *Amazon Simple Storage Service API Reference*\.

  
+ Your source bucket might contain objects that were created with server\-side encryption using keys stored in AWS KMS\. By default, Amazon S3 doesn't replicate these objects\. You can optionally direct Amazon S3 to replicate these objects\. First, explicitly opt into this feature by adding the `SourceSelectionCriteria` element, and then provide the AWS KMS CMK \(for the AWS Region of the destination bucket\) to use for encrypting object replicas\. 

  ```
  ...
  <SourceSelectionCriteria>
    <SseKmsEncryptedObjects>
      <Status>Enabled</Status>
    </SseKmsEncryptedObjects>
  </SourceSelectionCriteria>
  <Destination>
    <Bucket>arn:aws:s3:::dest-bucket-name</Bucket>
    <EncryptionConfiguration>
      <ReplicaKmsKeyID>AWS KMS key IDs to use for encrypting object replicas</ReplicaKmsKeyID>
    </EncryptionConfiguration>
  </Destination>
  ...
  ```

  For more information, see [Replicating objects created with server\-side encryption \(SSE\) using encryption keys stored in AWS KMS](replication-config-for-kms-objects.md)\.

## Example replication configurations<a name="replication-config-example-configs"></a>

To get started, you can add the following example replication configurations to your bucket, as appropriate\.

**Important**  
To add a replication configuration to a bucket, you must have the `iam:PassRole` permission\. This permission allows you to pass the IAM role that grants Amazon S3 replication permissions\. You specify the IAM role by providing the Amazon Resource Name \(ARN\) that is used in the `Role` element in the replication configuration XML\. For more information, see [Granting a User Permissions to Pass a Role to an AWS Service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html) in the *IAM User Guide*\.

**Example 1: Replication configuration with one rule**  
The following basic replication configuration specifies one rule\. The rule specifies an IAM role that Amazon S3 can assume and a single destination bucket for object replicas\. The rule `Status` indicates that the rule is in effect\.  

```
<?xml version="1.0" encoding="UTF-8"?>
<ReplicationConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  <Role>arn:aws:iam::AcctID:role/role-name</Role>
  <Rule>
    <Status>Enabled</Status>

    <Destination><Bucket>arn:aws:s3:::destinationbucket</Bucket></Destination>

  </Rule>
</ReplicationConfiguration>
```
To choose a subset of objects to replicate, you can add a filter\. In the following configuration, the filter specifies an object key prefix\. This rule applies to objects that have the prefix `Tax/` in their key names\.   

```
<?xml version="1.0" encoding="UTF-8"?>
<ReplicationConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  <Role>arn:aws:iam::AcctID:role/role-name</Role>
  <Rule>
    <Status>Enabled</Status>
    <Priority>1</Priority>
    <DeleteMarkerReplication>
       <Status>string</Status>
    </DeleteMarkerReplication>

    <Filter>
       <Prefix>Tax/</Prefix>
    </Filter>

    <Destination><Bucket>arn:aws:s3:::destinationbucket</Bucket></Destination>

  </Rule>
</ReplicationConfiguration>
```
If you specify the `Filter` element, you must also include the `Priority` and `DeleteMarkerReplication` elements\. In this example, priority is irrelevant because there is only one rule\.  
In the following configuration, the filter specifies one prefix and two tags\. The rule applies to the subset of objects that have the specified key prefix and tags\. Specifically, it applies to object that have the `Tax/` prefix in their key names and the two specified object tags\. Priority doesn't apply because there is only one rule\.  

```
<?xml version="1.0" encoding="UTF-8"?>
<ReplicationConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  <Role>arn:aws:iam::AcctID:role/role-name</Role>
  <Rule>
    <Status>Enabled</Status>
    <Priority>1</Priority>
    <DeleteMarkerReplication>
       <Status>string</Status>
    </DeleteMarkerReplication>

    <Filter>
        <And>
          <Prefix>Tax/</Prefix>
          <Tag>
             <Tag>
                <Key>tagA</Key>
                <Value>valueA</Value>
             </Tag>
          </Tag>
          <Tag>
             <Tag>
                <Key>tagB</Key>
                <Value>valueB</Value>
             </Tag>
          </Tag>
       </And>

    </Filter>

    <Destination><Bucket>arn:aws:s3:::destinationbucket</Bucket></Destination>

  </Rule>
</ReplicationConfiguration>
```
You can specify a storage class for the object replicas as follows\.  

```
<?xml version="1.0" encoding="UTF-8"?>

<ReplicationConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  <Role>arn:aws:iam::account-id:role/role-name</Role>
  <Rule>
    <Status>Enabled</Status>
    <Destination>
       <Bucket>arn:aws:s3:::destinationbucket</Bucket>
       <StorageClass>storage-class</StorageClass>
    </Destination>
  </Rule>
</ReplicationConfiguration>
```
You can specify any storage class that Amazon S3 supports\.  
 

**Example 2: Replication configuration with two rules**  

**Example**  
In the following replication configuration:  
+ Each rule filters on a different key prefix so that each rule applies to a distinct subset of objects\. Amazon S3 replicates objects with key names `Tax/doc1.pdf` and `Project/project1.txt`, but it doesn't replicate objects with the key name `PersonalDoc/documentA`\. 
+  Rule priority is irrelevant because the rules apply to two distinct sets of objects\. The next example shows what happens when rule priority is applied\. 
+ The second rule specifies a storage class for object replicas\. Amazon S3 uses the specified storage class for those object replicas\.
   

```
<?xml version="1.0" encoding="UTF-8"?>

<ReplicationConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  <Role>arn:aws:iam::account-id:role/role-name</Role>
  <Rule>
    <Status>Enabled</Status>
    <Priority>1</Priority>
    <DeleteMarkerReplication>
       <Status>string</Status>
    </DeleteMarkerReplication>
    <Filter>
        <Prefix>Tax</Prefix>
    </Filter>
    <Status>Enabled</Status>
    <Destination>
      <Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>
    </Destination>
     ...
  </Rule>
 <Rule>
    <Status>Enabled</Status>
    <Priority>2</Priority>
    <DeleteMarkerReplication>
       <Status>string</Status>
    </DeleteMarkerReplication>
    <Filter>
        <Prefix>Project</Prefix>
    </Filter>
    <Status>Enabled</Status>
    <Destination>
      <Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>
     <StorageClass>STANDARD_IA</StorageClass>
    </Destination>
     ...
  </Rule>


</ReplicationConfiguration>
```

**Example 3: Replication configuration with two rules with overlapping prefixes**  <a name="overlap-rule-example"></a>
In this configuration, the two rules specify filters with overlapping key prefixes, `star` and `starship`\. Both rules apply to objects with the key name `starship-x`\. In this case, Amazon S3 uses the rule priority to determine which rule to apply\. The higher the number, the higher the priority\.  

```
<ReplicationConfiguration>

  <Role>arn:aws:iam::AcctID:role/role-name</Role>

  <Rule>
    <Status>Enabled</Status>
    <Priority>1</Priority>
    <DeleteMarkerReplication>
       <Status>string</Status>
    </DeleteMarkerReplication>
    <Filter>
        <Prefix>star</Prefix>
    </Filter>
    <Destination>
      <Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>
    </Destination>
  </Rule>
  <Rule>
    <Status>Enabled</Status>
    <Priority>2</Priority>
    <DeleteMarkerReplication>
       <Status>string</Status>
    </DeleteMarkerReplication>
    <Filter>
        <Prefix>starship</Prefix>
    </Filter>    
    <Destination>
      <Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>
    </Destination>
  </Rule>
</ReplicationConfiguration>
```

**Example 4: Example walkthroughs**  
For example walkthroughs, see [Replication walkthroughs](replication-example-walkthroughs.md)\.

For more information about the XML structure of replication configuration, see [PutBucketReplication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTreplication.html) in the *Amazon Simple Storage Service API Reference*\. 

## Backward compatibility<a name="replication-backward-compat-considerations"></a>

The latest version of the replication configuration XML is V2\. XML V2 replication configurations are those that contain the `Filter` element for rules, and rules that specify S3 Replication Time Control \(S3 RTC\)\. In V2 replication configurations, Amazon S3 doesn't replicate delete markers for tag\-based rules\. Therefore, you must set the `DeleteMarkerReplication` element to `Disabled` if using a tag\-based V2 rule\.

To see your replication configuration version, you can use the `GetBucketReplication` API\. For more information see, [GetBucketReplication](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetBucketReplication.html) in the *Amazon Simple Storage Service API Reference*\. 

 For backward compatibility, Amazon S3 continues to support the XML V1 replication configuration\. If you have used XML V1 replication configuration, consider the following issues that affect backward compatibility:
+ Replication configuration XML V2 includes the `Filter` element for rules\. With the `Filter` element, you can specify object filters based on the object key prefix, tags, or both to scope the objects that the rule applies to\. Replication configuration XML V1 supported filtering based only on the key prefix\. In that case, you add the `Prefix` directly as a child element of the `Rule` element, as in the following example\.

  ```
  <?xml version="1.0" encoding="UTF-8"?>
  <ReplicationConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
    <Role>arn:aws:iam::AcctID:role/role-name</Role>
    <Rule>
      <Status>Enabled</Status>
      <Prefix>key-prefix</Prefix>
      <Destination><Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket></Destination>
  
    </Rule>
  </ReplicationConfiguration>
  ```

  For backward compatibility, `Amazon S3` continues to support the V1 configuration\. 
+ When you delete an object from your source bucket without specifying an object version ID, Amazon S3 adds a delete marker\. If you use V1 of the replication configuration XML, Amazon S3 replicates delete markers that resulted from user actions\. In other words, if the user deleted the object, and not if Amazon S3 deleted it because the object expired as part of lifecycle action\.

  ```
  ...
      <Rule>
          <ID>Rule-1</ID>
          <Status>rule-Enabled-or-Disabled</Status>
          <Priority>integer</Priority>
          <DeleteMarkerReplication>
             <Status>Disabled</Status>
          </DeleteMarkerReplication>        
          <Destination>        
             <Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket> 
          </Destination>    
      </Rule>
  ...
  ```

 



# Restoring previous versions<a name="RestoringPreviousVersions"></a>

One of the value propositions of versioning is the ability to retrieve previous versions of an object\. There are two approaches to doing so:
+ Copy a previous version of the object into the same bucket

  The copied object becomes the current version of that object and all object versions are preserved\.
+ Permanently delete the current version of the object

  When you delete the current object version, you, in effect, turn the previous version into the current version of that object\.

Because all object versions are preserved, you can make any earlier version the current version by copying a specific version of the object into the same bucket\. In the following figure, the source object \(ID = 111111\) is copied into the same bucket\. Amazon S3 supplies a new ID \(88778877\) and it becomes the current version of the object\. So, the bucket has both the original object version \(111111\) and its copy \(88778877\)\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_COPY2.png)

A subsequent `GET` will retrieve version 88778877\.

The following figure shows how deleting the current version \(121212\) of an object, which leaves the previous version \(111111\) as the current object\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_COPY_delete2.png)

A subsequent `GET` will retrieve version 111111\.

For more information on retrieving previous versions of an object, see [Retrieving object versions](RetrievingObjectVersions.md)\.


# Downloading objects in Requester Pays buckets<a name="ObjectsinRequesterPaysBuckets"></a>

Because requesters are charged for downloading data from Requester Pays buckets, the requests must contain a special parameter, `x-amz-request-payer`, which confirms that the requester knows he or she will be charged for the download\. To access objects in Requester Pays buckets, requests must include one of the following\.
+ For GET, HEAD, and POST requests, include `x-amz-request-payer : requester` in the header
+ For signed URLs, include `x-amz-request-payer=requester` in the request

If the request succeeds and the requester is charged, the response includes the header `x-amz-request-charged:requester`\. If `x-amz-request-payer` is not in the request, Amazon S3 returns a 403 error and charges the bucket owner for the request\.

**Note**  
Bucket owners do not need to add `x-amz-request-payer` to their requests\.  
Ensure that you have included `x-amz-request-payer` and its value in your signature calculation\. For more information, see [Constructing the CanonicalizedAmzHeaders Element](RESTAuthentication.md#RESTAuthenticationConstructingCanonicalizedAmzHeaders)\.

**To download objects from a Requester Pays bucket**
+  Use a `GET` request to download an object from a Requester Pays bucket, as shown in the following request\.

  ```
  1. GET / [destinationObject] HTTP/1.1
  2. Host: [BucketName].s3.amazonaws.com
  3. x-amz-request-payer : requester
  4. Date: Wed, 01 Mar 2009 12:00:00 GMT
  5. Authorization: AWS [Signature]
  ```

If the GET request succeeds and the requester is charged, the response includes `x-amz-request-charged:requester`\.

Amazon S3 can return an `Access Denied` error for requests that try to get objects from a Requester Pays bucket\. For more information, see [Error Responses](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html)\.


# Enabling cross\-origin resource sharing \(CORS\)<a name="ManageCorsUsing"></a>

Enable cross\-origin resource sharing by setting a CORS configuration on your bucket using the AWS Management Console, the REST API, or the AWS SDKs\.

**Topics**
+ [Enabling cross\-origin resource sharing \(CORS\) using the AWS Management Console](ManageCorsUsingConsole.md)
+ [Enabling cross\-origin resource sharing \(CORS\) using the AWS SDK for Java](ManageCorsUsingJava.md)
+ [Enabling cross\-origin resource sharing \(CORS\) using the AWS SDK for \.NET](ManageCorsUsingDotNet.md)
+ [Enabling cross\-origin resource sharing \(CORS\) using the REST API](EnableCorsUsingREST.md)


# Using Amazon S3 dual\-stack endpoints<a name="dual-stack-endpoints"></a>

Amazon S3 dual\-stack endpoints support requests to S3 buckets over IPv6 and IPv4\. This section describes how to use dual\-stack endpoints\.

**Topics**
+ [Amazon S3 dual\-stack endpoints](#dual-stack-endpoints-description)
+ [Using dual\-stack endpoints from the AWS CLI](#dual-stack-endpoints-cli)
+ [Using dual\-stack endpoints from the AWS SDKs](#dual-stack-endpoints-sdks)
+ [Using dual\-stack endpoints from the REST API](#dual-stack-endpoints-examples-rest-api)

## Amazon S3 dual\-stack endpoints<a name="dual-stack-endpoints-description"></a>

When you make a request to a dual\-stack endpoint, the bucket URL resolves to an IPv6 or an IPv4 address\. For more information about accessing a bucket over IPv6, see [Making requests to Amazon S3 over IPv6](ipv6-access.md)\.

When using the REST API, you directly access an Amazon S3 endpoint by using the endpoint name \(URI\)\. You can access an S3 bucket through a dual\-stack endpoint by using a virtual hosted\-style or a path\-style endpoint name\. Amazon S3 supports only regional dual\-stack endpoint names, which means that you must specify the region as part of the name\. 

Use the following naming conventions for the dual\-stack virtual hosted\-style and path\-style endpoint names:
+ Virtual hosted\-style dual\-stack endpoint: 

   *bucketname*\.s3\.dualstack\.*aws\-region*\.amazonaws\.com

   
+ Path\-style dual\-stack endpoint: 

  s3\.dualstack\.*aws\-region*\.amazonaws\.com/*bucketname*

For more information, about endpoint name style, see [Accessing a bucket](UsingBucket.md#access-bucket-intro)\. For a list of Amazon S3 endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\. 

**Important**  
You can use transfer acceleration with dual\-stack endpoints\. For more information, see [Getting Started with Amazon S3 Transfer Acceleration](transfer-acceleration.md#transfer-acceleration-getting-started)\.

When using the AWS Command Line Interface \(AWS CLI\) and AWS SDKs, you can use a parameter or flag to change to a dual\-stack endpoint\. You can also specify the dual\-stack endpoint directly as an override of the Amazon S3 endpoint in the config file\. The following sections describe how to use dual\-stack endpoints from the AWS CLI and the AWS SDKs\.

## Using dual\-stack endpoints from the AWS CLI<a name="dual-stack-endpoints-cli"></a>

This section provides examples of AWS CLI commands used to make requests to a dual\-stack endpoint\. For instructions on setting up the AWS CLI, see [Setting Up the AWS CLI](setup-aws-cli.md)\.

You set the configuration value `use_dualstack_endpoint` to `true` in a profile in your AWS Config file to direct all Amazon S3 requests made by the `s3` and `s3api` AWS CLI commands to the dual\-stack endpoint for the specified region\. You specify the region in the config file or in a command using the `--region` option\. 

When using dual\-stack endpoints with the AWS CLI, both `path` and `virtual` addressing styles are supported\. The addressing style, set in the config file, controls if the bucket name is in the hostname or part of the URL\. By default, the CLI will attempt to use virtual style where possible, but will fall back to path style if necessary\. For more information, see [AWS CLI Amazon S3 Configuration](https://docs.aws.amazon.com/cli/latest/topic/s3-config.html)\.

You can also make configuration changes by using a command, as shown in the following example, which sets `use_dualstack_endpoint` to `true` and `addressing_style` to `virtual` in the default profile\.

```
$ aws configure set default.s3.use_dualstack_endpoint true
$ aws configure set default.s3.addressing_style virtual
```

If you want to use a dual\-stack endpoint for specified AWS CLI commands only \(not all commands\), you can use either of the following methods: 
+ You can use the dual\-stack endpoint per command by setting the `--endpoint-url` parameter to `https://s3.dualstack.aws-region.amazonaws.com` or `http://s3.dualstack.aws-region.amazonaws.com` for any `s3` or `s3api` command\.

  ```
  $ aws s3api list-objects --bucket bucketname --endpoint-url https://s3.dualstack.aws-region.amazonaws.com
  ```
+ You can set up separate profiles in your AWS Config file\. For example, create one profile that sets `use_dualstack_endpoint` to `true` and a profile that does not set `use_dualstack_endpoint`\. When you run a command, specify which profile you want to use, depending upon whether or not you want to use the dual\-stack endpoint\. 

**Note**  
When using the AWS CLI you currently cannot use transfer acceleration with dual\-stack endpoints\. However, support for the AWS CLI is coming soon\. For more information, see [Using Transfer Acceleration from the AWS Command Line Interface \(AWS CLI\) ](transfer-acceleration-examples.md#transfer-acceleration-examples-aws-cli)\. 

## Using dual\-stack endpoints from the AWS SDKs<a name="dual-stack-endpoints-sdks"></a>

This section provides examples of how to access a dual\-stack endpoint by using the AWS SDKs\. 

### AWS SDK for Java dual\-stack endpoint example<a name="dual-stack-endpoints-examples-java"></a>

The following example shows how to enable dual\-stack endpoints when creating an Amazon S3 client using the AWS SDK for Java\.

For instructions on creating and testing a working Java sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;

public class DualStackEndpoints {

    public static void main(String[] args) {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            // Create an Amazon S3 client with dual-stack endpoints enabled.
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .withDualstackEnabled(true)
                    .build();

            s3Client.listObjects(bucketName);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

If you are using the AWS SDK for Java on Windows, you might have to set the following Java virtual machine \(JVM\) property: 

```
java.net.preferIPv6Addresses=true
```

### AWS \.NET SDK dual\-stack endpoint example<a name="dual-stack-endpoints-examples-dotnet"></a>

When using the AWS SDK for \.NET you use the `AmazonS3Config` class to enable the use of a dual\-stack endpoint as shown in the following example\. 

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DualStackEndpointTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            var config = new AmazonS3Config
            {
                UseDualstackEndpoint = true,
                RegionEndpoint = bucketRegion
            };
            client = new AmazonS3Client(config);
            Console.WriteLine("Listing objects stored in a bucket");
            ListingObjectsAsync().Wait();
        }

        private static async Task ListingObjectsAsync()
        {
            try
            {
                var request = new ListObjectsV2Request
                {
                    BucketName = bucketName,
                    MaxKeys = 10
                };
                ListObjectsV2Response response;
                do
                {
                    response = await client.ListObjectsV2Async(request);

                    // Process the response.
                    foreach (S3Object entry in response.S3Objects)
                    {
                        Console.WriteLine("key = {0} size = {1}",
                            entry.Key, entry.Size);
                    }
                    Console.WriteLine("Next Continuation Token: {0}", response.NextContinuationToken);
                    request.ContinuationToken = response.NextContinuationToken;
                } while (response.IsTruncated == true);
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("An AmazonS3Exception was thrown. Exception: " + amazonS3Exception.ToString());
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
            }
        }
    }
}
```

For a full \.NET sample for listing objects, see [Listing Keys Using the AWS SDK for \.NET](ListingObjectKeysUsingNetSDK.md)\. 

For information about how to create and test a working \.NET sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

## Using dual\-stack endpoints from the REST API<a name="dual-stack-endpoints-examples-rest-api"></a>

For information about making requests to dual\-stack endpoints by using the REST API, see [Making requests to dual\-stack endpoints by using the REST API](RESTAPI.md#rest-api-dual-stack)\.


# Managing Amazon S3 object locks<a name="object-lock-managing"></a>

S3 Object Lock lets you store objects in Amazon S3 using a *write once, read many* \(WORM\) model\. You can use it to view, configure, and manage the object lock status of your Amazon S3 objects\. For more information about S3 Object Lock capabilities, see [S3 Object Lock overview](object-lock-overview.md)\.

**Topics**
+ [Viewing the lock information for an object](#object-lock-managing-view)
+ [Bypassing governance mode](#object-lock-managing-bypass)
+ [Configuring events and notifications](#object-lock-managing-events)
+ [Setting retention limits](#object-lock-managing-retention-limits)
+ [Managing delete markers and object lifecycles](#object-lock-managing-lifecycle)
+ [Using S3 Object Lock with replication](#object-lock-managing-replication)

## Viewing the lock information for an object<a name="object-lock-managing-view"></a>

You can view the Object Lock status of an Amazon S3 object version using the `GET Object` or `HEAD Object` commands\. Both commands return the retention mode, `Retain Until Date`, and the legal\-hold status for the specified object version\. 

To view an object version's retention mode and retention period, you must have the `s3:GetObjectRetention` permission\. To view an object version's legal hold status, you must have the `s3:GetObjectLegalHold` permission\. If you `GET` or `HEAD` an object version but don't have the necessary permissions to view its lock status, the request succeeds\. However, it doesn't return information that you don't have permission to view\.

To view a bucket's default retention configuration \(if it has one\), request the bucket's Object Lock configuration\. To do this, you must have the `s3:GetBucketObjectLockConfiguration` permission\. If you make a request for an Object Lock configuration against a bucket that doesn't have S3 Object Lock enabled, Amazon S3 returns an error\. For more information about permissions, see [Example — Object Operations](using-with-s3-actions.md#using-with-s3-actions-related-to-objects)\. 

You can configure Amazon S3 inventory reports on your buckets to include the `Retain Until Date`, `object lock Mode`, and `Legal Hold Status` for all objects in a bucket\. For more information, see [ Amazon S3 inventory](storage-inventory.md)\.

## Bypassing governance mode<a name="object-lock-managing-bypass"></a>

You can perform operations on object versions that are locked in governance mode as if they were unprotected if you have the `s3:BypassGovernanceRetention` permission\. These operations include deleting an object version, shortening the retention period, or removing the Object Lock by placing a new lock with empty parameters\. To bypass governance mode, you must explicitly indicate in your request that you want to bypass this mode\. To do this, include the `x-amz-bypass-governance-retention:true` header with your request, or use the equivalent parameter with requests made through the AWS CLI, or AWS SDKs\. The AWS Management Console automatically applies this header for requests made through the console if you have the permission required to bypass governance mode\.

**Note**  
Bypassing governance mode doesn't affect an object version's legal hold status\. If an object version has a legal hold enabled, the legal hold remains in force and prevents requests to overwrite or delete the object version\.

## Configuring events and notifications<a name="object-lock-managing-events"></a>

You can configure Amazon S3 events for object\-level operations in an S3 Object Lock S3 bucket\. When `PUT Object`, `HEAD Object`, and `GET Object` calls include Object Lock metadata, events for these calls include those metadata values\. When Object Lock metadata is added to or updated for an object, those actions also trigger events\. These events occur whenever you `PUT` or `GET` object retention or legal\-hold information\.

For more information about Amazon S3 events, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.

You can use Amazon S3 event notifications to track access and changes to your Object Lock configurations and data using AWS CloudTrail\. For information about CloudTrail, see the [AWS CloudTrail Documentation](https://docs.aws.amazon.com/cloudtrail/index.html)\. 

You can also use Amazon CloudWatch to generate alerts based on this data\. For information about CloudWatch, see the [Amazon CloudWatch Documentation](https://docs.aws.amazon.com/cloudwatch/index.html)\.



## Setting retention limits<a name="object-lock-managing-retention-limits"></a>

You can set minimum and maximum allowable retention periods for a bucket using a bucket policy\. You do this using the `s3:object-lock-remaining-retention-days` condition key\. The following example shows a bucket policy that uses the `s3:object-lock-remaining-retention-days` condition key to set a maximum retention period of 10 days\.

```
{
    "Version": "2012-10-17",
    "Id": "<SetRetentionLimits",
    "Statement": [
        {
            "Sid": "<SetRetentionPeriod",
            "Effect": "Deny",
            "Principal": "*",
            "Action": [
                "s3:PutObjectRetention"
            ],
            "Resource": "arn:aws:s3:::<awsexamplebucket1>/*",
            "Condition": {
                "NumericGreaterThan": {
                    "s3:object-lock-remaining-retention-days": "10"
                }
            }
        }
    ]
}
```

**Note**  
If your bucket is the destination bucket for a replication policy and you want to set up minimum and maximum allowable retention periods for object replicas that are created using replication, you must include the `s3:ReplicateObject` action in your bucket policy\.

For more information, see the following topics:
+ [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)
+ [Example — Object Operations](using-with-s3-actions.md#using-with-s3-actions-related-to-objects)
+ [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)

## Managing delete markers and object lifecycles<a name="object-lock-managing-lifecycle"></a>

Although you can't delete a protected object version, you can still create a delete marker for that object\. Placing a delete marker on an object doesn't delete any object version\. However, it makes Amazon S3 behave in most ways as though the object has been deleted\. For more information, see [Working with delete markers](DeleteMarker.md)\.

**Note**  
Delete markers are not WORM\-protected, regardless of any retention period or legal hold in place on the underlying object\.

Object lifecycle management configurations continue to function normally on protected objects, including placing delete markers\. However, protected object versions remain safe from being deleted or overwritten by a lifecycle configuration\. For more information about managing object lifecycles, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

## Using S3 Object Lock with replication<a name="object-lock-managing-replication"></a>

You can use S3 Object Lock with replication to enable automatic, asynchronous copying of locked objects and their retention metadata, across S3 buckets in different or the same AWS Regions\. When you use replication, objects in a *source bucket* are replicated to a *destination bucket*\. For more information, see [Replication](replication.md)\. 

To set up S3 Object Lock with replication, you can choose one of the following options\.

Option 1: Enable Object Lock first\.

1. Enable Object Lock on the destination bucket, or on both the source and the destination bucket\. 

1. Set up replication between the source and the destination buckets\.

Option 2: Set up replication first\.

1. Set up replication between the source and destination buckets\.

1. Enable Object Lock on just the destination bucket, or on both the source and destination buckets\.

To complete step 2 in the preceding options, you must contact [AWS Support](https://console.aws.amazon.com/support/home)\. This is required to make sure that replication is configured correctly\. 

Before you contact AWS Support, review the following requirements for setting up Object Lock with replication:
+ The Amazon S3 destination bucket must have Object Lock enabled on it\.
+ You must grant two new permissions on the source S3 bucket in the AWS Identity and Access Management \(IAM\) role that you use to set up replication\. The two new permissions are `s3:GetObjectRetention` and `s3:GetObjectLegalHold`\. If the role has an `s3:Get*` permission, it satisfies the requirement\. For more information, see [Setting up permissions for replication](setting-repl-config-perm-overview.md)\.

For more information about S3 Object Lock, see [Locking objects using S3 Object Lock](object-lock.md)\.


# Working with objects using Amazon S3 on Outposts<a name="S3OutpostsObjectJava"></a>

You can use the SDK for Java to put and manage your S3 on Outposts objects\. From these examples, you can put objects and get objects from an Outpost bucket\. 

**Topics**
+ [Put an object in to an Amazon S3 on Outposts bucket](#S3OutpostsPutObjectJava)
+ [Get the Amazon S3 on Outposts bucket](#S3OutpostsGetObjectJava)
+ [Copy the object on Amazon S3 on Outposts](#S3OutpostsCopyObjectJava)
+ [Delete the object on Amazon S3 on Outposts](#S3OutpostsDeleteObjectJava)
+ [Deletes objects on Amazon S3 on Outposts](#S3OutpostsDeleteObjectsJava)
+ [Lists objects in an Amazon S3 on Outposts bucket](#S3OutpostsListObjectsJava)
+ [This initiates a multipart upload objects in an Amazon S3 on Outposts bucket](#S3OutpostsInitiateMultipartUploadJava)
+ [Copy an object in an Amazon S3 on Outposts bucket](#S3OutpostsCopyPartJava)
+ [List parts of an object in an Amazon S3 on Outposts bucket](#S3OutpostsListPartsJava)
+ [Retrieve a list of in\-progress multipart uploads in an Amazon S3 on Outposts bucket](#S3OutpostsListMultipartUploadsJava)
+ [Abort a multipart upload of an object in an Amazon S3 on Outposts bucket](#S3OutpostsAbortMultipartUploadJava)
+ [Using Head Bucket operation for an Amazon S3 on Outposts bucket](#S3OutpostsHeadBucketJava)

## Put an object in to an Amazon S3 on Outposts bucket<a name="S3OutpostsPutObjectJava"></a>

The following example puts an S3 on Outposts object using the SDK for Java\. For more information, see [Upload an object Using the AWS SDK for Java](https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadObjSingleOpJava.html)

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.PutObjectRequest;

import java.io.File;

public class PutObject {
    public static void main(String[] args) {
        String accessPointArn = "*** Access point ARN ***";
        String stringObjKeyName = "*** String object key name ***";
        String fileObjKeyName = "*** File object key name ***";
        String fileName = "*** Path to file to upload ***";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            // Upload a text string as a new object.
            s3Client.putObject(accessPointArn, stringObjKeyName, "Uploaded String Object");

            // Upload a file as a new object with ContentType and title specified.
            PutObjectRequest request = new PutObjectRequest(accessPointArn, fileObjKeyName, new File(fileName));
            ObjectMetadata metadata = new ObjectMetadata();
            metadata.setContentType("plain/text");
            metadata.addUserMetadata("title", "someTitle");
            request.setMetadata(metadata);
            s3Client.putObject(request);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Get the Amazon S3 on Outposts bucket<a name="S3OutpostsGetObjectJava"></a>

The following S3 on Outposts example gets a bucket using the SDK for Java\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.GetObjectRequest;
import com.amazonaws.services.s3.model.ResponseHeaderOverrides;
import com.amazonaws.services.s3.model.S3Object;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;

public class GetObject {
    public static void main(String[] args) throws IOException {
        String accessPointArn = "*** Access point ARN ***";
        String key = "*** Object key ***";

        S3Object fullObject = null, objectPortion = null, headerOverrideObject = null;
        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            // Get an object and print its contents.
            System.out.println("Downloading an object");
            fullObject = s3Client.getObject(new GetObjectRequest(accessPointArn, key));
            System.out.println("Content-Type: " + fullObject.getObjectMetadata().getContentType());
            System.out.println("Content: ");
            displayTextInputStream(fullObject.getObjectContent());

            // Get a range of bytes from an object and print the bytes.
            GetObjectRequest rangeObjectRequest = new GetObjectRequest(accessPointArn, key)
                    .withRange(0, 9);
            objectPortion = s3Client.getObject(rangeObjectRequest);
            System.out.println("Printing bytes retrieved.");
            displayTextInputStream(objectPortion.getObjectContent());

            // Get an entire object, overriding the specified response headers, and print the object's content.
            ResponseHeaderOverrides headerOverrides = new ResponseHeaderOverrides()
                    .withCacheControl("No-cache")
                    .withContentDisposition("attachment; filename=example.txt");
            GetObjectRequest getObjectRequestHeaderOverride = new GetObjectRequest(accessPointArn, key)
                    .withResponseHeaders(headerOverrides);
            headerOverrideObject = s3Client.getObject(getObjectRequestHeaderOverride);
            displayTextInputStream(headerOverrideObject.getObjectContent());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        } finally {
            // To ensure that the network connection doesn't remain open, close any open input streams.
            if (fullObject != null) {
                fullObject.close();
            }
            if (objectPortion != null) {
                objectPortion.close();
            }
            if (headerOverrideObject != null) {
                headerOverrideObject.close();
            }
        }
    }

    private static void displayTextInputStream(InputStream input) throws IOException {
        // Read the text input stream one line at a time and display each line.
        BufferedReader reader = new BufferedReader(new InputStreamReader(input));
        String line = null;
        while ((line = reader.readLine()) != null) {
            System.out.println(line);
        }
        System.out.println();
    }
}
```



## Copy the object on Amazon S3 on Outposts<a name="S3OutpostsCopyObjectJava"></a>

The following S3 on Outposts example gets a bucket using the SDK for Java\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CopyObjectRequest;

public class CopyObject {
    public static void main(String[] args) {
        String accessPointArn = "*** Access point ARN ***";
        String sourceKey = "*** Source object key ***";
        String destinationKey = "*** Destination object key ***";

        try {
            // This code expects that you have AWS credentials set up per:// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            // Copy the object into a new object in the same bucket.
            CopyObjectRequest copyObjectRequest = new CopyObjectRequest(accessPointArn, sourceKey, accessPointArn, destinationKey);
            s3Client.copyObject(copyObjectRequest);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```



## Delete the object on Amazon S3 on Outposts<a name="S3OutpostsDeleteObjectJava"></a>

The following S3 on Outposts example deletes an object using the SDK for Java\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.DeleteObjectRequest;

public class DeleteObject {
    public static void main(String[] args) {
        String accessPointArn = "*** Access point ARN ***";
        String keyName = "*** Key name ****";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            s3Client.deleteObject(new DeleteObjectRequest(accessPointArn, keyName));
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```



## Deletes objects on Amazon S3 on Outposts<a name="S3OutpostsDeleteObjectsJava"></a>

The following S3 on Outposts example deletes objects using the SDK for Java\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.DeleteObjectsRequest;
import com.amazonaws.services.s3.model.DeleteObjectsRequest.KeyVersion;
import com.amazonaws.services.s3.model.DeleteObjectsResult;

import java.util.ArrayList;

public class DeleteObjects {

    public static void main(String[] args) {
//        String accessPointArn = "*** Access point ARN ***";
        String accessPointArn = "arn:aws:s3-outposts:us-east-1:785856369849:outpost/ec2/accesspoint/mig-test-60";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            // Upload three sample objects.
            ArrayList<KeyVersion> keys = new ArrayList<KeyVersion≫();
            for (int i = 0; i < 3; i++) {
                String keyName = "delete object example " + i;
                s3Client.putObject(accessPointArn, keyName, "Object number " + i + " to be deleted.");
                keys.add(new KeyVersion(keyName));
            }
            System.out.println(keys.size() + " objects successfully created.");

            // Delete the sample objects.
            DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest(accessPointArn)
                    .withKeys(keys)
                    .withQuiet(false);

            // Verify that the objects were deleted successfully.
            DeleteObjectsResult delObjRes = s3Client.deleteObjects(multiObjectDeleteRequest);
            int successfulDeletes = delObjRes.getDeletedObjects().size();
            System.out.println(successfulDeletes + " objects successfully deleted.");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```



## Lists objects in an Amazon S3 on Outposts bucket<a name="S3OutpostsListObjectsJava"></a>

The following S3 on Outposts example lists objects using the SDK for Java in the Outposts bucket\. 

**Important**  
This section describes the latest revision of the API\. We recommend that you use this revised API for application development\. For backward compatibility, Amazon S3 continues to support the prior version of this API, [ListObjects](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html)\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListObjectsV2Request;
import com.amazonaws.services.s3.model.ListObjectsV2Result;
import com.amazonaws.services.s3.model.S3ObjectSummary;

public class ListObjectsV2 {

    public static void main(String[] args) {
        String accessPointArn = "*** Access point ARN ***";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            System.out.println("Listing objects");

            // maxKeys is set to 2 to demonstrate the use of
            // ListObjectsV2Result.getNextContinuationToken()
            ListObjectsV2Request req = new ListObjectsV2Request().withBucketName(accessPointArn).withMaxKeys(2);
            ListObjectsV2Result result;

            do {
                result = s3Client.listObjectsV2(req);

                for (S3ObjectSummary objectSummary : result.getObjectSummaries()) {
                    System.out.printf(" - %s (size: %d)\n", objectSummary.getKey(), objectSummary.getSize());
                }
                // If there are more than maxKeys keys in the bucket, get a continuation token
                // and list the next objects.
                String token = result.getNextContinuationToken();
                System.out.println("Next Continuation Token: " + token);
                req.setContinuationToken(token);
            } while (result.isTruncated());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```



## This initiates a multipart upload objects in an Amazon S3 on Outposts bucket<a name="S3OutpostsInitiateMultipartUploadJava"></a>

The following S3 on Outposts example initiate, uploads, and completes a multipart upload using the SDK for Java in the Outposts bucket\. For more information, see [Upload a File](https://docs.aws.amazon.com/AmazonS3/latest/dev/llJavaUploadFile.html)\.

**Important**  
This section describes the latest revision of the API\. We recommend that you use this revised API for application development\. For backward compatibility, Amazon S3 continues to support the prior version of this API, [ListObjects](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html)\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.util.ArrayList;
import java.util.List;

public class MultipartUploadCopy {
    public static void main(String[] args) {
        String accessPointArn = "*** Source access point ARN ***";
        String sourceObjectKey = "*** Source object key ***";
        String destObjectKey = "*** Target object key ***";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            // Initiate the multipart upload.
            InitiateMultipartUploadRequest initRequest = new InitiateMultipartUploadRequest(accessPointArn, destObjectKey);
            InitiateMultipartUploadResult initResult = s3Client.initiateMultipartUpload(initRequest);

            // Get the object size to track the end of the copy operation.
            GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest(accessPointArn, sourceObjectKey);
            ObjectMetadata metadataResult = s3Client.getObjectMetadata(metadataRequest);
            long objectSize = metadataResult.getContentLength();

            // Copy the object using 5 MB parts.
            long partSize = 5 * 1024 * 1024;
            long bytePosition = 0;
            int partNum = 1;
            List<CopyPartResult> copyResponses = new ArrayList<CopyPartResult>();
            while (bytePosition < objectSize) {
                // The last part might be smaller than partSize, so check to make sure
                // that lastByte isn't beyond the end of the object.
                long lastByte = Math.min(bytePosition + partSize - 1, objectSize - 1);

                // Copy this part.
                CopyPartRequest copyRequest = new CopyPartRequest()
                        .withSourceBucketName(accessPointArn)
                        .withSourceKey(sourceObjectKey)
                        .withDestinationBucketName(accessPointArn)
                        .withDestinationKey(destObjectKey)
                        .withUploadId(initResult.getUploadId())
                        .withFirstByte(bytePosition)
                        .withLastByte(lastByte)
                        .withPartNumber(partNum++);
                copyResponses.add(s3Client.copyPart(copyRequest));
                bytePosition += partSize;
            }

            // Complete the upload request to concatenate all uploaded parts and make the copied object available.
            CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest(
                    accessPointArn,
                    destObjectKey,
                    initResult.getUploadId(),
                    getETags(copyResponses));
            s3Client.completeMultipartUpload(completeRequest);
            System.out.println("Multipart copy complete.");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    // This is a helper function to construct a list of ETags.
    private static List<PartETag> getETags(List<CopyPartResult> responses) {
        List<PartETag> etags = new ArrayList<PartETag>();
        for (CopyPartResult response : responses) {
            etags.add(new PartETag(response.getPartNumber(), response.getETag()));
        }
        return etags;
    }
```



## Copy an object in an Amazon S3 on Outposts bucket<a name="S3OutpostsCopyPartJava"></a>

The following S3 on Outposts example copies and object using the SDK for Java from the Outposts bucket\. This is an example adapted from the [Copy an object using multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/CopyingObjctsUsingLLJavaMPUapi.html) example for Amazon S3\.

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.util.ArrayList;
import java.util.List;

public class MultipartUploadCopy {
    public static void main(String[] args) {
        String accessPointArn = "*** Source access point ARN ***";
        String sourceObjectKey = "*** Source object key ***";
        String destObjectKey = "*** Target object key ***";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            // Initiate the multipart upload.
            InitiateMultipartUploadRequest initRequest = new InitiateMultipartUploadRequest(accessPointArn, destObjectKey);
            InitiateMultipartUploadResult initResult = s3Client.initiateMultipartUpload(initRequest);

            // Get the object size to track the end of the copy operation.
            GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest(accessPointArn, sourceObjectKey);
            ObjectMetadata metadataResult = s3Client.getObjectMetadata(metadataRequest);
            long objectSize = metadataResult.getContentLength();

            // Copy the object using 5 MB parts.
            long partSize = 5 * 1024 * 1024;
            long bytePosition = 0;
            int partNum = 1;
            List<CopyPartResult> copyResponses = new ArrayList<CopyPartResult>();
            while (bytePosition < objectSize) {
                // The last part might be smaller than partSize, so check to make sure
                // that lastByte isn't beyond the end of the object.
                long lastByte = Math.min(bytePosition + partSize - 1, objectSize - 1);

                // Copy this part.
                CopyPartRequest copyRequest = new CopyPartRequest()
                        .withSourceBucketName(accessPointArn)
                        .withSourceKey(sourceObjectKey)
                        .withDestinationBucketName(accessPointArn)
                        .withDestinationKey(destObjectKey)
                        .withUploadId(initResult.getUploadId())
                        .withFirstByte(bytePosition)
                        .withLastByte(lastByte)
                        .withPartNumber(partNum++);
                copyResponses.add(s3Client.copyPart(copyRequest));
                bytePosition += partSize;
            }

            // Complete the upload request to concatenate all uploaded parts and make the copied object available.
            CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest(
                    accessPointArn,
                    destObjectKey,
                    initResult.getUploadId(),
                    getETags(copyResponses));
            s3Client.completeMultipartUpload(completeRequest);
            System.out.println("Multipart copy complete.");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    // This is a helper function to construct a list of ETags.
    private static List<PartETag> getETags(List<CopyPartResult> responses) {
        List<PartETag> etags = new ArrayList<PartETag>();
        for (CopyPartResult response : responses) {
            etags.add(new PartETag(response.getPartNumber(), response.getETag()));
        }
        return etags;
    }
}
```



## List parts of an object in an Amazon S3 on Outposts bucket<a name="S3OutpostsListPartsJava"></a>

The following S3 on Outposts example lists the parts of an object using the SDK for Java from the Outposts bucket\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.util.List;

public class ListParts {
    public static void main(String[] args) {
        String accessPointArn = "*** Access point ARN ***";
        String keyName = "*** Key name ***";
        String uploadId = "*** Upload ID ***";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            ListPartsRequest listPartsRequest = new ListPartsRequest(accessPointArn, keyName, uploadId);
            PartListing partListing = s3Client.listParts(listPartsRequest);
            List<PartSummary> partSummaries = partListing.getParts();

            System.out.println(partSummaries.size() + " multipart upload parts");
            for (PartSummary p : partSummaries) {
                System.out.println("Upload part: Part number = \"" + p.getPartNumber() + "\", ETag = " + p.getETag());
            }

        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```



## Retrieve a list of in\-progress multipart uploads in an Amazon S3 on Outposts bucket<a name="S3OutpostsListMultipartUploadsJava"></a>

The following S3 on Outposts example The following example shows how to retrieve a list of in\-progress multipart uploads using the SDK for Java from an Outposts bucket\. This is an example adapted from the [ List multipart uploads](https://docs.aws.amazon.com/AmazonS3/latest/dev/LLlistMPuploadsJava.html) example for Amazon S3\.

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListMultipartUploadsRequest;
import com.amazonaws.services.s3.model.MultipartUpload;
import com.amazonaws.services.s3.model.MultipartUploadListing;

import java.util.List;

public class ListMultipartUploads {
    public static void main(String[] args) {
                String accessPointArn = "*** Access point ARN ***";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            // Retrieve a list of all in-progress multipart uploads.
            ListMultipartUploadsRequest allMultipartUploadsRequest = new ListMultipartUploadsRequest(accessPointArn);
            MultipartUploadListing multipartUploadListing = s3Client.listMultipartUploads(allMultipartUploadsRequest);
            List<MultipartUpload> uploads = multipartUploadListing.getMultipartUploads();

            // Display information about all in-progress multipart uploads.
            System.out.println(uploads.size() + " multipart upload(s) in progress.");
            for (MultipartUpload u : uploads) {
                System.out.println("Upload in progress: Key = \"" + u.getKey() + "\", id = " + u.getUploadId());
            }
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```



## Abort a multipart upload of an object in an Amazon S3 on Outposts bucket<a name="S3OutpostsAbortMultipartUploadJava"></a>

The following S3 on Outposts example The following example shows how to abort the multipart upload for an object using SDK for Java from an Outposts bucket\. This is an example adapted from the [ Abort a multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/LLAbortMPUJava.html) example for Amazon S3\.

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListMultipartUploadsRequest;
import com.amazonaws.services.s3.model.MultipartUpload;
import com.amazonaws.services.s3.model.MultipartUploadListing;

import java.util.List;

public class ListMultipartUploads {
    public static void main(String[] args) {
                String accessPointArn = "*** Access point ARN ***";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            // Retrieve a list of all in-progress multipart uploads.
            ListMultipartUploadsRequest allMultipartUploadsRequest = new ListMultipartUploadsRequest(accessPointArn);
            MultipartUploadListing multipartUploadListing = s3Client.listMultipartUploads(allMultipartUploadsRequest);
            List<MultipartUpload> uploads = multipartUploadListing.getMultipartUploads();

            // Display information about all in-progress multipart uploads.
            System.out.println(uploads.size() + " multipart upload(s) in progress.");
            for (MultipartUpload u : uploads) {
                System.out.println("Upload in progress: Key = \"" + u.getKey() + "\", id = " + u.getUploadId());
            }
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```



## Using Head Bucket operation for an Amazon S3 on Outposts bucket<a name="S3OutpostsHeadBucketJava"></a>

The following S3 on Outposts example shows how to determine if a bucket exists and you have permission to access it\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.HeadBucketRequest;

public class HeadBucket {
    public static void main(String[] args) {
        String accessPointArn = "*** Access point ARN ***";

        try {
            // This code expects that you have AWS credentials set up per:
            // https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .enableUseArnRegion()
                    .build();

            s3Client.headBucket(new HeadBucketRequest(accessPointArn));
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```




# Adding objects to versioning\-suspended buckets<a name="AddingObjectstoVersionSuspendedBuckets"></a>

Once you suspend versioning on a bucket, Amazon S3 automatically adds a `null` version ID to every subsequent object stored thereafter \(using `PUT`, `POST`, or `COPY`\) in that bucket\.

The following figure shows how Amazon S3 adds the version ID of `null` to an object when it is added to a version\-suspended bucket\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_PUT_versionSuspended.png)

If a null version is already in the bucket and you add another object with the same key, the added object overwrites the original null version\. 

If there are versioned objects in the bucket, the version you `PUT` becomes the current version of the object\. The following figure shows how adding an object to a bucket that contains versioned objects does not overwrite the object already in the bucket\. In this case, version 111111 was already in the bucket\. Amazon S3 attaches a version ID of null to the object being added and stores it in the bucket\. Version 111111 is not overwritten\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_PUT_versionSuspended3.png)

If a null version already exists in a bucket, the null version is overwritten, as shown in the following figure\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_PUT_versionSuspended4.png)

Note that although the key and version ID \(`null`\) of null version are the same before and after the `PUT`, the contents of the null version originally stored in the bucket is replaced by the contents of the object `PUT` into the bucket\.


# Working with Amazon S3 on Outposts<a name="WorkingWithS3Outposts"></a>

Amazon S3 supports global buckets, the bucket name needs to be unique across all Regions within a partition\. You can access a bucket using just its name\. In S3 on Outposts, bucket names are unique to an Outpost and require the Outpost\-id along with the bucket name to identify them\. 

**Topics**
+ [Accessing Amazon S3 on Outposts](#AccessingS3Outposts)

Access Points simplify managing data access at scale for shared datasets in S3\. Access points are named network endpoints that are attached to buckets that you can use to perform Amazon S3 object operations, such as GetObject and PutObject\. With S3 on Outposts bucket endpoint and object API endpoint being different, you must use access points to access any object in a Outposts bucket, unlike S3 buckets in S3 that can be accessed directly\. Access points only support virtual\-host\-style addressing\. The ARN format for S3 on Outposts buckets will be: `arn:aws:s3-outposts:<region>:<account>:outpost/<outpost-id>/bucket/<bucket-name>`\. The ARN format for S3 on Outposts access points will be: `arn:aws:s3-outposts:<region>:<account>:outpost/<outpost-id>/accesspoint/<accesspoint-name>`\.

The existing bucket management APIs do not support the concept of location beyond Regions\. Thus, these APIs cannot be used to create and manage buckets that are scoped to account, Outposts and Region\. S3 on Outposts will host a separate endpoint for management of Outposts bucket APIs distinct from S3\. This will be `s3-outposts.<region>.amazonaws.com`\. You must create these endpoints to be able to access your Outposts bucket and perform object operations\. This will also allow the API model and behaviors to be the same by allowing the same actions to work in S3 and S3 on Outposts\. This is done by signing the bucket and objects using the correct ARNs\.

The reason to pass ARNs for the API is to Amazon S3 to determine if the request is for S3 \(s3\-control\.<region>\.amazonaws\.com\) or S3 on Outposts \(s3\-outposts\.<region>\.amazonaws\.com\) allowing it sign and route the request appropriately\. Whenever the request is sent to the Amazon S3 control plane, the SDK will extract the components from the ARN and include an additional header “x\-amz\-outpost\-id” with the value of the “outpost\-id” extracted from the ARN\. The service name from the ARN will be used to sign the request before it is routed to the S3 on Outposts endpoint\. This is applicable for all APIs handled by the s3control client\. 



The list of extended APIs for Amazon S3 on Outposts and their changes relative to S3 is in the table below\.


|  API |  S3 parameter value |  S3 on Outposts parameter value | 
| --- | --- | --- | 
|  *CreateBucket*  |  *bucket name*  |  *name as ARN, outpost\-id*  | 
|  *ListRegionalBuckets \(new API\)*  |  *NA*  |  *outpost\-id*  | 
|  *DeleteBucket*  |  *bucket name*  |  *name as ARN*  | 
|  *DeleteBucketLifecycleConfiguration*  |  *bucket name*  |  *bucket name as ARN*  | 
|  *GetBucketLifecycleConfiguration*  |  *bucket name*  |  *bucket name as ARN*  | 
|  * PutBucketLifecycleConfiguration*  |  *bucket name*  |  *bucket name as ARN*  | 
|  *GetBucketPolicy*  |  *bucket name*  |  *bucket name as ARN*  | 
|  *PutBucketPolicy*  |  *bucket name*  |  *bucket name as ARN*  | 
|  *DeleteBucketPolicy*  |  *bucket name*  |  *bucket name as ARN*  | 
|  *GetBucketTagging*  |  *bucket name*  |  *bucket name as ARN*  | 
|  *PutBucketTagging*  |  *bucket name*  |  *bucket name as ARN*  | 
|  *DeleteBucketTagging*  |  *bucket name*  |  *bucket name as ARN*  | 
|  *CreateAccessPoint*  |  *access point name*  |  *access point name as ARN*  | 
|  *DeleteAccessPoint*  |  *access point name*  |  *access point name as ARN*  | 
|  *GetAccessPoint*  |  *access point name*  |  *access point name as ARN*  | 
|  *GetAccessPoint*  |  *access point name*  |  *access point name as ARN*  | 
|  *ListAccessPoints*  |  *access point name*  |  *access point name as ARN*  | 
|  *PutAccessPointPolicy*  |  *access point name*  |  *access point name as ARN*  | 
|  *GetAccessPointPolicy*  |  *access point name*  |  *access point name as ARN*  | 
|  *DeleteAccessPointPolicy*  |  *access point name*  |  *access point name as ARN*  | 



## Accessing Amazon S3 on Outposts<a name="AccessingS3Outposts"></a>

Amazon S3 on Outposts supports VPC\-only access points as the only means to access Outposts buckets\. S3 on Outposts endpoints enable you to privately connect your VPC to your Outposts bucket without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection\. Instances in your VPC do not require public IP addresses to communicate with resources in your Outposts keeping traffic between your VPC and your S3 on Outposts buckets within the Amazon network\. S3 on Outposts endpoints are virtual devices\. They are horizontally scaled, redundant, and highly available VPC components\. They allow communication between instances in your VPC and S3 on Outposts without imposing availability risks or bandwidth constraints on your network traffic\. 

**Note**  
You will not be able to access your S3 on Outpost bucket and objects until:  
The access point is created for the VPC\.
An Endpoint exists for the same VPC\.

**Topics**
+ [Connection management for Amazon S3 on Outposts through Cross\-account elastic network interfaces](#S3OutpostsXENI)
+ [Permissions required for endpoints](#S3OutpostsClusters)
+ [Encryption options with Amazon S3 on Outposts](#S3OutpostsEncryption)
+ [Managing capacity on S3 on Outposts](#S3OutpostsCapacity)

### Connection management for Amazon S3 on Outposts through Cross\-account elastic network interfaces<a name="S3OutpostsXENI"></a>

S3 on Outposts endpoints will be named resources with proper ARNs\. During creation of these endpoints, Outposts will setup four cross\-account Elastic Network Interfaces \(X\-ENI\)\. X\-ENI are like other ENI with one exception: S3 on Outposts attaches this X\-ENI to instances it runs in the service account and has a presence in your VPC\. S3 on Outposts will DNS load balance your requests over the X\-ENI\. S3 on Outposts creates the X\-ENI in your account that is visible to from the ENI console\. 

### Permissions required for endpoints<a name="S3OutpostsClusters"></a>

To attach the X\-ENI to cluster accounts, S3 on Amazon will additionally need to modify the cross\-account Elastic Network Interfaces \(X\-ENI\) during creation to be used with the account ID of the cluster account\. Due to the CIDR restrictions, each ENI is unique and on a unique IP\. The source VPC for the IP and ENI id will be recorded and will be associated with the cluster id\. 

### Encryption options with Amazon S3 on Outposts<a name="S3OutpostsEncryption"></a>

 By default, all data stored in S3 on Outposts is encrypted using server\-side encryption with SSE\-S3\. You can optionally use server\-side encryption with customer\-provided encryption keys \(SSE\-C\) by specifying an encryption key as part of your object API requests\. Server\-side encryption encrypts only the object data, not object metadata\. 

### Managing capacity on S3 on Outposts<a name="S3OutpostsCapacity"></a>

If there is not enough space to store an object on your Outpost, the API will return an insufficient capacity exemption \(ICE\)\. To avoid this, you can create [CloudWatch alerts](https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudwatch-monitoring.html#s3-outposts-cloudwatch-metrics) that alert you when storage utilization exceeds a threshold\. You can use this to free up space by explicitly deleting data, using a lifecycle expiration policy, or copying data from your Outposts bucket to an S3 bucket in an AWS Region using AWS DataSync\. For more information about transferring data from your S3 on Outposts buckets using DataSync, see [Getting Started with AWS DataSync](https://docs.aws.amazon.com/datasync/latest/userguide/getting-started.html) in the *AWS DataSync User Guide* 


# Managing objects in a versioning\-suspended bucket<a name="VersionSuspendedBehavior"></a>

**Topics**
+ [Adding objects to versioning\-suspended buckets](AddingObjectstoVersionSuspendedBuckets.md)
+ [Retrieving objects from versioning\-suspended buckets](RetrievingObjectsfromVersioningSuspendedBuckets.md)
+ [Deleting objects from versioning\-suspended buckets](DeletingObjectsfromVersioningSuspendedBuckets.md)

You suspend versioning to stop accruing new versions of the same object in a bucket\. You might do this because you only want a single version of an object in a bucket, or you might not want to accrue charges for multiple versions\. 

When you suspend versioning, existing objects in your bucket do not change\. What changes is how Amazon S3 handles objects in future requests\. The topics in this section explain various object operations in a versioning\-suspended bucket\.


# Listing Keys Using the REST API<a name="ListingObjectKeysUsingREST"></a>

You can use the AWS SDK to list the object keys in a bucket\. However, if your application requires it, you can send REST requests directly\. You can send a GET request to return some or all of the objects in a bucket or you can use selection criteria to return a subset of the objects in a bucket\. For more information, go to [GET Bucket \(List Objects\) Version 2](https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html)\.


# Setting up the tools for the example walkthroughs<a name="policy-eval-walkthrough-download-awscli"></a>

The introductory examples \(see [Example walkthroughs: Managing access to your Amazon S3 resources ](example-walkthroughs-managing-access.md)\) use the AWS Management Console to create resources and grant permissions\. And to test permissions, the examples use the command line tools, AWS Command Line Interface \(CLI\) and AWS Tools for Windows PowerShell, so you don't need to write any code\. To test permissions, you must set up one of these tools\. 

**To set up the AWS CLI**

1. Download and configure the AWS CLI\. For instructions, see the following topics in the *AWS Command Line Interface User Guide*\. 

    [Getting Set Up with the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-set-up.html) 

    [Installing the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/installing.html) 

   [Configuring the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html)

1. Set the default profile\. 

   You will store user credentials in the AWS CLI config file\. Create a default profile in the config file using your AWS account credentials\. See [Configuration and Credential Files](https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html) for instructions on finding and editing your AWS CLI config file\.

   ```
   [default]
   aws_access_key_id = access key ID
   aws_secret_access_key = secret access key
   region = us-west-2
   ```

1. Verify the setup by entering the following command at the command prompt\. Both these commands don't provide credentials explicitly, so the credentials of the default profile are used\.
   + Try the help command

     ```
     aws help
     ```
   + Use `aws s3 ls` to get a list of buckets on the configured account\.

     ```
     aws s3 ls
     ```

As you go through the example walkthroughs, you will create users, and you will save user credentials in the config files by creating profiles, as the following example shows\. Note that these profiles have names \(AccountAadmin and AccountBadmin\):

```
[profile AccountAadmin]
aws_access_key_id = User AccountAadmin access key ID
aws_secret_access_key = User AccountAadmin secret access key
region = us-west-2

[profile AccountBadmin]
aws_access_key_id = Account B access key ID
aws_secret_access_key = Account B secret access key
region = us-east-1
```

To run a command using these user credentials, you add the `--profile` parameter specifying the profile name\. The following AWS CLI command retrieves a listing of objects in `examplebucket` and specifies the `AccountBadmin` profile\. 

```
aws s3 ls s3://examplebucket --profile AccountBadmin
```

Alternatively, you can configure one set of user credentials as the default profile by changing the `AWS_DEFAULT_PROFILE` environment variable from the command prompt\. Once you've done this, whenever you perform AWS CLI commands without the `--profile` parameter, the AWS CLI will use the profile you set in the environment variable as the default profile\.

```
$ export AWS_DEFAULT_PROFILE=AccountAadmin
```

**To set up AWS Tools for Windows PowerShell**

1. Download and configure the AWS Tools for Windows PowerShell\. For instructions, go to [Download and Install the AWS Tools for Windows PowerShell](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-getting-set-up.html#pstools-installing-download) in the *AWS Tools for Windows PowerShell User Guide*\. 
**Note**  
 In order to load the AWS Tools for Windows PowerShell module, you need to enable PowerShell script execution\. For more information, go to [Enable Script Execution](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-getting-set-up.html#enable-script-execution) in the *AWS Tools for Windows PowerShell User Guide*\.

1. For these exercises, you will specify AWS credentials per session using the `Set-AWSCredentials` command\. The command saves the credentials to a persistent store \(`-StoreAs `parameter\)\.

   ```
   Set-AWSCredentials -AccessKey AccessKeyID -SecretKey SecretAccessKey -storeas string
   ```

1. Verify the setup\.
   + Run the `Get-Command` to retrieve a list of available commands you can use for Amazon S3 operations\. 

     ```
     Get-Command -module awspowershell -noun s3* -StoredCredentials string
     ```
   + Run the `Get-S3Object` command to retrieve a list of objects in a bucket\.

     ```
     Get-S3Object -BucketName bucketname -StoredCredentials string
     ```

For a list of commands, go to [Amazon Simple Storage Service Cmdlets](https://docs.aws.amazon.com/powershell/latest/reference/Index.html)\. 

Now you are ready to try the exercises\. Follow the links provided at the beginning of the section\.


# Managing data access with Amazon S3 access points<a name="access-points"></a>

Amazon S3 Access Points simplify managing data access at scale for shared datasets in S3\. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations, such as `GetObject` and `PutObject`\. Each access point has distinct permissions and network controls that S3 applies for any request that is made through that access point\. Each access point enforces a customized access point policy that works in conjunction with the bucket policy that is attached to the underlying bucket\. You can configure any access point to accept requests only from a virtual private cloud \(VPC\) to restrict Amazon S3 data access to a private network\. You can also configure custom block public access settings for each access point\.

**Note**  
You can only use access points to perform operations on objects\. You can't use access points to perform other Amazon S3 operations, such as modifying or deleting buckets\. For a complete list of S3 operations that support access points, see [Access point compatibility with S3 operations and AWS services](using-access-points.md#access-points-service-api-support)\.
Access points work with some, but not all, AWS services and features\. For example, you can't configure Cross\-Region Replication to operate through an access point\. For a complete list of AWS services that are compatible with S3 access points, see [Access point compatibility with S3 operations and AWS services](using-access-points.md#access-points-service-api-support)\.

This section explains how to work with Amazon S3 access points\. For information about working with buckets, see [Working with Amazon S3 Buckets](UsingBucket.md)\. For information about working with objects, see [Working with Amazon S3 objects](UsingObjects.md)\.

**Topics**
+ [Creating access points](creating-access-points.md)
+ [Using access points](using-access-points.md)
+ [Access points restrictions and limitations](access-points-restrictions-limitations.md)


# Making requests using IAM user temporary credentials \- AWS SDK for Ruby<a name="AuthUsingTempSessionTokenRuby"></a>

An IAM user or an AWS account can request temporary security credentials using AWS SDK for Ruby and use them to access Amazon S3\. These credentials expire after the session duration\. 

By default, the session duration is one hour\. If you use IAM user credentials, you can specify the duration when requesting the temporary security credentials from 15 minutes to the maximum session duration for the role\. For more information about temporary security credentials, see [Temporary Security Credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html) in the *IAM User Guide*\. For more information about making requests, see [Making requests](MakingRequests.md)\.

**Note**  
If you obtain temporary security credentials using your AWS account security credentials, the temporary security credentials are valid for only one hour\. You can specify session duration only if you use IAM user credentials to request a session\.

The following Ruby example creates a temporary user to list the items in a specified bucket for one hour\. To use this example, you must have AWS credentials that have the necessary permissions to create new AWS Security Token Service \(AWS STS\) clients, and list Amazon S3 buckets\.

```
require 'aws-sdk-core'
require 'aws-sdk-s3'
require 'aws-sdk-iam'

# Checks whether a user exists in IAM.
#
# @param iam [Aws::IAM::Client] An initialized IAM client.
# @param user_name [String] The user's name.
# @return [Boolean] true if the user exists; otherwise, false.
# @example
#   iam_client = Aws::IAM::Client.new(region: 'us-east-1')
#   exit 1 unless user_exists?(iam_client, 'my-user')
def user_exists?(iam_client, user_name)
  response = iam_client.get_user(user_name: user_name)
  return true if response.user.user_name
rescue Aws::IAM::Errors::NoSuchEntity
  # User doesn't exist.
rescue StandardError => e
  puts 'Error while determining whether the user ' \
    "'#{user_name}' exists: #{e.message}"
end

# Creates a user in IAM.
#
# @param iam_client [Aws::IAM::Client] An initialized IAM client.
# @param user_name [String] The user's name.
# @return [AWS:IAM::Types::User] The new user.
# @example
#   iam_client = Aws::IAM::Client.new(region: 'us-east-1')
#   user = create_user(iam_client, 'my-user')
#   exit 1 unless user.user_name
def create_user(iam_client, user_name)
  response = iam_client.create_user(user_name: user_name)
  return response.user
rescue StandardError => e
  puts "Error while creating the user '#{user_name}': #{e.message}"
end

# Gets a user in IAM.
#
# @param iam_client [Aws::IAM::Client] An initialized IAM client.
# @param user_name [String] The user's name.
# @return [AWS:IAM::Types::User] The existing user.
# @example
#   iam_client = Aws::IAM::Client.new(region: 'us-east-1')
#   user = get_user(iam_client, 'my-user')
#   exit 1 unless user.user_name
def get_user(iam_client, user_name)
  response = iam_client.get_user(user_name: user_name)
  return response.user
rescue StandardError => e
  puts "Error while getting the user '#{user_name}': #{e.message}"
end

# Checks whether a role exists in IAM.
#
# @param iam_client [Aws::IAM::Client] An initialized IAM client.
# @param role_name [String] The role's name.
# @return [Boolean] true if the role exists; otherwise, false.
# @example
#   iam_client = Aws::IAM::Client.new(region: 'us-east-1')
#   exit 1 unless role_exists?(iam_client, 'my-role')
def role_exists?(iam_client, role_name)
  response = iam_client.get_role(role_name: role_name)
  return true if response.role.role_name
rescue StandardError => e
  puts 'Error while determining whether the role ' \
    "'#{role_name}' exists: #{e.message}"
end

# Gets credentials for a role in IAM.
#
# @param sts_client [Aws::STS::Client] An initialized AWS STS client.
# @param role_arn [String] The role's Amazon Resource Name (ARN).
# @param role_session_name [String] A name for this role's session.
# @param duration_seconds [Integer] The number of seconds this session is valid.
# @return [AWS::AssumeRoleCredentials] The credentials.
# @example
#   sts_client = Aws::STS::Client.new(region: 'us-east-1')
#   credentials = get_credentials(
#     sts_client,
#     'arn:aws:iam::123456789012:role/AmazonS3ReadOnly',
#     'ReadAmazonS3Bucket',
#     3600
#   )
#   exit 1 if credentials.nil?
def get_credentials(sts_client, role_arn, role_session_name, duration_seconds)
  Aws::AssumeRoleCredentials.new(
    client: sts_client,
    role_arn: role_arn,
    role_session_name: role_session_name,
    duration_seconds: duration_seconds
  )
rescue StandardError => e
  puts "Error while getting credentials: #{e.message}"
end

# Checks whether a bucket exists in Amazon S3.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param bucket_name [String] The name of the bucket.
# @return [Boolean] true if the bucket exists; otherwise, false.
# @example
#   s3_client = Aws::S3::Client.new(region: 'us-east-1')
#   exit 1 unless bucket_exists?(s3_client, 'doc-example-bucket')
def bucket_exists?(s3_client, bucket_name)
  response = s3_client.list_buckets
  response.buckets.each do |bucket|
    return true if bucket.name == bucket_name
  end
rescue StandardError => e
  puts "Error while checking whether the bucket '#{bucket_name}' " \
    "exists: #{e.message}"
end

# Lists the keys and ETags for the objects in an Amazon S3 bucket.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param bucket_name [String] The bucket's name.
# @return [Boolean] true if the objects were listed; otherwise, false.
# @example
#   s3_client = Aws::S3::Client.new(region: 'us-east-1')
#   exit 1 unless list_objects_in_bucket?(s3_client, 'doc-example-bucket')
def list_objects_in_bucket?(s3_client, bucket_name)
  puts "Accessing the contents of the bucket named '#{bucket_name}'..."
  response = s3_client.list_objects_v2(
    bucket: bucket_name,
    max_keys: 50
  )

  if response.count.positive?
    puts "Contents of the bucket named '#{bucket_name}' (first 50 objects):"
    puts 'Name => ETag'
    response.contents.each do |obj|
      puts "#{obj.key} => #{obj.etag}"
    end
  else
    puts "No objects in the bucket named '#{bucket_name}'."
  end
  return true
rescue StandardError => e
  puts "Error while accessing the bucket named '#{bucket_name}': #{e.message}"
end
```


# Retrieving the requestPayment Configuration<a name="BucketPayerValues"></a>

You can determine the `Payer` value that is set on a bucket by requesting the resource `requestPayment`\.

**To return the requestPayment resource**
+ Use a GET request to obtain the `requestPayment` resource, as shown in the following request\.

  ```
  1. GET ?requestPayment HTTP/1.1
  2. Host: [BucketName].s3.amazonaws.com
  3. Date: Wed, 01 Mar 2009 12:00:00 GMT
  4. Authorization: AWS [Signature]
  ```

If the request succeeds, Amazon S3 returns a response similar to the following\.

```
 1. HTTP/1.1 200 OK
 2. x-amz-id-2: [id]
 3. x-amz-request-id: [request_id]
 4. Date: Wed, 01 Mar 2009 12:00:00 GMT
 5. Content-Type: [type]
 6. Content-Length: [length]
 7. Connection: close
 8. Server: AmazonS3
 9. 
10. <?xml version="1.0" encoding="UTF-8"?>
11. <RequestPaymentConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
12. <Payer>Requester</Payer>
13. </RequestPaymentConfiguration>
```

This response shows that the `payer` value is set to `Requester`\. 


# Object key and metadata<a name="UsingMetadata"></a>

Each Amazon S3 object has data, a key, and metadata\. The *object key* \(or key name\) uniquely identifies the object in a bucket\. *Object metadata* is a set of name\-value pairs\. You can set object metadata at the time you upload it\. After you upload the object, you cannot modify object metadata\. The only way to modify object metadata is to make a copy of the object and set the metadata\. 

**Topics**
+ [Object keys](#object-keys)
+ [Object metadata](#object-metadata)

## Object keys<a name="object-keys"></a>

When you create an object, you specify the key name, which uniquely identifies the object in the bucket\. For example, on the [Amazon S3 console](https://console.aws.amazon.com/s3/home), when you highlight a bucket, a list of objects in your bucket appears\. These names are the *object keys*\. The name for a key is a sequence of Unicode characters whose UTF\-8 encoding is at most 1,024 bytes long\. 

The Amazon S3 data model is a flat structure: You create a bucket, and the bucket stores objects\. There is no hierarchy of subbuckets or subfolders\. However, you can infer logical hierarchy using key name prefixes and delimiters as the Amazon S3 console does\. The Amazon S3 console supports a concept of folders\. For more information about how to edit metadata from the Amazon S3 console, see [Editing object metadata](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-object-metadata.html) in the *Amazon Simple Storage Service Console User Guide*\.

**Note**  
Amazon S3 supports buckets and objects, and there is no hierarchy\. However, by using prefixes and delimiters in an object key name, the Amazon S3 console and the AWS SDKs can infer hierarchy and introduce the concept of folders\.
The Amazon S3 console implements folder object creation by creating a zero\-byte object with the folder *prefix and delimiter* value as the key\. These folder objects don't appear in the console\. Otherwise they behave like any other objects and can be viewed and manipulated through the REST API, AWS CLI, and AWS SDKs\.

### Object key naming guidelines<a name="object-key-guidelines"></a>

You can use any UTF\-8 character in an object key name\. However, using certain characters in key names can cause problems with some applications and protocols\. The following guidelines help you maximize compliance with DNS, web\-safe characters, XML parsers, and other APIs\. 

#### Safe characters<a name="object-key-guidelines-safe-characters"></a>

The following character sets are generally safe for use in key names\.


|  |  | 
| --- |--- |
| Alphanumeric characters |    0\-9   a\-z   A\-Z    | 
| Special characters |    Forward slash \(`/`\)   Exclamation point \(`!`\)   Hyphen \(`-`\)   Underscore \(`_`\)   Period \(`.`\)   Asterisk \(`*`\)   Single quote \(`'`\)   Open parenthesis \(`(`\)   Close parenthesis \(`)`\)    | 

The following are examples of valid object key names:
+ `4my-organization`
+ `my.great_photos-2014/jan/myvacation.jpg`
+ `videos/2014/birthday/video1.wmv`

**Important**  
If an object key name ends with a single period \(\.\), or two periods \(\.\.\), you can’t download the object using the Amazon S3 console\. To download an object with a key name ending with “\.” or “\.\.”, you must use the AWS Command Line Interface \(AWS CLI\), AWS SDKs, or REST API\.

#### Characters that might require special handling<a name="object-key-guidelines-special-handling"></a>

The following characters in a key name might require additional code handling and likely need to be URL encoded or referenced as HEX\. Some of these are non\-printable characters that your browser might not handle, which also requires special handling:
+ Ampersand \("&"\) 
+ Dollar \("$"\) 
+ ASCII character ranges 00–1F hex \(0–31 decimal\) and 7F \(127 decimal\) 
+ 'At' symbol \("@"\) 
+ Equals \("="\) 
+ Semicolon \(";"\) 
+ Colon \(":"\) 
+ Plus \("\+"\) 
+ Space – Significant sequences of spaces might be lost in some uses \(especially multiple spaces\) 
+ Comma \(","\) 
+ Question mark \("?"\) 

#### Characters to avoid<a name="object-key-guidelines-avoid-characters"></a>

Avoid the following characters in a key name because of significant special handling for consistency across all applications\. 
+ Backslash \("\\"\) 
+ Left curly brace \("\{"\) 
+ Non\-printable ASCII characters \(128–255 decimal characters\)
+ Caret \("^"\) 
+ Right curly brace \("\}"\) 
+ Percent character \("%"\) 
+ Grave accent / back tick \("`"\) 
+ Right square bracket \("\]"\) 
+ Quotation marks 
+ 'Greater Than' symbol \(">"\) 
+ Left square bracket \("\["\) 
+ Tilde \("\~"\) 
+ 'Less Than' symbol \("<"\) 
+ 'Pound' character \("\#"\) 
+ Vertical bar / pipe \("\|"\) 

## Object metadata<a name="object-metadata"></a>

There are two kinds of metadata: *system metadata* and *user\-defined metadata*\. 

### System\-defined object metadata<a name="SysMetadata"></a>

For each object stored in a bucket, Amazon S3 maintains a set of system metadata\. Amazon S3 processes this system metadata as needed\. For example, Amazon S3 maintains object creation date and size metadata and uses this information as part of object management\. 

There are two categories of system metadata: 

1. Metadata such as object creation date is system controlled, where only Amazon S3 can modify the value\. 

1. Other system metadata, such as the storage class configured for the object and whether the object has server\-side encryption enabled, are examples of system metadata whose values you control\. If your bucket is configured as a website, sometimes you might want to redirect a page request to another page or an external URL\. In this case, a webpage is an object in your bucket\. Amazon S3 stores the page redirect value as system metadata whose value you control\. 

   When you create objects, you can configure values of these system metadata items or update the values when you need to\. For more information about storage classes, see [Amazon S3 storage classes](storage-class-intro.md)\. 

   For more information about server\-side encryption, see [Protecting data using encryption](UsingEncryption.md)\. 

**Note**  
The PUT request header is limited to 8 KB in size\. Within the PUT request header, the system\-defined metadata is limited to 2 KB in size\. The size of system\-defined metadata is measured by taking the sum of the number of bytes in the US\-ASCII encoding of each key and value\. 

The following table provides a list of system\-defined metadata and whether you can update it\.


| Name | Description | Can user modify the value? | 
| --- | --- | --- | 
| Date | Current date and time\. | No | 
| Content\-Length | Object size in bytes\. | No | 
| Content\-Type | Object type\. | Yes | 
| Last\-Modified |  Object creation date or the last modified date, whichever is the latest\.  | No | 
| Content\-MD5 | The base64\-encoded 128\-bit MD5 digest of the object\. | Yes | 
| x\-amz\-server\-side\-encryption | Indicates whether server\-side encryption is enabled for the object, and whether that encryption is from the AWS Key Management Service \(AWS KMS\) or from Amazon S3 managed encryption \(SSE\-S3\)\. For more information, see [Protecting data using server\-side encryption](serv-side-encryption.md)\.  | Yes | 
| x\-amz\-version\-id | Object version\. When you enable versioning on a bucket, Amazon S3 assigns a version number to objects added to the bucket\. For more information, see [Using versioning](Versioning.md)\. | No | 
| x\-amz\-delete\-marker | In a bucket that has versioning enabled, this Boolean marker indicates whether the object is a delete marker\.  | No | 
| x\-amz\-storage\-class | Storage class used for storing the object\. For more information, see [Amazon S3 storage classes](storage-class-intro.md)\. | Yes | 
| x\-amz\-website\-redirect\-location |  Redirects requests for the associated object to another object in the same bucket or an external URL\. For more information, see [\(Optional\) Configuring a webpage redirect](how-to-page-redirect.md)\. | Yes | 
| x\-amz\-server\-side\-encryption\-aws\-kms\-key\-id | If x\-amz\-server\-side\-encryption is present and has the value of aws:kms, this indicates the ID of the AWS KMS symmetric customer master key \(CMK\) that was used for the object\. | Yes | 
| x\-amz\-server\-side\-encryption\-customer\-algorithm | Indicates whether server\-side encryption with customer\-provided encryption keys \(SSE\-C\) is enabled\. For more information, see [Protecting data using server\-side encryption with customer\-provided encryption keys \(SSE\-C\)](ServerSideEncryptionCustomerKeys.md)\.  | Yes | 

### User\-defined object metadata<a name="UserMetadata"></a>

When uploading an object, you can also assign metadata to the object\. You provide this optional information as a name\-value \(key\-value\) pair when you send a PUT or POST request to create the object\. When you upload objects using the REST API, the optional user\-defined metadata names must begin with "x\-amz\-meta\-" to distinguish them from other HTTP headers\. When you retrieve the object using the REST API, this prefix is returned\. When you upload objects using the SOAP API, the prefix is not required\. When you retrieve the object using the SOAP API, the prefix is removed, regardless of which API you used to upload the object\. 

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

When metadata is retrieved through the REST API, Amazon S3 combines headers that have the same name \(ignoring case\) into a comma\-delimited list\. If some metadata contains unprintable characters, it is not returned\. Instead, the `x-amz-missing-meta` header is returned with a value of the number of unprintable metadata entries\.

User\-defined metadata is a set of key\-value pairs\. Amazon S3 stores user\-defined metadata keys in lowercase\.

Amazon S3 allows arbitrary Unicode characters in your metadata values\.

To avoid issues around the presentation of these metadata values, you should conform to using US\-ASCII characters when using REST and UTF\-8 when using SOAP or browser\-based uploads via POST\.

When using non US\-ASCII characters in your metadata values, the provided Unicode string is examined for non US\-ASCII characters\. If the string contains only US\-ASCII characters, it is presented as is\. If the string contains non US\-ASCII characters, it is first character\-encoded using UTF\-8 and then encoded into US\-ASCII\.

Example:

```
PUT /Key HTTP/1.1
Host: awsexamplebucket1.s3.amazonaws.com
x-amz-meta-nonascii: ÄMÄZÕÑ S3

HEAD /Key HTTP/1.1
Host: awsexamplebucket1.s3.amazonaws.com
x-amz-meta-nonascii: =?UTF-8?B?w4PChE3Dg8KEWsODwpXDg8KRIFMz?=

PUT /Key HTTP/1.1
Host: awsexamplebucket1.s3.amazonaws.com
x-amz-meta-ascii: AMAZONS3

HEAD /Key HTTP/1.1
Host: awsexamplebucket1.s3.amazonaws.com
x-amz-meta-ascii: AMAZONS3
```

**Note**  
The PUT request header is limited to 8 KB in size\. Within the PUT request header, the user\-defined metadata is limited to 2 KB in size\. The size of user\-defined metadata is measured by taking the sum of the number of bytes in the UTF\-8 encoding of each key and value\. 

For information about adding metadata to your object after it’s been uploaded, see [How Do I Add Metadata to an S3 Object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-object-metadata.html) in the *Amazon Simple Storage Service Console User Guide*\.


# Browser\-based uploads using POST \(AWS signature version 2\)<a name="UsingHTTPPOST"></a>

Amazon S3 supports POST, which allows your users to upload content directly to Amazon S3\. POST is designed to simplify uploads, reduce upload latency, and save you money on applications where users upload data to store in Amazon S3\.

**Note**  
The request authentication discussed in this section is based on AWS Signature Version 2, a protocol for authenticating inbound API requests to AWS services\.   
Amazon S3 now supports Signature Version 4, a protocol for authenticating inbound API requests to AWS services, in all AWS regions\. At this time, AWS regions created before January 30, 2014 will continue to support the previous protocol, Signature Version 2\. Any new regions after January 30, 2014 will support only Signature Version 4 and therefore all requests to those regions must be made with Signature Version 4\. For more information, see [Authenticating Requests in Browser\-Based Uploads Using POST \(AWS Signature Version 4\)](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-authentication-HTTPPOST.html) in the *Amazon Simple Storage Service API Reference*\. 

The following figure shows an upload using Amazon S3 POST\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/s3_post.png)


**Uploading using POST**  

|  |  | 
| --- |--- |
| 1 | The user opens a web browser and accesses your web page\. | 
| 2 | Your web page contains an HTTP form that contains all the information necessary for the user to upload content to Amazon S3\. | 
| 3 | The user uploads content directly to Amazon S3\. | 

**Note**  
Query string authentication is not supported for POST\.


# Event message structure<a name="notification-content-structure"></a>

The notification message that Amazon S3 sends to publish an event is in the JSON format\. The following example shows the structure of the JSON message\. 

For general information about configuring event notifications, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.

Note the following about the example:
+ The `eventVersion` key value contains a major and minor version in the form `<major>`\.`<minor>`\.

  The major version is incremented if Amazon S3 makes a change to the event structure that is not backward compatible\. This includes removing a JSON field that is already present or changing how the contents of a field are represented \(for example, a date format\)\.

  The minor version is incremented if Amazon S3 adds new fields to the event structure\. This might occur if new information is provided for some or all existing events, or if new information is provided on only newly introduced event types\. Applications should ignore new fields to stay forward compatible with new minor versions of the event structure\.

  If new event types are introduced but the structure of the event is otherwise unmodified, the event version does not change\.

  To ensure that your applications can parse the event structure correctly, we recommend that you do an equal\-to comparison on the major version number\. To ensure that the fields expected by your application are present, we also recommend doing a greater\-than\-or\-equal\-to comparison on the minor version\.
+ The `responseElements` key value is useful if you want to trace a request by following up with AWS Support\. Both `x-amz-request-id` and `x-amz-id-2` help Amazon S3 trace an individual request\. These values are the same as those that Amazon S3 returns in the response to the request that initiates the events, so they can be used to match the event to the request\.
+ The `s3` key provides information about the bucket and object involved in the event\. The object key name value is URL encoded\. For example, "red flower\.jpg" becomes "red\+flower\.jpg" \(Amazon S3 returns "`application/x-www-form-urlencoded`" as the content type in the response\)\.
+ The `sequencer` key provides a way to determine the sequence of events\. Event notifications are not guaranteed to arrive in the order that the events occurred\. However, notifications from events that create objects \(`PUT`s\) and delete objects contain a `sequencer`, which can be used to determine the order of events for a given object key\. 

  If you compare the `sequencer` strings from two event notifications on the same object key, the event notification with the greater `sequencer` hexadecimal value is the event that occurred later\. If you are using event notifications to maintain a separate database or index of your Amazon S3 objects, you will probably want to compare and store the `sequencer` values as you process each event notification\. 

  Note the following:
  + You cannot use `sequencer` to determine order for events on different object keys\.
  + The sequencers can be of different lengths\. So to compare these values, you first left pad the shorter value with zeros, and then do a lexicographical comparison\.
+ The `glacierEventData` key is only visible for `s3:ObjectRestore:Completed` events\. 
+ The `restoreEventData` key contains attributes related to your restore request\.
+ The `replicationEventData` key is only visible for replication events\.

The following example shows version 2\.2 of the event message JSON structure, which is the version currently being used by Amazon S3\.

```
{  
   "Records":[  
      {  
         "eventVersion":"2.2",
         "eventSource":"aws:s3",
         "awsRegion":"us-west-2",
         "eventTime":"The time, in ISO-8601 format, for example, 1970-01-01T00:00:00.000Z, when Amazon S3 finished processing the request",
         "eventName":"event-type",
         "userIdentity":{  
            "principalId":"Amazon-customer-ID-of-the-user-who-caused-the-event"
         },
         "requestParameters":{  
            "sourceIPAddress":"ip-address-where-request-came-from"
         },
         "responseElements":{  
            "x-amz-request-id":"Amazon S3 generated request ID",
            "x-amz-id-2":"Amazon S3 host that processed the request"
         },
         "s3":{  
            "s3SchemaVersion":"1.0",
            "configurationId":"ID found in the bucket notification configuration",
            "bucket":{  
               "name":"bucket-name",
               "ownerIdentity":{  
                  "principalId":"Amazon-customer-ID-of-the-bucket-owner"
               },
               "arn":"bucket-ARN"
            },
            "object":{  
               "key":"object-key",
               "size":"object-size",
               "eTag":"object eTag",
               "versionId":"object version if bucket is versioning-enabled, otherwise null",
               "sequencer": "a string representation of a hexadecimal value used to determine event sequence, only used with PUTs and DELETEs"
            }
         },
         "glacierEventData": {
            "restoreEventData": {
               "lifecycleRestorationExpiryTime": "The time, in ISO-8601 format, for example, 1970-01-01T00:00:00.000Z, of Restore Expiry",
               "lifecycleRestoreStorageClass": "Source storage class for restore"
            }
         }
      }
   ]
}
```

The following example shows version 2\.0 of the event message structure, which is no longer used by Amazon S3\.

```
{  
   "Records":[  
      {  
         "eventVersion":"2.0",
         "eventSource":"aws:s3",
         "awsRegion":"us-west-2",
         "eventTime":"The time, in ISO-8601 format, for example, 1970-01-01T00:00:00.000Z, when S3 finished processing the request",
         "eventName":"event-type",
         "userIdentity":{  
            "principalId":"Amazon-customer-ID-of-the-user-who-caused-the-event"
         },
         "requestParameters":{  
            "sourceIPAddress":"ip-address-where-request-came-from"
         },
         "responseElements":{  
            "x-amz-request-id":"Amazon S3 generated request ID",
            "x-amz-id-2":"Amazon S3 host that processed the request"
         },
         "s3":{  
            "s3SchemaVersion":"1.0",
            "configurationId":"ID found in the bucket notification configuration",
            "bucket":{  
               "name":"bucket-name",
               "ownerIdentity":{  
                  "principalId":"Amazon-customer-ID-of-the-bucket-owner"
               },
               "arn":"bucket-ARN"
            },
            "object":{  
               "key":"object-key",
               "size":"object-size",
               "eTag":"object eTag",
               "versionId":"object version if bucket is versioning-enabled, otherwise null",
               "sequencer": "a string representation of a hexadecimal value used to determine event sequence, only used with PUTs and DELETEs"
            }
         }
      }
   ]
}
```

The following are example messages:
+ Test message—When you configure an event notification on a bucket, Amazon S3 sends the following test message\.

  ```
  1. {  
  2.    "Service":"Amazon S3",
  3.    "Event":"s3:TestEvent",
  4.    "Time":"2014-10-13T15:57:02.089Z",
  5.    "Bucket":"bucketname",
  6.    "RequestId":"5582815E1AEA5ADF",
  7.    "HostId":"8cLeGAmw098X5cv4Zkwcmo8vvZa3eH3eKxsPzbB9wrR+YstdA6Knx4Ip8EXAMPLE"
  8. }
  ```
+ Example message when an object is created using the PUT request—The following message is an example of a message Amazon S3 sends to publish an `s3:ObjectCreated:Put` event\.

  ```
   1. {  
   2.    "Records":[  
   3.       {  
   4.          "eventVersion":"2.1",
   5.          "eventSource":"aws:s3",
   6.          "awsRegion":"us-west-2",
   7.          "eventTime":"1970-01-01T00:00:00.000Z",
   8.          "eventName":"ObjectCreated:Put",
   9.          "userIdentity":{  
  10.             "principalId":"AIDAJDPLRKLG7UEXAMPLE"
  11.          },
  12.          "requestParameters":{  
  13.             "sourceIPAddress":"127.0.0.1"
  14.          },
  15.          "responseElements":{  
  16.             "x-amz-request-id":"C3D13FE58DE4C810",
  17.             "x-amz-id-2":"FMyUVURIY8/IgAtTv8xRjskZQpcIZ9KG4V5Wp6S7S/JRWeUWerMUE5JgHvANOjpD"
  18.          },
  19.          "s3":{  
  20.             "s3SchemaVersion":"1.0",
  21.             "configurationId":"testConfigRule",
  22.             "bucket":{  
  23.                "name":"mybucket",
  24.                "ownerIdentity":{  
  25.                   "principalId":"A3NL1KOZZKExample"
  26.                },
  27.                "arn":"arn:aws:s3:::mybucket"
  28.             },
  29.             "object":{  
  30.                "key":"HappyFace.jpg",
  31.                "size":1024,
  32.                "eTag":"d41d8cd98f00b204e9800998ecf8427e",
  33.                "versionId":"096fKKXTRTtl3on89fVO.nfljtsv6qko",
  34.                "sequencer":"0055AED6DCD90281E5"
  35.             }
  36.          }
  37.       }
  38.    ]
  39. }
  ```

For a definition of each IAM identification prefix \(AIDA, AROA, AGPA, etc\.\), see [Understanding Unique ID Prefixes](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-prefixesl)\.


# Setting lifecycle configuration on a bucket<a name="how-to-set-lifecycle-configuration-intro"></a>

**Topics**
+ [Manage an object's lifecycle using the Amazon S3 console](manage-lifecycle-using-console.md)
+ [Set lifecycle configurations using the AWS CLI](set-lifecycle-cli.md)
+ [Managing object lifecycles using the AWS SDK for Java](manage-lifecycle-using-java.md)
+ [Manage an object's lifecycle using the AWS SDK for \.NET](manage-lifecycle-using-dot-net.md)
+ [Manage an object's lifecycle using the AWS SDK for Ruby](manage-lifecycle-using-ruby.md)
+ [Manage an object's lifecycle using the REST API](manage-lifecycle-using-rest.md)

This section explains how you can set S3 Lifecycle configuration on a bucket programmatically using AWS SDKs, or by using the Amazon S3 console or the AWS CLI\. Note the following:
+ When you add an S3 Lifecycle configuration to a bucket, there is usually some lag before a new or updated Lifecycle configuration is fully propagated to all the Amazon S3 systems\. Expect a delay of a few minutes before the configuration fully takes effect\. This delay can also occur when you delete an S3 Lifecycle configuration\.
+ When you disable or delete a Lifecycle rule, after a small delay, Amazon S3 stops scheduling new objects for deletion or transition\. Any objects that were already scheduled are unscheduled and are not deleted or transitioned\.
+ When you add a Lifecycle configuration to a bucket, the configuration rules apply to both existing objects and objects that you add later\. For example, if you add a Lifecycle configuration rule today with an expiration action that causes objects with a specific prefix to expire 30 days after creation, Amazon S3 will queue for removal any existing objects that are more than 30 days old\.
+ There may be a lag between when the Lifecycle configuration rules are satisfied and when the action triggered by satisfying the rule is taken\. However, changes in billing happen as soon as the Lifecycle configuration rule is satisfied even if the action is not yet taken\. One example is you are not charged for storage after the object expiration time even if the object is not deleted immediately\. Another example is you are charged Amazon S3 Glacier storage rates as soon as the object transition time elapses, even if the object is not immediately transitioned to the S3 Glacier storage class\. Lifecycle transitions to the S3 Intelligent\-Tiering storage class are the exception and changes in billing do not happen until the object has transitioned into the S3 Intelligent\-Tiering storage class\. 

For information about S3 Lifecycle configuration, see [Object lifecycle management](object-lifecycle-mgmt.md)\.




# Website endpoints<a name="WebsiteEndpoints"></a>

When you configure your bucket as a static website, the website is available at the AWS Region\-specific website endpoint of the bucket\. Website endpoints are different from the endpoints where you send REST API requests\. For more information about the differences between the endpoints, see [Key differences between a website endpoint and a REST API endpoint](#WebsiteRestEndpointDiff)\.

Depending on your Region, your Amazon S3 website endpoint follows one of these two formats\.
+ **s3\-website dash \(\-\) Region** ‐ `http://bucket-name.s3-website-Region.amazonaws.com`
+ **s3\-website dot \(\.\) Region** ‐ `http://bucket-name.s3-website.Region.amazonaws.com`

These URLs return the default index document that you configure for the website\. For a complete list of Amazon S3 website endpoints, see [Amazon S3 Website Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html#s3_website_region_endpoints)\.

For your customers to access content at the website endpoint, you must make all your content publicly readable\. To do so, you can edit the S3 Block Public Access settings for the bucket\. For more information, see [Using Amazon S3 block public access](access-control-block-public-access.md)\. Then, use a bucket policy or an access control list \(ACL\) on an object to grant the necessary permissions\. For more information, see [Setting permissions for website access](WebsiteAccessPermissionsReqd.md)\.

**Important**  
Amazon S3 website endpoints do not support HTTPS\. For information about using HTTPS with an Amazon S3 bucket, see the following:  
[How do I use CloudFront to serve HTTPS requests for my Amazon S3 bucket?](https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-requests-s3)
[Requiring HTTPS for communication between viewers and CloudFront](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html)
Requester Pays buckets  do not allow access through the website endpoint\. Any request to such a bucket receives a 403 Access Denied response\. For more information, see [Requester Pays buckets](RequesterPaysBuckets.md)\.

**Topics**
+ [Website endpoint examples](#website-endpoint-examples)
+ [Adding a DNS CNAME](#website-endpoint-dns-cname)
+ [Using a custom domain with Route 53](#custom-domain-s3-endpoint)
+ [Key differences between a website endpoint and a REST API endpoint](#WebsiteRestEndpointDiff)

## Website endpoint examples<a name="website-endpoint-examples"></a>

The following examples show how you can access an Amazon S3 bucket that is configured as a static website\.

**Example — Requesting an object at the root level**  
To request a specific object that is stored at the root level in the bucket, use the following URL structure\.  

```
http://bucket-name.s3-website.Region.amazonaws.com/object-name
```
For example, the following URL requests the `photo.jpg` object that is stored at the root level in the bucket\.  

```
http://example-bucket.s3-website.us-west-2.amazonaws.com/photo.jpg
```

**Example — Requesting an object in a prefix**  
To request an object that is stored in a folder in your bucket, use this URL structure\.  

```
http://bucket-name.s3-website.Region.amazonaws.com/folder-name/object-name
```
The following URL requests the `docs/doc1.html` object in your bucket\.   

```
http://example-bucket.s3-website.us-west-2.amazonaws.com/docs/doc1.html
```

## Adding a DNS CNAME<a name="website-endpoint-dns-cname"></a>

If you have a registered domain, you can add a DNS CNAME entry to point to the Amazon S3 website endpoint\. For example, if you registered the domain `www.example-bucket.com`, you could create a bucket `www.example-bucket.com`, and add a DNS CNAME record that points to `www.example-bucket.com.s3-website.Region.amazonaws.com`\. All requests to `http://www.example-bucket.com` are routed to `www.example-bucket.com.s3-website.Region.amazonaws.com`\. 

For more information, see [Customizing Amazon S3 URLs with CNAMEs](VirtualHosting.md#VirtualHostingCustomURLs)\. 

## Using a custom domain with Route 53<a name="custom-domain-s3-endpoint"></a>

Instead of accessing the website using an Amazon S3 website endpoint, you can use your own domain registered with Amazon Route 53 to serve your content—for example, `example.com`\. You can use Amazon S3 with Route 53 to host a website at the root domain\. For example, if you have the root domain `example.com` and you host your website on Amazon S3, your website visitors can access the site from their browser by entering either `http://www.example.com` or `http://example.com`\. 

For an example walkthrough, see [Configuring a static website using a custom domain registered with Route 53](website-hosting-custom-domain-walkthrough.md)\. 

## Key differences between a website endpoint and a REST API endpoint<a name="WebsiteRestEndpointDiff"></a>

An Amazon S3 website endpoint is optimized for access from a web browser\. The following table summarizes the key differences between a REST API endpoint and a website endpoint\. 


| Key difference | REST API endpoint | Website endpoint | 
| --- | --- | --- | 
| Access control |  Supports both public and private content  | Supports only publicly readable content  | 
| Error message handling |  Returns an XML\-formatted error response  | Returns an HTML document | 
| Redirection support |  Not applicable  | Supports both object\-level and bucket\-level redirects | 
| Requests supported  |  Supports all bucket and object operations  | Supports only GET and HEAD requests on objects | 
| Responses to GET and HEAD requests at the root of a bucket | Returns a list of the object keys in the bucket | Returns the index document that is specified in the website configuration | 
| Secure Sockets Layer \(SSL\) support | Supports SSL connections | Does not support SSL connections | 

For a complete list of Amazon S3 endpoints, see [Amazon S3 Service Endpoints and Quotas](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\.


# Listing Keys Using the AWS SDK for \.NET<a name="ListingObjectKeysUsingNetSDK"></a>

**Example**  
The following C\# example lists the object keys for a bucket\. In the example, we use pagination to retrieve a set of object keys\. If there are more keys to return, Amazon S3 includes a continuation token in the response\. The code uses the continuation token in the subsequent request to fetch the next set of object keys\.  
 For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ListObjectsTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;

        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            ListingObjectsAsync().Wait();
        }

        static async Task ListingObjectsAsync()
        {
            try
            {
                ListObjectsV2Request request = new ListObjectsV2Request
                {
                    BucketName = bucketName,
                    MaxKeys = 10
                };
                ListObjectsV2Response response;
                do
                {
                    response = await client.ListObjectsV2Async(request);

                    // Process the response.
                    foreach (S3Object entry in response.S3Objects)
                    {
                        Console.WriteLine("key = {0} size = {1}",
                            entry.Key, entry.Size);
                    }
                    Console.WriteLine("Next Continuation Token: {0}", response.NextContinuationToken);
                    request.ContinuationToken = response.NextContinuationToken;
                } while (response.IsTruncated);
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("S3 error occurred. Exception: " + amazonS3Exception.ToString());
                Console.ReadKey();
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
                Console.ReadKey();
            }
        }
    }
}
```


# Conditional Functions<a name="s3-glacier-select-sql-reference-conditional"></a>

Amazon S3 Select and S3 Glacier Select support the following conditional functions\.

**Topics**
+ [CASE](#s3-glacier-select-sql-reference-case)
+ [COALESCE](#s3-glacier-select-sql-reference-coalesce)
+ [NULLIF](#s3-glacier-select-sql-reference-nullif)

## CASE<a name="s3-glacier-select-sql-reference-case"></a>

The CASE expression is a conditional expression, similar to if/then/else statements found in other languages\. CASE is used to specify a result when there are multiple conditions\. There are two types of CASE expressions: simple and searched\.

In simple CASE expressions, an expression is compared with a value\. When a match is found, the specified action in the THEN clause is applied\. If no match is found, the action in the ELSE clause is applied\.

In searched CASE expressions, each CASE is evaluated based on a Boolean expression, and the CASE statement returns the first matching CASE\. If no matching CASEs are found among the WHEN clauses, the action in the ELSE clause is returned\.

### Syntax<a name="s3-glacier-select-sql-reference-case-syntax"></a>

Simple CASE statement used to match conditions:

```
CASE expression
WHEN value THEN result
[WHEN...]
[ELSE result]
END
```

Searched CASE statement used to evaluate each condition:

```
CASE
WHEN boolean condition THEN result
[WHEN ...]
[ELSE result]
END
```

### Examples<a name="s3-glacier-select-sql-reference-case-examples"></a>

Use a simple CASE expression is used to replace New York City with Big Apple in a query\. Replace all other city names with other\.

```
select venuecity,
case venuecity
when 'New York City'
then 'Big Apple' else 'other'
end from venue
order by venueid desc;

venuecity        |   case
-----------------+-----------
Los Angeles      | other
New York City    | Big Apple
San Francisco    | other
Baltimore        | other
...
(202 rows)
```

Use a searched CASE expression to assign group numbers based on the PRICEPAID value for individual ticket sales:

```
select pricepaid,
case when pricepaid <10000 then 'group 1'
when pricepaid >10000 then 'group 2'
else 'group 3'
end from sales
order by 1 desc;

pricepaid |  case
-----------+---------
12624.00 | group 2
10000.00 | group 3
10000.00 | group 3
9996.00 | group 1
9988.00 | group 1
...
(172456 rows)
```

## COALESCE<a name="s3-glacier-select-sql-reference-coalesce"></a>

Evaluates the arguments in order and returns the first non\-unknown, that is, the first non\-null or non\-missing\. This function does not propagate null and missing\.

### Syntax<a name="s3-glacier-select-sql-reference-coalesce-syntax"></a>

```
COALESCE ( expression, expression, ... )
```

### Parameters<a name="s3-glacier-select-sql-reference-coalesce-parameters"></a>

 *expression*   
The target expression that the function operates on\.

### Examples<a name="s3-glacier-select-sql-reference-coalesce-examples"></a>

```
COALESCE(1)                -- 1
COALESCE(null)             -- null
COALESCE(null, null)       -- null
COALESCE(missing)          -- null
COALESCE(missing, missing) -- null
COALESCE(1, null)          -- 1
COALESCE(null, null, 1)    -- 1
COALESCE(null, 'string')   -- 'string'
COALESCE(missing, 1)       -- 1
```

## NULLIF<a name="s3-glacier-select-sql-reference-nullif"></a>

Given two expressions, returns NULL if the two expressions evaluate to the same value; otherwise, returns the result of evaluating the first expression\.

### Syntax<a name="s3-glacier-select-sql-reference-nullif-syntax"></a>

```
NULLIF ( expression1, expression2 )
```

### Parameters<a name="s3-glacier-select-sql-reference-nullif-parameters"></a>

 *expression1, expression2*   
The target expressions that the function operates on\.

### Examples<a name="s3-glacier-select-sql-reference-nullif-examples"></a>

```
NULLIF(1, 1)             -- null
NULLIF(1, 2)             -- 1
NULLIF(1.0, 1)           -- null
NULLIF(1, '1')           -- 1
NULLIF([1], [1])         -- null
NULLIF(1, NULL)          -- 1
NULLIF(NULL, 1)          -- null
NULLIF(null, null)       -- null
NULLIF(missing, null)    -- null
NULLIF(missing, missing) -- null
```


# Put object tagging<a name="batch-ops-put-object-tagging"></a>

The Put Object Tagging operation replaces the Amazon S3 object tags of each object listed in the manifest\. An Amazon S3 object tag is a key\-value pair of strings that you can use to store metadata about an object\.

To create a Put Object Tagging job, you provide a set of tags that you want to apply\. S3 Batch Operations apply the same set of tags to each object\. The tag set that you provide replaces whatever tag sets are already associated with the objects in the manifest\. S3 Batch Operations do not support adding tags to objects while leaving the existing tags in place\.

If the objects in your manifest are in a versioned bucket, you can apply the tag set to specific versions of each object\. You do this by specifying a version ID for each object in the manifest\. If you don't include a version ID for any object, then S3 Batch Operations will apply the tag set to the latest version of each object\. 

## Restrictions and limitations<a name="batch-ops-set-tagging-restrictions"></a>
+ The role that you specify to run the Put Object Tagging job must have permissions to perform the underlying Amazon S3 PUT Object tagging operation\. For more information about the permissions required, see [PUT Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUTtagging.html) in the *Amazon Simple Storage Service API Reference*\.
+ S3 Batch Operations use the Amazon S3 PUT Object tagging operation to apply tags to each object in the manifest\. Therefore, all restrictions and limitations that apply to the underlying PUT Object tagging operation also apply to S3 Batch Operations Put Object Tagging jobs\. For more information, see the [Related resources](#batch-ops-put-object-tagging-related-resources) section of this page\.

## Related resources<a name="batch-ops-put-object-tagging-related-resources"></a>
+ [Object tagging](object-tagging.md)
+ [GET Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtagging.html) in the *Amazon Simple Storage Service API Reference*
+ [PUT Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUTtagging.html) in the *Amazon Simple Storage Service API Reference*


# Upload a file to an S3 bucket using the AWS SDK for \.NET \(high\-level API\)<a name="HLuploadFileDotNet"></a>

To upload a file to an S3 bucket, use the `TransferUtility` class\. When uploading data from a file, you must provide the object's key name\. If you don't, the API uses the file name for the key name\. When uploading data from a stream, you must provide the object's key name\.

To set advanced upload options—such as the part size, the number of threads when uploading the parts concurrently, metadata, the storage class, or ACL—use the `TransferUtilityUploadRequest` class\. 

The following C\# example uploads a file to an Amazon S3 bucket in multiple parts\. It shows how to use various `TransferUtility.Upload` overloads to upload a file\. Each successive call to upload replaces the previous upload\. For information about the example's compatibility with a specific version of the AWS SDK for \.NET and instructions for creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Transfer;
using System;
using System.IO;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class UploadFileMPUHighLevelAPITest
    {
        private const string bucketName = "*** provide bucket name ***";
        private const string keyName = "*** provide a name for the uploaded object ***";
        private const string filePath = "*** provide the full path name of the file to upload ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            UploadFileAsync().Wait();
        }

        private static async Task UploadFileAsync()
        {
            try
            {
                var fileTransferUtility =
                    new TransferUtility(s3Client);

                // Option 1. Upload a file. The file name is used as the object key name.
                await fileTransferUtility.UploadAsync(filePath, bucketName);
                Console.WriteLine("Upload 1 completed");

                // Option 2. Specify object key name explicitly.
                await fileTransferUtility.UploadAsync(filePath, bucketName, keyName);
                Console.WriteLine("Upload 2 completed");

                // Option 3. Upload data from a type of System.IO.Stream.
                using (var fileToUpload = 
                    new FileStream(filePath, FileMode.Open, FileAccess.Read))
                {
                    await fileTransferUtility.UploadAsync(fileToUpload,
                                               bucketName, keyName);
                }
                Console.WriteLine("Upload 3 completed");

                // Option 4. Specify advanced settings.
                var fileTransferUtilityRequest = new TransferUtilityUploadRequest
                {
                    BucketName = bucketName,
                    FilePath = filePath,
                    StorageClass = S3StorageClass.StandardInfrequentAccess,
                    PartSize = 6291456, // 6 MB.
                    Key = keyName,
                    CannedACL = S3CannedACL.PublicRead
                };
                fileTransferUtilityRequest.Metadata.Add("param1", "Value1");
                fileTransferUtilityRequest.Metadata.Add("param2", "Value2");

                await fileTransferUtility.UploadAsync(fileTransferUtilityRequest);
                Console.WriteLine("Upload 4 completed");
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }

        }
    }
}
```

## More info<a name="HLuploadFileDotNet-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# Using the AWS SDK for \.NET<a name="UsingTheMPDotNetAPI"></a>

The AWS SDK for \.NET provides the API for the Amazon S3 bucket and object operations\. For object operations, in addition to providing the API to upload objects in a single operation, the SDK provides the API to upload large objects in parts \(see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\)\. 

**Topics**
+ [The \.NET API Organization](#DotNetAPIOrg)
+ [Running the Amazon S3 \.NET Code Examples](#TestingDotNetApiSamples)

The AWS SDK for \.NET gives you the option of using a high\-level or low\-level API\. 

**Low\-Level API**  
The low\-level APIs correspond to the underlying Amazon S3 REST operations, including the create, update, and delete operations that apply to buckets and objects\. When you upload large objects using the low\-level multipart upload API \(see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\), it provides greater control\. For example, it lets you pause and resume multipart uploads, vary part sizes during the upload, or begin uploads when you don't know the size of the data in advance\. If you do not have these requirements, use the high\-level API for uploading objects\.

**High\-Level API**  
For uploading objects, the SDK provides a higher level of abstraction by providing the `TransferUtility` class\. The high\-level API is a simpler API, where in just a few lines of code, you can upload files and streams to Amazon S3\. You should use this API to upload data unless you need to control the upload as described in the preceding Low\-Level API section\.

For smaller data size, the `TransferUtility` API uploads data in a single operation\. However, the `TransferUtility` switches to using the multipart upload API when the data size reaches a certain threshold\. By default, it uses multiple threads to concurrently upload the parts\. If a part upload fails, the API retries the failed part upload up to three times\. However, these are configurable options\. 

**Note**  
When you're using a stream for the source of data, the `TransferUtility` class does not do concurrent uploads\.

## The \.NET API Organization<a name="DotNetAPIOrg"></a>

When writing Amazon S3 applications using the AWS SDK for \.NET, you use the `AWSSDK.dll`\. The following namespaces in this assembly provide the multipart upload API:
+ **Amazon\.S3\.Transfer—**Provides the high\-level API to upload your data in parts\. 

  It includes the `TransferUtility` class that enables you to specify a file, directory, or stream for uploading your data\. It also includes the `TransferUtilityUploadRequest` and `TransferUtilityUploadDirectoryRequest` classes to configure advanced settings, such as the number of concurrent threads, part size, object metadata, the storage class \(STANDARD, REDUCED\_REDUNDANCY\), and object access control list \(ACL\)\.
+ **Amazon\.S3—**Provides the implementation for the low\-level APIs\. 

  It provides methods that correspond to the Amazon S3 REST multipart upload API \(see [Using the REST API for multipart upload](UsingRESTAPImpUpload.md)\)\.
+ **Amazon\.S3\.Model—**Provides the low\-level API classes to create requests and process responses\. For example, it provides the `InitiateMultipartUploadRequest` and `InitiateMultipartUploadResponse` classes you can use when initiating a multipart upload, and the `UploadPartRequest` and `UploadPartResponse` classes when uploading parts\. 
+ **Amazon\.S3\.Encryption—** Provides `AmazonS3EncryptionClient`\.
+ **Amazon\.S3\.Util—** Provides various utility classes such as `AmazonS3Util` and `BucketRegionDetector`\.

For more information about the AWS SDK for \.NET API, see [AWS SDK for \.NET Version 3 API Reference](https://docs.aws.amazon.com/sdkfornet/v3/apidocs/Index.html)\. 

## Running the Amazon S3 \.NET Code Examples<a name="TestingDotNetApiSamples"></a>

The \.NET code examples in this guide are compatible with the AWS SDK for \.NET version 3\.0\. For information about setting up and running the code examples, see [Getting Started with the AWS SDK for \.NET](https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/net-dg-setup.html) in the *AWS SDK for \.NET Developer Guide*\. 


# Amazon S3 Storage Lens examples using the AWS CLI<a name="S3LensCLIExamples"></a>

Amazon S3 Storage Lens aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format\. You can use the dashboard to visualize insights and trends, flag outliers, and provides recommendations for optimizing storage costs and applying data protection best practices\. You can use S3 Storage Lens through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\.\. For more information, see [Assessing storage activity and usage with Amazon S3 Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens.html)\. 

The following examples show how you can use S3 Storage Lens with the AWS Command Line Interface\.

**Topics**
+ [Helper files for using Amazon S3 Storage Lens](#S3LensHelperFilesCLI)
+ [Using Amazon S3 Storage Lens configurations using the AWS CLI](#S3LensConfigurationsCLI)
+ [Using Amazon S3 Storage Lens with your AWS Organizations using the AWS CLI](#S3LensOrganizationsCLI)

## Helper files for using Amazon S3 Storage Lens<a name="S3LensHelperFilesCLI"></a>

Use the following json files for key inputs for your examples\.



### S3 Storage Lens sample configuration json<a name="S3LensHelperFilesSampleConfigurationCLI"></a>

**Example config\.json**  
Contains details of a S3 Storage Lens Organizations\-level *Advanced Metrics and Recommendations* configuration\.  
Additional charges apply for Advanced Metrics and Recommendations\. For more information, see [Advanced Metrics and Recommendations](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_basics_metrics_recommendations.html#storage_lens_basics_metrics_selection)\.

```
        {
     "Id": "SampleS3StorageLensConfiguration",//Use this property to identify S3 Storage Lens configuration.
     "AwsOrg": {//Use this property when enabling S3 Storage Lens for AWS Organizations
        "Arn": "arn:aws:organizations::222222222222:organization/o-abcdefgh"
     },
     "AccountLevel": {
        "ActivityMetrics": {
           "IsEnabled":true
        },
        "BucketLevel": {
           "ActivityMetrics": {
              "IsEnabled":true//Mark this as false if you only want Free Metrics metrics.
           },
           "PrefixLevel":{
              "StorageMetrics":{
                 "IsEnabled":true,//Mark this as false if you only want Free Metrics metrics.
                 "SelectionCriteria":{
                    "MaxDepth":5,
                    "MinStorageBytesPercentage":1.25,
                    "Delimiter":"/"
                 }
              }
           }
        }
     },
     "Exclude": {//Replace with include if you prefer to include regions.
        "Regions": [
           "eu-west-1"
        ],
        "Buckets": [/This attribute is not supported for  Organizations-level configurations.
           "arn:aws:s3:::source_bucket1" 
        ]
     },
     "IsEnabled": true,//Whether the configuration is enabled
     "DataExport": {//Details about the metrics export
        "S3BucketDestination": {
           "OutputSchemaVersion": "V_1",
           "Format": "CSV",//You can add "Parquet" if you prefer.
           "AccountId": "ExampleAWSAccountNo8",
           "Arn": "arn:aws:s3:::destination-bucket-name",
           "Prefix": "prefix-for-your-export-destination",
           "Encryption": {
              "SSES3": {}
           }
        }
     }
  }
```

### S3 Storage Lens sample configuration tags json<a name="S3LensHelperFilesSampleConfigurationTagsCLI"></a>

**Example tags\.json**  

```
[
    {
        "Key": "key1",
        "Value": "value1"
    },
    {
        "Key": "key2",
        "Value": "value2"
    }
]
```

### S3 Storage Lens sample configuration IAM permissions<a name="S3LensHelperFilesSampleConfigurationIAMPermissionsCLI"></a>

**Example permissions\.json**  
S3 Storage Lens IAM permissions\.  

```
{
  "Version": "2012-10-17",  "Statement": [
    {
      "Effect": "Allow",      
      "Action": [       
        "iam:*",        
        "sts:AssumeRole"      
      ],      
      "Resource": "*"    
    },    
    {
      "Effect": "Allow",      
      "Action":
      [
        "s3:GetStorageLensConfiguration*",
        "s3:DeleteStorageLensConfiguration*",        
        "s3:PutStorageLensConfiguration*"      
      ],      
      "Condition": {
        "StringEquals": {
          "aws:ResourceTag/key1": "value1"        
        }
      },      
      "Resource": "*"  
    }
  ]
}
```

## Using Amazon S3 Storage Lens configurations using the AWS CLI<a name="S3LensConfigurationsCLI"></a>

You can use the AWS CLI to list, create, get and update your S3 Storage Lens configurations\. The following examples use the helper json files for key inputs\.

**Topics**
+ [Put an S3 Storage Lens configuration](#S3PutStorageLensConfigurationTagsCLI)
+ [Put an S3 Storage Lens configuration without tags](#S3PutStorageLensConfigurationWOTagsCLI)
+ [Gets an S3 Storage Lens configuration](#S3GetStorageLensConfigurationCLI)
+ [Lists S3 Storage Lens configurations without next token](#S3ListStorageLensConfigurationsWOTokenCLI)
+ [Lists S3 Storage Lens configurations](#S3ListStorageLensConfigurationsCLI)
+ [Delete an S3 Storage Lens configuration](#S3DeleteStorageLensConfigurationCLI)
+ [Put tags to an S3 Storage Lens configuration](#S3PutStorageLensConfigurationTaggingCLI)
+ [Get tags for an S3 Storage Lens configuration](#S3GetStorageLensConfigurationTaggingCLI)
+ [Delete tags for an S3 Storage Lens configuration](#S3DeleteStorageLensConfigurationTaggingCLI)

### Put an S3 Storage Lens configuration<a name="S3PutStorageLensConfigurationTagsCLI"></a>

**Example Puts an S3 Storage Lens configuration**  

```
aws s3control put-storage-lens-configuration --account-id=222222222222 --config-id=your-configuration-id --region=us-east-1 --storage-lens-configuration=file://./config.json --tags=file://./tags.json
```

### Put an S3 Storage Lens configuration without tags<a name="S3PutStorageLensConfigurationWOTagsCLI"></a>

**Example Puts an S3 Storage Lens configuration\.**  

```
aws s3control put-storage-lens-configuration --account-id=222222222222 --config-id=your-configuration-id --region=us-east-1 --storage-lens-configuration=file://./config.json
```

### Gets an S3 Storage Lens configuration<a name="S3GetStorageLensConfigurationCLI"></a>

**Example Get an S3 Storage Lens configuration**  

```
aws s3control get-storage-lens-configuration --account-id=222222222222 --config-id=your-configuration-id --region=us-east-1
```

### Lists S3 Storage Lens configurations without next token<a name="S3ListStorageLensConfigurationsWOTokenCLI"></a>

**Example Lists S3 Storage Lens configurations without next token**  

```
aws s3control list-storage-lens-configurations --account-id=222222222222 --region=us-east-1
```

### Lists S3 Storage Lens configurations<a name="S3ListStorageLensConfigurationsCLI"></a>

**Example Lists S3 Storage Lens configurations**  

```
aws s3control list-storage-lens-configurations --account-id=222222222222 --region=us-east-1 --next-token=abcdefghij1234
```

### Delete an S3 Storage Lens configuration<a name="S3DeleteStorageLensConfigurationCLI"></a>

**Example Delete an S3 Storage Lens configuration**  

```
aws s3control delete-storage-lens-configuration --account-id=222222222222 --region=us-east-1 --config-id=your-configuration-id
```

### Put tags to an S3 Storage Lens configuration<a name="S3PutStorageLensConfigurationTaggingCLI"></a>

**Example Put tags to an S3 Storage Lens configuration**  

```
aws s3control put-storage-lens-configuration-tagging --account-id=222222222222 --region=us-east-1 --config-id=your-configuration-id --tags=file://./tags.json
```

### Get tags for an S3 Storage Lens configuration<a name="S3GetStorageLensConfigurationTaggingCLI"></a>

**Example Get tags for an S3 Storage Lens configuration**  

```
aws s3control get-storage-lens-configuration-tagging --account-id=222222222222 --region=us-east-1 --config-id=your-configuration-id
```

### Delete tags for an S3 Storage Lens configuration<a name="S3DeleteStorageLensConfigurationTaggingCLI"></a>

**Example Delete tags for an S3 Storage Lens configuration**  

```
aws s3control delete-storage-lens-configuration-tagging --account-id=222222222222 --region=us-east-1 --config-id=your-configuration-id
```

## Using Amazon S3 Storage Lens with your AWS Organizations using the AWS CLI<a name="S3LensOrganizationsCLI"></a>

Use Amazon S3 Storage Lens to collect storage metrics and usage data for all accounts that are part of your AWS Organizations hierarchy\. For more information, see [Using Amazon S3 Storage Lens with AWS Organizations](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_with_organizations.html)\. 

**Topics**
+ [Enable Organizations trusted access for S3 Storage Lens](#OrganizationsEnableTrustedAccessS3LensCLI)
+ [Disable Organizations trusted access for S3 Storage Lens](#OrganizationsDisableTrustedAccessS3LensCLI)
+ [Register Organizations delegated administrators for S3 Storage Lens](#OrganizationsRegisterDelegatedAdministratorS3LensCLI)
+ [De\-register Organizations delegated administrators for S3 Storage Lens](#OrganizationsDeregisterDelegatedAdministratorS3LensCLI)



### Enable Organizations trusted access for S3 Storage Lens<a name="OrganizationsEnableTrustedAccessS3LensCLI"></a>

**Example Enable Organizations trusted access for S3 Storage Lens**  

```
aws organizations enable-aws-service-access --service-principal storage-lens.s3.amazonaws.com
```

### Disable Organizations trusted access for S3 Storage Lens<a name="OrganizationsDisableTrustedAccessS3LensCLI"></a>

**Example Disable Organizations trusted access for S3 Storage Lens**  

```
aws organizations disable-aws-service-access --service-principal storage-lens.s3.amazonaws.com
```

### Register Organizations delegated administrators for S3 Storage Lens<a name="OrganizationsRegisterDelegatedAdministratorS3LensCLI"></a>

**Example Register Organizations delegated administrators for S3 Storage Lens**  

```
aws organizations register-delegated-administrator --service-principal storage-lens.s3.amazonaws.com —account-id 123456789012
```

### De\-register Organizations delegated administrators for S3 Storage Lens<a name="OrganizationsDeregisterDelegatedAdministratorS3LensCLI"></a>

**Example De\-register Organizations delegated administrators for S3 Storage Lens**  

```
aws organizations deregister-delegated-administrator --service-principal storage-lens.s3.amazonaws.com —account-id 123456789012
```


# Configuring an index document<a name="IndexDocumentSupport"></a>

When you enable website hosting, you must also configure and upload an index document\. An *index document* is a webpage that Amazon S3 returns when a request is made to the root of a website or any subfolder\. For example, if a user enters `http://www.example.com` in the browser, the user is not requesting any specific page\. In that case, Amazon S3 serves up the index document, which is sometimes referred to as the *default page*\.

When you enable static website hosting for your bucket, you enter the name of the index document \(for example, `index.html`\)\. After you enable static website hosting for your bucket, you upload an HTML file with the index document name to your bucket\. 

The trailing slash at the root\-level URL is optional\. For example, if you configure your website with `index.html` as the index document, either of the following URLs returns `index.html`\.

```
1. http://example-bucket.s3-website.Region.amazonaws.com/
2. http://example-bucket.s3-website.Region.amazonaws.com
```

For more information about Amazon S3 website endpoints, see [Website endpoints](WebsiteEndpoints.md)\.

## Index document and folders<a name="IndexDocumentsandFolders"></a>

In Amazon S3, a bucket is a flat container of objects\. It does not provide any hierarchical organization as the file system on your computer does\. However, you can create a logical hierarchy by using object key names that imply a folder structure\. 

For example, consider a bucket with three objects that have the following key names\. Although these are stored with no physical hierarchical organization, you can infer the following logical folder structure from the key names:
+ `sample1.jpg` — Object is at the root of the bucket\.
+ `photos/2006/Jan/sample2.jpg` — Object is in the `photos/2006/Jan` subfolder\.
+ `photos/2006/Feb/sample3.jpg` — Object is in the `photos/2006/Feb` subfolder\. 

In the Amazon S3 console, you can also create a folder in a bucket\. For example, you can create a folder named `photos`\. You can upload objects to the bucket or to the `photos` folder within the bucket\. If you add the object `sample.jpg` to the bucket, the key name is `sample.jpg`\. If you upload the object to the `photos` folder, the object key name is `photos/sample.jpg`\.

If you create a folder structure in your bucket, you must have an index document at each level\. In each folder, the index document must have the same name, for example, `index.html`\. When a user specifies a URL that resembles a folder lookup, the presence or absence of a trailing slash determines the behavior of the website\. For example, the following URL, with a trailing slash, returns the `photos/index.html` index document\. 

```
1. http://bucket-name.s3-website.Region.amazonaws.com/photos/
```

However, if you exclude the trailing slash from the preceding URL, Amazon S3 first looks for an object `photos` in the bucket\. If the `photos` object is not found, it searches for an index document, `photos/index.html`\. If that document is found, Amazon S3 returns a `302 Found` message and points to the `photos/` key\. For subsequent requests to `photos/`, Amazon S3 returns `photos/index.html`\. If the index document is not found, Amazon S3 returns an error\.

## Configure an index document<a name="configuring-index-document"></a>

To configure an index document using the S3 console, follow the steps below\. You can also configure an index document using the REST API, the AWS SDKs, the AWS CLI, or AWS CloudFormation\. For more information, see the following:
+ [Configuring a static website using the REST API and AWS SDKs](https://docs.aws.amazon.com/AmazonS3/latest/dev/ManagingBucketWebsiteConfig.html)
+ [AWS::S3::Bucket WebsiteConfiguration](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-websiteconfiguration.html) in the *AWS CloudFormation User Guide*
+ [put\-bucket\-website](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/put-bucket-website.html) in the *AWS CLI Command Reference*

When you enable static website hosting for your bucket, you enter the name of the index document \(for example, **index\.html**\)\. After you enable static website hosting for the bucket, you upload an HTML file with this index document name to your bucket\.

**To configure the index document**

1. Create an `index.html` file\.

   If you don't have an `index.html` file, you can use the following HTML to create one:

   ```
   <html xmlns="http://www.w3.org/1999/xhtml" >
   <head>
       <title>My Website Home Page</title>
   </head>
   <body>
     <h1>Welcome to my website</h1>
     <p>Now hosted on Amazon S3!</p>
   </body>
   </html>
   ```

1. Save the index file locally with the *exact* index document name that you entered when you enabled static website hosting for your bucket \(for example, `index.html`\)\.

   The index document file name must exactly match the index document name that you enter in the **Static website hosting** dialog box\. The index document name is case sensitive\. For example, if you enter `index.html` for the **Index document** name in the **Static website hosting** dialog box, your index document file name must also be `index.html` and not `Index.html`\.

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose the name of the bucket that you want to use to host a static website\.

1. Enable static website hosting for your bucket, and enter the exact name of your index document \(for example, `index.html`\)\. For more information, see [Enabling website hosting](EnableWebsiteHosting.md)\.

   After enabling static website hosting, proceed to step 6\. 

1. To upload the index document to your bucket, do one of the following:
   + Drag and drop the index file into the console bucket listing\.
   + Choose **Upload**, and follow the prompts to choose and upload the index file\.

   For step\-by\-step instructions, see [How Do I Upload Files and Folders to an Amazon S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html) in the *Amazon Simple Storage Service Console User Guide*\.

1. \(Optional\) Upload other website content to your bucket\.

Next, you must set permissions for website access\. For more information, see [Setting permissions for website access](WebsiteAccessPermissionsReqd.md)\. You can also optionally configure an [error document](CustomErrorDocSupport.md), [web traffic logging](LoggingWebsiteTraffic.md), or a [redirect](how-to-page-redirect.md)\.


# Error response<a name="ErrorResponse"></a>

**Topics**
+ [Error code](ErrorCode.md)
+ [Error message](ErrorMessage.md)
+ [Further details](ErrorDetails.md)

When an Amazon S3 request is in error, the client receives an error response\. The exact format of the error response is API specific: For example, the REST error response differs from the SOAP error response\. However, all error responses have common elements\.

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 


# Making requests using AWS account or IAM user temporary credentials \- AWS SDK for PHP<a name="AuthUsingTempSessionTokenPHP"></a>

This topic guides explains how to use classes from version 3 of the AWS SDK for PHP to request temporary security credentials and use them to access Amazon S3\. It assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\. 

An IAM user or an AWS account can request temporary security credentials using version 3 of the AWS SDK for PHP\. It can then use the temporary credentials to access Amazon S3\. The credentials expire when the session duration expires\. 

By default, the session duration is one hour\. If you use IAM user credentials, you can specify the duration when requesting the temporary security credentials from 15 minutes to the maximum session duration for the role\. For more information about temporary security credentials, see [Temporary Security Credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html) in the *IAM User Guide*\. For more information about making requests, see [Making requests](MakingRequests.md)\.

**Note**  
If you obtain temporary security credentials using your AWS account security credentials, the temporary security credentials are valid for only one hour\. You can specify the session duration only if you use IAM user credentials to request a session\.

**Example**  
The following PHP example lists object keys in the specified bucket using temporary security credentials\. The example obtains temporary security credentials for a default one\-hour session, and uses them to send authenticated request to Amazon S3\. For information about running the PHP examples in this guide, see [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\.  
If you want to test the example using IAM user credentials, you need to create an IAM user under your AWS account\. For information about how to create an IAM user, see [Creating Your First IAM User and Administrators Group](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html) in the *IAM User Guide*\. For an example of setting the session duration when using IAM user credentials to request a session, see [Making requests using federated user temporary credentials \- AWS SDK for PHP](AuthUsingTempFederationTokenPHP.md)\.   

```
 require 'vendor/autoload.php';

use Aws\Sts\StsClient;
use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';

$sts = new StsClient([
    'version' => 'latest',
    'region' => 'us-east-1'
]);
    
$sessionToken = $sts->getSessionToken();

$s3 = new S3Client([
    'region' => 'us-east-1',
    'version' => 'latest',
    'credentials' => [
        'key'    => $sessionToken['Credentials']['AccessKeyId'],
        'secret' => $sessionToken['Credentials']['SecretAccessKey'],
        'token'  => $sessionToken['Credentials']['SessionToken']
    ]
]);

$result = $s3->listBuckets();


try {
    // Retrieve a paginator for listing objects.
    $objects = $s3->getPaginator('ListObjects', [
        'Bucket' => $bucket
    ]);
    
    echo "Keys retrieved!" . PHP_EOL;
    
    // List objects
    foreach ($objects as $object) {
        echo $object['Key'] . PHP_EOL;
    }
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}
```

## Related resources<a name="RelatedResources-AuthUsingTempSessionTokenPHP"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Configuring your bucket to use an S3 Bucket Key with SSE\-KMS for new objects<a name="configuring-bucket-key"></a>

When you configure server\-side encryption using SSE\-KMS, you can configure your bucket to use an S3 Bucket Key for SSE\-KMS on new objects\. S3 Bucket Keys decrease the request traffic from Amazon S3 to AWS Key Management Service \(AWS KMS\) and reduce the cost of SSE\-KMS\. For more information, see [Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys](bucket-key.md)\.

You can configure your bucket to use an S3 Bucket Key for SSE\-KMS on new objects by using the Amazon S3 console, REST API, AWS SDK, AWS CLI, or AWS CloudFormation\. If you want to enable or disable an S3 Bucket Key for existing objects, you can use a COPY operation\. For more information, see [Configuring an S3 Bucket Key at the object level using the REST API, AWS SDKs, or AWS CLI](configuring-bucket-key-object.md)\. 

**Prerequisite:**  
Before you configure your bucket to use an S3 Bucket Key, review [Changes to note before enabling an S3 Bucket Key](bucket-key.md#bucket-key-changes)\.

**Topics**
+ [Using the Amazon S3 console](#enable-bucket-key-console)
+ [Using the REST API](#enable-bucket-key-rest)
+ [Using the AWS SDKs](#enable-bucket-key-sdk)
+ [Using the CLI](#enable-bucket-key-cli)
+ [Using AWS CloudFormation](#enable-bucket-key-cloudformation)

## Using the Amazon S3 console<a name="enable-bucket-key-console"></a>

You can use the Amazon S3 console to enable an S3 Bucket Key for a new or existing bucket\. For more information, see [Configuring an S3 Bucket Key in the S3 console](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/s3-bucket-key.html)\.

## Using the REST API<a name="enable-bucket-key-rest"></a>

You can use [PutBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketEncryption.html) to enable or disable an S3 Bucket Key for your bucket\. To configure an S3 Bucket Key with `PutBucketEncryption`, specify the [ServerSideEncryptionRule](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ServerSideEncryptionRule.html), which includes default encryption with server\-side encryption using AWS KMS customer master keys \(CMKs\)\. You can also optionally use a customer managed CMK by specifying the KMS key ID for the CMK\.  

For more information and example syntax, see [PutBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketEncryption.html)\. 

## Using the AWS SDKs<a name="enable-bucket-key-sdk"></a>

The following example enables default bucket encryption with SSE\-KMS and an S3 Bucket Key using the AWS SDK for Java\.

------
#### [ Java ]

```
AmazonS3 s3client = AmazonS3ClientBuilder.standard()
    .withRegion(Regions.DEFAULT_REGION)
    .build();
    
ServerSideEncryptionByDefault serverSideEncryptionByDefault = new ServerSideEncryptionByDefault()
    .withSSEAlgorithm(SSEAlgorithm.KMS);
ServerSideEncryptionRule rule = new ServerSideEncryptionRule()
    .withApplyServerSideEncryptionByDefault(serverSideEncryptionByDefault)
    .withBucketKeyEnabled(true);
ServerSideEncryptionConfiguration serverSideEncryptionConfiguration =
    new ServerSideEncryptionConfiguration().withRules(Collections.singleton(rule));

SetBucketEncryptionRequest setBucketEncryptionRequest = new SetBucketEncryptionRequest()
    .withServerSideEncryptionConfiguration(serverSideEncryptionConfiguration)
    .withBucketName(bucketName);
            
s3client.setBucketEncryption(setBucketEncryptionRequest);
```

------

## Using the CLI<a name="enable-bucket-key-cli"></a>

The following example enables default bucket encryption with SSE\-KMS and an S3 Bucket Key using the AWS CLI\.

```
aws s3api put-bucket-encryption --bucket <bucket-name> --server-side-encryption-configuration '{
        "Rules": [
            {
                "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "aws:kms",
                    "KMSMasterKeyID": "<KMS-Key-ARN>"
                },
                "BucketKeyEnabled": true
            }
        ]
    }'
```

## Using AWS CloudFormation<a name="enable-bucket-key-cloudformation"></a>

When you use the [AWS::S3::Bucket](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html) resource to create a new bucket and specify default encryption with SSE\-KMS, you can configure your bucket to use an S3 Bucket Key for new objects in the bucket\. 

You configure the `BucketKeyEnabled` property as part of the [AWS::S3::Bucket ServerSideEncryptionRule](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-serversideencryptionrule.html), which specifies the default server\-side encryption configuration for a bucket\.

**BucketKeyEnabled**  
Specifies whether Amazon S3 should use an S3 Bucket Key with server\-side encryption using AWS KMS \(SSE\-KMS\) for new objects in the bucket\. Existing objects are not affected\. Setting the `BucketKeyEnabled` element to `TRUE` causes Amazon S3 to use an S3 Bucket Key\. By default, it is not enabled\. For more information, see [Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys](bucket-key.md)\.  

**Required: **No

**Type:** [ServerSideEncryptionByDefault](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket-serversideencryptionbydefault.html) 

**Update requires:** [No interruption](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-update-behaviors.html#update-no-interrupt)

**Example \- Create a bucket that uses an S3 Bucket Key with SSE\-KMS default encryption**  

```
{
  "AWSTemplateFormatVersion": "2010-09-09",
  "Description": "S3 bucket with default encryption",
  "Resources": {
    "EncryptedS3Bucket": {
      "Type": "AWS::S3::Bucket",
      "Properties": {
        "BucketName": {
          "Fn::Sub": "encryptedbucket-${AWS::Region}-${AWS::AccountId}"
        },
        "BucketEncryption": {
          "ServerSideEncryptionConfiguration": [
            {
              "ServerSideEncryptionByDefault": {
                "SSEAlgorithm": "aws:kms",
                "KMSMasterKeyID": "KMS-KEY-ARN"
              },
              "BucketKeyEnabled": true
            }
          ]
        }
      },
      "DeletionPolicy": "Delete"
    }
  }
}
```

```
AWSTemplateFormatVersion: 2010-09-09
Description: S3 bucket with default encryption
Resources:
  EncryptedS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'encryptedbucket-${AWS::Region}-${AWS::AccountId}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: aws:kms
              KMSMasterKeyID: KMS-KEY-ARN
            BucketKeyEnabled: true
    DeletionPolicy: Delete
```


# Appendices<a name="Appendices"></a>

This Amazon Simple Storage Service Developer Guide appendix include the following sections\.

**Topics**
+ [Appendix a: Using the SOAP API](SOAPAPI3.md)
+ [Appendix b: Authenticating requests \(AWS signature version 2\)](auth-request-sig-v2.md)


# Operations<a name="batch-ops-operations"></a>

S3 Batch Operations supports several different operations\. The topics in this section describe each of these operations\.

**Topics**
+ [Put object copy](batch-ops-copy-object.md)
+ [Initiate restore object](batch-ops-initiate-restore-object.md)
+ [Invoking a Lambda function from Amazon S3 batch operations](batch-ops-invoke-lambda.md)
+ [Put object ACL](batch-ops-put-object-acl.md)
+ [Put object tagging](batch-ops-put-object-tagging.md)
+ [Managing S3 Object Lock retention dates](batch-ops-retention-date.md)
+ [Managing S3 Object Lock legal hold](batch-ops-legal-hold.md)


# Assessing your storage activity and usage with Amazon S3 Storage Lens<a name="storage_lens"></a>

Amazon S3 Storage Lens aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format\. You can use the dashboard to visualize insights and trends, flag outliers, and receive recommendations for optimizing storage costs and applying data protection best practices\. You can use S3 Storage Lens through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\.

Amazon S3 Storage Lens provides a single view of usage and activity across your Amazon S3 storage\. With drill\-down options to generate insights at the organization, account, bucket, object, or even prefix level\. S3 Storage Lens analyzes storage metrics to deliver contextual recommendations to help optimize storage costs and apply best practices on data protection\. 

On the [S3 console](https://console.aws.amazon.com/s3), S3 Storage Lens provides an interactive *default dashboard* that is updated daily\. Other dashboards can be scoped by account \(for AWS Organizations users\), AWS Regions, and S3 buckets to provide [usage metrics](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_basics_metrics_recommendations.html#storage_lens_basics_metrics_types) for free\. For an additional charge, you can upgrade to receive *advanced metrics and recommendations*\. These include usage metrics with prefix\-level aggregation, activity metrics aggregated by bucket, and contextual recommendations \(available only in the dashboard\)\. S3 Storage Lens can be used to get a summary of storage insights, detect outliers, enhance data protection, and optimize storage costs\. For more information, see [Using S3 Storage Lens in the console](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/storage_lens_console.html)\. For more information about S3 Storage Lens pricing, see [Amazon S3 pricing](http://aws.amazon.com/s3/pricing)\.

In addition to the dashboard in the S3 console, you can export metrics in CSV or Apache Parquet format to an S3 bucket of your choice for further analysis\. For information, see [Viewing Amazon S3 Storage Lens metrics using a data export](storage_lens_view_metrics_export.md)\.

Use S3 Storage Lens to generate summary insights, such as finding out how much storage you have across your entire organization, or what are the fastest growing buckets and prefixes\. Identify outliers in your storage metrics, and then drill down to further investigate the source of the spike in usage or activity\. 

You can assess your storage based on data protection best practices in Amazon S3, such as analyzing the percentage of your buckets that have encryption or object lock enabled\. And you can identify potential cost savings opportunities, such as by analyzing your request activity per bucket to find buckets where objects could be transitioned to a lower\-cost storage class\. 

**Topics**
+ [Understanding Amazon S3 Storage Lens](storage_lens_basics_metrics_recommendations.md)
+ [Using Amazon S3 Storage Lens with AWS Organizations](storage_lens_with_organizations.md)
+ [Setting permissions to use Amazon S3 Storage Lens](storage_lens_iam_permissions.md)
+ [Viewing storage usage and activity metrics with Amazon S3 Storage Lens](storage_lens_view_metrics.md)
+ [Amazon S3 Storage Lens metrics glossary](storage_lens_metrics_glossary.md)
+ [Amazon S3 Storage Lens examples and console walk\-through](S3LensExamples.md)



# Transitioning object versions<a name="transitioning-object-versions"></a>

You can define lifecycle configuration rules for objects that have a well\-defined lifecycle to transition object versions to the `S3 Glacier` storage class at a specific time in the object's lifetime\. For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.


# Internetwork traffic privacy<a name="inter-network-traffic-privacy"></a>

This topic describes how Amazon S3 secures connections from the service to other locations\.

## Traffic between service and on\-premises clients and applications<a name="inter-network-traffic-privacy-on-prem"></a>

You have two connectivity options between your private network and AWS: 
+ An AWS Site\-to\-Site VPN connection\. For more information, see [What is AWS Site\-to\-Site VPN?](https://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html)
+ An AWS Direct Connect connection\. For more information, see [What is AWS Direct Connect?](https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html)

Access to Amazon S3 via the network is through AWS published APIs\. Clients must support Transport Layer Security \(TLS\) 1\.0\. We recommend TLS 1\.2 or above\. Clients must also support cipher suites with Perfect Forward Secrecy \(PFS\), such as Ephemeral Diffie\-Hellman \(DHE\) or Elliptic Curve Diffie\-Hellman Ephemeral \(ECDHE\)\. Most modern systems such as Java 7 and later support these modes\. Additionally, you must sign requests using an access key ID and a secret access key that are associated with an IAM principal, or you can use the [AWS Security Token Service \(STS\)](https://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html) to generate temporary security credentials to sign requests\.

## Traffic between AWS resources in the same Region<a name="inter-network-traffic-privacy-within-region"></a>

An virtual private cloud \(VPC\) \(VPC\) endpoint for Amazon S3 is a logical entity within a VPC that allows connectivity only to Amazon S3\. The VPC routes requests to Amazon S3 and routes responses back to the VPC\. For more information, see [VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html) in the *VPC User Guide*\. For example bucket policies that you can use to control S3 bucket access from VPC endpoints, see [Example Bucket Policies for VPC Endpoints for Amazon S3](example-bucket-policies-vpc-endpoint.md)\. 


# Deleting multiple objects using the REST API<a name="DeletingMultipleObjectsUsingREST"></a>

You can use the AWS SDKs to delete multiple objects using the Multi\-Object Delete API\. However, if your application requires it, you can send REST requests directly\. For more information, go to [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html) in the *Amazon Simple Storage Service API Reference*\. 


# \(Optional\) Logging web traffic<a name="LoggingWebsiteTraffic"></a>

You can optionally enable Amazon S3 server access logging for a bucket that is configured as a static website\. Server access logging provides detailed records for the requests that are made to your bucket\. For more information, see [Amazon S3 server access logging](ServerLogs.md)\. If you plan to use Amazon CloudFront to [speed up your website](website-hosting-cloudfront-walkthrough.md), you can also use CloudFront logging\. For more information, see [Configuring and Using Access Logs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html) in the *Amazon CloudFront Developer Guide*\.

**To enable server access logging for your static website bucket**

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the same Region where you created the bucket that is configured as a static website, create a bucket for logging, for example `logs.example.com`\.

1. Create a folder for the server access logging log files \(for example, `logs`\)\.

   When you group your log data files in a folder, they are easier to locate\.

1. \(Optional\) If you want to use CloudFront to improve your website performance, create a folder for the CloudFront log files \(for example, `cdn`\)\.

1. In the **Buckets** list, choose your bucket\.

1. Choose **Properties**\.

1. Under **Server access logging**, choose **Edit**\.

1. Choose **Enable**\.

1. Under the **Target bucket**, choose the bucket and folder destination for the server access logs:
   + Browse to the folder and bucket location:

     1. Choose **Browse S3**\.

     1. Choose the bucket name, and then choose the logs folder\. 

     1. Choose **Choose path**\.
   + Enter the S3 bucket path, for example, **s3://logs\.example\.com/logs/**\.

1. Choose **Save changes**\.

   In your log bucket, you can now access your logs\. Amazon S3 writes website access logs to your log bucket every 2 hours\.


# Configuring a static website using a custom domain registered with Route 53<a name="website-hosting-custom-domain-walkthrough"></a>

Suppose that you want to host your static website on Amazon S3\. You've registered a domain with Amazon Route 53 \(for example, `example.com`\), and you want requests for `http://www.example.com` and `http://example.com` to be served from your Amazon S3 content\. You can use this walkthrough to learn how to host a static website and create redirects on Amazon S3 for a website with a custom domain name that is registered with Route 53\. You can work with an existing website that you want to host on Amazon S3, or you use this walkthrough to start from scratch\. 

Once you complete this walkthrough, you can optionally use Amazon CloudFront to improve the performance of your website\. For more information, see [Speeding up your website with Amazon CloudFront](website-hosting-cloudfront-walkthrough.md)\.

**Note**  
Amazon S3 does not support HTTPS access to the website\. If you want to use HTTPS, you can use Amazon CloudFront to serve a static website hosted on Amazon S3\.  
For more information, see [How do I use CloudFront to serve a static website hosted on Amazon S3?](http://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/) and [Requiring HTTPS for communication between viewers and CloudFront](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html)\.

**Automating static website set up with an AWS CloudFormation template**  
You can use an AWS CloudFormation template to automate your static website setup\. The AWS CloudFormation template sets up the components that you need to host a secure static website so that you can focus more on your website’s content and less on configuring components\.

The AWS CloudFormation template includes the following components:
+ Amazon S3‐Creates an Amazon S3 bucket to host your static website\.
+ CloudFront‐ Creates a CloudFront distribution to speed up your static website\.
+ Lambda@Edge‐Uses [Lambda@Edge](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html) to add security headers to every server response\. Security headers are a group of headers in the web server response that tell web browsers to take extra security precautions\. For more information, refer to this blog post: [Adding HTTP security headers using Lambda@Edge and Amazon CloudFront](http://aws.amazon.com/blogs/networking-and-content-delivery/adding-http-security-headers-using-lambdaedge-and-amazon-cloudfront/)\.

This AWS CloudFormation template is available for you to download and use\. For information and instructions, see [Getting started with a secure static website](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/getting-started-secure-static-website-cloudformation-template.html) in the *Amazon CloudFront Developer Guide*\.

**Topics**
+ [Before you begin](#root-domain-walkthrough-before-you-begin)
+ [Step 1: Register a custom domain with Route 53](#website-hosting-custom-domain-walkthrough-domain-registry)
+ [Step 2: Create two buckets](#root-domain-walkthrough-create-buckets)
+ [Step 3: Configure your root domain bucket for website hosting](#root-domain-walkthrough-configure-bucket-aswebsite)
+ [Step 4: Configure your subdomain bucket for website redirect](#root-domain-walkthrough-configure-redirect)
+ [Step 5: Configure logging for website traffic](#root-domain-walkthrough-configure-logging)
+ [Step 6: Upload index and website content](#upload-website-content)
+ [Step 7: Edit S3 Block Public Access settings](#root-domain-walkthrough-configure-bucket-permissions)
+ [Step 8: Attach a bucket policy](#add-bucket-policy-root-domain)
+ [Step 9: Test your domain endpoint](#root-domain-walkthrough-test-website)
+ [Step 10: Add alias records for your domain and subdomain](#root-domain-walkthrough-add-record-to-hostedzone)
+ [Step 11: Test the website](#root-domain-testing)
+ [Speeding up your website with Amazon CloudFront](website-hosting-cloudfront-walkthrough.md)
+ [Cleaning up your example resources](getting-started-cleanup.md)

## Before you begin<a name="root-domain-walkthrough-before-you-begin"></a>

As you follow the steps in this example, you work with the following services:

**Amazon Route 53 –** You use Route 53 to register domains and to define where you want to route internet traffic for your domain\. The example shows how to create Route 53 alias records that route traffic for your domain \(`example.com`\) and subdomain \(`www.example.com`\) to an Amazon S3 bucket that contains an HTML file\.

**Amazon S3 –** You use Amazon S3 to create buckets, upload a sample website page, configure permissions so that everyone can see the content, and then configure the buckets for website hosting\.

## Step 1: Register a custom domain with Route 53<a name="website-hosting-custom-domain-walkthrough-domain-registry"></a>

If you don't already have a registered domain name, such as `example.com`, register one with Route 53\. For more information, see [Registering a New Domain](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-register.html) in the *Amazon Route 53 Developer Guide*\. After you register your domain name, you can create and configure your Amazon S3 buckets for website hosting\. 

## Step 2: Create two buckets<a name="root-domain-walkthrough-create-buckets"></a>

To support requests from both the root domain and subdomain, you create two buckets\.
+ **Domain bucket** ‐ `example.com`
+ **Subdomain bucket** ‐ `www.example.com` 

These bucket names must exactly match your domain name\. In this example, the domain name is `example.com`\. You host your content out of the root domain bucket \(`example.com`\)\. You create a redirect request for the subdomain bucket \(`www.example.com`\)\. If someone enters `www.example.com` in their browser, they are redirected to `example.com` and see the content that is hosted in the Amazon S3 bucket with that name\. 

**To create your buckets for website hosting**

The following instructions provide an overview of how to create your buckets for website hosting\. For detailed, step\-by\-step instructions on creating a bucket, see [How Do I Create an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\.

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Create your root domain bucket: 

   1. Choose **Create bucket**\.

   1. Enter the **Bucket name** \(for example, `example.com`\)\.

   1. Choose the Region where you want to create the bucket\. 

      Choose a Region close to you to minimize latency and costs, or to address regulatory requirements\. The Region that you choose determines your Amazon S3 website endpoint\. For more information, see [Website endpoints](WebsiteEndpoints.md)\.

   1. To accept the default settings and create the bucket, choose **Create**\.

1. Create your subdomain bucket: 

   1. Choose **Create bucket**\.

   1. Enter the **Bucket name** \(for example, `www.example.com`\)\.

   1. Choose the Region where you want to create the bucket\. 

      Choose a Region close to you to minimize latency and costs, or to address regulatory requirements\. The Region that you choose determines your Amazon S3 website endpoint\. For more information, see [Website endpoints](WebsiteEndpoints.md)\.

   1. To accept the default settings and create the bucket, choose **Create**\.

In the next step, you configure `example.com` for website hosting\. 

## Step 3: Configure your root domain bucket for website hosting<a name="root-domain-walkthrough-configure-bucket-aswebsite"></a>

In this step, you configure your root domain bucket \(`example.com`\) as a website\. This bucket will contain your website content\. When you configure a bucket for website hosting, you can access the website using the [Website endpoints](WebsiteEndpoints.md)\. 

**To enable static website hosting**

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose the name of the bucket that you want to enable static website hosting for\.

1. Choose **Properties**\.

1. Under **Static website hosting**, choose **Edit**\.

1. Choose **Use this bucket to host a website**\. 

1. Under **Static website hosting**, choose **Enable**\.

1. In **Index document**, enter the file name of the index document, typically `index.html`\. 

   The index document name is case sensitive and must exactly match the file name of the HTML index document that you plan to upload to your S3 bucket\. When you configure a bucket for website hosting, you must specify an index document\. Amazon S3 returns this index document when requests are made to the root domain or any of the subfolders\. For more information, see [Configuring an index document](https://docs.aws.amazon.com/AmazonS3/latest/dev/IndexDocumentSupport.html)\.

1. \(Optional\) If you want to provide your own custom error document for 4XX class errors, in **Error document**, enter the custom error document file name\. 

   The error document name is case sensitive and must exactly match the file name of the HTML error document that you plan to upload to your S3 bucket\. If you don't specify a custom error document and an error occurs, Amazon S3 returns a default HTML error document\. For more information, see [Configuring a custom error document](https://docs.aws.amazon.com/AmazonS3/latest/dev/CustomErrorDocSupport.html)\.

1. \(Optional\) If you want to specify advanced redirection rules, in **Redirection rules**, enter XML to describe the rules\.

   For example, you can conditionally route requests according to specific object key names or prefixes in the request\. For more information, see [Configuring advanced conditional redirects](https://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html#advanced-conditional-redirects)\.

1. Choose **Save changes**\.

   Amazon S3 enables static website hosting for your bucket\. At the bottom of the page, under **Static website hosting**, you see the website endpoint for your bucket\.

1. Under **Static website hosting**, note the **Endpoint**\.

   The **Endpoint** is the Amazon S3 website endpoint for your bucket\. After you finish configuring your bucket as a static website, you can use this endpoint to test your website\.

In the next step, you configure your subdomain \(`www.example.com`\) to redirect requests to your domain \(`example.com`\)\. 

## Step 4: Configure your subdomain bucket for website redirect<a name="root-domain-walkthrough-configure-redirect"></a>

After you configure your root domain bucket for website hosting, you can configure your subdomain bucket to redirect all requests to the domain\. In this example, all requests for `www.example.com` are redirected to `example.com`\.

**To configure a redirect request**

1. On the Amazon S3 console, in the **Buckets** list, choose your subdomain bucket name \(`www.example.com` in this example\)\.

1. Choose **Properties**\.

1. Under **Static website hosting**, choose **Edit**\.

1. Choose **Redirect requests for an object**\. 

1. In the **Target bucket** box, enter your root domain, for example, **example\.com**\.

1. For **Protocol**, choose **http**\.

1. Choose **Save changes**\.

## Step 5: Configure logging for website traffic<a name="root-domain-walkthrough-configure-logging"></a>

If you want to track the number of visitors accessing your website, you can optionally enable logging for your root domain bucket\. For more information, see [Amazon S3 Server Access Logging](https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html)\. If you plan to use Amazon CloudFront to speed up your website, you can also use CloudFront logging\.

**To enable server access logging for your root domain bucket**

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the same Region where you created the bucket that is configured as a static website, create a bucket for logging, for example `logs.example.com`\.

1. Create a folder for the server access logging log files \(for example, `logs`\)\.

1. \(Optional\) If you want to use CloudFront to improve your website performance, create a folder for the CloudFront log files \(for example, `cdn`\)\.

1. In the **Buckets** list, choose your root domain bucket\.

1. Choose **Properties**\.

1. Under **Server access logging**, choose **Edit**\.

1. Choose **Enable**\.

1. Under the **Target bucket**, choose the bucket and folder destination for the server access logs:
   + Browse to the folder and bucket location:

     1. Choose **Browse S3**\.

     1. Choose the bucket name, and then choose the logs folder\. 

     1. Choose **Choose path**\.
   + Enter the S3 bucket path, for example, `s3://logs.example.com/logs/`\.

1. Choose **Save changes**\.

   In your log bucket, you can now access your logs\. Amazon S3 writes website access logs to your log bucket every 2 hours\.

## Step 6: Upload index and website content<a name="upload-website-content"></a>

In this step, you upload your index document and optional website content to your root domain bucket\. 

When you enable static website hosting for your bucket, you enter the name of the index document \(for example, **index\.html**\)\. After you enable static website hosting for the bucket, you upload an HTML file with this index document name to your bucket\.

**To configure the index document**

1. Create an `index.html` file\.

   If you don't have an `index.html` file, you can use the following HTML to create one:

   ```
   <html xmlns="http://www.w3.org/1999/xhtml" >
   <head>
       <title>My Website Home Page</title>
   </head>
   <body>
     <h1>Welcome to my website</h1>
     <p>Now hosted on Amazon S3!</p>
   </body>
   </html>
   ```

1. Save the index file locally with the *exact* index document name that you entered when you enabled static website hosting for your bucket \(for example, `index.html`\)\.

   The index document file name must exactly match the index document name that you enter in the **Static website hosting** dialog box\. The index document name is case sensitive\. For example, if you enter `index.html` for the **Index document** name in the **Static website hosting** dialog box, your index document file name must also be `index.html` and not `Index.html`\.

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose the name of the bucket that you want to use to host a static website\.

1. Enable static website hosting for your bucket, and enter the exact name of your index document \(for example, `index.html`\)\. For more information, see [Enabling website hosting](EnableWebsiteHosting.md)\.

   After enabling static website hosting, proceed to step 6\. 

1. To upload the index document to your bucket, do one of the following:
   + Drag and drop the index file into the console bucket listing\.
   + Choose **Upload**, and follow the prompts to choose and upload the index file\.

   For step\-by\-step instructions, see [How Do I Upload Files and Folders to an Amazon S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/upload-objects.html) in the *Amazon Simple Storage Service Console User Guide*\.

1. \(Optional\) Upload other website content to your bucket\.

## Step 7: Edit S3 Block Public Access settings<a name="root-domain-walkthrough-configure-bucket-permissions"></a>

In this example, you edit block public access settings for the domain bucket \(`example.com`\) to allow public access\.

By default, Amazon S3 blocks public access to your account and buckets\. If you want to use a bucket to host a static website, you can use these steps to edit your block public access settings\. 

**Warning**  
Before you complete this step, review [Using Amazon S3 Block Public Access](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) to ensure that you understand and accept the risks involved with allowing public access\. When you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket\. We recommend that you block all public access to your buckets\.

1. Open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Choose the name of the bucket that you have configured as a static website\.

1. Choose **Permissions**\.

1. Under **Block public access \(bucket settings\)**, choose **Edit**\.

1. Clear **Block *all* public access**, and choose **Save changes**\.
**Warning**  
Before you complete this step, review [Using Amazon S3 Block Public Access](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) to ensure you understand and accept the risks involved with allowing public access\. When you turn off block public access settings to make your bucket public, anyone on the internet can access your bucket\. We recommend that you block all public access to your buckets\.  
![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/edit-public-access-clear.png)

   Amazon S3 turns off Block Public Access settings for your bucket\. To create a public, static website, you might also have to [edit the Block Public Access settings](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access-account.html) for your account before adding a bucket policy\. If account settings for Block Public Access are currently turned on, you see a note under **Block public access \(bucket settings\)**\.

## Step 8: Attach a bucket policy<a name="add-bucket-policy-root-domain"></a>

After you edit S3 Block Public Access settings, you can add a bucket policy to grant public read access to your bucket\. When you grant public read access, anyone on the internet can access your bucket\.

**Important**  
The following policy is an example only and allows full access to the contents of your bucket\. Before you proceed with this step, review [How can I secure the files in my Amazon S3 bucket?](https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/) to ensure that you understand the best practices for securing the files in your S3 bucket and risks involved in granting public access\.

1. Under **Buckets**, choose the name of your bucket\.

1. Choose **Permissions**\.

1. Under **Bucket Policy**, choose **Edit**\.

1. To grant public read access for your website, copy the following bucket policy, and paste it in the **Bucket policy editor**\.

   ```
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Sid": "PublicReadGetObject",
               "Effect": "Allow",
               "Principal": "*",
               "Action": [
                   "s3:GetObject"
               ],
               "Resource": [
                   "arn:aws:s3:::example.com/*"
               ]
           }
       ]
   }
   ```

1. Update the `Resource` to your bucket name\.

   In the preceding example bucket policy, *example\.com* is the bucket name\. To use this bucket policy with your own bucket, you must update this name to match your bucket name\.

1. Choose **Save changes**\.

   A message appears indicating that the bucket policy has been successfully added\.

   If you see an error that says `Policy has invalid resource`, confirm that the bucket name in the bucket policy matches your bucket name\. For information about adding a bucket policy, see [How do I add an S3 bucket policy?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html)

   If you get an error message and cannot save the bucket policy, check your account and bucket Block Public Access settings to confirm that you allow public access to the bucket\.

In the next step, you can figure out your website endpoints and test your domain endpoint\.

## Step 9: Test your domain endpoint<a name="root-domain-walkthrough-test-website"></a>

After you configure your domain bucket to host a public website, you can test your endpoint\. For more information, see [Website endpoints](WebsiteEndpoints.md)\. You will only be able to test the endpoint for your domain bucket since your subdomain bucket is set up for website redirect and not static website hosting\. 

**Note**  
Amazon S3 does not support HTTPS access to the website\. If you want to use HTTPS, you can use Amazon CloudFront to serve a static website hosted on Amazon S3\.  
For more information, see [How do I use CloudFront to serve a static website hosted on Amazon S3?](http://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/) and [Requiring HTTPS for communication between viewers and CloudFront](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html)\.

1. Under **Buckets**, choose the name of your bucket\.

1. Choose **Properties**\.

1. At the bottom of the page, under **Static website hosting**, choose your **Bucket website endpoint**\.

   Your index document opens in a separate browser window\.

In the next step, you use Amazon Route 53 to enable customers to use both of your custom URLs to navigate to your site\. 

## Step 10: Add alias records for your domain and subdomain<a name="root-domain-walkthrough-add-record-to-hostedzone"></a>

In this step, you create the alias records that you add to the hosted zone for your domain maps `example.com` and `www.example.com`\. Instead of using IP addresses, the alias records use the Amazon S3 website endpoints\. Amazon Route 53 maintains a mapping between the alias records and the IP addresses where the Amazon S3 buckets reside\. You create two alias records, one for your root domain and one for your subdomain\.

### Add an alias record for your root domain and subdomain<a name="add-alis-record"></a>

**To add an alias record for your root domain \(`example.com`\)**

1. Open the Route 53 console at [https://console\.aws\.amazon\.com/route53/](https://console.aws.amazon.com/route53/)\.
**Note**  
If you don't already use Route 53, see [Step 1: Register a Domain](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/getting-started.html#getting-started-find-domain-name) in the *Amazon Route 53 Developer Guide*\. After completing your setup, you can resume the instructions\.

1. Choose **Hosted zones**\.

1. In the list of hosted zones, choose the name of the hosted zone that matches your domain name\.

1. Choose **Create record**\.

1. Choose **Simple routing**, and choose **Next**\.

1. Choose **Define simple record**\.

1. In **Record name** accept the default value, which is the name of your hosted zone and your domain\.

1. In **Value/Route traffic to**, choose **Alias to S3 website endpoint**\.

1. Choose the Region\.

1. Choose the S3 bucket\.

   The bucket name should match the name that appears in the **Name** box\. In the **Alias Target** listing, the bucket name is followed by the Amazon S3 website endpoint for the Region where the bucket was created, for example `example.com (s3-website-us-west-2.amazonaws.com)`\. **Alias Target** lists a bucket if:
   + You configured the bucket as a static website\.
   + The bucket name is the same as the name of the record that you're creating\.
   + The current AWS account created the bucket\.

   If your bucket does not appear in the **Alias Target** listing, enter the Amazon S3 website endpoint for the Region where the bucket was created, for example, `s3-website-us-west-2.amazonaws.com`\. For a complete list of Amazon S3 website endpoints, see [Amazon S3 Website Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html#s3_website_region_endpoints)\. For more information about the alias target, see [Alias Target](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-alias.html#rrsets-values-alias-alias-target) in the *Amazon Route 53 Developer Guide*\.

1. In **Record type**, choose **A ‐ Routes traffic to an IPv4 address and some AWS resources**\.

1. For **Evaluate target health**, choose **No**\.

1. Choose **Define simple record**\.

**To add an alias record for your subdomain \(`www.example.com`\)**

1. In the hosted zone for your root domain \(`example.com`\), choose **Create record**\.

1. In **Record name** for your subdomain, type `www`\.

1. In **Value/Route traffic to**, choose **Alias to S3 website endpoint**\.

1. Choose the Region\.

1. Choose the S3 bucket, for example `example.com (s3-website-us-west-2.amazonaws.com)`\.

1. In **Record type**, choose **A ‐ Routes traffic to an IPv4 address and some AWS resources**\.

1. For **Evaluate target health**, choose **No**\.

1. Choose **Define simple record**\.

**Note**  
Changes generally propagate to all Route 53 servers within 60 seconds\. When propagation is done, you can route traffic to your Amazon S3 bucket by using the names of the alias records that you created in this procedure\.

### Add an alias record for your root domain and subdomain \(old Route 53 console\)<a name="add-alis-record-old"></a>

**To add an alias record for your root domain \(`example.com`\)**

The Route 53 console has been redesigned\. In the Route 53 console you can temporarily use the old console\. If you choose to work with the old Route 53 console, use the procedure below\.

1. Open the Route 53 console at [https://console\.aws\.amazon\.com/route53/](https://console.aws.amazon.com/route53/)\.
**Note**  
If you don't already use Route 53, see [Step 1: Register a Domain](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/getting-started.html#getting-started-find-domain-name) in the *Amazon Route 53 Developer Guide*\. After completing your setup, you can resume the instructions\.

1. Choose **Hosted Zones**\.

1. In the list of hosted zones, choose the name of the hosted zone that matches your domain name\.

1. Choose **Create Record Set**\.

1. Specify the following values:  
**Name**  
Accept the default value, which is the name of your hosted zone and your domain\.   
For the root domain, you don't need to enter any additional information in the **Name** field\.  
**Type**  
Choose **A – IPv4 address**\.  
**Alias**  
Choose **Yes**\.  
**Alias Target**  
In the **S3 website endpoints** section of the list, choose your bucket name\.   
The bucket name should match the name that appears in the **Name** box\. In the **Alias Target** listing, the bucket name is followed by the Amazon S3 website endpoint for the Region where the bucket was created, for example `example.com (s3-website-us-west-2.amazonaws.com)`\. **Alias Target** lists a bucket if:  
   + You configured the bucket as a static website\.
   + The bucket name is the same as the name of the record that you're creating\.
   + The current AWS account created the bucket\.
If your bucket does not appear in the **Alias Target** listing, enter the Amazon S3 website endpoint for the Region where the bucket was created, for example, `s3-website-us-west-2.amazonaws.com`\. For a complete list of Amazon S3 website endpoints, see [Amazon S3 Website Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html#s3_website_region_endpoints)\. For more information about the alias target, see [Alias Target](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-alias.html#rrsets-values-alias-alias-target) in the *Amazon Route 53 Developer Guide*\.  
**Routing Policy**  
Accept the default value of **Simple**\.  
**Evaluate Target Health**  
Accept the default value of **No**\.

1. Choose **Create**\.

**To add an alias record for your subdomain \(`www.example.com`\)**

1. In the hosted zone for your root domain \(`example.com`\), choose **Create Record Set**\.

1. Specify the following values:  
**Name**  
For the subdomain, enter `www` in the box\.   
**Type**  
Choose **A – IPv4 address**\.  
**Alias**  
Choose **Yes**\.  
**Alias Target**  
In the **S3 website endpoints** section of the list, choose the same bucket name that appears in the **Name** field—for example, `www.example.com (s3-website-us-west-2.amazonaws.com)`\.  
**Routing Policy**  
Accept the default value of **Simple**\.  
**Evaluate Target Health**  
Accept the default value of **No**\.

1. Choose **Create**\.

**Note**  
Changes generally propagate to all Route 53 servers within 60 seconds\. When propagation is done, you can route traffic to your Amazon S3 bucket by using the names of the alias records that you created in this procedure\.

## Step 11: Test the website<a name="root-domain-testing"></a>

Verify that the website and the redirect work correctly\. In your browser, enter your URLs\. In this example, you can try the following URLs:
+ **Domain** \(`http://example.com`\) – Displays the index document in the `example.com` bucket\.
+ **Subdomain **\(`http://www.example.com`\) – Redirects your request to `http://example.com`\. You see the index document in the `example.com` bucket\.

If your website or redirect links don't work, you can try the following:
+ **Clear cache**–Clear the cache of your web browser\.
+ **Check name servers**–If your web page and redirect links don't work after you've cleared your cache, you can compare the name servers for your domain and the name servers for your hosted zone\. If the name servers don't match, you might need to update your domain name servers to match those listed under your hosted zone\. For more information, see [Adding or changing name servers and glue records for a domain](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-name-servers-glue-records.html)\.

Once you've successfully tested your root domain and subdomain, you can set up an [Amazon CloudFront](http://aws.amazon.com/cloudfront) distribution to improve the performance of your website and provide logs that you can use to review website traffic\. For more information, see [Speeding up your website with Amazon CloudFront](website-hosting-cloudfront-walkthrough.md)\.


# Access points restrictions and limitations<a name="access-points-restrictions-limitations"></a>

Amazon S3 access points have the following restrictions and limitations: 
+ You can only create access points for buckets that you own\.
+ Each access point is associated with exactly one bucket, which you must specify when you create the access point\. After you create an access point, you can't associate it with a different bucket\. However, you can delete an access point and then create another one with the same name associated with a different bucket\.
+ After you create an access point, you can't change its virtual private cloud \(VPC\) configuration\.
+ Access point policies are limited to 20 KB in size\.
+ You can create a maximum of 1,000 access points per AWS account per Region\. If you need more than 1,000 access points for a single account in a single Region, you can request a service quota increase\. For more information about service quotas and requesting an increase, see [AWS Service Quotas](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) in the *AWS General Reference*\.
+ You can't use an access point as a destination for S3 Cross\-Region Replication\. For more information about replication, see [Replication](replication.md)\.
+ You can only address access points using virtual\-host\-style URLs\. For more information about virtual\-host\-style addressing, see [Accessing a Bucket](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro)\.
+ APIs that control access point functionality \(for example, `PutAccessPoint` and `GetAccessPointPolicy`\) don't support cross\-account calls\.
+ You must use AWS Signature Version 4 when making requests to an access point using the REST APIs\. For more information about authenticating requests, see [Authenticating Requests \(AWS Signature Version 4\)](https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html) in the *Amazon Simple Storage Service API Reference*\.
+ Access points only support access over HTTPS\.
+ Access points don't support anonymous access\.


# Amazon S3 Transfer Acceleration Examples<a name="transfer-acceleration-examples"></a>

This section provides examples of how to enable Amazon S3 Transfer Acceleration on a bucket and use the acceleration endpoint for the enabled bucket\. Some of the AWS SDK supported languages \(for example, Java and \.NET\) use an accelerate endpoint client configuration flag so you don't need to explicitly set the endpoint for Transfer Acceleration to *bucketname*\.s3\-accelerate\.amazonaws\.com\. For more information about Transfer Acceleration, see [Amazon S3 Transfer Acceleration](transfer-acceleration.md)\.

**Topics**
+ [Using the Amazon S3 Console](#transfer-acceleration-examples-console)
+ [Using Transfer Acceleration from the AWS Command Line Interface \(AWS CLI\)](#transfer-acceleration-examples-aws-cli)
+ [Using Transfer Acceleration from the AWS SDK for Java](#transfer-acceleration-examples-java)
+ [Using Transfer Acceleration from the AWS SDK for \.NET](#transfer-acceleration-examples-dotnet)
+ [Using Transfer Acceleration from the AWS SDK for JavaScript](#transfer-acceleration-examples-javascript)
+ [Using Transfer Acceleration from the AWS SDK for Python \(Boto\)](#transfer-acceleration-examples-python)
+ [Using Other AWS SDKs](#transfer-acceleration-examples-sdks)

## Using the Amazon S3 Console<a name="transfer-acceleration-examples-console"></a>

For information about enabling Transfer Acceleration on a bucket using the Amazon S3 console, see [Enabling Transfer Acceleration](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-transfer-acceleration.html) in the *Amazon Simple Storage Service Console User Guide*\.

## Using Transfer Acceleration from the AWS Command Line Interface \(AWS CLI\)<a name="transfer-acceleration-examples-aws-cli"></a>

This section provides examples of AWS CLI commands used for Transfer Acceleration\. For instructions on setting up the AWS CLI, see [Setting Up the AWS CLI](setup-aws-cli.md)\.

### Enabling Transfer Acceleration on a Bucket Using the AWS CLI<a name="transfer-acceleration-examples-aws-cli-1"></a>

Use the AWS CLI [ put\-bucket\-accelerate\-configuration](https://docs.aws.amazon.com/cli/latest/reference/s3api/put-bucket-accelerate-configuration.html) command to enable or suspend Transfer Acceleration on a bucket\. The following example sets `Status=Enabled` to enable Transfer Acceleration on a bucket\. You use `Status=Suspended` to suspend Transfer Acceleration\.

**Example**  

```
$ aws s3api put-bucket-accelerate-configuration --bucket bucketname --accelerate-configuration Status=Enabled
```

### Using the Transfer Acceleration from the AWS CLI<a name="transfer-acceleration-examples-aws-cli-2"></a>

Setting the configuration value `use_accelerate_endpoint` to `true` in a profile in your AWS Config File will direct all Amazon S3 requests made by s3 and s3api AWS CLI commands to the accelerate endpoint: `s3-accelerate.amazonaws.com`\. Transfer Acceleration must be enabled on your bucket to use the accelerate endpoint\. 

All request are sent using the virtual style of bucket addressing:  `my-bucket.s3-accelerate.amazonaws.com`\. Any `ListBuckets`, `CreateBucket`, and `DeleteBucket` requests will not be sent to the accelerate endpoint as the endpoint does not support those operations\. For more information about `use_accelerate_endpoint`, see [AWS CLI S3 Configuration](https://docs.aws.amazon.com/cli/latest/topic/s3-config.html)\. 

The following example sets `use_accelerate_endpoint` to `true` in the default profile\.

**Example**  

```
$ aws configure set default.s3.use_accelerate_endpoint true
```

If you want to use the accelerate endpoint for some AWS CLI commands but not others, you can use either one of the following two methods: 
+ You can use the accelerate endpoint per command by setting the `--endpoint-url` parameter to `https://s3-accelerate.amazonaws.com` or `http://s3-accelerate.amazonaws.com` for any s3 or s3api command\.
+ You can setup separate profiles in your AWS Config File\. For example, create one profile that sets `use_accelerate_endpoint` to `true` and a profile that does not set `use_accelerate_endpoint`\. When you run a command specify which profile you want to use, depending upon whether or not you want to use the accelerate endpoint\. 

### AWS CLI Examples of Uploading an Object to a Bucket Enabled for Transfer Acceleration<a name="transfer-acceleration-examples-aws-cli-3"></a>

The following example uploads a file to a bucket enabled for Transfer Acceleration by using the default profile that has been configured to use the accelerate endpoint\.

**Example**  

```
$ aws s3 cp file.txt s3://bucketname/keyname --region region
```

The following example uploads a file to a bucket enabled for Transfer Acceleration by using the `--endpoint-url` parameter to specify the accelerate endpoint\.

**Example**  

```
$ aws configure set s3.addressing_style virtual
$ aws s3 cp file.txt s3://bucketname/keyname --region region --endpoint-url http://s3-accelerate.amazonaws.com
```

## Using Transfer Acceleration from the AWS SDK for Java<a name="transfer-acceleration-examples-java"></a>

### <a name="transfer-acceleration-examples-java-uploads"></a>

**Example**  
The following example shows how to use an accelerate endpoint to upload an object to Amazon S3\. The example does the following:  
+ Creates an `AmazonS3Client` that is configured to use accelerate endpoints\. All buckets that the client accesses must have transfer acceleration enabled\.
+ Enables transfer acceleration on a specified bucket\. This step is necessary only if the bucket you specify doesn't already have transfer acceleration enabled\.
+ Verifies that transfer acceleration is enabled for the specified bucket\.
+ Uploads a new object to the specified bucket using the bucket's accelerate endpoint\.
For more information about using Transfer Acceleration, see [Getting Started with Amazon S3 Transfer Acceleration](transfer-acceleration.md#transfer-acceleration-getting-started)\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketAccelerateConfiguration;
import com.amazonaws.services.s3.model.BucketAccelerateStatus;
import com.amazonaws.services.s3.model.GetBucketAccelerateConfigurationRequest;
import com.amazonaws.services.s3.model.SetBucketAccelerateConfigurationRequest;

public class TransferAcceleration {
    public static void main(String[] args) {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ***";

        try {
            // Create an Amazon S3 client that is configured to use the accelerate endpoint.
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .enableAccelerateMode()
                    .build();

            // Enable Transfer Acceleration for the specified bucket.
            s3Client.setBucketAccelerateConfiguration(
                    new SetBucketAccelerateConfigurationRequest(bucketName,
                            new BucketAccelerateConfiguration(
                                    BucketAccelerateStatus.Enabled)));

            // Verify that transfer acceleration is enabled for the bucket.
            String accelerateStatus = s3Client.getBucketAccelerateConfiguration(
                    new GetBucketAccelerateConfigurationRequest(bucketName))
                    .getStatus();
            System.out.println("Bucket accelerate status: " + accelerateStatus);

            // Upload a new object using the accelerate endpoint.
            s3Client.putObject(bucketName, keyName, "Test object for transfer acceleration");
            System.out.println("Object \"" + keyName + "\" uploaded with transfer acceleration.");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Using Transfer Acceleration from the AWS SDK for \.NET<a name="transfer-acceleration-examples-dotnet"></a>



The following example shows how to use the AWS SDK for \.NET to enable Transfer Acceleration on a bucket\. For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

**Example**  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class TransferAccelerationTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            EnableAccelerationAsync().Wait();
        }

        static async Task EnableAccelerationAsync()
        {
                try
                {
                    var putRequest = new PutBucketAccelerateConfigurationRequest
                    {
                        BucketName = bucketName,
                        AccelerateConfiguration = new AccelerateConfiguration
                        {
                            Status = BucketAccelerateStatus.Enabled
                        }
                    };
                    await s3Client.PutBucketAccelerateConfigurationAsync(putRequest);

                    var getRequest = new GetBucketAccelerateConfigurationRequest
                    {
                        BucketName = bucketName
                    };
                    var response = await s3Client.GetBucketAccelerateConfigurationAsync(getRequest);

                    Console.WriteLine("Acceleration state = '{0}' ", response.Status);
                }
                catch (AmazonS3Exception amazonS3Exception)
                {
                    Console.WriteLine(
                        "Error occurred. Message:'{0}' when setting transfer acceleration",
                        amazonS3Exception.Message);
                }
        }
    }
}
```

When uploading an object to a bucket that has Transfer Acceleration enabled, you specify using acceleration endpoint at the time of creating a client as shown:



```
var client = new AmazonS3Client(new AmazonS3Config
            {
                RegionEndpoint = TestRegionEndpoint,
                UseAccelerateEndpoint = true
            }
```



## Using Transfer Acceleration from the AWS SDK for JavaScript<a name="transfer-acceleration-examples-javascript"></a>

For an example of enabling Transfer Acceleration by using the AWS SDK for JavaScript, see [Calling the putBucketAccelerateConfiguration operation](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#putBucketAccelerateConfiguration-property) in the *AWS SDK for JavaScript API Reference*\.

## Using Transfer Acceleration from the AWS SDK for Python \(Boto\)<a name="transfer-acceleration-examples-python"></a>

For an example of enabling Transfer Acceleration by using the SDK for Python, see [ put\_bucket\_accelerate\_configuration](http://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.put_bucket_accelerate_configuration) in the *AWS SDK for Python \(Boto3\) API Reference*\.

## Using Other AWS SDKs<a name="transfer-acceleration-examples-sdks"></a>

For information about using other AWS SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code/)\. 


# Using Amazon S3 Storage Lens with AWS Organizations<a name="storage_lens_with_organizations"></a>

You can use Amazon S3 Storage Lens to collect storage metrics and usage data for all AWS accounts that are part of your AWS Organizations hierarchy\. To do this, you must be using AWS Organizations, and you must enable S3 Storage Lens trusted access using your AWS Organizations management\. 

After enabling trusted access, you can add delegated administrator access to accounts in your organization\. These accounts can then create organization\-wide dashboards and configurations for S3 Storage Lens\. 

For more information about enabling trusted access, see [Amazon S3 Storage Lens and AWS Organizations](https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-s3lens.html) in the *AWS Organizations User Guide*\.

**Topics**
+ [Enabling trusted access for S3 Storage Lens](#storage_lens_with_organizations_enabling_trusted_access)
+ [Disabling trusted access for S3 Storage Lens](#storage_lens_with_organizations_disabling_trusted_access)
+ [Registering a delegated administrator for S3 Storage Lens](#storage_lens_with_organizations_registering_delegated_admins)
+ [Deregistering a delegated administrator for S3 Storage Lens](#storage_lens_with_organizations_deregistering_delegated_admins)

## Enabling trusted access for S3 Storage Lens<a name="storage_lens_with_organizations_enabling_trusted_access"></a>

By enabling trusted access, you allow Amazon S3 Storage Lens to have access to your AWS Organizations hierarchy, membership, and structure through the AWS Organizations APIs\. S3 Storage Lens will be a trusted service for your entire organization’s structure\. 

Whenever a dashboard configuration is created, S3 Storage Lens creates service\-linked roles in your organization’s management or delegated administrator accounts\. The service\-linked role grants S3 Storage Lens permissions to describe organizations, list accounts, verify a list of service access for the organizations, and get delegated administrators for the organization\. S3 Storage Lens can then collect cross\-account storage usage and activity metrics for dashboards within accounts in your organizations\. For more information, see [ Using service\-linked roles for Amazon S3 Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/dev/using-service-linked-roles.html)\. 

After enabling trusted access, you can assign delegate administrator access to accounts in your organization\. When an account is marked as a delegate administrator for a service, the account receives authorization to access all read\-only organization APIs\. This provides visibility to the members and structures of your organization so that they can create S3 Storage Lens dashboards on your behalf\.

**Note**  
Only the management account can enable trusted access for Amazon S3 Storage Lens\.

## Disabling trusted access for S3 Storage Lens<a name="storage_lens_with_organizations_disabling_trusted_access"></a>

By disabling trusted access, you limit S3 Storage Lens to working only on an account level\. In addition, each account holder can only see the S3 Storage Lens benefits limited to the scope of their account, and not their entire organization\. Any dashboards requiring trusted access are no longer updated, but will retain their historic data per their respective [ retention periods](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_basics_metrics_recommendations.html#storage_lens_basics_retention_period)\. 

Removing an account as a delegated administrator will limit their S3 Storage Lens dashboard metrics access to only work on an account level\. Any organizational dashboards that they created are no longer updated, but they will retain their historic data per their respective retention periods\. 

**Note**  
This action also automatically stops all organization\-level dashboards from collecting and aggregating storage metrics\. 
Your management and delegated administrator accounts will still be able to see the historic data for your exiting organization\-level dashboards according to their respective retention periods\.

## Registering a delegated administrator for S3 Storage Lens<a name="storage_lens_with_organizations_registering_delegated_admins"></a>

You can create organization\-level dashboards using your organization’s management account or a delegated administrator account\. Delegated administrator accounts allow other accounts besides your management account to create organization\-level dashboards\. Only the management account of an organization can register and deregister other accounts as delegated administrators for the organization\.

To register a delegated administrator using the Amazon S3 console, see [Register accounts for delegated administrator access](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/storage_lens_console_organizations_registering_delegated_admins.html) in the *Amazon Simple Storage Service Console User Guide*\.

You can also register a delegated administrator using the AWS Organizations REST API, AWS CLI, or SDKs from the management account\. For more information, see [RegisterDelegatedAdministrator](https://docs.aws.amazon.com/organizations/latest/APIReference/API_RegisterDelegatedAdministrator.html) in the *AWS Organizations API Reference*\.

**Note**  
Before you can designate a delegated administrator using the AWS Organizations REST API, AWS CLI, or SDKs, you must call the [EnableAWSOrganizationsAccess](https://docs.aws.amazon.com/servicecatalog/latest/dg/API_EnableAWSOrganizationsAccess.html) operation\.

## Deregistering a delegated administrator for S3 Storage Lens<a name="storage_lens_with_organizations_deregistering_delegated_admins"></a>

You can also de\-register a delegated administrator account\. Delegated administrator accounts allow other accounts besides your management account to create organization\-level dashboards\. Only the management account of an organization can de\-register accounts as delegated administrators for the organization\.

To de\-register a delegated admin using the S3 console, see [ Deregister accounts for delegated administrator access](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/storage_lens_console_organizations_deregistering_delegated_admins.html) in the *Amazon Simple Storage Service Console User Guide*\.

You can also de\-register a delegated administrator using the AWS Organizations REST API, AWS CLI, or SDKs from the management account\. For more information, see [ DeregisterDelegatedAdministrator](https://docs.aws.amazon.com/organizations/latest/APIReference/API_DeregisterDelegatedAdministrator.html) in the *AWS Organizations API Reference*\.

**Note**  
This action also automatically stop all organization\-level dashboards created by that delegated administrator from aggregating new storage metrics\.
The delegate administrator accounts will still be able to see the historic data for those dashboards according to their respective retention periods\.


# Removing delete markers<a name="RemDelMarker"></a>

To delete a delete marker, it must have a version ID and you must specify that ID in a `DELETE Object versionId` request\. If you use a `DELETE` request to delete a delete marker \(without specifying the version ID of the delete marker\), Amazon S3 does not delete the delete marker, but instead, inserts another delete marker\.

The following figure shows how a simple `DELETE` on a delete marker removes nothing, but adds a new delete marker to a bucket\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_deleteMarker.png)

In a versioning\-enabled bucket, this new delete marker would have a unique version ID\. So, it's possible to have multiple delete markers of the same object in one bucket\. 

If the current object version is the only object version and it is also a delete marker \(also referred as an *expired object delete marker*, where all object versions are deleted and you only have a delete marker remaining\), Amazon S3 removes the expired object delete marker\. You can also use the expiration action to direct Amazon S3 to remove any expired object delete markers\. For an example, see [Example 7: Removing expired object delete markers](lifecycle-configuration-examples.md#lifecycle-config-conceptual-ex7)\. 

To permanently delete a delete marker, you must include its version ID in a `DELETE Object versionId` request\. The following figure shows how a `DELETE Object versionId` request permanently removes a delete marker\. Only the owner of a bucket can permanently remove a delete marker\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_deleteMarkerVersioned.png)

The effect of removing the delete marker is that a simple `GET` request will now retrieve the current version \(121212\) of the object\.

**To permanently remove a delete marker**

1. Set `versionId` to the ID of the version to the delete marker you want to remove\.

1. Send a `DELETE Object versionId` request\.

**Example Removing a delete marker**  
The following example removes the delete marker for `photo.gif` version 4857693\.  

```
1. DELETE /photo.gif?versionId=4857693 HTTP/1.1
2. Host: bucket.s3.amazonaws.com
3. Date: Wed, 28 Oct 2009 22:32:00 GMT
4. Authorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=
```

When you delete a delete marker, Amazon S3 includes in the response:

```
1. 204 NoContent 
2. x-amz-version-id: versionID 
3. x-amz-delete-marker: true
```


# Example 3: Bucket owner granting its users permissions to objects it does not own<a name="example-walkthroughs-managing-access-example3"></a>

**Topics**
+ [Step 0: Preparing for the walkthrough](#access-policies-walkthrough-cross-account-acl-step0)
+ [Step 1: Do the account a tasks](#access-policies-walkthrough-cross-account-acl-acctA-tasks)
+ [Step 2: Do the account b tasks](#access-policies-walkthrough-cross-account-acl-acctB-tasks)
+ [Step 3: Test permissions](#access-policies-walkthrough-cross-account-acl-verify)
+ [Step 4: Clean up](#access-policies-walkthrough-cross-account-acl-cleanup)

The scenario for this example is that a bucket owner wants to grant permission to access objects, but not all objects in the bucket are owned by the bucket owner\. How can a bucket owner grant permission on objects it does not own? For this example, the bucket owner is trying to grant permission to users in its own account\.

A bucket owner can enable other AWS accounts to upload objects\. These objects are owned by the accounts that created them\. The bucket owner does not own objects that were not created by the bucket owner\. Therefore, for the bucket owner to grant access to these objects, the object owner must first grant permission to the bucket owner using an object ACL\. For more information, see [Amazon S3 bucket and object ownership](access-control-overview.md#about-resource-owner)\.

In this example, the bucket owner delegates permission to users in its own account\. The following is a summary of the walkthrough steps:

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/access-policy-ex3.png)

1. Account A administrator user attaches a bucket policy with two statements\.
   + Allow cross\-account permission to Account B to upload objects\.
   + Allow a user in its own account to access objects in the bucket\.

1. Account B administrator user uploads objects to the bucket owned by Account A\.

1. Account B administrator updates the object ACL adding grant that gives the bucket owner full\-control permission on the object\.

1. User in Account A verifies by accessing objects in the bucket, regardless of who owns them\.

For this example, you need two accounts\. The following table shows how we refer to these accounts and the administrator users in these accounts\. Per IAM guidelines \(see [About using an administrator user to create resources and grant permissions](example-walkthroughs-managing-access.md#about-using-root-credentials)\) we do not use the account root credentials in this walkthrough\. Instead, you create an administrator user in each account and use those credentials in creating resources and granting them permissions\.


| AWS account ID | Account referred to as | Administrator user in the account  | 
| --- | --- | --- | 
|  *1111\-1111\-1111*  |  Account A  |  AccountAadmin  | 
|  *2222\-2222\-2222*  |  Account B  |  AccountBadmin  | 

All the tasks of creating users and granting permissions are done in the AWS Management Console\. To verify permissions, the walkthrough uses the command line tools, AWS Command Line Interface \(CLI\) and AWS Tools for Windows PowerShell, so you don't need to write any code\. 

## Step 0: Preparing for the walkthrough<a name="access-policies-walkthrough-cross-account-acl-step0"></a>

1. Make sure you have two AWS accounts and each account has one administrator user as shown in the table in the preceding section\.

   1. Sign up for an AWS account, if needed\. 

      1.  Go to [https://aws\.amazon\.com/s3/](https://aws.amazon.com/s3/) and click **Create an AWS Account**\. 

      1. Follow the on\-screen instructions\. AWS will notify you by email when your account is active and available for you to use\.

   1. Using Account A credentials, sign in to the [IAM console](https://console.aws.amazon.com/iam/home?#home) and do the following to create an administrator user:
      + Create user AccountAadmin and note down security credentials\. For more information about adding users, see [Creating an IAM User in Your AWS Account](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html) in the *IAM User Guide*\. 
      + Grant AccountAadmin administrator privileges by attaching a user policy giving full access\. For instructions, see [Working with Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html) in the *IAM User Guide*\. 
      + In the IAM console **Dashboard**, note down the** IAM User Sign\-In URL**\. Users in this account must use this URL when signing in to the AWS Management Console\. For more information, see [How Users Sign in to Your Account](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_how-users-sign-in.html) in *IAM User Guide*\. 

   1. Repeat the preceding step using Account B credentials and create administrator user AccountBadmin\.

1. Set up either the AWS Command Line Interface \(CLI\) or the AWS Tools for Windows PowerShell\. Make sure you save administrator user credentials as follows:
   + If using the AWS CLI, create two profiles, AccountAadmin and AccountBadmin, in the config file\.
   + If using the AWS Tools for Windows PowerShell, make sure you store credentials for the session as AccountAadmin and AccountBadmin\.

   For instructions, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\. 

## Step 1: Do the account a tasks<a name="access-policies-walkthrough-cross-account-acl-acctA-tasks"></a>

### Step 1\.1: Sign in to the AWS Management Console<a name="access-policies-walkthrough-cross-account-permissions-acctA-tasks-sign-in-example3"></a>

Using the IAM user sign\-in URL for Account A first sign in to the AWS Management Console as AccountAadmin user\. This user will create a bucket and attach a policy to it\. 

### Step 1\.2: Create a bucket, a user, and add a bucket policy granting user permissions<a name="access-policies-walkthrough-cross-account-acl-create-bucket"></a>

1. In the Amazon S3 console, create a bucket\. This exercise assumes the bucket is created in the US East \(N\. Virginia\) region and the name is `DOC-EXAMPLE-BUCKET1`\.

   For instructions, see [How Do I Create an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\. 

1. In the IAM console, create a user Dave\. 

   For instructions, see [Creating IAM Users \(AWS Management Console\)](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console) in the *IAM User Guide*\. 

1. Note down the Dave credentials\. 

1. In the Amazon S3 console, attach the following bucket policy to `DOC-EXAMPLE-BUCKET1` bucket\. For instructions, see [How Do I Add an S3 Bucket Policy?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-bucket-policy.html) in the *Amazon Simple Storage Service Console User Guide*\. Follow the steps to add a bucket policy\. For information about how to find account IDs, see [Finding Your AWS Account ID](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingYourAccountIdentifiers)\. 

   The policy grants Account B the `s3:PutObject` and `s3:ListBucket` permissions\. The policy also grants user Dave the `s3:GetObject` permission\. 

   ```
   {
      "Version": "2012-10-17",
      "Statement": [
         {
            "Sid": "Statement1",
            "Effect": "Allow",
            "Principal": {
               "AWS": "arn:aws:iam::AccountB-ID:root"
            },
            "Action": [
               "s3:PutObject",
               "s3:ListBucket"
            ],
            "Resource": [
               "arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*",
               "arn:aws:s3:::DOC-EXAMPLE-BUCKET1"
            ]
         },
         {
            "Sid": "Statement3",
            "Effect": "Allow",
            "Principal": {
               "AWS": "arn:aws:iam::AccountA-ID:user/Dave"
            },
            "Action": [
               "s3:GetObject"
            ],
            "Resource": [
               "arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*"
            ]
         }  
      ]
   }
   ```

## Step 2: Do the account b tasks<a name="access-policies-walkthrough-cross-account-acl-acctB-tasks"></a>

Now that Account B has permissions to perform operations on Account A's bucket, the Account B administrator will do the following;
+ Upload an object to Account A's bucket\. 
+ Add a grant in the object ACL to allow Account A, the bucket owner, full control\. 

**Using the AWS CLI**

1. Using the `put-object` AWS CLI command, upload an object\. The \-`-body` parameter in the command identifies the source file to upload\. For example, if the file is on `C:` drive of a Windows machine, you would specify `c:\HappyFace.jpg`\. The `--key` parameter provides the key name for the object\. 

   ```
   aws s3api put-object --bucket DOC-EXAMPLE-BUCKET1 --key HappyFace.jpg --body HappyFace.jpg --profile AccountBadmin
   ```

1. Add a grant to the object ACL to allow the bucket owner full control of the object\. For information about how to find a canonical user ID, see [Finding Your Account Canonical User ID](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingCanonicalId)\.

   ```
   aws s3api put-object-acl --bucket DOC-EXAMPLE-BUCKET1 --key HappyFace.jpg --grant-full-control id="AccountA-CanonicalUserID" --profile AccountBadmin
   ```

**Using the AWS Tools for Windows PowerShell**

1. Using the `Write-S3Object` AWS Tools for Windows PowerShell command, upload an object\. 

   ```
   Write-S3Object -BucketName DOC-EXAMPLE-BUCKET1 -key HappyFace.jpg -file HappyFace.jpg -StoredCredentials AccountBadmin
   ```

1. Add a grant to the object ACL to allow the bucket owner full control of the object\.

   ```
   Set-S3ACL -BucketName DOC-EXAMPLE-BUCKET1 -Key HappyFace.jpg -CannedACLName "bucket-owner-full-control" -StoredCreden
   ```

## Step 3: Test permissions<a name="access-policies-walkthrough-cross-account-acl-verify"></a>

Now verify user Dave in Account A can access the object owned by Account B\.

**Using the AWS CLI**

1. Add user Dave credentials to the AWS CLI config file and create a new profile, `UserDaveAccountA`\. For more information, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\.

   ```
   [profile UserDaveAccountA]
   aws_access_key_id = access-key
   aws_secret_access_key = secret-access-key
   region = us-east-1
   ```

1. Run the `get-object` AWS CLI command to download `HappyFace.jpg` and save it locally\. You provide user Dave credentials by adding the `--profile` parameter\.

   ```
   aws s3api get-object --bucket DOC-EXAMPLE-BUCKET1 --key HappyFace.jpg Outputfile.jpg --profile UserDaveAccountA
   ```

**Using the AWS Tools for Windows PowerShell**

1. Store user Dave AWS credentials, as UserDaveAccountA, to persistent store\. 

   ```
   Set-AWSCredentials -AccessKey UserDave-AccessKey -SecretKey UserDave-SecretAccessKey -storeas UserDaveAccountA
   ```

1. Run the Read\-S3Object command to download the `HappyFace.jpg` object and save it locally\. You provide user Dave credentials by adding the `-StoredCredentials` parameter\. 

   ```
   Read-S3Object -BucketName DOC-EXAMPLE-BUCKET1 -Key HappyFace.jpg -file HappyFace.jpg  -StoredCredentials UserDaveAccountA
   ```

## Step 4: Clean up<a name="access-policies-walkthrough-cross-account-acl-cleanup"></a>

1. After you are done testing, you can do the following to clean up\.

   1. Sign in to the AWS Management Console \([AWS Management Console](https://console.aws.amazon.com/)\) using Account A credentials, and do the following:
     + In the Amazon S3 console, remove the bucket policy attached to *DOC\-EXAMPLE\-BUCKET1*\. In the bucket **Properties**, delete the policy in the **Permissions** section\. 
     + If the bucket is created for this exercise, in the Amazon S3 console, delete the objects and then delete the bucket\. 
     + In the IAM console, remove the AccountAadmin user\.

1. Sign in to the AWS Management Console \([AWS Management Console](https://console.aws.amazon.com/)\) using Account B credentials\. In the IAM console, delete user AccountBadmin\.


# Object Versioning<a name="ObjectVersioning"></a>

Use Amazon S3 Versioning to keep multiple versions of an object in one bucket\. For example, you could store `my-image.jpg` \(version 111111\) and `my-image.jpg` \(version 222222\) in a single bucket\. S3 Versioning protects you from the consequences of unintended overwrites and deletions\. You can also use it to archive objects so that you have access to previous versions\. 

**Note**  
The SOAP API does not support S3 Versioning\. SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features are not supported for SOAP\.

To customize your data retention approach and control storage costs, use object versioning with [Object lifecycle management](object-lifecycle-mgmt.md)\. For information about creating S3 Lifecycle policies using the AWS Management Console, see [How do I create a lifecycle policy for an S3 bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html) in the *Amazon Simple Storage Service Console User Guide*\.

If you have an object expiration lifecycle policy in your non\-versioned bucket and you want to maintain the same permanent delete behavior when you enable versioning, you must add a noncurrent expiration policy\. The noncurrent expiration lifecycle policy will manage the deletes of the noncurrent object versions in the version\-enabled bucket\. \(A version\-enabled bucket maintains one current and zero or more noncurrent object versions\.\)

You must explicitly enable S3 Versioning on your bucket\. By default, S3 Versioning is disabled\. Regardless of whether you have enabled Versioning, each object in your bucket has a version ID\. If you have not enabled Versioning, Amazon S3 sets the value of the version ID to null\. If S3 Versioning is enabled, Amazon S3 assigns a version ID value for the object\. This value distinguishes it from other versions of the same key\.

Enabling and suspending versioning is done at the bucket level\. When you enable versioning on an existing bucket, objects that are already stored in the bucket are unchanged\. The version IDs \(null\), contents, and permissions remain the same\. After you enable S3 Versioning for a bucket, each object that is added to the bucket gets a version ID, which distinguishes it from other versions of the same key\. 

Only Amazon S3 generates version IDs, and they can’t be edited\. Version IDs are Unicode, UTF\-8 encoded, URL\-ready, opaque strings that are no more than 1,024 bytes long\. The following is an example: `3/L4kqtJlcpXroDTDmJ+rmSpXd3dIbrHY+MTRCxf3vjVBH40Nr8X8gdRQBpUMLUo`\.

**Note**  
For simplicity, all the examples use much shorter IDs\.

When you `PUT` an object in a versioning\-enabled bucket, the noncurrent version is not overwritten\. The following figure shows that when a new version of `photo.gif` is `PUT` into a bucket that already contains an object with the same name, the original object \(ID = 111111\) remains in the bucket, Amazon S3 generates a new version ID \(121212\), and adds the newer version to the bucket\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_PUT_versionEnabled3.png)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)

This functionality prevents you from accidentally overwriting or deleting objects and affords you the opportunity to retrieve a previous version of an object\. 

When you `DELETE` an object, all versions remain in the bucket and Amazon S3 inserts a delete marker, as shown in the following figure\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_versioningEnabled.png)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)

The delete marker becomes the current version of the object\. By default, `GET` requests retrieve the most recently stored version\. Performing a simple `GET Object` request when the current version is a delete marker returns a `404 Not Found` error, as shown in the following figure\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_NoObjectFound2.png)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)

However, you can `GET` a noncurrent version of an object by specifying its version ID\. In the following figure, you `GET` a specific object version, 111111\. Amazon S3 returns that object version even though it's not the current version\. 

For more information, see [ Retrieving object versions](https://docs.aws.amazon.com/AmazonS3/latest/dev/RetrievingObjectVersions.html)\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_GET_Versioned3.png)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)

You can permanently delete an object by specifying the version you want to delete\. Only the owner of an Amazon S3 bucket can permanently delete a version\. The following figure shows how `DELETE versionId` permanently deletes an object from a bucket and that Amazon S3 doesn't insert a delete marker\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_versioningEnabled2.png)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/)

You can add additional security by configuring a bucket to enable MFA \(multi\-factor authentication\) Delete\. When you do, the bucket owner must include two forms of authentication in any request to delete a version or change the versioning state of the bucket\. For more information, see [MFA Delete](Versioning.md#MultiFactorAuthenticationDelete)\.

**Important**  
If you notice a significant increase in the number of HTTP 503\-slow down responses received for Amazon S3 PUT or DELETE object requests to a bucket that has S3 Versioning enabled, you might have one or more objects in the bucket for which there are millions of versions\. For more information, see [Troubleshooting Amazon S3](troubleshooting.md)\.

For more information, see [Using Versioning](Versioning.md)\.


# Upload an object using the AWS SDK for PHP<a name="UploadObjSingleOpPHP"></a>

 This topic guides you through using classes from the AWS SDK for PHP to upload an object of up to 5 GB in size\. For larger files you must use multipart upload API\. For more information, see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\.

 This topic assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\.

**Example of Creating an Object in an Amazon S3 bucket by Uploading Data**  
The following PHP example creates an object in a specified bucket by uploading data using the `putObject()` method\. For information about running the PHP examples in this guide, go to [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\.   

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
                        
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

try {
    // Upload data.
    $result = $s3->putObject([
        'Bucket' => $bucket,
        'Key'    => $keyname,
        'Body'   => 'Hello, world!',
        'ACL'    => 'public-read'
    ]);

    // Print the URL to the object.
    echo $result['ObjectURL'] . PHP_EOL;
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}
```

## Related Resources<a name="RelatedResources-UploadObjSingleOpPHP"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Restoring archived objects<a name="restoring-objects"></a>

When you archive Amazon S3 objects to the S3 Glacier or S3 Glacier Deep Archive, or when objects are archived to the S3 Intelligent\-Tiering Archive Access or Deep Archive Access tiers, the objects are not accessible in real time\. For objects in Archive Access or Deep Archive Access tiers, you must first initiate a restore request, and then wait until the object is moved into the Frequent Access tier\. For objects in S3 Glacier or S3 Glacier Deep Archive, you must first initiate a restore request, and then wait until a temporary copy of the object is available\. For more information about how all Amazon S3 storage classes compare, see [Amazon S3 storage classes](storage-class-intro.md)\. 

When you are restoring from S3 Intelligent\-Tiering Archive Access tier or S3 Intelligent\-Tiering Deep Archive Access tier, the object moves back into the S3 Intelligent\-Tiering Frequent Access tier\. Afterwards, if the object is not accessed after 30 consecutive days, it automatically moves into the Infrequent Access tier\. It moves into the S3 Intelligent\-Tiering Archive Access tier after a minimum of 90 consecutive days of no access, and it moves into the Deep Archive Access tier after a minimum of 180 consecutive days of no access\.

**Note**  
Unlike in S3 Glacier and S3 Glacier Deep Archive storage classes, restore requests for S3 Intelligent\-Tiering objects don't accept the `days` value\. 

When you use S3 Glacier or S3 Glacier Deep Archive, Amazon S3 restores a temporary copy of the object only for the specified duration\. After that, it deletes the restored object copy\. You can modify the expiration period of a restored copy by reissuing a restore\. In this case, Amazon S3 updates the expiration period relative to the current time\. 

**Note**  
When you restore an archive from S3 Glacier or S3 Glacier Deep Archive, you pay for both the archived object and a copy that you restored temporarily \(Reduced Redundancy Storage \[RRS\] or Standard, whichever is the lower\-cost storage in the Region\)\. For information about pricing, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

Amazon S3 calculates the expiration time of the restored object copy by adding the number of days specified in the restoration request to the current time\. It then rounds the resulting time to the next day at midnight Universal Coordinated Time \(UTC\)\. For example, suppose that an object was created on October 15, 2012 10:30 AM UTC, and the restoration period was specified as 3 days\. In this case, the restored copy expires on October 19, 2012 00:00 UTC, at which time Amazon S3 deletes the object copy\. 

If a temporary copy of the restored object is created, the object's storage class remains the same\. \(A [HEAD Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html) or the [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) API operations request returns S3 Glacier or S3 Glacier Deep Archive as the storage class\.\) 

The time it takes a restore job to finish depends on which archive storage class or storage tier you use and which retrieval option you specify: `Expedited` \(only available for S3 Glacier and S3 Intelligent\-Tiering Archive Access\), `Standard`, or `Bulk`\. You can be notified when your restore is complete using Amazon S3 event notifications\. For more information, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.

When required, you can restore large segments of the data stored for a secondary copy\. However, keep in mind that S3 Glacier and S3 Glacier Deep Archive storage classes and Archive Access and Deep Archive Access tiers are designed for 35 random restore requests per pebibyte \(PiB\) stored per day\.

To restore more than one Amazon S3 object with a single request, you can use S3 Batch Operations\. You provide S3 Batch Operations with a list of objects to operate on\. S3 Batch Operations calls the respective API to perform the specified operation\. A single Batch Operations job can perform the specified operation on billions of objects containing exabytes of data\. 

The S3 Batch Operations feature tracks progress, sends notifications, and stores a detailed completion report of all actions, providing a fully managed, auditable, serverless experience\. You can use S3 Batch Operations through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\. For more information, see [S3 Batch Operations basics](batch-ops-basics.md)\.

The following sections provide more information about restoring objects\.

**Topics**
+ [Archive retrieval options](#restoring-objects-retrieval-options)
+ [Upgrading the speed of an in\-progress restore](#restoring-objects-upgrade-tier)
+ [Restore an archived object using the Amazon S3 console](restoring-objects-console.md)
+ [Restore an archived object using the AWS SDK for Java](restoring-objects-java.md)
+ [Restore an archived object using the AWS SDK for \.NET](restore-object-dotnet.md)
+ [Restore an archived object using the REST API](restoring-objects-rest.md)

## Archive retrieval options<a name="restoring-objects-retrieval-options"></a>

The following are the available retrieval options when restoring an archived object in Amazon S3: 
+ **`Expedited`** \- *Expedited* retrievals allow you to quickly access your data stored in the S3 Glacier storage class or S3 Intelligent\-Tiering Archive Access tier when occasional urgent requests for a subset of archives are required\. For all but the largest archived objects \(250 MB\+\), data accessed using expedited retrievals is typically made available within 1–5 minutes\. Provisioned capacity helps ensure that retrieval capacity for expedited retrievals from S3 Glacier is available when you need it\. For more information, see [Provisioned capacity](#restoring-objects-expedited-capacity)\.
+ **`Standard`** \- *Standard* retrievals allow you to access any of your archived objects within several hours\. This is the default option for retrieval requests that do not specify the retrieval option\. Standard retrievals typically finish within 3–5 hours for objects stored in the S3 Glacier storage class or S3 Intelligent\-Tiering Archive Access tier\. They typically finish within 12 hours for objects stored in the S3 Glacier Deep Archive or S3 Intelligent\-Tiering Deep Archive Access storage class\. Standard retrievals are free for objects stored in S3 Intelligent\-Tiering\.
+ **`Bulk`** \- *Bulk* retrievals are the lowest\-cost retrieval option in Amazon S3 Glacier, enabling you to retrieve large amounts, even petabytes, of data inexpensively\. Bulk retrievals typically finish within 5–12 hours for objects stored in the S3 Glacier storage class or S3 Intelligent\-Tiering Archive Access tier\. They typically finish within 48 hours for objects stored in the S3 Glacier Deep Archive storage class or S3 Intelligent\-Tiering Deep Archive Access tier\. Bulk retrievals are free for objects stored in S3 Intelligent\-Tiering\.

The following table summarizes the archival retrieval options\. For complete information about pricing, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.


**Retrieval options**  

| Storage class or tier | Expedited | Standard | Bulk | 
| --- | --- | --- | --- | 
|  S3 Glacier or S3 Intelligent\-Tiering Archive Access  |  1–5 minutes  |  3–5 hours  |  5–12 hours  | 
|  S3 Glacier Deep Archive or S3 Intelligent\-Tiering Deep Archive Access  |  Not available  |  Within 12 hours  |  Within 48 hours  | 

To make an `Expedited`, `Standard`, or `Bulk` retrieval, set the `Tier` request element in the [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html) REST API request to the option you want, or the equivalent in the AWS CLI or AWS SDKs\. If you purchased provisioned capacity, all expedited retrievals are automatically served through your provisioned capacity\. 

You can restore an archived object programmatically or by using the Amazon S3 console\. Amazon S3 processes only one restore request at a time per object\. You can use both the console and the Amazon S3 API to check the restoration status and to find out when Amazon S3 will delete the restored copy\. 

### Provisioned capacity<a name="restoring-objects-expedited-capacity"></a>

Provisioned capacity helps ensure that your retrieval capacity for expedited retrievals from S3 Glacier is available when you need it\. Each unit of capacity provides that at least three expedited retrievals can be performed every 5 minutes, and it provides up to 150 MB/s of retrieval throughput\.

If your workload requires highly reliable and predictable access to a subset of your data in minutes, you should purchase provisioned retrieval capacity\. Without provisioned capacity, expedited retrievals might not be accepted during periods of high demand\. If you require access to expedited retrievals under all circumstances, we recommend that you purchase provisioned retrieval capacity\. 

You can purchase provisioned capacity using the Amazon S3 console, the Amazon S3 Glacier console, the [Purchase Provisioned Capacity](https://docs.aws.amazon.com/amazonglacier/latest/dev/api-PurchaseProvisionedCapacity.html) REST API, the AWS SDKs, or the AWS CLI\. For provisioned capacity pricing information, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

## Upgrading the speed of an in\-progress restore<a name="restoring-objects-upgrade-tier"></a>

Using Amazon S3 restore speed upgrade, you can change the restore speed to a faster speed while the restore is in progress\. A restore speed upgrade overrides an in\-progress restore with a faster restore tier\. You cannot slow down an in\-progress restore\.

To upgrade the speed of an in\-progress restoration, issue another restore request to the same object that sets a new `Tier` request element in the [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html) REST API, or the equivalent in the AWS CLI or AWS SDKs\. When issuing a request to upgrade the restore tier, you must choose a tier that is faster than the tier that the in\-progress restore is using\. You must not change any other parameters, such as the `Days` request element\. 

**Note**  
Standard and bulk restores for S3 Intelligent\-Tiering are free of charge\. However, subsequent restore requests called on an object that is already being restored are billed as a GET request\.

You can be notified of the completion of the restore by using Amazon S3 event notifications\. Restores are charged at the price of the upgraded tier\. For information about restore pricing, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.


# Troubleshooting replication<a name="replication-troubleshoot"></a>

If object replicas don't appear in the destination bucket after you configure replication, use these troubleshooting tips to identify and fix issues\.
+ The majority of objects replicate within 15 minutes, but they can sometimes take a couple of hours\. In rare cases, the replication can take longer\. The time it takes Amazon S3 to replicate an object depends on several factors, including source and destination Region pair, and the size of the object\. For large objects, replication can take up to several hours\. 

  If the object that is being replicated is large, wait a while before checking to see whether it appears in the destination\. You can also check the source object replication status\. If the object replication status is `PENDING`, Amazon S3 has not completed the replication\. If the object replication status is `FAILED`, check the replication configuration set on the source bucket\.
+ In the replication configuration on the source bucket, verify the following:
  + The Amazon Resource Name \(ARN\) of the destination buckets are correct\.
  + The key name prefix is correct\. For example, if you set the configuration to replicate objects with the prefix `Tax`, then only objects with key names such as `Tax/document1` or `Tax/document2` are replicated\. An object with the key name `document3` is not replicated\.
  + The status is `Enabled`\.
+ If the destination bucket is owned by another AWS account, verify that the bucket owner has a bucket policy on that destination bucket that allows the source bucket owner to replicate objects\. For an example, see [Example 2: Configuring replication when the source and destination buckets are owned by different accounts](replication-walkthrough-2.md)\.
+ If an object replica doesn't appear in the destination buckets, the following might have prevented replication:
  + Amazon S3 doesn't replicate an object in a source bucket that is a replica created by another replication configuration\. For example, if you set replication configuration from bucket A to bucket B to bucket C, Amazon S3 doesn't replicate object replicas in bucket B to bucket C\.
  + A source bucket owner can grant other AWS accounts permission to upload objects\. By default, the source bucket owner doesn't have permissions for the objects created by other accounts\. The replication configuration replicates only the objects for which the source bucket owner has access permissions\. The source bucket owner can grant other AWS accounts permissions to create objects conditionally, requiring explicit access permissions on those objects\. For an example policy, see [Granting Cross\-Account Permissions to Upload Objects While Ensuring the Bucket Owner Has Full Control](example-bucket-policies.md#example-bucket-policies-use-case-8)\.
+ Suppose that in the replication configuration, you add a rule to replicate a subset of objects having a specific tag\. In this case, you must assign the specific tag key and value at the time of creating the object for Amazon S3 to replicate the object\. If you first create an object and then add the tag to the existing object, Amazon S3 does not replicate the object\.
+ Replication fails if the bucket policy denies access to the replication role for any of the following actions:

  Source bucket:

  ```
  1.            "s3:GetReplicationConfiguration",
  2.            "s3:ListBucket",
  3.            "s3:GetObjectVersion",
  4.            "s3:GetObjectVersionAcl",
  5.            "s3:GetObjectVersionTagging"
  ```

  Destination buckets:

  ```
  1.            "s3:ReplicateObject",
  2.            "s3:ReplicateDelete",
  3.            "s3:ReplicateTags"
  ```

+ If you are restricting access via IAM for replication you need to ensure both the STS and IAM forms of the replication role are allowed as principals within the policy. The IAM role will need to both be assumed by S3 and run within the full context in order to execute the replication tasks. If these are not set appropriately, S3 will show a replication failure error.

  Role Definitions:

  ```
  1.            "arn:aws:iam::123456789012:role/path/role-name/*",
  2.            "arn:aws:sts::123456789012:assumed-role/path/role-name/*"
  "
  ```

## Related topics<a name="replication-troubleshoot-related-topics"></a>

[Replication](replication.md)



# Set lifecycle configurations using the AWS CLI<a name="set-lifecycle-cli"></a>

You can use the following AWS CLI commands to manage S3 Lifecycle configurations:
+ `put-bucket-lifecycle-configuration`
+ `get-bucket-lifecycle-configuration`
+ `delete-bucket-lifecycle`

For instructions on setting up the AWS CLI, see [Setting Up the AWS CLI](setup-aws-cli.md)\.

The Amazon S3 Lifecycle configuration is an XML file\. But when using the AWS CLI, you cannot specify the XML\. You must specify the JSON instead\. The following are example XML Lifecycle configurations and equivalent JSON that you can specify in an AWS CLIcommand\.
+ Consider the following example S3 Lifecycle configuration\.

  ```
  <LifecycleConfiguration>
      <Rule>
          <ID>ExampleRule</ID>
          <Filter>
             <Prefix>documents/</Prefix>
          </Filter>
          <Status>Enabled</Status>
          <Transition>        
             <Days>365</Days>        
             <StorageClass>GLACIER</StorageClass>
          </Transition>    
          <Expiration>
               <Days>3650</Days>
          </Expiration>
      </Rule>
  </LifecycleConfiguration>
  ```

  The equivalent JSON is shown\.

  ```
  {
      "Rules": [
          {
              "Filter": {
                  "Prefix": "documents/"
              },
              "Status": "Enabled",
              "Transitions": [
                  {
                      "Days": 365,
                      "StorageClass": "GLACIER"
                  }
              ],
              "Expiration": {
                  "Days": 3650
              },
              "ID": "ExampleRule"
          }
      ]
  }
  ```
+ Consider the following example S3 Lifecycle configuration\.

  ```
  <LifecycleConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
      <Rule>
          <ID>id-1</ID>
          <Expiration>
              <Days>1</Days>
          </Expiration>
          <Filter>
              <And>
                  <Prefix>myprefix</Prefix>
                  <Tag>
                      <Key>mytagkey1</Key>
                      <Value>mytagvalue1</Value>
                  </Tag>
                  <Tag>
                      <Key>mytagkey2</Key>
                      <Value>mytagvalue2</Value>
                  </Tag>
              </And>
          </Filter>
          <Status>Enabled</Status>    
      </Rule>
  </LifecycleConfiguration>
  ```

  The equivalent JSON is shown\.

  ```
  {
      "Rules": [
          {
              "ID": "id-1",
              "Filter": {
                  "And": {
                      "Prefix": "myprefix", 
                      "Tags": [
                          {
                              "Value": "mytagvalue1", 
                              "Key": "mytagkey1"
                          }, 
                          {
                              "Value": "mytagvalue2", 
                              "Key": "mytagkey2"
                          }
                      ]
                  }
              }, 
              "Status": "Enabled", 
              "Expiration": {
                  "Days": 1
              }
          }
      ]
  }
  ```

You can test the `put-bucket-lifecycle-configuration` as follows\.

**To test the configuration**

1. Save the JSON Lifecycle configuration in a file \(`lifecycle.json`\)\. 

1. Run the following AWS CLI command to set the Lifecycle configuration on your bucket\.

   ```
   $ aws s3api put-bucket-lifecycle-configuration  \
   --bucket bucketname  \
   --lifecycle-configuration file://lifecycle.json
   ```

1. To verify, retrieve the S3 Lifecycle configuration using the `get-bucket-lifecycle-configuration` AWS CLI command as follows\.

   ```
   $ aws s3api get-bucket-lifecycle-configuration  \
   --bucket bucketname
   ```

1. To delete the S3 Lifecycle configuration use the `delete-bucket-lifecycle` AWS CLI command as follows\.

   ```
   aws s3api delete-bucket-lifecycle \
   --bucket bucketname
   ```


# Creating access points<a name="creating-access-points"></a>

Amazon S3 provides functionality for creating and managing access points\. By default, you can create up to 1,000 access points per Region for each of your AWS accounts\. If you need more than 1,000 access points for a single account in a single Region, you can request a service quota increase\. For more information about service quotas and requesting an increase, see [AWS Service Quotas](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) in the *AWS General Reference*\.

You can create S3 access points using the AWS Management Console, AWS Command Line Interface \(AWS CLI\), AWS SDKs, or Amazon S3 REST API\. The following examples demonstrate how to create an access point using the AWS CLI\.

For information about how to create access points using the AWS Management Console, see [Introduction to Amazon S3 Access Points](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-points.html) in the *Amazon Simple Storage Service Console User Guide*\. For more information about how to create access points using the REST API, see [CreateAccessPoint](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_CreateAccessPoint.html) in the *Amazon Simple Storage Service API Reference*\.

**Note**  
Because you might want to publicize your access point name in order to allow users to use the access point, we recommend that you avoid including sensitive information in the access point name\.

## Rules for naming Amazon S3 access points<a name="access-points-names"></a>

Access point names must meet the following conditions:
+ Must be unique within a single AWS account and Region
+ Must comply with DNS naming restrictions
+ Must begin with a number or lowercase letter
+ Must be between 3 and 50 characters long
+ Can't begin or end with a dash
+ Can't contain underscores, uppercase letters, or periods

**Example**  
***Example: Create an Access Point***  
The following example creates an access point named `example-ap` for bucket `example-bucket` in account `123456789012`\. To create the access point, you send a request to Amazon S3, specifying the access point name, the name of the bucket that you want to associate the access point with, and the account ID for the AWS account that owns the bucket\. For information about naming rules, see [Rules for naming Amazon S3 access points](#access-points-names)\.  

```
aws s3control create-access-point --name example-ap --account-id 123456789012 --bucket example-bucket
```

## Creating access points restricted to a virtual private cloud<a name="access-points-vpc"></a>

When you create an access point, you can choose to make the access point accessible from the internet, or you can specify that all requests made through that access point must originate from a specific virtual private cloud \(VPC\)\. An access point that's accessible from the internet is said to have a network origin of `Internet`\. It can be used from anywhere on the internet, subject to any other access restrictions in place for the access point, underlying bucket, and related resources, such as the requested objects\. An access point that's only accessible from a specified VPC has a network origin of `VPC`, and Amazon S3 rejects any request made to the access point that doesn't originate from that VPC\.

**Important**  
You can only specify an access point's network origin when you create the access point\. After you create the access point, you can't change its network origin\.

To restrict an access point to VPC\-only access, you include the `VpcConfiguration` parameter with the request to create the access point\. In the `VpcConfiguration` parameter, you specify the VPC ID that you want to be able to use the access point\. Amazon S3 then rejects requests made through the access point unless they originate from that VPC\.

You can retrieve an access point's network origin using the AWS CLI, AWS SDKs, or REST APIs\. If an access point has a VPC configuration specified, its network origin is `VPC`\. Otherwise, the access point's network origin is `Internet`\.

**Example**  
***Example: Create an Access Point Restricted to VPC Access***  
The following example creates an access point named `example-vpc-ap` for bucket `example-bucket` in account `123456789012` that allows access only from VPC `vpc-1a2b3c`\. The example then verifies that the new access point has a network origin of `VPC`\.  

```
aws s3control create-access-point --name example-vpc-ap --account-id 123456789012 --bucket example-bucket --vpc-configuration VpcId=vpc-1a2b3c
```

```
aws s3control get-access-point --name example-vpc-ap --account-id 123456789012

{
    "Name": "example-vpc-ap",
    "Bucket": "example-bucket",
    "NetworkOrigin": "VPC",
    "VpcConfiguration": {
        "VpcId": "vpc-1a2b3c"
    },
    "PublicAccessBlockConfiguration": {
        "BlockPublicAcls": true,
        "IgnorePublicAcls": true,
        "BlockPublicPolicy": true,
        "RestrictPublicBuckets": true
    },
    "CreationDate": "2019-11-27T00:00:00Z"
}
```

To use an access point with a VPC, you must modify the access policy for your VPC endpoint\. VPC endpoints allow traffic to flow from your VPC to Amazon S3\. They have access\-control policies that control how resources within the VPC are allowed to interact with S3\. Requests from your VPC to S3 only succeed through an access point if the VPC endpoint policy grants access to both the access point and the underlying bucket\.

The following example policy statement configures a VPC endpoint to allow calls to `GetObject` for a bucket named `awsexamplebucket1` and an access point named `example-vpc-ap`\.

```
{
    "Version": "2012-10-17",
    "Statement": [
    {
        "Principal": "*",
        "Action": [
            "s3:GetObject"
        ],
        "Effect": "Allow",
        "Resource": [
            "arn:aws:s3:::awsexamplebucket1/*",
            "arn:aws:s3:us-west-2:123456789012:accesspoint/example-vpc-ap/object/*"
        ]
    }]
}
```

**Note**  
The `"Resource"` declaration in this example uses an Amazon Resource Name \(ARN\) to specify the access point\. For more information about access point ARNs, see [Using access points](using-access-points.md)\. 

For more information about VPC endpoint policies, see [Using Endpoint Policies for Amazon S3](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html#vpc-endpoints-policies-s3) in the *virtual private cloud \(VPC\) User Guide*\.

## Managing public access to access points<a name="access-points-bpa-settings"></a>

Amazon S3 access points support independent *block public access* settings for each access point\. When you create an access point, you can specify block public access settings that apply to that access point\. For any request made through an access point, Amazon S3 evaluates the block public access settings for that access point, the underlying bucket, and the bucket owner's account\. If any of these settings indicate that the request should be blocked, Amazon S3 rejects the request\.

For more information about the S3 Block Public Access feature, see [Using Amazon S3 block public access](access-control-block-public-access.md)\.

**Important**  
All block public access settings are enabled by default for access points\. You must explicitly disable any settings that you don't want to apply to an access point\.
Amazon S3 currently doesn't support changing an access point's block public access settings after the access point has been created\.

**Example**  
***Example: Create an Access Point with Custom Block Public Access Settings***  
This example creates an access point named `example-ap` for bucket `example-bucket` in account `123456789012` with non\-default Block Public Access settings\. The example then retrieves the new access point's configuration to verify its Block Public Access settings\.  

```
aws s3control create-access-point --name example-ap --account-id 123456789012 --bucket example-bucket --public-access-block-configuration BlockPublicAcls=false,IgnorePublicAcls=false,BlockPublicPolicy=true,RestrictPublicBuckets=true
```

```
aws s3control get-access-point --name example-ap --account-id 123456789012

{
    "Name": "example-ap",
    "Bucket": "example-bucket",
    "NetworkOrigin": "Internet",
    "PublicAccessBlockConfiguration": {
        "BlockPublicAcls": false,
        "IgnorePublicAcls": false,
        "BlockPublicPolicy": true,
        "RestrictPublicBuckets": true
    },
    "CreationDate": "2019-11-27T00:00:00Z"
}
```

## Configuring IAM policies for using access points<a name="access-points-policies"></a>

Amazon S3 access points support AWS Identity and Access Management \(IAM\) resource policies that allow you to control the use of the access point by resource, user, or other conditions\. For an application or user to be able to access objects through an access point, both the access point and the underlying bucket must permit the request\.

**Important**  
Adding an S3 access point to a bucket doesn't change the bucket's behavior when accessed through the existing bucket name or ARN\. All existing operations against the bucket will continue to work as before\. Restrictions that you include in an access point policy apply only to requests made through that access point\. 

### Condition keys<a name="access-points-condition-keys"></a>

S3 access points introduce three new condition keys that can be used in IAM policies to control access to your resources:

**s3:DataAccessPointArn**  
This is a string that you can use to match on an access point ARN\. The following example matches all access points for AWS account `123456789012` in Region `us-west-2`:  

```
"Condition" : {
    "StringLike": {
        "s3:DataAccessPointArn": "arn:aws:s3:us-west-2:123456789012:accesspoint/*"
    }
}
```

**s3:DataAccessPointAccount**  
This is a string operator that you can use to match on the account ID of the owner of an access point\. The following example matches all access points owned by AWS account `123456789012`\.  

```
"Condition" : {
    "StringEquals": {
        "s3:DataAccessPointAccount": "123456789012"
    }
}
```

**s3:AccessPointNetworkOrigin**  
This is a string operator that you can use to match on the network origin, either `Internet` or `VPC`\. The following example matches only access points with a VPC origin\.  

```
"Condition" : {
    "StringEquals": {
        "s3:AccessPointNetworkOrigin": "VPC"
    }
}
```

For more information about using condition keys with Amazon S3, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\.

### Delegating access control to access points<a name="access-points-delegating-control"></a>

You can delegate access control for a bucket to the bucket's access points\. The following example bucket policy allows full access to all access points owned by the bucket owner's account\. Thus, all access to this bucket is controlled by the policies attached to its access points\. We recommend configuring your buckets this way for all use cases that don't require direct access to the bucket\.

**Example Bucket policy delegating access control to access points**  

```
{
    "Version": "2012-10-17",
    "Statement" : [
    {
        "Effect": "Allow",
        "Principal" : { "AWS": "*" },
        "Action" : "*",
        "Resource" : [ "Bucket ARN", "Bucket ARN/*"],
        "Condition": {
            "StringEquals" : { "s3:DataAccessPointAccount" : "Bucket owner's account ID" }
        }
    }]
}
```

### Access point policy examples<a name="access-point-policy-examples"></a>

The following examples demonstrate how to create IAM policies to control requests made through an access point\.

**Note**  
Permissions granted in an access point policy are only effective if the underlying bucket also allows the same access\. You can accomplish this in two ways:  
\(Recommended\) Delegate access control from the bucket to the access point as described in [Delegating access control to access points](#access-points-delegating-control)\.
Add the same permissions contained in the access point policy to the underlying bucket's policy\. The first access point policy example demonstrates how to modify the underlying bucket policy to allow the necessary access\.

**Example Access point policy grant**  
The following access point policy grants IAM user `Alice` in account `123456789012` permissions to `GET` and `PUT` objects with the prefix `Alice/` through access point `my-access-point` in account `123456789012`\.  

```
{
    "Version":"2012-10-17",
    "Statement": [
    {
        "Effect": "Allow",
        "Principal": {
            "AWS": "arn:aws:iam::123456789012:user/Alice"
        },
        "Action": ["s3:GetObject", "s3:PutObject"],
        "Resource": "arn:aws:s3:us-west-2:123456789012:accesspoint/my-access-point/object/Alice/*"
    }]
}
```

**Note**  
For the access point policy to effectively grant access to `Alice`, the underlying bucket must also allow the same access to `Alice`\. You can delegate access control from the bucket to the access point as described in [Delegating access control to access points](#access-points-delegating-control)\. Or, you can add the following policy to the underlying bucket to grant the necessary permissions to Alice\. Note that the `Resource` entry differs between the access point and bucket policies\.   

```
{
    "Version": "2012-10-17",
    "Statement": [
    {
        "Effect": "Allow",
        "Principal": {
            "AWS": "arn:aws:iam::123456789012:user/Alice"
        },
        "Action": ["s3:GetObject", "s3:PutObject"],
        "Resource": "arn:aws:s3:::awsexamplebucket1/Alice/*"
    }]    
}
```

**Example Access point policy with tag condition**  
The following access point policy grants IAM user `Bob` in account `123456789012` permissions to `GET` objects through access point `my-access-point` in account `123456789012` that have the tag key `data` set with a value of `finance`\.  

```
{
    "Version":"2012-10-17",
    "Statement": [
    {
        "Effect":"Allow",
        "Principal" : {
            "AWS": "arn:aws:iam::123456789012:user/Bob"
        },
        "Action":"s3:GetObject",
        "Resource" : "arn:aws:s3:us-west-2:123456789012:accesspoint/my-access-point/object/*",
        "Condition" : {
            "StringEquals": {
                "s3:ExistingObjectTag/data": "finance"
            }
        }
    }]
}
```

**Example Access point policy allowing bucket listing**  
The following access point policy allows IAM user `Charles` in account `123456789012` permission to view the objects contained in the bucket underlying access point `my-access-point` in account `123456789012`\.  

```
{
    "Version":"2012-10-17",
    "Statement": [
    {
        "Effect": "Allow",
        "Principal": {
            "AWS": "arn:aws:iam::123456789012:user/Charles"
        },
        "Action": "s3:ListBucket",
        "Resource": "arn:aws:s3:us-west-2:123456789012:accesspoint/my-access-point"
    }]
}
```

**Example Service control policy**  
The following service control policy requires all new access points to be created with a VPC network origin\. With this policy in place, users in your organization can't create new access points that are accessible from the internet\.  

```
{
    "Version": "2012-10-17",
    "Statement": [
    {
        "Effect": "Deny",
        "Action": "s3:CreateAccessPoint",
        "Resource": "*",
        "Condition": {
            "StringNotEquals": {
                "s3:AccessPointNetworkOrigin": "VPC"
            }
        }
    }]
}
```

**Example Bucket policy to limit S3 operations to VPC network origins**  
The following bucket policy limits access to all S3 object operations for bucket `examplebucket` to access points with a VPC network origin\.  
Before using a statement like this example, make sure you don't need to use features that aren't supported by access points, such as Cross\-Region Replication\.

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Principal": "*",
            "Action": [
                "s3:AbortMultipartUpload",
                "s3:BypassGovernanceRetention",
                "s3:DeleteObject",
                "s3:DeleteObjectTagging",
                "s3:DeleteObjectVersion",
                "s3:DeleteObjectVersionTagging",
                "s3:GetObject",
                "s3:GetObjectAcl",
                "s3:GetObjectLegalHold",
                "s3:GetObjectRetention",
                "s3:GetObjectTagging",
                "s3:GetObjectVersion",
                "s3:GetObjectVersionAcl",
                "s3:GetObjectVersionTagging",
                "s3:ListMultipartUploadParts",
                "s3:PutObject",
                "s3:PutObjectAcl",
                "s3:PutObjectLegalHold",
                "s3:PutObjectRetention",
                "s3:PutObjectTagging",
                "s3:PutObjectVersionAcl",
                "s3:PutObjectVersionTagging",
                "s3:RestoreObject"
            ],
            "Resource": "arn:aws:s3:::examplebucket/*",
            "Condition": {
                "StringNotEquals": {
                    "s3:AccessPointNetworkOrigin": "VPC"
                }
            }
        }
    ]
}
```


# Using cost allocation S3 bucket tags<a name="CostAllocTagging"></a>

To track the storage cost or other criteria for individual projects or groups of projects, label your Amazon S3 buckets using cost allocation tags\. A *cost allocation tag* is a key\-value pair that you associate with an S3 bucket\. After you activate cost allocation tags, AWS uses the tags to organize your resource costs on your cost allocation report\. Cost allocation tags can only be used to label buckets\. For information about tags used for labeling objects, see [Object tagging](object-tagging.md)\.

The *cost allocation report* lists the AWS usage for your account by product category and AWS Identity and Access Management \(IAM\) user\. The report contains the same line items as the detailed billing report \(see [Understanding your AWS billing and usage reports for Amazon S3](aws-usage-report-understand.md)\) and additional columns for your tag keys\.

AWS provides two types of cost allocation tags, an AWS\-generated tag and user\-defined tags\. AWS defines, creates, and applies the AWS\-generated `createdBy` tag for you after an Amazon S3 CreateBucket event\. You define, create, and apply *user\-defined* tags to your S3 bucket\.

You must activate both types of tags separately in the Billing and Cost Management console before they can appear in your billing reports\. For more information about AWS\-generated tags, see [ AWS\-Generated Cost Allocation Tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/aws-tags.html)\. 
+ To create tags in the console, see [How Do I View the Properties for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/view-bucket-properties.html) in the *Amazon Simple Storage Service Console User Guide*\.
+ To create tags using the Amazon S3 API, see [PUT Bucket tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTtagging.html) in the *Amazon Simple Storage Service API Reference*\.
+ To create tags using the AWS CLI, see [put\-bucket\-tagging](https://docs.aws.amazon.com/cli/latest/reference/s3api/put-bucket-tagging.html) in the AWS CLI Command Reference\.
+ For more information about activating tags, see [Using cost allocation tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html) in the *AWS Billing and Cost Management User Guide*\.

**User\-defined cost allocation tags**  
A user\-defined cost allocation tag has the following components:
+ The tag key\. The tag key is the name of the tag\. For example, in the tag project/Trinity, project is the key\. The tag key is a case\-sensitive string that can contain 1 to 128 Unicode characters\. 
+ The tag value\. The tag value is a required string\. For example, in the tag project/Trinity, Trinity is the value\. The tag value is a case\-sensitive string that can contain from 0 to 256 Unicode characters\.

For details on the allowed characters for user\-defined tags and other restrictions, see [User\-Defined Tag Restrictions](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocation-tag-restrictions.html) in the *AWS Billing and Cost Management User Guide*\. For more information about user\-defined tags, see [User\-Defined Cost Allocation Tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html) in the *AWS Billing and Cost Management User Guide*\.

**S3 bucket tags**  
Each S3 bucket has a tag set\. A *tag set* contains all of the tags that are assigned to that bucket\. A tag set can contain as many as 50 tags, or it can be empty\. Keys must be unique within a tag set, but values in a tag set don't have to be unique\. For example, you can have the same value in tag sets named project/Trinity and cost\-center/Trinity\.

Within a bucket, if you add a tag that has the same key as an existing tag, the new value overwrites the old value\.

AWS doesn't apply any semantic meaning to your tags\. We interpret tags strictly as character strings\. 

To add, list, edit, or delete tags, you can use the Amazon S3 console, the AWS Command Line Interface \(AWS CLI\), or the Amazon S3 API\. 

## More Info<a name="CostAllocTagging-more-info"></a>
+ [ Using Cost Allocation Tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html) in the *AWS Billing and Cost Management User Guide*
+ [Understanding your AWS billing and usage reports for Amazon S3](aws-usage-report-understand.md)
+ [AWS Billing reports for Amazon S3](aws-billing-reports.md)


# Copying objects using the multipart upload API<a name="CopyingObjctsMPUapi"></a>

 The examples in this section show you how to copy objects greater than 5 GB using the multipart upload API\. You can copy objects less than 5 GB in a single operation\. For more information, see [Copying Objects in a Single Operation](CopyingObjectsUsingAPIs.md)\. 

**Topics**
+ [Copy an object using the AWS SDK for Java multipart upload API](CopyingObjctsUsingLLJavaMPUapi.md)
+ [Copy an Amazon S3 object using the AWS SDK for \.NET multipart upload API](CopyingObjctsUsingLLNetMPUapi.md)
+ [Copy object using the REST multipart upload API](CopyingObjctsUsingRESTMPUapi.md)


# Using the AWS Amplify JavaScript Library<a name="using-aws-amplify"></a>

AWS Amplify is an open source JavaScript library for web and mobile developers who build cloud\-enabled applications\. AWS Amplify provides customizable UI components and a declarative interface to work with an S3 bucket, along with other high\-level categories for AWS services\. 

 To get started using the AWS Amplify JavaScript library, choose one of the following links: 
+ [Getting Started with the AWS Amplify Library for the Web](https://docs.aws.amazon.com/aws-mobile/latest/developerguide/web-getting-started.html)
+ [Getting Started with the AWS Amplify Library for React Native ](https://docs.aws.amazon.com/aws-mobile/latest/developerguide/react-native-getting-started.html)

For more information about AWS Amplify, see [AWS Amplify](https://github.com/aws/aws-amplify) on [GitHub](https://github.com/about)\.

## More Info<a name="using-aws-amplify-moreinfo"></a>

[Using the AWS Mobile SDKs for iOS and Android ](using-mobile-sdks.md)


# Enabling cross\-origin resource sharing \(CORS\) using the AWS SDK for Java<a name="ManageCorsUsingJava"></a>

You can use the AWS SDK for Java to manage cross\-origin resource sharing \(CORS\) for a bucket\. For more information about CORS, see [Cross\-origin resource sharing \(CORS\)](cors.md)\.

**Example**  
 The following example:   
+ Creates a CORS configuration and sets the configuration on a bucket
+ Retrieves the configuration and modifies it by adding a rule
+ Adds the modified configuration to the bucket
+ Deletes the configuration
 For instructions on how to create and test a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketCrossOriginConfiguration;
import com.amazonaws.services.s3.model.CORSRule;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

public class CORS {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        // Create two CORS rules.
        List<CORSRule.AllowedMethods> rule1AM = new ArrayList<CORSRule.AllowedMethods>();
        rule1AM.add(CORSRule.AllowedMethods.PUT);
        rule1AM.add(CORSRule.AllowedMethods.POST);
        rule1AM.add(CORSRule.AllowedMethods.DELETE);
        CORSRule rule1 = new CORSRule().withId("CORSRule1").withAllowedMethods(rule1AM)
                .withAllowedOrigins(Arrays.asList("http://*.example.com"));

        List<CORSRule.AllowedMethods> rule2AM = new ArrayList<CORSRule.AllowedMethods>();
        rule2AM.add(CORSRule.AllowedMethods.GET);
        CORSRule rule2 = new CORSRule().withId("CORSRule2").withAllowedMethods(rule2AM)
                .withAllowedOrigins(Arrays.asList("*")).withMaxAgeSeconds(3000)
                .withExposedHeaders(Arrays.asList("x-amz-server-side-encryption"));

        List<CORSRule> rules = new ArrayList<CORSRule>();
        rules.add(rule1);
        rules.add(rule2);

        // Add the rules to a new CORS configuration.
        BucketCrossOriginConfiguration configuration = new BucketCrossOriginConfiguration();
        configuration.setRules(rules);

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Add the configuration to the bucket.
            s3Client.setBucketCrossOriginConfiguration(bucketName, configuration);

            // Retrieve and display the configuration.
            configuration = s3Client.getBucketCrossOriginConfiguration(bucketName);
            printCORSConfiguration(configuration);

            // Add another new rule.
            List<CORSRule.AllowedMethods> rule3AM = new ArrayList<CORSRule.AllowedMethods>();
            rule3AM.add(CORSRule.AllowedMethods.HEAD);
            CORSRule rule3 = new CORSRule().withId("CORSRule3").withAllowedMethods(rule3AM)
                    .withAllowedOrigins(Arrays.asList("http://www.example.com"));

            rules = configuration.getRules();
            rules.add(rule3);
            configuration.setRules(rules);
            s3Client.setBucketCrossOriginConfiguration(bucketName, configuration);

            // Verify that the new rule was added by checking the number of rules in the configuration.
            configuration = s3Client.getBucketCrossOriginConfiguration(bucketName);
            System.out.println("Expected # of rules = 3, found " + configuration.getRules().size());

            // Delete the configuration.
            s3Client.deleteBucketCrossOriginConfiguration(bucketName);
            System.out.println("Removed CORS configuration.");

            // Retrieve and display the configuration to verify that it was
            // successfully deleted.
            configuration = s3Client.getBucketCrossOriginConfiguration(bucketName);
            printCORSConfiguration(configuration);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void printCORSConfiguration(BucketCrossOriginConfiguration configuration) {
        if (configuration == null) {
            System.out.println("Configuration is null.");
        } else {
            System.out.println("Configuration has " + configuration.getRules().size() + " rules\n");

            for (CORSRule rule : configuration.getRules()) {
                System.out.println("Rule ID: " + rule.getId());
                System.out.println("MaxAgeSeconds: " + rule.getMaxAgeSeconds());
                System.out.println("AllowedMethod: " + rule.getAllowedMethods());
                System.out.println("AllowedOrigins: " + rule.getAllowedOrigins());
                System.out.println("AllowedHeaders: " + rule.getAllowedHeaders());
                System.out.println("ExposeHeader: " + rule.getExposedHeaders());
                System.out.println();
            }
        }
    }
}
```


# Configure Requester Pays by using the Amazon S3 console<a name="configure-requester-pays-console"></a>

 You can configure a bucket for Requester Pays by using the Amazon S3 console\.

**To enable Requester Pays for an S3 bucket**

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose the name of the bucket that you want to enable Requester Pays for\.

1. Choose **Properties**\.

1. Under **Requester pays**, choose **Edit**\.

1. Choose **Enable**, and choose **Save changes**\.

   Amazon S3 enables Requester Pays for your bucket and displays your **Bucket overview**\. Under **Requester pays**, you see **Enabled**


# S3 Object Lock overview<a name="object-lock-overview"></a>

You can use S3 Object Lock to store objects using a *write\-once\-read\-many* \(WORM\) model\. It can help you prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely\. You can use S3 Object Lock to meet regulatory requirements that require WORM storage, or add an extra layer of protection against object changes and deletion\. 

For information about managing the lock status of your Amazon S3 objects, see [Managing Amazon S3 object locks](object-lock-managing.md)\.

**Note**  
 S3 buckets with S3 Object Lock cannot be used as destination buckets for [Amazon S3 server access logging](ServerLogs.md)

The following sections describe the main features of S3 Object Lock\.

**Topics**
+ [Retention modes](#object-lock-retention-modes)
+ [Retention periods](#object-lock-retention-periods)
+ [Legal holds](#object-lock-legal-holds)
+ [Bucket configuration](#object-lock-bucket-config)
+ [Required permissions](#object-lock-permissions)

## Retention modes<a name="object-lock-retention-modes"></a>

S3 Object Lock provides two *retention modes*:
+ Governance mode
+ Compliance mode

These retention modes apply different levels of protection to your objects\. You can apply either retention mode to any object version that is protected by Object Lock\.

### <a name="object-lock-governance-mode"></a>

In *governance* mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions\. With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary\. You can also use governance mode to test retention\-period settings before creating a compliance\-mode retention period\. To override or remove governance\-mode retention settings, a user must have the `s3:BypassGovernanceRetention` permission and must explicitly include `x-amz-bypass-governance-retention:true` as a request header with any request that requires overriding governance mode\. 

**Note**  
The Amazon S3 console by default includes the `x-amz-bypass-governance-retention:true` header\. If you try to delete objects protected by *governance* mode and have `s3:BypassGovernanceRetention` or `s3:GetBucketObjectLockConfiguration` permissions, the operation will succeed\. 

### <a name="object-lock-compliance-mode"></a>

In *compliance* mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account\. When an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened\. Compliance mode ensures that an object version can't be overwritten or deleted for the duration of the retention period\.

**Note**  
Updating an object version's metadata, as occurs when you place or alter an Object Lock, doesn't overwrite the object version or reset its `Last-Modified` timestamp\.

## Retention periods<a name="object-lock-retention-periods"></a>

A *retention period* protects an object version for a fixed amount of time\. When you place a retention period on an object version, Amazon S3 stores a timestamp in the object version's metadata to indicate when the retention period expires\. After the retention period expires, the object version can be overwritten or deleted unless you also placed a legal hold on the object version\.

You can place a retention period on an object version either explicitly or through a bucket default setting\. When you apply a retention period to an object version explicitly, you specify a *Retain Until Date* for the object version\. Amazon S3 stores the Retain Until Date setting in the object version's metadata and protects the object version until the retention period expires\.

When you use bucket default settings, you don't specify a Retain Until Date\. Instead, you specify a duration, in either days or years, for which every object version placed in the bucket should be protected\. When you place an object in the bucket, Amazon S3 calculates a Retain Until Date for the object version by adding the specified duration to the object version's creation timestamp\. It stores the Retain Until Date in the object version's metadata\. The object version is then protected exactly as though you explicitly placed a lock with that retention period on the object version\.

**Note**  
If your request to place an object version in a bucket contains an explicit retention mode and period, those settings override any bucket default settings for that object version\.

Like all other Object Lock settings, retention periods apply to individual object versions\. Different versions of a single object can have different retention modes and periods\.

For example, suppose that you have an object that is 15 days into a 30\-day retention period, and you `PUT` an object into Amazon S3 with the same name and a 60\-day retention period\. In this case, your `PUT` succeeds, and Amazon S3 creates a new version of the object with a 60\-day retention period\. The older version maintains its original retention period and becomes deletable in 15 days\.

You can extend a retention period after you've applied a retention setting to an object version\. To do this, submit a new lock request for the object version with a `Retain Until Date` that is later than the one currently configured for the object version\. Amazon S3 replaces the existing retention period with the new, longer period\. Any user with permissions to place an object retention period can extend a retention period for an object version locked in either mode\.

## Legal holds<a name="object-lock-legal-holds"></a>

Object Lock also enables you to place a *legal hold* on an object version\. Like a retention period, a legal hold prevents an object version from being overwritten or deleted\. However, a legal hold doesn't have an associated retention period and remains in effect until removed\. Legal holds can be freely placed and removed by any user who has the `s3:PutObjectLegalHold` permission\. For a complete list of Amazon S3 permissions, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\.

Legal holds are independent from retention periods\. As long as the bucket that contains the object has Object Lock enabled, you can place and remove legal holds regardless of whether the specified object version has a retention period set\. Placing a legal hold on an object version doesn't affect the retention mode or retention period for that object version\. For example, suppose that you place a legal hold on an object version while the object version is also protected by a retention period\. If the retention period expires, the object doesn't lose its WORM protection\. Rather, the legal hold continues to protect the object until an authorized user explicitly removes it\. Similarly, if you remove a legal hold while an object version has a retention period in effect, the object version remains protected until the retention period expires\.

## Bucket configuration<a name="object-lock-bucket-config"></a>

To use Object Lock, you must enable it for a bucket\. You can also optionally configure a default retention mode and period that applies to new objects that are placed in the bucket\.

### Enabling S3 Object Lock<a name="object-lock-bucket-config-enable"></a>

Before you can lock any objects, you have to configure a bucket to use S3 Object Lock\. To do this, you specify when you create the bucket that you want to enable Object Lock\. After you configure a bucket for Object Lock, you can lock objects in that bucket using retention periods, legal holds, or both\.

**Note**  
You can only enable Object Lock for new buckets\. If you want to turn on Object Lock for an existing bucket, contact AWS Support\.
When you create a bucket with Object Lock enabled, Amazon S3 automatically enables versioning for the bucket\.
Once you create a bucket with Object Lock enabled, you can't disable Object Lock or suspend versioning for the bucket\.

For information about enabling Object Lock on the console, see [How Do I Lock an Amazon S3 Object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/object-lock.html) in the *Amazon Simple Storage Service Console User Guide*\.

### Default retention settings<a name="object-lock-bucket-config-defaults"></a>

When you turn on Object Lock for a bucket, the bucket can store protected objects\. However, the setting doesn't automatically protect objects that you put into the bucket\. If you want to automatically protect object versions that are placed in the bucket, you can configure a default retention period\. Default settings apply to all new objects that are placed in the bucket, unless you explicitly specify a different retention mode and period for an object when you create it\.

**Tip**  
If you want to enforce the bucket default retention mode and period for all new object versions placed in a bucket, set the bucket defaults and deny users permission to configure object retention settings\. Amazon S3 then applies the default retention mode and period to new object versions placed in the bucket, and rejects any request to put an object that includes a retention mode and setting\.

Bucket default settings require both a mode and a period\. A bucket default mode is either *governance* or *compliance*\. For more information, see [Retention modes](#object-lock-retention-modes)\. 

A default retention period is described not as a timestamp, but as a period either in days or in years\. When you place an object version in a bucket with a default retention period, Object Lock calculates a *Retain Until Date*\. It does this by adding the default retention period to the creation timestamp for the object version\. Amazon S3 stores the resulting timestamp as the object version's Retain Until Date, as if you had calculated the timestamp manually and placed it on the object version yourself\.

Default settings apply only to new objects that are placed in the bucket\. Placing a default retention setting on a bucket doesn't place any retention settings on objects that already exist in the bucket\.

**Important**  
Object locks apply to individual object versions only\. If you place an object in a bucket that has a default retention period, and you don't explicitly specify a retention period for that object, Amazon S3 creates the object with a retention period that matches the bucket default\. After the object is created, its retention period is independent from the bucket's default retention period\. Changing a bucket's default retention period doesn't change the existing retention period for any objects in that bucket\.

**Note**  
If you configure a default retention period on a bucket, requests to upload objects in such a bucket must include the `Content-MD5` header\. For more information, see [Put Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) in the *Amazon Simple Storage Service API Reference*\. 

## Required permissions<a name="object-lock-permissions"></a>

 Object Lock operations require specific permissions\. For more information about required permissions, see [Example — Object Operations](using-with-s3-actions.md#using-with-s3-actions-related-to-objects)\. For information about using conditions with permissions, see [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)\.


# Amazon S3 on Outposts restrictions and limitations<a name="S3OnOutpostsRestrictionsLimitations"></a>

Consider the following restrictions and limitations as you set up Amazon S3 on Outposts\.

**Topics**
+ [Specifications](#S3OnOutpostsSpecifications)
+ [Data consistency model](#S3OnOutpostsDataConsistency)
+ [Supported API operations](#S3OnOutpostsAPILimitations)
+ [Unsupported Amazon S3 features](#S3OnOutpostsFeatureLimitations)
+ [Network restrictions](#S3OnOutpostsConnectivityRestrictions)

## Amazon S3 on Outposts specifications<a name="S3OnOutpostsSpecifications"></a>
+ Maximum Outposts bucket size is 50 TB\.
+ Maximum number of Outposts buckets per Outpost is 100\.
+ Outposts buckets can only be accessed using access points and endpoints\.
+ Maximum number of access points per Outposts bucket is 10\.
+ Access point policies are limited to 20 KB in size\.
+ The S3 on Outposts bucket owner account is always the owner of all objects in the bucket\.
+ Only the S3 on Outposts bucket owner account can perform operations on the bucket\.
+ Object size limitations are consistent with Amazon S3\.
+ All objects stored on S3 on Outposts are stored in the `OUTPOSTS` storage class\.
+ All objects stored in the `OUTPOSTS` storage class are stored using server\-side encryption with Amazon S3\-managed encryption keys \(SSE\-S3\) by default\. You can also explicity choose to store objects using server\-side encryption with customer\-provided encryption keys \(SSE\-C\)\.
+ If there is not enough space to store an object on your Outpost, the API will return an insufficient capacity exception \(ICE\)\. 

## Amazon S3 on Outposts data consistency model<a name="S3OnOutpostsDataConsistency"></a>

Amazon S3 on Outposts provides read\-after\-write consistency for PUTS of new objects in your Amazon S3 bucket with one caveat\. The caveat is that if you make a HEAD or GET request to a key name before the object is created, then create the object shortly after that, a subsequent GET might not return the object due to eventual consistency\.

Amazon S3 on Outposts offers eventual consistency for overwrite PUTS and DELETES in all Regions\.

Updates to a single key are atomic\. For example, if you PUT to an existing key, a subsequent read might return the old data or the updated data, but it never returns corrupted or partial data\. With the eventual consistency model, you might observe the following behaviors:
+ A process writes a new object to S3OutpostsLong; and immediately lists keys within its bucket\. Until the change is fully propagated, the object might not appear in the list\.
+ A process replaces an existing object and immediately tries to read it\. Until the change is fully propagated, Amazon S3 on Outposts might return the previous data\.
+ A process deletes an existing object and immediately tries to read it\. Until the deletion is fully propagated, Amazon S3 on Outposts might return the deleted data\.
+ A process deletes an existing object and immediately lists keys within its bucket\. Until the deletion is fully propagated, Amazon S3 on Outposts might list the deleted object\.

## API operations supported by Amazon S3 on Outposts<a name="S3OnOutpostsAPILimitations"></a>

Amazon S3 on Outposts is designed to use the same object APIs as Amazon S3\. Therefore, you can use many of your existing code and policies by passing the S3 on Outposts Amazon Resource Name \(ARN\) as your identifier\.

Amazon S3 on Outposts supports the following API operations:
+ `AbortMultipartUpload`
+ `CompleteMultipartUpload`
+ `CopyObject`
+ `CreateMultipartUpload`
+ `DeleteObject`
+ `DeleteObjects`
+ `DeleteObjectTagging`
+ `GetObject`
+ `GetObjectTagging`
+ `HeadObject`
+ `HeadBucket`
+ `ListMultipartUploads`
+ `ListObjects`
+ `ListObjectsV2`
+ `ListParts`
+ `PutObject`
+ `PutObjectTagging`
+ `UploadPart`
+ `UploadPartCopy`

## Amazon S3 features not supported by Amazon S3 on Outposts<a name="S3OnOutpostsFeatureLimitations"></a>

Several Amazon S3 features are currently not supported by Amazon S3 on Outposts\. Any attempts to use them are rejected\.
+ Access control list \(ACL\)
+ CORS
+ Batch Operations
+ Inventory reports
+ Changing the default bucket encryption
+ Public buckets
+ MFA Delete
+ Lifecycle transitions limited to object deletion and aborting incomplete multipart uploads
+ Object Lock legal hold
+ Object Lock retention
+ Object Versioning
+ SSE\-KMS
+ Replication
+ Replication Time Control
+ Amazon CloudWatch Request Metrics
+ Metrics configuration
+ Transfer acceleration
+ Event notifications
+ Requester Pays buckets
+ S3 Select
+ Torrent
+ Lambda events
+ Server access logging
+ Presigned URLs
+ HTTP POST requests
+ SOAP
+ Website access

## Amazon S3 on Outposts network restrictions<a name="S3OnOutpostsConnectivityRestrictions"></a>
+ You will need to create and configure an endpoint in order to route requests to an Amazon S3 on Outposts access point\. The following limits apply endpoints for S3 on Outposts:
  + Each virtual private cloud \(VPC\) on your AWS Outposts can have one associated endpoint, and you can have up to three endpoints per AWS Outposts\.
  + Multiple access point can be mapped to the same endpoint\.
  + endpoints can only be added to VPCs with CIDR blocks in the subspaces of the following CIDR ranges\.
    + 10\.0\.10\.0/24
    + 172\.16\.0\.0/12
    + 192\.168\.0\.0/16
+ An endpoint can only be created within a single CIDR block\.
+ Connections from peered VPCs to an endpoint are not supported\.
+ The subnet used to create an endpoint must contain four IP addresses for S3 on Outposts to use\.
+ A CIDR range used to create an endpoint for an outpost cannot be reused for another endpoint within that VPC\.


# How Amazon S3 Authorizes a Request for an Object Operation<a name="access-control-auth-workflow-object-operation"></a>

When Amazon S3 receives a request for an object operation, it converts all the relevant permissions—resource\-based  permissions \(object access control list \(ACL\), bucket policy, bucket ACL\) and IAM user policies—into a set of policies to be evaluated at run time\. It then evaluates the resulting set of policies in a series of steps\. In each step, it evaluates a subset of policies in three specific contexts—user context, bucket context, and object context\.

1. **User context** – If the requester is an IAM principal, the principal must have permission from the parent AWS account to which it belongs\. In this step, Amazon S3 evaluates a subset of policies owned by the parent account \(also referred as the context authority\)\. This subset of policies includes the user policy that the parent attaches to the principal\. If the parent also owns the resource in the request \(bucket, object\), Amazon S3 evaluates the corresponding resource policies \(bucket policy, bucket ACL, and object ACL\) at the same time\. 
**Note**  
If the parent AWS account owns the resource \(bucket or object\), it can grant resource permissions to its IAM principal by using either the user policy or the resource policy\. 

1. **Bucket context** – In this context, Amazon S3 evaluates policies owned by the AWS account that owns the bucket\.

   If the AWS account that owns the object in the request is not same as the bucket owner, in the bucket context Amazon S3 checks the policies if the bucket owner has explicitly denied access to the object\. If there is an explicit deny set on the object, Amazon S3 does not authorize the request\. 

1. **Object context** – The requester must have permissions from the object owner to perform a specific object operation\. In this step, Amazon S3 evaluates the object ACL\. 
**Note**  
If bucket and object owners are the same, access to the object can be granted in the bucket policy, which is evaluated at the bucket context\. If the owners are different, the object owners must use an object ACL to grant permissions\. If the AWS account that owns the object is also the parent account to which the IAM principal belongs, it can configure object permissions in a user policy, which is evaluated at the user context\. For more information about using these access policy alternatives, see [Guidelines for using the available access policy options](access-policy-alternatives-guidelines.md)\.

 The following is an illustration of the context\-based evaluation for an object operation\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/AccessControlAuthorizationFlowObjectResource.png)

## Example 1: Object Operation Request<a name="access-control-auth-workflow-object-operation-example1"></a>

In this example, IAM user Jill, whose parent AWS account is 1111\-1111\-1111, sends an object operation request \(for example, Get object\) for an object owned by AWS account 3333\-3333\-3333 in a bucket owned by AWS account 2222\-2222\-2222\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/example50-policy-eval-logic.png)

Jill will need permission from the parent AWS account, the bucket owner, and the object owner\. Amazon S3 evaluates the context as follows:

1. Because the request is from an IAM principal, Amazon S3 evaluates the user context to verify that the parent AWS account 1111\-1111\-1111 has given Jill permission to perform the requested operation\. If she has that permission, Amazon S3 evaluates the bucket context\. Otherwise, Amazon S3 denies the request\.

1.  In the bucket context, the bucket owner, AWS account 2222\-2222\-2222, is the context authority\. Amazon S3 evaluates the bucket policy to determine if the bucket owner has explicitly denied Jill access to the object\. 

1. In the object context, the context authority is AWS account 3333\-3333\-3333, the object owner\. Amazon S3 evaluates the object ACL to determine if Jill has permission to access the object\. If she does, Amazon S3 authorizes the request\. 


# Further details<a name="ErrorDetails"></a>

Many error responses contain additional structured data meant to be read and understood by a developer diagnosing programming errors\. For example, if you send a Content\-MD5 header with a REST PUT request that doesn't match the digest calculated on the server, you receive a BadDigest error\. The error response also includes as detail elements the digest we calculated, and the digest you told us to expect\. During development, you can use this information to diagnose the error\. In production, a well\-behaved program might include this information in its error log\.


# Listing Keys Using the AWS SDK for PHP<a name="ListingObjectKeysUsingPHP"></a>

This topic guides you through using classes from version 3 of the AWS SDK for PHP to list the object keys contained in an Amazon S3 bucket\. 

 This topic assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\. 

To list the object keys contained in a bucket using the AWS SDK for PHP you first must list the objects contained in the bucket and then extract the key from each of the listed objects\. When listing objects in a bucket you have the option of using the low\-level [Aws\\S3\\S3Client::listObjects\(\)](https://docs.aws.amazon.com/aws-sdk-php/v3/api/api-s3-2006-03-01.html#listobjects) method or the high\-level [Aws\\ResultPaginator](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.ResultPaginator.html) class\. 

The low\-level `listObjects()` method maps to the underlying Amazon S3 REST API\. Each `listObjects()` request returns a page of up to 1,000 objects\. If you have more than 1,000 objects in the bucket, your response will be truncated and you will need to send another `listObjects()` request to retrieve the next set of 1,000 objects\. 

You can use the high\-level `ListObjects` paginator to make your task of listing the objects contained in a bucket a bit easier\. To use the `ListObjects` paginator to create a list of objects you run the Amazon S3 client [getPaginator\(\)](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.AwsClientInterface.html#_getPaginator) method that is inherited from [Aws/AwsClientInterface](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.AwsClientInterface.html) class with the `ListObjects` command as the first argument and an array to contain the returned objects from the specified bucket as the second argument\. When used as a `ListObjects` paginator the `getPaginator()` method returns all the objects contained in the specified bucket\. There is no 1,000 object limit, so you don't need to worry if the response is truncated or not\.

The following tasks guide you through using the PHP Amazon S3 client methods to list the objects contained in a bucket from which you can list the object keys\.

**Example of Listing Object Keys**  
The following PHP example demonstrates how to list the keys from a specified bucket\. It shows how to use the high\-level `getIterator()` method to list the objects in a bucket and then how to extract the key from each of the objects in the list\. It also show how to use the low\-level `listObjects()` method to list the objects in a bucket and then how to extract the key from each of the objects in the list returned\. For information about running the PHP examples in this guide, go to [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\.   

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';

// Instantiate the client.
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Use the high-level iterators (returns ALL of your objects).
try {
    $results = $s3->getPaginator('ListObjects', [
        'Bucket' => $bucket
    ]);

    foreach ($results as $result) {
        foreach ($result['Contents'] as $object) {
            echo $object['Key'] . PHP_EOL;
        }
    }
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}

// Use the plain API (returns ONLY up to 1000 of your objects).
try {
    $objects = $s3->listObjects([
        'Bucket' => $bucket
    ]);
    foreach ($objects['Contents']  as $object) {
        echo $object['Key'] . PHP_EOL;
    }
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}
```

## Related Resources<a name="RelatedResources-ListingObjectKeysUsingPHP"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [ Paginators](https://docs.aws.amazon.com/aws-sdk-php/v3/guide/guide/paginators.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Making requests using the AWS SDKs<a name="MakingAuthenticatedRequests"></a>

**Topics**
+ [Making requests using AWS account or IAM user credentials](AuthUsingAcctOrUserCredentials.md)
+ [Making requests using IAM user temporary credentials](AuthUsingTempSessionToken.md)
+ [Making requests using federated user temporary credentials](AuthUsingTempFederationToken.md)

You can send authenticated requests to Amazon S3 using either the AWS SDK or by making the REST API calls directly in your application\. The AWS SDK API uses the credentials that you provide to compute the signature for authentication\. If you use the REST API directly in your applications, you must write the necessary code to compute the signature for authenticating your request\. For a list of available AWS SDKs go to, [Sample Code and Libraries](https://aws.amazon.com/code/)\. 


# The REST error response<a name="UsingRESTError"></a>

**Topics**
+ [Response headers](#UsingRESTErrorResponseHeaders)
+ [Error response](ErrorResponse.md)

If a REST request results in an error, the HTTP reply has: 
+ An XML error document as the response body 
+ Content\-Type: application/xml
+ An appropriate 3xx, 4xx, or 5xx HTTP status code

Following is an example of a REST Error Response\.

```
1. <?xml version="1.0" encoding="UTF-8"?>
2. <Error>
3.   <Code>NoSuchKey</Code>
4.   <Message>The resource you requested does not exist</Message>
5.   <Resource>/mybucket/myfoto.jpg</Resource> 
6.   <RequestId>4442587FB7D0A2F9</RequestId>
7. </Error>
```

For more information about Amazon S3 errors, go to [ErrorCodeList](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html)\.

## Response headers<a name="UsingRESTErrorResponseHeaders"></a>

Following are response headers returned by all operations:
+ `x-amz-request-id:` A unique ID assigned to each request by the system\. In the unlikely event that you have problems with Amazon S3, Amazon can use this to help troubleshoot the problem\.
+ `x-amz-id-2:` A special token that will help us to troubleshoot problems\.


# Replicating objects created with server\-side encryption \(SSE\) using encryption keys stored in AWS KMS<a name="replication-config-for-kms-objects"></a>

By default, Amazon S3 doesn't replicate objects that are stored at rest using server\-side encryption with customer master keys \(CMKs\) stored in AWS KMS\. This section explains additional configuration that you add to direct Amazon S3 to replicate these objects\. 

**Important**  
Replication of encrypted data is a server\-side process that occurs entirely within Amazon S3\. Replication does not support client\-side encryption\. 

For an example with step\-by\-step instructions, see [Example 4: Replicating encrypted objects](replication-walkthrough-4.md)\. For information about creating a replication configuration, see [Replication](replication.md)\. 

**Topics**
+ [Specifying additional information in the replication configuration](#replication-kms-extra-config)
+ [Granting additional permissions for the IAM role](#replication-kms-extra-permissions)
+ [Granting additional permissions for cross\-account scenarios](#replication-kms-cross-acct-scenario)
+ [AWS KMS transaction limit considerations](#replication-kms-considerations)

## Specifying additional information in the replication configuration<a name="replication-kms-extra-config"></a>

In the replication configuration, you do the following:
+ In the `Destination` configuration, add the symmetric customer managed AWS KMS CMK that you want Amazon S3 to use to encrypt object replicas\. 
+ Explicitly opt in by enabling replication of objects encrypted using AWS KMS CMKs by adding the `SourceSelectionCriteria` element\.

 

```
<ReplicationConfiguration>
   <Rule>
      ...
      <SourceSelectionCriteria>
         <SseKmsEncryptedObjects>
           <Status>Enabled</Status>
         </SseKmsEncryptedObjects>
      </SourceSelectionCriteria>

      <Destination>
          ...
          <EncryptionConfiguration>
             <ReplicaKmsKeyID>AWS KMS key ID for the AWS region of the destination bucket.</ReplicaKmsKeyID>
          </EncryptionConfiguration>
       </Destination>
      ...
   </Rule>
</ReplicationConfiguration>
```

**Important**  
The AWS KMS CMK must have been created in the same AWS Region as the destination buckets\.   
The AWS KMS CMK *must* be valid\. The `PUT` Bucket replication API doesn't check the validity of AWS KMS CMKs\. If you use an invalid CMK, you will receive the 200 OK status code in response, but replication fails\.

The following example shows a replication configuration, which includes optional configuration elements\.

```
<?xml version="1.0" encoding="UTF-8"?>
<ReplicationConfiguration>
   <Role>arn:aws:iam::account-id:role/role-name</Role>
   <Rule>
      <ID>Rule-1</ID>
      <Priority>1</Priority>
      <Status>Enabled</Status>
      <DeleteMarkerReplication>
         <Status>Disabled</Status>
      </DeleteMarkerReplication>
      <Filter>
         <Prefix>Tax</Prefix>
      </Filter>
      <Destination>
         <Bucket>arn:aws:s3:::destination-bucket</Bucket>
         <EncryptionConfiguration>
            <ReplicaKmsKeyID>The AWS KMS key ID for the AWS region of the destination buckets (S3 uses it to encrypt object replicas).</ReplicaKmsKeyID>
         </EncryptionConfiguration>
      </Destination>
      <SourceSelectionCriteria>
         <SseKmsEncryptedObjects>
            <Status>Enabled</Status>
         </SseKmsEncryptedObjects>
      </SourceSelectionCriteria>
   </Rule>
</ReplicationConfiguration>
```

This replication configuration has one rule\. The rule applies to objects with the `Tax` key prefix\. Amazon S3 uses the AWS KMS key ID to encrypt these object replicas\.

## Granting additional permissions for the IAM role<a name="replication-kms-extra-permissions"></a>

To replicate objects that are encrypted at rest under AWS Key Management Service \(AWS KMS\), grant the following additional permissions to the IAM role you specify in the replication configuration\. You grant these permissions by updating the permission policy associated with the IAM role\. Objects created with server\-side encryption using customer\-provided \(SSE\-C\) encryption keys are not replicated\.
+ Permission for the `s3:GetObjectVersionForReplication` action for source objects\. Permission for this action allows Amazon S3 to replicate both unencrypted objects and objects created with server\-side encryption using Amazon S3 managed encryption \(SSE\-S3\) keys or CMKs stored in AWS KMS \(SSE\-KMS\)\.
**Note**  
We recommend that you use the `s3:GetObjectVersionForReplication` action instead of the `s3:GetObjectVersion` action because it provides Amazon S3 with only the minimum permissions necessary for replication\. In addition, permission for the `s3:GetObjectVersion` action allows replication of unencrypted and SSE\-S3\-encrypted objects, but not of objects created using a CMK stored in AWS KMS\. 
+ Permissions for the following AWS KMS actions:
  + `kms:Decrypt` permissions for the AWS KMS CMK used to decrypt the source object
  + `kms:Encrypt` permissions for the AWS KMS CMK used to encrypt the object replica

  We recommend that you restrict these permissions only to the destination buckets and objects using AWS KMS condition keys\. The following example policy shows statements for using AWS KMS with separate destination buckets\. 

  ```
  {
      "Action": ["kms:Decrypt"],
      "Effect": "Allow",
      "Condition": {
          "StringLike": {
              "kms:ViaService": "s3.source-bucket-region.amazonaws.com",
              "kms:EncryptionContext:aws:s3:arn": [
                  "arn:aws:s3:::source-bucket-name/key-prefix1*",
              ]
          }
      },
      "Resource": [
          "List of AWS KMS key ARNs used to encrypt source objects.", 
      ]
  },
  {
      "Action": ["kms:Encrypt"],
      "Effect": "Allow",
      "Condition": {
          "StringLike": {
              "kms:ViaService": "s3.destination-bucket-1-region.amazonaws.com",
              "kms:EncryptionContext:aws:s3:arn": [
                  "arn:aws:s3:::destination-bucket-name-1/key-prefix1*",
              ]
          }
      },
      "Resource": [
           "AWS KMS key ARNs (for the AWS Region of the destination bucket 1). Used to encrypt object replicas created in destination bucket 1.", 
      ]
  },
  {
      "Action": ["kms:Encrypt"],
      "Effect": "Allow",
      "Condition": {
          "StringLike": {
              "kms:ViaService": "s3.destination-bucket-2-region.amazonaws.com",
              "kms:EncryptionContext:aws:s3:arn": [
                  "arn:aws:s3:::destination-bucket-2-name/key-prefix1*",
              ]
          }
      },
      "Resource": [
           "AWS KMS key ARNs (for the AWS Region of destination bucket 2). Used to encrypt object replicas created in destination bucket 2.",
      ]
  }
  ```

  The AWS account that owns the IAM role must have permissions for these AWS KMS actions \(`kms:Encrypt` and `kms:Decrypt`\) for AWS KMS CMKs listed in the policy\. If the AWS KMS CMKs are owned by another AWS account, the CMK owner must grant these permissions to the AWS account that owns the IAM role\. For more information about managing access to these CMKs, see [Using IAM Policies with AWS KMS](https://docs.aws.amazon.com/kms/latest/developerguide/iam-policies.html) in the* AWS Key Management Service Developer Guide*\.

  The following is a complete IAM policy that grants the necessary permissions to replicate unencrypted objects, objects created with server\-side encryption using Amazon S3 managed encryption keys,and CMKs stored in AWS KMS\.
**Note**  
Objects created with server\-side encryption using customer\-provided \(SSE\-C\) encryption keys are not replicated\.

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Effect":"Allow",
         "Action":[
            "s3:GetReplicationConfiguration",
            "s3:ListBucket"
         ],
         "Resource":[
            "arn:aws:s3:::source-bucket"
         ]
      },
      {
         "Effect":"Allow",
         "Action":[
            "s3:GetObjectVersionForReplication",
            "s3:GetObjectVersionAcl"
         ],
         "Resource":[
            "arn:aws:s3:::source-bucket/key-prefix1*"
         ]
      },
      {
         "Effect":"Allow",
         "Action":[
            "s3:ReplicateObject",
            "s3:ReplicateDelete"
         ],
         "Resource":"arn:aws:s3:::destination-bucket/key-prefix1*"
      },
      {
         "Action":[
            "kms:Decrypt"
         ],
         "Effect":"Allow",
         "Condition":{
            "StringLike":{
               "kms:ViaService":"s3.source-bucket-region.amazonaws.com",
               "kms:EncryptionContext:aws:s3:arn":[
                  "arn:aws:s3:::source-bucket-name/key-prefix1*"
               ]
            }
         },
         "Resource":[
           "List of AWS KMS key ARNs used to encrypt source objects."
         ]
      },
      {
         "Action":[
            "kms:Encrypt"
         ],
         "Effect":"Allow",
         "Condition":{
            "StringLike":{
               "kms:ViaService":"s3.destination-bucket-region.amazonaws.com",
               "kms:EncryptionContext:aws:s3:arn":[
                  "arn:aws:s3:::destination-bucket-name/prefix1*"
               ]
            }
         },
         "Resource":[
            "AWS KMS key ARNs (for the AWS Region of the destination buckets) to use for encrypting object replicas"
         ]
      }
   ]
}
```

For more information, see [Using IAM policies with AWS KMS](https://docs.aws.amazon.com/kms/latest/developerguide/iam-policies.html)\.

## Granting additional permissions for cross\-account scenarios<a name="replication-kms-cross-acct-scenario"></a>

In a cross\-account scenario, where *source* and *destination* buckets are owned by different AWS accounts, you can use a customer managed CMK to encrypt object replicas\. However, the CMK owner must grant the source bucket owner permission to use the CMK\. <a name="cross-acct-kms-key-permission"></a>

**To grant the source bucket owner permission to use the AWS KMS CMK \(IAM console\)**

1. Sign in to the AWS Management Console and open the AWS Key Management Service \(AWS KMS\) console at [https://console\.aws\.amazon\.com/kms](https://console.aws.amazon.com/kms)\.

1. To change the AWS Region, use the Region selector in the upper\-right corner of the page\.

1.  To view the keys in your account that you create and manage, in the navigation pane choose **Customer managed keys**\.

1. Choose the CMK\.

1. Under **General configuration**, choose the **Key policy** tab\.

1. Choose **Other AWS Accounts**\.

1. Choose **Add another AWS Account**\.

1. In **arn:aws:iam::**, enter the source bucket account ID\.

1. Choose **Save Changes**\.

**To grant the source bucket owner permission to use the AWS KMS CMK \(AWS CLI\)**
+ For information, see [put\-key\-policy](http://docs.aws.amazon.com/cli/latest/reference/kms/put-key-policy.html) in the* AWS CLI Command Reference*\. For information about the underlying API, see [PutKeyPolicy](http://docs.aws.amazon.com/kms/latest/APIReference/API_PutKeyPolicy.html) in the *[AWS Key Management Service API Reference](https://docs.aws.amazon.com/kms/latest/APIReference/)\.*

## AWS KMS transaction limit considerations<a name="replication-kms-considerations"></a>

When you add many new objects with AWS KMS encryption after enabling cross\-region replication \(CRR\), you might experience throttling \(HTTP 503 Slow Down errors\)\. Throttling occurs when the number of AWS KMS transactions per second exceeds the current limit\. For more information, see [Limits]( http://docs.aws.amazon.com/kms/latest/developerguide/limits.html) in the *AWS Key Management Service Developer Guide*\.

To request a limit increase, use Service Quotas\. For more information, see [Amazon Web Services Limits](https://docs.aws.amazon.com/kms/latest/developerguide/limits.html)\. If Service Quotas isn't supported in your Region, [open an AWS Support case](https://console.aws.amazon.com/support/home#/)\. 


# Managing object lifecycles using the AWS SDK for Java<a name="manage-lifecycle-using-java"></a>

You can use the AWS SDK for Java to manage the S3 Lifecycle configuration of a bucket\. For more information about managing S3 Lifecycle configuration, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

**Note**  
When you add S3 Lifecycle configuration to a bucket, Amazon S3 replaces the bucket's current Lifecycle configuration, if there is one\. To update a configuration, you retrieve it, make the desired changes, and then add the revised configuration to the bucket\.

**Example**  
The following example shows how to use the AWS SDK for Java to add, update, and delete the Lifecycle configuration of a bucket\. The example does the following:  
+ Adds a Lifecycle configuration to a bucket\. 
+ Retrieves the Lifecycle configuration and updates it by adding another rule\. 
+ Adds the modified Lifecycle configuration to the bucket\. Amazon S3 replaces the existing configuration\. 
+ Retrieves the configuration again and verifies that it has the right number of rules by the printing number of rules\.
+ Deletes the Lifecycle configuration and verifies that it has been deleted by attempting to retrieve it again\.
 For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketLifecycleConfiguration;
import com.amazonaws.services.s3.model.BucketLifecycleConfiguration.Transition;
import com.amazonaws.services.s3.model.StorageClass;
import com.amazonaws.services.s3.model.Tag;
import com.amazonaws.services.s3.model.lifecycle.LifecycleAndOperator;
import com.amazonaws.services.s3.model.lifecycle.LifecycleFilter;
import com.amazonaws.services.s3.model.lifecycle.LifecyclePrefixPredicate;
import com.amazonaws.services.s3.model.lifecycle.LifecycleTagPredicate;

import java.io.IOException;
import java.util.Arrays;

public class LifecycleConfiguration {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        // Create a rule to archive objects with the "glacierobjects/" prefix to Glacier immediately.
        BucketLifecycleConfiguration.Rule rule1 = new BucketLifecycleConfiguration.Rule()
                .withId("Archive immediately rule")
                .withFilter(new LifecycleFilter(new LifecyclePrefixPredicate("glacierobjects/")))
                .addTransition(new Transition().withDays(0).withStorageClass(StorageClass.Glacier))
                .withStatus(BucketLifecycleConfiguration.ENABLED);

        // Create a rule to transition objects to the Standard-Infrequent Access storage class 
        // after 30 days, then to Glacier after 365 days. Amazon S3 will delete the objects after 3650 days.
        // The rule applies to all objects with the tag "archive" set to "true". 
        BucketLifecycleConfiguration.Rule rule2 = new BucketLifecycleConfiguration.Rule()
                .withId("Archive and then delete rule")
                .withFilter(new LifecycleFilter(new LifecycleTagPredicate(new Tag("archive", "true"))))
                .addTransition(new Transition().withDays(30).withStorageClass(StorageClass.StandardInfrequentAccess))
                .addTransition(new Transition().withDays(365).withStorageClass(StorageClass.Glacier))
                .withExpirationInDays(3650)
                .withStatus(BucketLifecycleConfiguration.ENABLED);

        // Add the rules to a new BucketLifecycleConfiguration.
        BucketLifecycleConfiguration configuration = new BucketLifecycleConfiguration()
                .withRules(Arrays.asList(rule1, rule2));

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Save the configuration.
            s3Client.setBucketLifecycleConfiguration(bucketName, configuration);

            // Retrieve the configuration.
            configuration = s3Client.getBucketLifecycleConfiguration(bucketName);

            // Add a new rule with both a prefix predicate and a tag predicate.
            configuration.getRules().add(new BucketLifecycleConfiguration.Rule().withId("NewRule")
                    .withFilter(new LifecycleFilter(new LifecycleAndOperator(
                            Arrays.asList(new LifecyclePrefixPredicate("YearlyDocuments/"),
                                    new LifecycleTagPredicate(new Tag("expire_after", "ten_years"))))))
                    .withExpirationInDays(3650)
                    .withStatus(BucketLifecycleConfiguration.ENABLED));

            // Save the configuration.
            s3Client.setBucketLifecycleConfiguration(bucketName, configuration);

            // Retrieve the configuration.
            configuration = s3Client.getBucketLifecycleConfiguration(bucketName);

            // Verify that the configuration now has three rules.
            configuration = s3Client.getBucketLifecycleConfiguration(bucketName);
            System.out.println("Expected # of rules = 3; found: " + configuration.getRules().size());

            // Delete the configuration.
            s3Client.deleteBucketLifecycleConfiguration(bucketName);

            // Verify that the configuration has been deleted by attempting to retrieve it.
            configuration = s3Client.getBucketLifecycleConfiguration(bucketName);
            String s = (configuration == null) ? "No configuration found." : "Configuration found.";
            System.out.println(s);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Using MFA delete<a name="UsingMFADelete"></a>

If a bucket's versioning configuration is MFA Delete–enabled, the bucket owner must include the `x-amz-mfa` request header in requests to permanently delete an object version or change the versioning state of the bucket\. Requests that include `x-amz-mfa` must use HTTPS\. The header's value is the concatenation of your authentication device's serial number, a space, and the authentication code displayed on it\. If you do not include this request header, the request fails\.

For more information about authentication devices, see [https://aws\.amazon\.com/iam/details/mfa/](https://aws.amazon.com/iam/details/mfa/)\.

**Example Deleting an object from an MFA delete enabled bucket**  
The following example shows how to delete `my-image.jpg` \(with the specified version\), which is in a bucket configured with MFA Delete enabled\. Note the space between *\[SerialNumber\]* and *\[AuthenticationCode\]*\. For more information, see [DELETE Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html)\.  

```
1. DELETE /my-image.jpg?versionId=3HL4kqCxf3vjVBH40Nrjfkd HTTPS/1.1
2. Host: bucketName.s3.amazonaws.com
3. x-amz-mfa: 20899872 301749
4. Date: Wed, 28 Oct 2009 22:32:00 GMT
5. Authorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=
```

For more information about enabling MFA delete, see [MFA delete](Versioning.md#MultiFactorAuthenticationDelete)\.


# Upload a file<a name="llJavaUploadFile"></a>

The following example shows how to use the low\-level Java classes to upload a file\. It performs the following steps:
+ Initiates a multipart upload using the `AmazonS3Client.initiateMultipartUpload()` method, and passes in an `InitiateMultipartUploadRequest` object\.
+ Saves the upload ID that the `AmazonS3Client.initiateMultipartUpload()` method returns\. You provide this upload ID for each subsequent multipart upload operation\.
+ Uploads the parts of the object\. For each part, you call the `AmazonS3Client.uploadPart()` method\. You provide part upload information using an `UploadPartRequest` object\. 
+ For each part, saves the ETag from the response of the `AmazonS3Client.uploadPart()` method in a list\. You use the ETag values to complete the multipart upload\.
+ Calls the `AmazonS3Client.completeMultipartUpload()` method to complete the multipart upload\. 

**Example**  
For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class LowLevelMultipartUpload {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ***";
        String filePath = "*** Path to file to upload ***";

        File file = new File(filePath);
        long contentLength = file.length();
        long partSize = 5 * 1024 * 1024; // Set part size to 5 MB. 

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

            // Create a list of ETag objects. You retrieve ETags for each object part uploaded,
            // then, after each individual part has been uploaded, pass the list of ETags to 
            // the request to complete the upload.
            List<PartETag> partETags = new ArrayList<PartETag>();

            // Initiate the multipart upload.
            InitiateMultipartUploadRequest initRequest = new InitiateMultipartUploadRequest(bucketName, keyName);
            InitiateMultipartUploadResult initResponse = s3Client.initiateMultipartUpload(initRequest);

            // Upload the file parts.
            long filePosition = 0;
            for (int i = 1; filePosition < contentLength; i++) {
                // Because the last part could be less than 5 MB, adjust the part size as needed.
                partSize = Math.min(partSize, (contentLength - filePosition));

                // Create the request to upload a part.
                UploadPartRequest uploadRequest = new UploadPartRequest()
                        .withBucketName(bucketName)
                        .withKey(keyName)
                        .withUploadId(initResponse.getUploadId())
                        .withPartNumber(i)
                        .withFileOffset(filePosition)
                        .withFile(file)
                        .withPartSize(partSize);

                // Upload the part and add the response's ETag to our list.
                UploadPartResult uploadResult = s3Client.uploadPart(uploadRequest);
                partETags.add(uploadResult.getPartETag());

                filePosition += partSize;
            }

            // Complete the multipart upload.
            CompleteMultipartUploadRequest compRequest = new CompleteMultipartUploadRequest(bucketName, keyName,
                    initResponse.getUploadId(), partETags);
            s3Client.completeMultipartUpload(compRequest);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Managing tags using the AWS SDK for Java<a name="tagging-manage-javasdk"></a>

The following example shows how to use the AWS SDK for Java to set tags for a new object and retrieve or replace tags for an existing object\. For more information about object tagging, see [Object tagging](object-tagging.md)\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\. 

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.io.File;
import java.util.ArrayList;
import java.util.List;

public class ManagingObjectTags {

    public static void main(String[] args) {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Object key ***";
        String filePath = "*** File path ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Create an object, add two new tags, and upload the object to Amazon S3.
            PutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName, new File(filePath));
            List<Tag> tags = new ArrayList<Tag>();
            tags.add(new Tag("Tag 1", "This is tag 1"));
            tags.add(new Tag("Tag 2", "This is tag 2"));
            putRequest.setTagging(new ObjectTagging(tags));
            PutObjectResult putResult = s3Client.putObject(putRequest);

            // Retrieve the object's tags.
            GetObjectTaggingRequest getTaggingRequest = new GetObjectTaggingRequest(bucketName, keyName);
            GetObjectTaggingResult getTagsResult = s3Client.getObjectTagging(getTaggingRequest);

            // Replace the object's tags with two new tags.
            List<Tag> newTags = new ArrayList<Tag>();
            newTags.add(new Tag("Tag 3", "This is tag 3"));
            newTags.add(new Tag("Tag 4", "This is tag 4"));
            s3Client.setObjectTagging(new SetObjectTaggingRequest(bucketName, keyName, new ObjectTagging(newTags)));
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```


# Setting access policy with SOAP<a name="SOAPAccessPolicy"></a>

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

Access control can be set at the time a bucket or object is written by including the "AccessControlList" element with the request to `CreateBucket`, `PutObjectInline`, or `PutObject`\. The AccessControlList element is described in [Identity and access management in Amazon S3](s3-access-control.md)\. If no access control list is specified with these operations, the resource is created with a default access policy that gives the requester FULL\_CONTROL access \(this is the case even if the request is a PutObjectInline or PutObject request for an object that already exists\)\.

Following is a request that writes data to an object, makes the object readable by anonymous principals, and gives the specified user FULL\_CONTROL rights to the bucket \(Most developers will want to give themselves FULL\_CONTROL access to their own bucket\)\.

**Example**  
Following is a request that writes data to an object and makes the object readable by anonymous principals\.  
 `Sample Request`   

```
 1. <PutObjectInline xmlns="https://doc.s3.amazonaws.com/2006-03-01">
 2.   <Bucket>quotes</Bucket>
 3.   <Key>Nelson</Key>
 4.   <Metadata>
 5.     <Name>Content-Type</Name>
 6.     <Value>text/plain</Value>
 7.   </Metadata>
 8.   <Data>aGEtaGE=</Data>
 9.   <ContentLength>5</ContentLength>
10.   <AccessControlList>
11.     <Grant>
12.       <Grantee xsi:type="CanonicalUser">
13.         <ID>75cc57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a</ID>
14.         <DisplayName>chriscustomer</DisplayName>
15.       </Grantee>
16.       <Permission>FULL_CONTROL</Permission>
17.     </Grant>
18.     <Grant>
19.       <Grantee xsi:type="Group">
20.         <URI>http://acs.amazonaws.com/groups/global/AllUsers<URI>
21.       </Grantee>
22.       <Permission>READ</Permission>
23.     </Grant>
24.   </AccessControlList>
25.   <AWSAccessKeyId>AKIAIOSFODNN7EXAMPLE</AWSAccessKeyId>
26.   <Timestamp>2009-03-01T12:00:00.183Z</Timestamp>
27.   <Signature>Iuyz3d3P0aTou39dzbqaEXAMPLE=</Signature>
28. </PutObjectInline>
```
 `Sample Response`   

```
1. <PutObjectInlineResponse xmlns="https://s3.amazonaws.com/doc/2006-03-01">
2.   <PutObjectInlineResponse>
3.     <ETag>&quot828ef3fdfa96f00ad9f27c383fc9ac7f&quot</ETag>
4.     <LastModified>2009-01-01T12:00:00.000Z</LastModified>
5.   </PutObjectInlineResponse>
6. </PutObjectInlineResponse>
```

The access control policy can be read or set for an existing bucket or object using the `GetBucketAccessControlPolicy`, `GetObjectAccessControlPolicy`, `SetBucketAccessControlPolicy`, and `SetObjectAccessControlPolicy` methods\. For more information, see the detailed explanation of these methods\.


# Listing objects in a versioning\-enabled bucket<a name="list-obj-version-enabled-bucket"></a>

**Topics**
+ [Using the console](#list-obj-version-enabled-bucket-console)
+ [Using the AWS SDKs](#list-obj-version-enabled-bucket-sdk-examples)
+ [Using the REST API](#ListingtheObjectsinaVersioningEnabledBucket)

This section provides an example of listing object versions from a versioning\-enabled bucket\. Amazon S3 stores object version information in the *versions* subresource \(see [Bucket configuration options](UsingBucket.md#bucket-config-options-intro)\) that is associated with the bucket\. 

## Using the console<a name="list-obj-version-enabled-bucket-console"></a>

For information about listing object versions using the Amazon S3 console, see [ How Do I See the Versions of an S3 Object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/view-object-versions.html) in the *Amazon Simple Storage Service Console User Guide*\. 

## Using the AWS SDKs<a name="list-obj-version-enabled-bucket-sdk-examples"></a>

The examples in this section show how to retrieve an object listing from a versioning\-enabled bucket\. Each request returns up to 1,000 versions, unless you specify a lower number\. If the bucket contains more versions than this limit, you send a series of requests to retrieve the list of all versions\. This process of returning results in "pages" is called *pagination*\. To show how pagination works, the examples limit each response to two object versions\. After retrieving the first page of results, each example checks to determine whether the version list was truncated\. If it was, the example continues retrieving pages until all versions have been retrieved\. 

**Note**  
The following examples also work with a bucket that isn't versioning\-enabled, or for objects that don't have individual versions\. In those cases, Amazon S3 returns the object listing with a version ID of `null`\.

 For information about using other AWS SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code/)\. 

### Using the AWS SDK for Java<a name="list-obj-version-enabled-bucket-java"></a>

For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\. 

**Example**  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListVersionsRequest;
import com.amazonaws.services.s3.model.S3VersionSummary;
import com.amazonaws.services.s3.model.VersionListing;

public class ListKeysVersioningEnabledBucket {

    public static void main(String[] args) {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Retrieve the list of versions. If the bucket contains more versions
            // than the specified maximum number of results, Amazon S3 returns
            // one page of results per request.
            ListVersionsRequest request = new ListVersionsRequest()
                    .withBucketName(bucketName)
                    .withMaxResults(2);
            VersionListing versionListing = s3Client.listVersions(request);
            int numVersions = 0, numPages = 0;
            while (true) {
                numPages++;
                for (S3VersionSummary objectSummary :
                        versionListing.getVersionSummaries()) {
                    System.out.printf("Retrieved object %s, version %s\n",
                            objectSummary.getKey(),
                            objectSummary.getVersionId());
                    numVersions++;
                }
                // Check whether there are more pages of versions to retrieve. If
                // there are, retrieve them. Otherwise, exit the loop.
                if (versionListing.isTruncated()) {
                    versionListing = s3Client.listNextBatchOfVersions(versionListing);
                } else {
                    break;
                }
            }
            System.out.println(numVersions + " object versions retrieved in " + numPages + " pages");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

### Using the AWS SDK for \.NET<a name="list-obj-version-enabled-bucket-dotnet"></a>

For information about how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

**Example**  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ListObjectsVersioningEnabledBucketTest
    {
        static string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main(string[] args)
        {
            s3Client = new AmazonS3Client(bucketRegion);
            GetObjectListWithAllVersionsAsync().Wait();
        }

        static async Task GetObjectListWithAllVersionsAsync()
        {
            try
            {
                ListVersionsRequest request = new ListVersionsRequest()
                {
                    BucketName = bucketName,
                    // You can optionally specify key name prefix in the request
                    // if you want list of object versions of a specific object.

                    // For this example we limit response to return list of 2 versions.
                    MaxKeys = 2
                };
                do
                {
                    ListVersionsResponse response = await s3Client.ListVersionsAsync(request); 
                    // Process response.
                    foreach (S3ObjectVersion entry in response.Versions)
                    {
                        Console.WriteLine("key = {0} size = {1}",
                            entry.Key, entry.Size);
                    }

                    // If response is truncated, set the marker to get the next 
                    // set of keys.
                    if (response.IsTruncated)
                    {
                        request.KeyMarker = response.NextKeyMarker;
                        request.VersionIdMarker = response.NextVersionIdMarker;
                    }
                    else
                    {
                        request = null;
                    }
                } while (request != null);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}
```

## Using the REST API<a name="ListingtheObjectsinaVersioningEnabledBucket"></a>

To list all the versions of all the objects in a bucket, you use the `versions` subresource in a `GET Bucket` request\. Amazon S3 can retrieve only a maximum of 1,000 objects, and each object version counts fully as an object\. Therefore, if a bucket contains two keys \(for example, `photo.gif` and `picture.jpg`\), and the first key has 990 versions and the second key has 400 versions, a single request would retrieve all 990 versions of `photo.gif` and only the most recent 10 versions of `picture.jpg`\.

Amazon S3 returns object versions in the order in which they were stored, with the most recently stored returned first\.

**To list all object versions in a bucket**
+ In a `GET Bucket` request, include the `versions` subresource\.

  ```
  1. GET /?versions HTTP/1.1
  2. Host: bucketName.s3.amazonaws.com
  3. Date: Wed, 28 Oct 2009 22:32:00 +0000
  4. Authorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=
  ```

### Retrieving a subset of objects in a bucket<a name="RetBucObjSubset"></a>

This section discusses the following two example scenarios:
+ You want to retrieve a subset of all object versions in a bucket, for example, retrieve all versions of a specific object\.
+ The number of object versions in the response exceeds the value for `max-key` \(1000 by default\), so that you have to submit a second request to retrieve the remaining object versions\.

 To retrieve a subset of object versions, you use the request parameters for GET Bucket\. For more information, see [GET Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html)\. 

#### Example 1: Retrieving all versions of only a specific object<a name="ReturningAllVersionsofanObject"></a>

You can retrieve all versions of an object using the `versions` subresource and the `prefix` request parameter using the following process\. For more information about `prefix`, see [GET Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html)\.


**Retrieving all versions of a key**  

|  |  | 
| --- |--- |
| 1 | Set the prefix parameter to the key of the object you want to retrieve\. | 
| 2 |  Send a `GET Bucket` request using the `versions` subresource and `prefix`\. `GET /?versions&prefix=objectName HTTP/1.1`  | 

**Example Retrieving objects using a prefix**  
The following example retrieves objects whose key is or begins with `myObject`\.  

```
1. GET /?versions&prefix=myObject HTTP/1.1
2. Host: bucket.s3.amazonaws.com
3. Date: Wed, 28 Oct 2009 22:32:00 GMT
4. Authorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=
```

You can use the other request parameters to retrieve a subset of all versions of the object\. For more information, see [GET Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html)\.

#### Example 2: Retrieving a listing of additional objects if the response is truncated<a name="ReturningAdditionalObjectVersionsAfterExceedingMaxKeys"></a>

If the number of objects that could be returned in a `GET` request exceeds the value of `max-keys`, the response contains `<isTruncated>true</isTruncated>`, and includes the first key \(in `NextKeyMarker`\) and the first version ID \(in `NextVersionIdMarker`\) that satisfy the request, but were not returned\. You use those returned values as the starting position in a subsequent request to retrieve the additional objects that satisfy the `GET` request\. 

Use the following process to retrieve additional objects that satisfy the original `GET Bucket versions` request from a bucket\. For more information about `key-marker`, `version-id-marker`, `NextKeyMarker`, and `NextVersionIdMarker`, see [GET Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html)\.


**Retrieving additional responses that satisfy the original GET request**  

|  |  | 
| --- |--- |
| 1 | Set the value of key\-marker to the key returned in NextKeyMarker in the previous response\. | 
| 2 | Set the value of version\-id\-marker to the version ID returned in NextVersionIdMarker in the previous response\. | 
| 3 | Send a GET Bucket versions request using key\-marker and version\-id\-marker\. | 

**Example Retrieving objects starting with a specified key and version ID**  

```
1. GET /?versions&key-marker=myObject&version-id-marker=298459348571 HTTP/1.1
2. Host: bucket.s3.amazonaws.com
3. Date: Wed, 28 Oct 2009 22:32:00 GMT
4. Authorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=
```


# Example Bucket Policies for VPC Endpoints for Amazon S3<a name="example-bucket-policies-vpc-endpoint"></a>

You can use Amazon S3 bucket policies to control access to buckets from specific virtual private cloud \(VPC\) \(VPC\) endpoints, or specific VPCs\. This section contains example bucket policies that can be used to control Amazon S3 bucket access from VPC endpoints\. To learn how to set up VPC endpoints, see [VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html) in the *VPC User Guide*\. 

VPC enables you to launch AWS resources into a virtual network that you define\. A VPC endpoint enables you to create a private connection between your VPC and another AWS service without requiring access over the internet, through a VPN connection, through a NAT instance, or through AWS Direct Connect\. 

A VPC endpoint for Amazon S3 is a logical entity within a VPC that allows connectivity only to Amazon S3\. The VPC endpoint routes requests to Amazon S3 and routes responses back to the VPC\. VPC endpoints change only how requests are routed\. Amazon S3 public endpoints and DNS names will continue to work with VPC endpoints\. For important information about using VPC endpoints with Amazon S3, see [Gateway VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html) and [Endpoints for Amazon S3](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html) in the *VPC User Guide*\. 

VPC endpoints for Amazon S3 provide two ways to control access to your Amazon S3 data: 
+ You can control the requests, users, or groups that are allowed through a specific VPC endpoint\. For information about this type of access control, see [Controlling Access to Services with VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html) in the *VPC User Guide*\.
+ You can control which VPCs or VPC endpoints have access to your buckets by using Amazon S3 bucket policies\. For examples of this type of bucket policy access control, see the following topics on restricting access\.

**Topics**
+ [Restricting Access to a Specific VPC Endpoint](#example-bucket-policies-restrict-accesss-vpc-endpoint)
+ [Restricting Access to a Specific VPC](#example-bucket-policies-restrict-access-vpc)
+ [Related Resources](#example-bucket-policies-restrict-access-vpc-related-resources)

**Important**  
When applying the Amazon S3 bucket policies for VPC endpoints described in this section, you might block your access to the bucket without intending to do so\. Bucket permissions that are intended to specifically limit bucket access to connections originating from your VPC endpoint can block all connections to the bucket\. For information about how to fix this issue, see [My bucket policy has the wrong VPC or VPC endpoint ID\. How can I fix the policy so that I can access the bucket?](https://aws.amazon.com/premiumsupport/knowledge-center/s3-regain-access/) in the *AWS Support Knowledge Center*\.

## Restricting Access to a Specific VPC Endpoint<a name="example-bucket-policies-restrict-accesss-vpc-endpoint"></a>

The following is an example of an Amazon S3 bucket policy that restricts access to a specific bucket, `DOC-EXAMPLE-BUCKET`, only from the VPC endpoint with the ID `vpce-1a2b3c4d`\. The policy denies all access to the bucket if the specified endpoint is not being used\. The `aws:SourceVpce` condition is used to specify the endpoint\. The `aws:SourceVpce` condition does not require an Amazon Resource Name \(ARN\) for the VPC endpoint resource, only the VPC endpoint ID\. For more information about using conditions in a policy, see [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)\.

**Important**  
Before using the following example policy, replace the VPC endpoint ID with an appropriate value for your use case\. Otherwise, you won't be able to access your bucket\.
This policy disables console access to the specified bucket, because console requests don't originate from the specified VPC endpoint\.

```
 1. {
 2.    "Version": "2012-10-17",
 3.    "Id": "Policy1415115909152",
 4.    "Statement": [
 5.      {
 6.        "Sid": "Access-to-specific-VPCE-only",
 7.        "Principal": "*",
 8.        "Action": "s3:*",
 9.        "Effect": "Deny",
10.        "Resource": ["arn:aws:s3:::DOC-EXAMPLE-BUCKET",
11.                     "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"],
12.        "Condition": {
13.          "StringNotEquals": {
14.            "aws:SourceVpce": "vpce-1a2b3c4d"
15.          }
16.        }
17.      }
18.    ]
19. }
```

## Restricting Access to a Specific VPC<a name="example-bucket-policies-restrict-access-vpc"></a>

You can create a bucket policy that restricts access to a specific VPC by using the `aws:SourceVpc` condition\. This is useful if you have multiple VPC endpoints configured in the same VPC, and you want to manage access to your Amazon S3 buckets for all of your endpoints\. The following is an example of a policy that allows VPC `vpc-111bbb22` to access `DOC-EXAMPLE-BUCKET` and its objects\. The policy denies all access to the bucket if the specified VPC is not being used\. The `vpc-111bbb22` condition key does not require an ARN for the VPC resource, only the VPC ID\.

**Important**  
Before using the following example policy, replace the VPC ID with an appropriate value for your use case\. Otherwise, you won't be able to access your bucket\.
This policy disables console access to the specified bucket, because console requests don't originate from the specified VPC\.

```
 1. {
 2.    "Version": "2012-10-17",
 3.    "Id": "Policy1415115909153",
 4.    "Statement": [
 5.      {
 6.        "Sid": "Access-to-specific-VPC-only",
 7.        "Principal": "*",
 8.        "Action": "s3:*",
 9.        "Effect": "Deny",
10.        "Resource": ["arn:aws:s3:::DOC-EXAMPLE-BUCKET",
11.                     "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"],
12.        "Condition": {
13.          "StringNotEquals": {
14.            "aws:SourceVpc": "vpc-111bbb22"
15.          }
16.        }
17.      }
18.    ]
19. }
```

## Related Resources<a name="example-bucket-policies-restrict-access-vpc-related-resources"></a>
+ [Bucket Policy Examples](example-bucket-policies.md)
+ [VPC Endpoints](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html) in the *VPC User Guide*


# Setting permissions to use Amazon S3 Storage Lens<a name="storage_lens_iam_permissions"></a>

Amazon S3 Storage Lens requires new permissions in AWS Identity and Access Management \(IAM\) to authorize access to S3 Storage Lens actions\. You can attach the policy to IAM users, groups, or roles to grant them permissions to enable or disable S3 Storage Lens, or to access any S3 Storage Lens dashboard or configuration\. 

The IAM user or role must belong to the account that created or owns the dashboard or configuration, unless your account is a member of AWS Organizations, and you were given access to create organization\-level dashboards by your management account as a delegated administrator\. 

**Note**  
You can’t use your account's root user credentials to view Amazon S3 Storage Lens dashboards\. To access S3 Storage Lens dashboards, you must grant the requisite IAM permissions to a new or existing IAM user\. Then, sign in with those user credentials to access S3 Storage Lens dashboards\. For more information, see [AWS Identity and Access Management best practices](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html)\. 
Using S3 Storage Lens on the Amazon S3 console can require multiple permissions\. For example; to edit a dashboard on the console, you need the following permissions:  
`s3:ListStorageLensConfigurations`
`s3:GetStorageLensConfiguration`
`s3:PutStorageLensConfiguration`

**Topics**
+ [Setting account permissions to use S3 Storage Lens](#storage_lens_iam_permissions_account)
+ [Setting permissions to use S3 Storage Lens with AWS Organizations](#storage_lens_iam_permissions_organizations)

## Setting account permissions to use S3 Storage Lens<a name="storage_lens_iam_permissions_account"></a>


**Amazon S3 Storage Lens related IAM permissions**  

| Action | IAM permissions | 
| --- | --- | 
| Create or update an S3 Storage Lens dashboard in the Amazon S3 console\. |  `s3:ListStorageLensConfigurations` `s3:GetStorageLensConfiguration` `s3:GetStorageLensConfiguration` `s3:PutStorageLensConfiguration` `s3:PutStorageLensConfigurationTagging`  | 
| Get tags of an S3 Storage Lens dashboard on the Amazon S3 console\. |  `s3:ListStorageLensConfigurations` `s3:GetStorageLensConfigurationTagging`  | 
| View an S3 Storage Lens dashboard on the Amazon S3 console\. |  `s3:ListStorageLensConfigurations` `s3:GetStorageLensConfiguration` `s3:GetStorageLensDashboard`  | 
| Delete an S3 Storage Lens dashboard on Amazon S3 console\. |  `s3:ListStorageLensConfigurations` `s3:GetStorageLensConfiguration` `s3:DeleteStorageLensConfiguration`  | 
| Create or update an S3 Storage Lens configuration in the AWS CLI or SDK\. |  `s3:PutStorageLensConfiguration` `s3:PutStorageLensConfigurationTagging`  | 
| Get tags of an S3 Storage Lens configuration in the AWS CLI or SDK\. |  `s3:GetStorageLensConfigurationTagging`  | 
| View an S3 Storage Lens configuration in the AWS CLI or SDK\. |  `s3:GetStorageLensConfiguration`  | 
| Delete an S3 Storage Lens configuration in AWS CLI or SDK\. |  `s3:DeleteStorageLensConfiguration`  | 

**Note**  
You can use resource tags in an IAM policy to manage permissions\.
An IAM user/role with these permissions can see metrics from buckets and prefixes that they might not have direct permission to read or list objects from\.
For S3 Storage Lens configurations with *advanced metrics and recommendations* aggregated at the prefix\-level, if the selected prefix matches object keys, it may show the object key as your prefix up to the delimiter and maximum depth selected\.
For metrics exports, which are stored in a bucket in your account, permissions are granted using the existing `s3:GetObject` permission in the IAM policy\. Similarly, for an AWS Organizations entity, the organization management or delegated administrator account can use IAM policies to manage access permissions for organization\-level dashboard and configurations\.

## Setting permissions to use S3 Storage Lens with AWS Organizations<a name="storage_lens_iam_permissions_organizations"></a>

You can use Amazon S3 Storage Lens to collect storage metrics and usage data for all accounts that are part of your AWS Organizations hierarchy\. The following are the actions and permissions related to using S3 Storage Lens with Organizations\.


**AWS Organizations\-related IAM permissions for using Amazon S3 Storage Lens**  

| Action | IAM Permissions | 
| --- | --- | 
| Enable trusted access for S3 Storage Lens for your organization\. |  `organizations:EnableAWSServiceAccess`  | 
| Disable trusted access S3 Storage Lens for your organization\. |  `organizations:DisableAWSServiceAccess`  | 
| Register a delegated administrator to create S3 Storage Lens dashboards or configurations for your organization\. |  `organizations:RegisterDelegatedAdministrator`  | 
| De\-register a delegated administrator to create S3 Storage Lens dashboards or configurations for your organization\. |  `organizations:DeregisterDelegatedAdministrator`  | 
| Additional permissions to create S3 Storage Lens organization\-wide configurations |  `organizations:DescribeOrganization` `organizations:ListAccounts` `organizations:ListAWSServiceAccessForOrganization` `organizations:ListDelegatedAdministrators` `iam:CreateServiceLinkedRole`  | 


# Using the AWS SDK for Ruby \- Version 3<a name="UsingTheMPRubyAPI"></a>

The AWS SDK for Ruby provides an API for Amazon S3 bucket and object operations\. For object operations, you can use the API to upload objects in a single operation or upload large objects in parts \(see [Using the AWS SDK for Ruby for Multipart Upload](uploadobjusingmpu-ruby-sdk.md)\)\. However, the API for a single operation upload can also accept large objects and behind the scenes manage the upload in parts for you, thereby reducing the amount of script you need to write\.

## The Ruby API Organization<a name="RubyAPIOrganization"></a>

When creating Amazon S3 applications using the AWS SDK for Ruby, you must install the SDK for Ruby gem\. For more information, see the [AWS SDK for Ruby \- Version 3](https://docs.aws.amazon.com/sdkforruby/api/index.html)\. Once installed, you can access the API, including the following key classes: 
+ **Aws::S3::Resource—**Represents the interface to Amazon S3 for the Ruby SDK and provides methods for creating and enumerating buckets\. 

  The `S3` class provides the `#buckets` instance method for accessing existing buckets or creating new ones\.
+ **Aws::S3::Bucket—**Represents an Amazon S3 bucket\.  

  The `Bucket` class provides the `#object(key)` and `#objects` methods for accessing the objects in a bucket, as well as methods to delete a bucket and return information about a bucket, like the bucket policy\.
+ **Aws::S3::Object—**Represents an Amazon S3 object identified by its key\.

  The `Object` class provides methods for getting and setting properties of an object, specifying the storage class for storing objects, and setting object permissions using access control lists\. The `Object` class also has methods for deleting, uploading and copying objects\. When uploading objects in parts, this class provides options for you to specify the order of parts uploaded and the part size\.

For more information about the AWS SDK for Ruby API, go to [AWS SDK for Ruby \- Version 2](https://docs.aws.amazon.com/sdkforruby/api/index.html)\.

## Testing the Ruby Script Examples<a name="TestingRubySamples"></a>

The easiest way to get started with the Ruby script examples is to install the latest AWS SDK for Ruby gem\. For information about installing or updating to the latest gem, go to [AWS SDK for Ruby \- Version 3](https://docs.aws.amazon.com/sdkforruby/api/index.html)\. The following tasks guide you through the creation and testing of the Ruby script examples assuming that you have installed the AWS SDK for Ruby\.


**General Process of Creating and Testing Ruby Script Examples**  

|  |  | 
| --- |--- |
|  1  |  To access AWS, you must provide a set of credentials for your SDK for Ruby application\. For more information, see [ Configuring the AWS SDK for Ruby](https://docs.aws.amazon.com/sdk-for-ruby/v3/developer-guide/setup-config.html)\.   | 
|  2  |  Create a new SDK for Ruby script and add the following lines to the top of the script\.  <pre>#!/usr/bin/env ruby<br /><br />require 'rubygems'<br />require 'aws-sdk-s3'<br />								</pre> The first line is the interpreter directive and the two `require` statements import two required gems into your script\.  | 
|  3  |  Copy the code from the section you are reading to your script\.   | 
|  4  | Update the code by providing any required data\. For example, if uploading a file, provide the file path and the bucket name\. | 
|  5  |  Run the script\. Verify changes to buckets and objects by using the AWS Management Console\. For more information about the AWS Management Console, go to [https://aws\.amazon\.com/console/](https://aws.amazon.com/console/)\.  | 

**Ruby Samples**

The following links contain samples to help get you started with the SDK for Ruby \- Version 3:
+ [Using the AWS SDK for Ruby Version 3](create-bucket-get-location-example.md#create-bucket-get-location-ruby)
+ [Upload an object using the AWS SDK for Ruby](UploadObjSingleOpRuby.md)


# Amazon S3 multipart upload limits<a name="qfacts"></a>

The following table provides multipart upload core specifications\. For more information, see [Multipart upload overview](mpuoverview.md)\.


| Item | Specification | 
| --- | --- | 
| Maximum object size | 5 TB  | 
| Maximum number of parts per upload | 10,000 | 
| Part numbers | 1 to 10,000 \(inclusive\) | 
| Part size | 5 MB to 5 GB\. There is no size limit on the last part of your multipart upload\. | 
| Maximum number of parts returned for a list parts request | 1000  | 
| Maximum number of multipart uploads returned in a list multipart uploads request | 1000  | 


# Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys<a name="bucket-key"></a>

Amazon S3 Bucket Keys reduce the cost of Amazon S3 server\-side encryption using AWS Key Management Service \(SSE\-KMS\)\. This new bucket\-level key for SSE can reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS\. With a few clicks in the AWS Management Console, and without any changes to your client applications, you can configure your bucket to use an S3 Bucket Key for AWS KMS\-based encryption on new objects\.

## S3 Bucket Keys for SSE\-KMS<a name="bucket-key-overview"></a>

Workloads that access millions or billions of objects encrypted with SSE\-KMS can generate large volumes of requests to AWS KMS\. When you use SSE\-KMS to protect your data without an S3 Bucket Key, Amazon S3 uses an individual AWS KMS [data key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys) for every object\. It makes a call to AWS KMS every time a request is made against a KMS\-encrypted object\. For information about how SSE\-KMS works, see [Protecting data with server\-side encryption using AWS KMS CMKs \(SSE\-KMS\)](UsingKMSEncryption.md)\. 

When you configure your bucket to use an S3 Bucket Key for SSE\-KMS on new objects, AWS KMS generates a bucket\-level key that is used to create unique data keys for objects in the bucket\. This S3 Bucket Key is used for a time\-limited period within Amazon S3, reducing the need for Amazon S3 to make requests to AWS KMS to complete encryption operations\. This reduces traffic from S3 to AWS KMS, allowing you to access AWS KMS\-encrypted objects in S3 at a fraction of the previous cost\. 

Amazon S3 will only share an S3 Bucket Key for objects accessed by the same AWS KMS customer master key \(CMK\)\.

![\[Diagram showing AWS KMS generating a bucket key that creates data keys for objects in a bucket in S3.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/S3-Bucket-Keys.png)

## Configuring S3 Bucket Keys<a name="configure-bucket-key"></a>

You can configure your bucket to use an S3 Bucket Key for SSE\-KMS on new objects through the Amazon S3 console, AWS SDKs, AWS CLI, or REST API\. You can also override the S3 Bucket Key configuration for specific objects in a bucket with an individual per\-object KMS key using the REST API, AWS SDK, or AWS CLI\. You can also view S3 Bucket Key settings\. 

Before you configure your bucket to use an S3 Bucket Key, review [Changes to note before enabling an S3 Bucket Key](#bucket-key-changes)\. 

### Configuring an S3 Bucket Key using the Amazon S3 console<a name="configure-bucket-key-console"></a>

When you create a new bucket, you can configure your bucket to use an S3 Bucket Key for SSE\-KMS on new objects\. You can also configure an existing bucket to use an S3 Bucket Key for SSE\-KMS on new objects by updating your bucket properties\. 

For more information, see [Configuring your bucket to use S3 Bucket Keys using the S3 console](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-bucket-key.html) in the *Amazon Simple Storage Service Console User Guide*\.

### REST API, AWS CLI, and AWS SDK support for S3 Bucket Keys<a name="configure-bucket-key-programmatic"></a>

You can use the REST API, AWS CLI, or AWS SDK to configure your bucket to use an S3 Bucket Key for SSE\-KMS on new objects\. You can also enable an S3 Bucket Key at the object level\.

For more information, see the following: 
+ [Configuring an S3 Bucket Key at the object level using the REST API, AWS SDKs, or AWS CLI](configuring-bucket-key-object.md)
+ [Configuring your bucket to use an S3 Bucket Key with SSE\-KMS for new objects](configuring-bucket-key.md)

The following APIs support S3 Bucket Keys for SSE\-KMS:
+ [PutBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketEncryption.html)
  + `ServerSideEncryptionRule` accepts the `BucketKeyEnabled` parameter for enabling and disabling an S3 Bucket Key\.
+ [GetBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetBucketEncryption.html)
  + `ServerSideEncryptionRule` returns the settings for `BucketKeyEnabled`\.
+ [PutObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html), [CopyObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CopyObject.html), [CreateMutlipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateMultipartUpload.html), and [PostObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)
  + `x-amz-server-side-encryption-bucket-key-enabled` request header enables or disables an S3 Bucket Key at the object level\.
+ [HeadObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadObject.html), [GetObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html), [UploadPartCopy](https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPartCopy.html), [UploadPart](https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPart.html), and [CompleteMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CompleteMultipartUpload.html)
  + `x-amz-server-side-encryption-bucket-key-enabled` response header indicates if an S3 Bucket Key is enabled or disabled for an object\.

### Working with AWS CloudFormation<a name="configure-bucket-key-cfn"></a>

In AWS CloudFormation, the `AWS::S3::Bucket` resource includes an encryption property called `BucketKeyEnabled` that you can use to enable or disable an S3 Bucket Key\. 

For more information, see [Using AWS CloudFormation](configuring-bucket-key.md#enable-bucket-key-cloudformation)\.

## Changes to note before enabling an S3 Bucket Key<a name="bucket-key-changes"></a>

After you enable an S3 Bucket Key, key material is used for a time\-limited period within Amazon S3, and Amazon S3 decreases requests to AWS KMS\. Additionally, Amazon S3 uses the bucket Amazon Resource Name \(ARN\) as the encryption context instead of the object ARN\. 

After you enable an S3 Bucket Key, you see related changes in the following areas:

### IAM or KMS key policies<a name="bucket-key-policies"></a>

If your existing IAM policies or AWS KMS key policies use your object Amazon Resource Name \(ARN\) as the encryption context to refine or limit access to your AWS KMS CMKs, these policies won’t work with an S3 Bucket Key\. S3 Bucket Keys use the bucket ARN as encryption context\. Before you enable an S3 Bucket Key, update your IAM policies or AWS KMS key policies to use your bucket ARN as encryption context\.

For more information about encryption context and S3 Bucket Keys, see [Encryption context \(x\-amz\-server\-side\-encryption\-context\)](KMSUsingRESTAPI.md#s3-kms-encryption-context)\.

### AWS KMS CloudTrail events<a name="bucket-key-cloudtrail"></a>

After you enable an S3 Bucket Key, your AWS KMS CloudTrail events log your bucket ARN instead of your object ARN\. Additionally, you see fewer KMS CloudTrail events for SSE\-KMS objects in your logs\. Because key material is time\-limited in Amazon S3, fewer requests are made to AWS KMS\.  

## Using an S3 Bucket Key with replication<a name="bucket-key-replication"></a>

You can use S3 Bucket Keys with Same\-Region Replication \(SRR\) and Cross\-Region Replication \(CRR\)\.

When Amazon S3 replicates an encrypted object, it generally preserves the encryption settings of the replica object in the destination bucket\. However, if the source object is not encrypted and your destination bucket uses default encryption or an S3 Bucket Key, Amazon S3 encrypts the object with the destination bucket’s configuration\. 

The following examples illustrate how an S3 Bucket Key works with replication\. For more information, see [Replicating objects created with server\-side encryption \(SSE\) using encryption keys stored in AWS KMS](replication-config-for-kms-objects.md)\. 

**Example 1 – Source object uses S3 Bucket Keys, destination bucket uses default encryption**  
If your source object uses an S3 Bucket Key but your destination bucket uses default encryption with SSE\-KMS, the replica object maintains its S3 Bucket Key encryption settings in the destination bucket\. The destination bucket still uses default encryption with SSE\-KMS\.   
 

**Example 2 – Source object is not encrypted, destination bucket uses an S3 Bucket Key with SSE\-KMS**  
If your source object is not encrypted and the destination bucket uses an S3 Bucket Key with SSE\-KMS, the source object is encrypted with an S3 Bucket Key using SSE\-KMS in the destination bucket\. This results in the `ETag` of the source object being different from the `ETag` of the replica object\. You must update applications that use the `ETag` to accommodate for this difference\.

## Working with S3 Bucket Keys<a name="using-bucket-key"></a>

For more information about enabling and working with S3 Bucket Keys, see the following sections:
+ [Configuring your bucket to use an S3 Bucket Key with SSE\-KMS for new objects](configuring-bucket-key.md)
+ [Configuring an S3 Bucket Key at the object level using the REST API, AWS SDKs, or AWS CLI](configuring-bucket-key-object.md)
+ [Viewing settings for an S3 Bucket Key ](viewing-bucket-key-settings.md)


# S3 Batch Operations examples<a name="batch-ops-examples"></a>

You can use S3 Batch Operations to perform large\-scale S3 Batch Operations on billions of S3 objects containing exabytes of data\. You can use the AWS Management Console, AWS Command Line Interface \(AWS CLI\), AWS SDKs, or REST API to manage your Batch Operations jobs\. 

This section contains the following examples of creating and managing Batch Operations jobs in Amazon S3\. In the examples, replace any variable values with those that suit your needs\.

**Topics**
+ [S3 Batch Operations examples using the AWS CLI](batch-ops-examples-cli.md)
+ [S3 Batch Operations examples using the AWS SDK for Java](batch-ops-examples-java.md)
+ [Example: Using job tags to control permissions for S3 Batch Operations](batch-ops-job-tags-examples.md)
+ [Example: Requesting S3 Batch Operations completion reports](batch-ops-examples-reports.md)
+ [Example: Copying objects across AWS accounts using S3 Batch Operations](batch-ops-examples-xcopy.md)
+ [Example: Tracking an S3 Batch Operations job in Amazon EventBridge through AWS CloudTrail](batch-ops-examples-event-bridge-cloud-trail.md)


# Using Service\-Linked Roles for Amazon S3 Storage Lens<a name="using-service-linked-roles"></a>

To use Amazon S3 Storage Lens to collect and aggregate metrics across all your accounts in AWS Organizations, you must first ensure that S3 Storage Lens has trusted access enabled by the Management account in your organization\. S3 Storage Lens creates a service\-linked role to allow it to enable it to get the list of AWS accounts belonging to your organization\. This list of accounts is used by S3 Storage Lens to collect metrics for S3 resources in all those member accounts when S3 Storage Lens dashboard or configurations are created or updated\.

Amazon S3 Storage Lens uses AWS Identity and Access Management \(IAM\)[ service\-linked roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#iam-term-service-linked-role)\. A service\-linked role is a unique type of IAM role that is linked directly to S3 Storage Lens\. Service\-linked roles are predefined by S3 Storage Lens and include all the permissions that the service requires to call other AWS services on your behalf\.

A service\-linked role makes setting up S3 Storage Lens easier because you don't have to manually add the necessary permissions\. S3 Storage Lens defines the permissions of its service\-linked roles, and unless defined otherwise, only S3 Storage Lens can assume its roles\. The defined permissions include the trust policy and the permissions policy, and that permissions policy cannot be attached to any other IAM entity\.

You can delete this service\-linked role only after first deleting their related resources\. This protects your S3 Storage Lens resources because you can't inadvertently remove permission to access the resources\.

For information about other services that support service\-linked roles, see [AWS Services That Work with IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html) and look for the services that have **Yes **in the **Service\-Linked Role** column\. Choose a **Yes** with a link to view the service\-linked role documentation for that service\.

## Service\-Linked Role Permissions for Amazon S3 Storage Lens<a name="slr-permissions"></a>

S3 Storage Lens uses the service\-linked role named **AWSServiceRoleForS3StorageLens** – This enables access to AWS services and resources used or managed by S3 Storage Lens\. This allows S3 Storage Lens to access AWS Organizations resources on your behalf\.

The S3 Storage Lens service\-linked role trusts the following service on your organization's storage:
+ `storage-lens.s3.amazonaws.com`

The role permissions policy allows S3 Storage Lens to complete the following actions:
+ `organizations:DescribeOrganization`

  `organizations:ListAccounts`

  `organizations:ListAWSServiceAccessForOrganization`

  `organizations:ListDelegatedAdministrators`

You must configure permissions to allow an IAM entity \(such as a user, group, or role\) to create, edit, or delete a service\-linked role\. For more information, see [Service\-Linked Role Permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked-roles.html#service-linked-role-permissions) in the *IAM User Guide*\.

## Creating a Service\-Linked Role for S3 Storage Lens<a name="create-slr"></a>

You don't need to manually create a service\-linked role\. When you complete one of the following tasks while signed into the AWS Organizations Management or the delegate administrator accounts, S3 Storage Lens creates the service\-linked role for you:
+ Create an S3 Storage Lens dashboard configuration for your organization in the Amazon S3 console\.
+ `PUT` an S3 Storage Lens configuration for your organization using the REST API, AWS CLI and SDKs\.

**Note**  
S3 Storage Lens will support a maximum of five delegated administrators per Organization\.

If you delete this service\-linked role, the preceding actions will recreate it as needed\.

### Example Policy for S3 Storage Lens service\-linked role<a name="slr-sample-policy"></a>

**Example Permissions policy for the S3 Storage Lens service\-linked role**  

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AwsOrgsAccess",
            "Effect": "Allow",
            "Action": [
                "organizations:DescribeOrganization",
                "organizations:ListAccounts",
                "organizations:ListAWSServiceAccessForOrganization",
                "organizations:ListDelegatedAdministrators"
            ],
            "Resource": [
                "*"
            ]
        }
    ]
}
```

## Editing a Service\-Linked Role for Amazon S3 Storage Lens<a name="edit-slr"></a>

S3 Storage Lens does not allow you to edit the AWSServiceRoleForS3StorageLens service\-linked role\. After you create a service\-linked role, you cannot change the name of the role because various entities might reference the role\. However, you can edit the description of the role using IAM\. For more information, see [Editing a Service\-Linked Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked-roles.html#edit-service-linked-role) in the *IAM User Guide*\.

## Deleting a Service\-Linked Role for Amazon S3 Storage Lens<a name="delete-slr"></a>

If you no longer need to use the service\-linked role, we recommend that you delete that role\. That way you don't have an unused entity that is not actively monitored or maintained\. However, you must clean up the resources for your service\-linked role before you can manually delete it\.

**Note**  
If the Amazon S3 Storage Lens service is using the role when you try to delete the resources, then the deletion might fail\. If that happens, wait for a few minutes and try the operation again\.

To delete the AWSServiceRoleForS3StorageLens you must delete all the organization level S3 Storage Lens configurations present in all Regions using the AWS Organizations Management or the delegate administrator accounts\.

The resources are organization\-level S3 Storage Lens configurations\. Use S3 Storage Lens to clean up the resources and then use the IAM console, CLI, REST API or AWS SDK to delete the role\. 

In the REST API, AWS CLI and SDKs, S3 Storage Lens configurations can be discovered using `ListStorageLensConfigurations` in all the Regions where your Organization has created S3 Storage Lens configurations\. Use the action `DeleteStorageLensConfiguration` to delete these configurations so you can then delete the role\.

**Note**  
To delete the service\-linked role you must delete all the organization\-level S3 Storage Lens configurations in all the Regions where they exist\.

**To delete Amazon S3 Storage Lens resources used by the AWSServiceRoleForS3StorageLens**

1. You must use the `ListStorageLensConfigurations` in every Region that you have S3 Storage Lens configurations to get a list of your organization level configurations\. This list may also be obtained from the Amazon S3 console\.

1. These configurations then must be deleted from the appropriate regional endpoints by invoking the `DeleteStorageLensConfiguration` API call or via the Amazon S3 console\. 

**To manually delete the service\-linked role using IAM**

After the configurations are deleted the AWSServiceRoleForS3StorageLens can be deleted from the IAM console or by invoking the IAM API `DeleteServiceLinkedRole`, the AWS CLI, or the AWS SDK\. For more information, see [Deleting a Service\-Linked Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/using-service-linked-roles.html#delete-service-linked-role) in the *IAM User Guide*\.

## Supported Regions for S3 Storage Lens Service\-Linked Roles<a name="slr-regions"></a>

S3 Storage Lens supports using service\-linked roles in all of the regions where the service is available\. For more information, see [Amazon S3 Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html)\.


# Example 1: Bucket owner granting its users bucket permissions<a name="example-walkthroughs-managing-access-example1"></a>

**Topics**
+ [Step 0: Preparing for the walkthrough](#grant-permissions-to-user-in-your-account-step0)
+ [Step 1: Create resources \(a bucket and an IAM user\) in account a and grant permissions](#grant-permissions-to-user-in-your-account-step1)
+ [Step 2: Test permissions](#grant-permissions-to-user-in-your-account-test)

In this exercise, an AWS account owns a bucket, and it has an IAM user in the account\. By default, the user has no permissions\. For the user to perform any tasks, the parent account must grant them permissions\. The bucket owner and parent account are the same\. Therefore, to grant the user permissions on the bucket, the AWS account can use a bucket policy, a user policy, or both\. The account owner will grant some permissions using a bucket policy and other permissions using a user policy\.

The following steps summarize the walkthrough:

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/access-policy-ex1.png)

1. Account administrator creates a bucket policy granting a set of permissions to the user\.

1. Account administrator attaches a user policy to the user granting additional permissions\.

1. User then tries permissions granted via both the bucket policy and the user policy\.

For this example, you will need an AWS account\. Instead of using the root credentials of the account, you will create an administrator user \(see [About using an administrator user to create resources and grant permissions](example-walkthroughs-managing-access.md#about-using-root-credentials)\)\. We refer to the AWS account and the administrator user as follows:


| Account ID | Account referred to as | Administrator user in the account | 
| --- | --- | --- | 
|  *1111\-1111\-1111*  |  Account A  |  AccountAadmin  | 



All the tasks of creating users and granting permissions are done in the AWS Management Console\. To verify permissions, the walkthrough uses the command line tools, AWS Command Line Interface \(CLI\) and AWS Tools for Windows PowerShell, to verify the permissions, so you don't need to write any code\.

## Step 0: Preparing for the walkthrough<a name="grant-permissions-to-user-in-your-account-step0"></a>

1. Make sure you have an AWS account and that it has a user with administrator privileges\.

   1. Sign up for an account, if needed\. We refer to this account as Account A\.

      1.  Go to [https://aws\.amazon\.com/s3](https://aws.amazon.com/s3) and click **Sign Up**\. 

      1. Follow the on\-screen instructions\.

         AWS will notify you by email when your account is active and available for you to use\.

   1. In Account A, create an administrator user AccountAadmin\. Using Account A credentials, sign in to the [IAM console](https://console.aws.amazon.com/iam/home?#home) and do the following: 

      1. Create user AccountAadmin and note down the user security credentials\. 

         For instructions, see [Creating an IAM User in Your AWS Account](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html) in the *IAM User Guide*\. 

      1. Grant AccountAadmin administrator privileges by attaching a user policy giving full access\. 

         For instructions, see [Working with Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html) in the *IAM User Guide*\. 

      1. Note down the **IAM User Sign\-In URL** for AccountAadmin\. You will need to use this URL when signing in to the AWS Management Console\. For more information about where to find it, see [How Users Sign in to Your Account](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_how-users-sign-in.html) in *IAM User Guide*\. Note down the URL for each of the accounts\.

1. Set up either the AWS Command Line Interface \(CLI\) or the AWS Tools for Windows PowerShell\. Make sure you save administrator user credentials as follows:
   + If using the AWS CLI, create a profile, AccountAadmin, in the config file\.
   + If using the AWS Tools for Windows PowerShell, make sure you store credentials for the session as AccountAadmin\.

   For instructions, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\. 

## Step 1: Create resources \(a bucket and an IAM user\) in account a and grant permissions<a name="grant-permissions-to-user-in-your-account-step1"></a>

Using the credentials of user AccountAadmin in Account A, and the special IAM user sign\-in URL, sign in to the AWS Management Console and do the following:

1. Create Resources \(a bucket and an IAM user\)

   1. In the Amazon S3 console create a bucket\. Note down the AWS region in which you created it\. For instructions, see [How Do I Create an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) in the *Amazon Simple Storage Service Console User Guide*\. 

   1. In the IAM console, do the following: 

      1. Create a user, Dave\.

         For instructions, see [Creating IAM Users \(AWS Management Console\)](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console) in the *IAM User Guide*\. 

      1. Note down the UserDave credentials\.

      1. Note down the Amazon Resource Name \(ARN\) for user Dave\. In the IAM console, select the user, and the **Summary** tab provides the user ARN\.

1. Grant Permissions\. 

   Because the bucket owner and the parent account to which the user belongs are the same, the AWS account can grant user permissions using a bucket policy, a user policy, or both\. In this example, you do both\. If the object is also owned by the same account, the bucket owner can grant object permissions in the bucket policy \(or an IAM policy\)\.

   1. In the Amazon S3 console, attach the following bucket policy to *awsexamplebucket1*\. 

      The policy has two statements\. 
      + The first statement grants Dave the bucket operation permissions `s3:GetBucketLocation` and `s3:ListBucket`\.
      + The second statement grants the `s3:GetObject` permission\. Because Account A also owns the object, the account administrator is able to grant the `s3:GetObject` permission\. 

      In the `Principal` statement, Dave is identified by his user ARN\. For more information about policy elements, see [Policies and Permissions in Amazon S3](access-policy-language-overview.md)\.

      ```
      {
         "Version": "2012-10-17",
         "Statement": [
            {
               "Sid": "statement1",
               "Effect": "Allow",
               "Principal": {
                  "AWS": "arn:aws:iam::AccountA-ID:user/Dave"
               },
               "Action": [
                  "s3:GetBucketLocation",
                  "s3:ListBucket"
               ],
               "Resource": [
                  "arn:aws:s3:::awsexamplebucket1"
               ]
            },
            {
               "Sid": "statement2",
               "Effect": "Allow",
               "Principal": {
                  "AWS": "arn:aws:iam::AccountA-ID:user/Dave"
               },
               "Action": [
                   "s3:GetObject"
               ],
               "Resource": [
                  "arn:aws:s3:::awsexamplebucket1/*"
               ]
            }
         ]
      }
      ```

   1. Create an inline policy for the user Dave by using the following policy\. The policy grants Dave the `s3:PutObject` permission\. You need to update the policy by providing your bucket name\.

      ```
      {
         "Version": "2012-10-17",
         "Statement": [
            {
               "Sid": "PermissionForObjectOperations",
               "Effect": "Allow",
               "Action": [
                  "s3:PutObject"
               ],
               "Resource": [
                  "arn:aws:s3:::awsexamplebucket1/*"
               ]
            }
         ]
      }
      ```

      For instructions, see [Working with Inline Policies](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_inline-using.html) in the *IAM User Guide*\. Note you need to sign in to the console using Account A credentials\.

## Step 2: Test permissions<a name="grant-permissions-to-user-in-your-account-test"></a>

Using Dave's credentials, verify that the permissions work\. You can use either of the following two procedures\.

**Test using the AWS CLI**

1. Update the AWS CLI config file by adding the following UserDaveAccountA profile\. For more information, see [Setting up the tools for the example walkthroughs](policy-eval-walkthrough-download-awscli.md)\.

   ```
   [profile UserDaveAccountA]
   aws_access_key_id = access-key
   aws_secret_access_key = secret-access-key
   region = us-east-1
   ```

1. Verify that Dave can perform the operations as granted in the user policy\. Upload a sample object using the following AWS CLI `put-object` command\. 

   The `--body` parameter in the command identifies the source file to upload\. For example, if the file is in the root of the C: drive on a Windows machine, you specify `c:\HappyFace.jpg`\. The `--key` parameter provides the key name for the object\.

   ```
   aws s3api put-object --bucket awsexamplebucket1 --key HappyFace.jpg --body HappyFace.jpg --profile UserDaveAccountA
   ```

   Run the following AWS CLI command to get the object\. 

   ```
   aws s3api get-object --bucket awsexamplebucket1 --key HappyFace.jpg OutputFile.jpg --profile UserDaveAccountA
   ```

**Test using the AWS Tools for Windows PowerShell**

1. Store Dave's credentials as AccountADave\. You then use these credentials to PUT and GET an object\.

   ```
   set-awscredentials -AccessKey AccessKeyID -SecretKey SecretAccessKey -storeas AccountADave
   ```

1. Upload a sample object using the AWS Tools for Windows PowerShell `Write-S3Object` command using user Dave's stored credentials\. 

   ```
   Write-S3Object -bucketname awsexamplebucket1 -key HappyFace.jpg -file HappyFace.jpg -StoredCredentials AccountADave
   ```

   Download the previously uploaded object\.

   ```
   Read-S3Object -bucketname awsexamplebucket1 -key HappyFace.jpg -file Output.jpg -StoredCredentials AccountADave
   ```


# The SOAP error response<a name="UsingSOAPError"></a>

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

In SOAP, an error result is returned to the client as a SOAP fault, with the HTTP response code 500\. If you do not receive a SOAP fault, then your request was successful\. The Amazon S3 SOAP fault code is comprised of a standard SOAP 1\.1 fault code \(either "Server" or "Client"\) concatenated with the Amazon S3\-specific error code\. For example: "Server\.InternalError" or "Client\.NoSuchBucket"\. The SOAP fault string element contains a generic, human readable error message in English\. Finally, the SOAP fault detail element contains miscellaneous information relevant to the error\.

For example, if you attempt to delete the object "Fred", which does not exist, the body of the SOAP response contains a "NoSuchKey" SOAP fault\.

**Example**  

```
1. <soapenv:Body>
2.   <soapenv:Fault>
3.     <Faultcode>soapenv:Client.NoSuchKey</Faultcode>
4.     <Faultstring>The specified key does not exist.</Faultstring>
5.     <Detail>
6.       <Key>Fred</Key>
7.     </Detail>
8.   </soapenv:Fault>
9. </soapenv:Body>
```

For more information about Amazon S3 errors, go to [ErrorCodeList](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html)\.


# Working with delete markers<a name="DeleteMarker"></a>

A delete marker is a placeholder \(marker\) for a versioned object that was named in a simple `DELETE` request\. Because the object was in a versioning\-enabled bucket, the object was not deleted\. The delete marker, however, makes Amazon S3 behave as if it had been deleted\. 

A delete marker has a key name \(or key\) and version ID like any other object\. However, a delete marker differs from other objects in the following ways:
+ It does not have data associated with it\.
+ It is not associated with an access control list \(ACL\) value\.
+ It does not retrieve anything from a `GET` request because it has no data; you get a 404 error\.
+ The only operation you can use on a delete marker is an Amazon S3 API `DELETE` call\. To do this you will need make the `DELETE` request using an AWS Identity and Access Management \(IAM\) user or role with the appropriate permissions\.

Delete markers accrue a nominal charge for storage in Amazon S3\. The storage size of a delete marker is equal to the size of the key name of the delete marker\. A key name is a sequence of Unicode characters\. The UTF\-8 encoding adds from 1 to 4 bytes of storage to your bucket for each character in the name\. For more information about key names, see [Object keys](UsingMetadata.md#object-keys)\. For information about deleting a delete marker, see [Removing delete markers](RemDelMarker.md)\.  

Only Amazon S3 can create a delete marker, and it does so whenever you send a `DELETE Object` request on an object in a versioning\-enabled or suspended bucket\. The object named in the `DELETE` request is not actually deleted\. Instead, the delete marker becomes the current version of the object\. \(The object's key name \(or key\) becomes the key of the delete marker\.\) If you try to get an object and its current version is a delete marker, Amazon S3 responds with:
+ A 404 \(Object not found\) error
+ A response header, x\-amz\-delete\-marker: true

The response header tells you that the object accessed was a delete marker\. This response header never returns `false`; if the value is `false`, Amazon S3 does not include this response header in the response\.

The following figure shows how a simple `GET` on an object, whose current version is a delete marker, returns a 404 No Object Found error\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_NoObjectFound.png)

The only way to list delete markers \(and other versions of an object\) is by using the `versions` subresource in a `GET Bucket versions` request\. A simple `GET` does not retrieve delete marker objects\. The following figure shows that a `GET Bucket` request does not return objects whose current version is a delete marker\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_GETBucketwithDeleteMarkers.png)


# Appendix a: Using the SOAP API<a name="SOAPAPI3"></a>

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

This section contains information specific to the Amazon S3 SOAP API\. 

**Note**  
SOAP requests, both authenticated and anonymous, must be sent to Amazon S3 using SSL\. Amazon S3 returns an error when you send a SOAP request over HTTP\. 


# Best practices and guidelines for S3 RTC<a name="rtc-best-practices"></a>

When replicating data in Amazon S3 using S3 Replication Time Control \(S3 RTC\), follow these best practice guidelines to optimize replication performance for your workloads\. 

**Topics**
+ [Amazon S3 Replication and request rate performance guidelines](#rtc-request-rate-performance)
+ [Estimating your replication request rates](#estimating-replication-request-rates)
+ [Exceeding S3 RTC data transfer rate limits](#exceed-rtc-data-transfer-limits)
+ [AWS KMS encrypted object replication request rates](#kms-object-replication-request-rates)

## Amazon S3 Replication and request rate performance guidelines<a name="rtc-request-rate-performance"></a>

When uploading and retrieving storage from Amazon S3, your applications can achieve thousands of transactions per second in request performance\. For example, an application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in an S3 bucket, including the requests that S3 replication makes on your behalf\. There are no limits to the number of prefixes in a bucket\. You can increase your read or write performance by parallelizing reads\. For example, if you create 10 prefixes in an S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second\. 

Amazon S3 automatically scales in response to sustained request rates above these guidelines, or sustained request rates concurrent with LIST requests\. While Amazon S3 is internally optimizing for the new request rate, you might receive HTTP 503 request responses temporarily until the optimization is complete\. This might occur with increases in request per second rates, or when you first enable S3 RTC\. During these periods, your replication latency might increase\. The S3 RTC service level agreement \(SLA\) doesn’t apply to time periods when Amazon S3 performance guidelines on requests per second are exceeded\. 

The S3 RTC SLA also doesn’t apply during time periods where your replication data transfer rate exceeds the default 1 Gbps limit\. If you expect your replication transfer rate to exceed 1 Gbps, you can contact [AWS Support Center](https://console.aws.amazon.com/support/home#/) or use [Service Quotas](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) to request an increase in your limit\. 

## Estimating your replication request rates<a name="estimating-replication-request-rates"></a>

Your total request rate including the requests that Amazon S3 replication makes on your behalf should be within the Amazon S3 request rate guidelines for both the replication source and destination buckets\. For each object replicated, Amazon S3 replication makes up to five GET/HEAD requests and one PUT request to the source bucket, and one PUT request to each destination bucket\.

For example, if you expect to replicate 100 objects per second, Amazon S3 replication might perform an additional 100 PUT requests on your behalf for a total of 200 PUTs per second to the source S3 bucket\. Amazon S3 replication also might perform up to 500 GET/HEAD \(5 GET/HEAD requests for each object replicated\.\) 

**Note**  
You incur costs for only one PUT request per object replicated\. For more information, see the pricing information in the [Amazon S3 FAQ on replication](https://aws.amazon.com/s3/faqs/#Replication)\. 

## Exceeding S3 RTC data transfer rate limits<a name="exceed-rtc-data-transfer-limits"></a>

If you expect your S3 Replication Time Control data transfer rate to exceed the default 1 Gbps limit, contact [AWS Support Center](https://console.aws.amazon.com/support/home#/) or use [Service Quotas](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) to request an increase in your limit\. 

## AWS KMS encrypted object replication request rates<a name="kms-object-replication-request-rates"></a>

When you replicate objects encrypted with server\-side encryption \(SSE\-KMS\) using Amazon S3 replication, AWS Key Management Service \(AWS KMS\) requests per second limits apply\. AWS KMS might reject an otherwise valid request because your request rate exceeds the limit for the number of requests per second\. When a request is throttled, AWS KMS returns a `ThrottlingException` error\. The AWS KMS request rate limit applies to requests you make directly and to requests made by Amazon S3 replication on your behalf\. 

For example, if you expect to replicate 1,000 objects per second, you can subtract 2,000 requests from your AWS KMS request rate limit\. The resulting request rate per second is available for your AWS KMS workloads excluding replication\. You can use [AWS KMS request metrics in Amazon CloudWatch](https://docs.aws.amazon.com/kms/latest/developerguide/monitoring-cloudwatch.html) to monitor the total AWS KMS request rate on your AWS account\.


# Example 5: S3 Replication Time Control \(S3 RTC\) configuration<a name="replication-walkthrough-5"></a>

S3 Replication Time Control \(S3 RTC\) helps you meet compliance or business requirements for data replication and provides visibility into Amazon S3 replication times\. S3 RTC replicates most objects that you upload to Amazon S3 in seconds, and 99\.99 percent of those objects within 15 minutes\. 

With S3 RTC, you can monitor the total number and size of objects that are pending replication, and the maximum replication time to the destination Region\. Replication metrics are available through the [AWS Management Console](https://console.aws.amazon.com/s3/) and [Amazon CloudWatch User Guide](https://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/)\. For more information, see [Amazon S3 CloudWatch replication metrics](cloudwatch-monitoring.md#s3-cloudwatch-replication-metrics)\. 

**Topics**

## Enable S3 RTC on the Amazon S3 console<a name="replication-ex5-console"></a>

For step\-by\-step instructions, see [How do I add a replication rule to an S3 bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html) in the *Amazon Simple Storage Service Console User Guide*\. This topic provides instructions for enabling S3 RTC in your replication configuration when buckets are owned by same and different AWS accounts\.

## Replicate objects with S3 Replication Time Control \(S3 RTC\) \(AWS CLI\)<a name="replication-ex5-cli"></a>

To use the AWS CLI to replicate objects with S3 RTC enabled, you create buckets, enable versioning on the buckets, create an IAM role that gives Amazon S3 permission to replicate objects, and add the replication configuration to the source bucket\. The replication configuration needs to have S3 Replication Time Control \(S3 RTC\) enabled\. 

**To replicate with S3 RTC enabled \(AWS CLI\)**
+ The following example sets `ReplicationTime` and `Metric`, and adds replication configuration to the source bucket\.

  ```
  {
      "Rules": [
          {
              "Status": "Enabled",
              "Filter": {
                  "Prefix": "Tax"
              },
              "DeleteMarkerReplication": {
                  "Status": "Disabled"
              },
              "Destination": {
                  "Bucket": "arn:aws:s3:::destination",
                  "Metrics": {
                      "Status": "Enabled",
                      "EventThreshold": {
                          "Minutes": 15
                      }
                  },
                  "ReplicationTime": {
                      "Status": "Enabled",
                      "Time": {
                          "Minutes": 15
                      }
                  }
              },
              "Priority": 1
          }
      ],
      "Role": "IAM-Role-ARN"
  }
  ```
**Important**  
 `Metrics:EventThreshold:Minutes` and `ReplicationTime:Time:Minutes` can only have 15 as a valid value\. 

## Replicate objects with S3 Replication Time Control \(S3 RTC\) \(AWS SDK\)<a name="replication-ex5-sdk"></a>

 The following Java example adds replication configuration with S3 Replication Time Control \(S3 RTC\)\.

```
import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.model.DeleteMarkerReplication;
import software.amazon.awssdk.services.s3.model.Destination;
import software.amazon.awssdk.services.s3.model.Metrics;
import software.amazon.awssdk.services.s3.model.MetricsStatus;
import software.amazon.awssdk.services.s3.model.PutBucketReplicationRequest;
import software.amazon.awssdk.services.s3.model.ReplicationConfiguration;
import software.amazon.awssdk.services.s3.model.ReplicationRule;
import software.amazon.awssdk.services.s3.model.ReplicationRuleFilter;
import software.amazon.awssdk.services.s3.model.ReplicationTime;
import software.amazon.awssdk.services.s3.model.ReplicationTimeStatus;
import software.amazon.awssdk.services.s3.model.ReplicationTimeValue;

public class Main {

  public static void main(String[] args) {
    S3Client s3 = S3Client.builder()
      .region(Region.US_EAST_1)
      .credentialsProvider(() -> AwsBasicCredentials.create(
          "AWS_ACCESS_KEY_ID",
          "AWS_SECRET_ACCESS_KEY")
      )
      .build();

    ReplicationConfiguration replicationConfig = ReplicationConfiguration
      .builder()
      .rules(
          ReplicationRule
            .builder()
            .status("Enabled")
            .priority(1)
            .deleteMarkerReplication(
                DeleteMarkerReplication
                    .builder()
                    .status("Disabled")
                    .build()
            )
            .destination(
                Destination
                    .builder()
                    .bucket("destination_bucket_arn")
                    .replicationTime(
                        ReplicationTime.builder().time(
                            ReplicationTimeValue.builder().minutes(15).build()
                        ).status(
                            ReplicationTimeStatus.ENABLED
                        ).build()
                    )
                    .metrics(
                        Metrics.builder().eventThreshold(
                            ReplicationTimeValue.builder().minutes(15).build()
                        ).status(
                            MetricsStatus.ENABLED
                        ).build()
                    )
                    .build()
            )
            .filter(
                ReplicationRuleFilter
                    .builder()
                    .prefix("testtest")
                    .build()
            )
        .build())
        .role("role_arn")
        .build();

    // Put replication configuration
    PutBucketReplicationRequest putBucketReplicationRequest = PutBucketReplicationRequest
      .builder()
      .bucket("source_bucket")
      .replicationConfiguration(replicationConfig)
      .build();

    s3.putBucketReplication(putBucketReplicationRequest);
  }
}
```

For more information, see [Meeting compliance requirements using S3 Replication Time Control \(S3 RTC\)](replication-time-control.md)\. 


# Upload examples \(AWS signature version 2\)****<a name="HTTPPOSTExamples"></a>

**Topics**
+ [File upload](#HTTPPOSTExamplesFileUpload)
+ [Text area upload](#HTTPPOSTExamplesTextArea)

**Note**  
The request authentication discussed in this section is based on AWS Signature Version 2, a protocol for authenticating inbound API requests to AWS services\.   
Amazon S3 now supports Signature Version 4, a protocol for authenticating inbound API requests to AWS services, in all AWS regions\. At this time, AWS regions created before January 30, 2014 will continue to support the previous protocol, Signature Version 2\. Any new regions after January 30, 2014 will support only Signature Version 4 and therefore all requests to those regions must be made with Signature Version 4\. For more information, see [Examples: Browser\-Based Upload using HTTP POST \(Using AWS Signature Version 4\)](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-post-example.html) in the *Amazon Simple Storage Service API Reference*\. 

## File upload<a name="HTTPPOSTExamplesFileUpload"></a>

This example shows the complete process for constructing a policy and form that can be used to upload a file attachment\.

### Policy and form construction<a name="HTTPPOSTExamplesFileUploadPolicy"></a>

The following policy supports uploads to Amazon S3 for the awsexamplebucket1 bucket\.

```
 1. { "expiration": "2007-12-01T12:00:00.000Z",
 2.   "conditions": [
 3.     {"bucket": "awsexamplebucket1"},
 4.     ["starts-with", "$key", "user/eric/"],
 5.     {"acl": "public-read"},
 6.     {"success_action_redirect": "https://awsexamplebucket1.s3.us-west-1.amazonaws.com/successful_upload.html"},
 7.     ["starts-with", "$Content-Type", "image/"],
 8.     {"x-amz-meta-uuid": "14365123651274"},
 9.     ["starts-with", "$x-amz-meta-tag", ""]
10.   ]
11. }
```

This policy requires the following:
+ The upload must occur before 12:00 UTC on December 1, 2007\.
+ The content must be uploaded to the awsexamplebucket1 bucket\.
+ The key must start with "user/eric/"\.
+ The ACL is set to public\-read\.
+ The success\_action\_redirect is set to https://awsexamplebucket1\.s3\.us\-west\-1\.amazonaws\.com/successful\_upload\.html\.
+ The object is an image file\.
+ The x\-amz\-meta\-uuid tag must be set to 14365123651274\. 
+ The x\-amz\-meta\-tag can contain any value\.

The following is a Base64\-encoded version of this policy\.

```
1. eyAiZXhwaXJhdGlvbiI6ICIyMDA3LTEyLTAxVDEyOjAwOjAwLjAwMFoiLAogICJjb25kaXRpb25zIjogWwogICAgeyJidWNrZXQiOiAiam9obnNtaXRoIn0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiRrZXkiLCAidXNlci9lcmljLyJdLAogICAgeyJhY2wiOiAicHVibGljLXJlYWQifSwKICAgIHsic3VjY2Vzc19hY3Rpb25fcmVkaXJlY3QiOiAiaHR0cDovL2pvaG5zbWl0aC5zMy5hbWF6b25hd3MuY29tL3N1Y2Nlc3NmdWxfdXBsb2FkLmh0bWwifSwKICAgIFsic3RhcnRzLXdpdGgiLCAiJENvbnRlbnQtVHlwZSIsICJpbWFnZS8iXSwKICAgIHsieC1hbXotbWV0YS11dWlkIjogIjE0MzY1MTIzNjUxMjc0In0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiR4LWFtei1tZXRhLXRhZyIsICIiXQogIF0KfQo=
```

Using your credentials create a signature, for example `0RavWzkygo6QX9caELEqKi9kDbU=` is the signature for the preceding policy document\.

The following form supports a POST request to the awsexamplebucket1\.net bucket that uses this policy\.

```
 1. <html>
 2.   <head>
 3.     ...
 4.     <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
 5.     ...
 6.   </head>
 7.   <body>
 8.   ...
 9.   <form action="https://awsexamplebucket1.s3.us-west-1.amazonaws.com/" method="post" enctype="multipart/form-data">
10.     Key to upload: <input type="input" name="key" value="user/eric/" /><br />
11.     <input type="hidden" name="acl" value="public-read" />
12.     <input type="hidden" name="success_action_redirect" value="https://awsexamplebucket1.s3.us-west-1.amazonaws.com/successful_upload.html" />
13.     Content-Type: <input type="input" name="Content-Type" value="image/jpeg" /><br />
14.     <input type="hidden" name="x-amz-meta-uuid" value="14365123651274" />
15.     Tags for File: <input type="input" name="x-amz-meta-tag" value="" /><br />
16.     <input type="hidden" name="AWSAccessKeyId" value="AKIAIOSFODNN7EXAMPLE" />
17.     <input type="hidden" name="Policy" value="POLICY" />
18.     <input type="hidden" name="Signature" value="SIGNATURE" />
19.     File: <input type="file" name="file" /> <br />
20.     <!-- The elements after this will be ignored -->
21.     <input type="submit" name="submit" value="Upload to Amazon S3" />
22.   </form>
23.   ...
24. </html>
```

### Sample request<a name="HTTPPOSTExamplesFileUploadRequest"></a>

This request assumes that the image uploaded is 117,108 bytes; the image data is not included\.

```
 1. POST / HTTP/1.1
 2. Host: awsexamplebucket1.s3.us-west-1.amazonaws.com
 3. User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.10) Gecko/20071115 Firefox/2.0.0.10
 4. Accept: text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5
 5. Accept-Language: en-us,en;q=0.5
 6. Accept-Encoding: gzip,deflate
 7. Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7
 8. Keep-Alive: 300
 9. Connection: keep-alive
10. Content-Type: multipart/form-data; boundary=9431149156168
11. Content-Length: 118698 
12. 
13. --9431149156168
14. Content-Disposition: form-data; name="key"
15. 
16. user/eric/MyPicture.jpg
17. --9431149156168
18. Content-Disposition: form-data; name="acl"
19. 
20. public-read
21. --9431149156168
22. Content-Disposition: form-data; name="success_action_redirect"
23. 
24. https://awsexamplebucket1.s3.us-west-1.amazonaws.com/successful_upload.html
25. --9431149156168
26. Content-Disposition: form-data; name="Content-Type"
27. 
28. image/jpeg
29. --9431149156168
30. Content-Disposition: form-data; name="x-amz-meta-uuid"
31. 
32. 14365123651274
33. --9431149156168
34. Content-Disposition: form-data; name="x-amz-meta-tag"
35. 
36. Some,Tag,For,Picture
37. --9431149156168
38. Content-Disposition: form-data; name="AWSAccessKeyId"
39. 
40. AKIAIOSFODNN7EXAMPLE
41. --9431149156168
42. Content-Disposition: form-data; name="Policy"
43. 
44. eyAiZXhwaXJhdGlvbiI6ICIyMDA3LTEyLTAxVDEyOjAwOjAwLjAwMFoiLAogICJjb25kaXRpb25zIjogWwogICAgeyJidWNrZXQiOiAiam9obnNtaXRoIn0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiRrZXkiLCAidXNlci9lcmljLyJdLAogICAgeyJhY2wiOiAicHVibGljLXJlYWQifSwKICAgIHsic3VjY2Vzc19hY3Rpb25fcmVkaXJlY3QiOiAiaHR0cDovL2pvaG5zbWl0aC5zMy5hbWF6b25hd3MuY29tL3N1Y2Nlc3NmdWxfdXBsb2FkLmh0bWwifSwKICAgIFsic3RhcnRzLXdpdGgiLCAiJENvbnRlbnQtVHlwZSIsICJpbWFnZS8iXSwKICAgIHsieC1hbXotbWV0YS11dWlkIjogIjE0MzY1MTIzNjUxMjc0In0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiR4LWFtei1tZXRhLXRhZyIsICIiXQogIF0KfQo=
45. --9431149156168
46. Content-Disposition: form-data; name="Signature"
47. 
48. 0RavWzkygo6QX9caELEqKi9kDbU=
49. --9431149156168
50. Content-Disposition: form-data; name="file"; filename="MyFilename.jpg"
51. Content-Type: image/jpeg
52. 
53. ...file content...
54. --9431149156168
55. Content-Disposition: form-data; name="submit"
56. 
57. Upload to Amazon S3
58. --9431149156168--
```

### Sample response<a name="HTTPPOSTExamplesFileUploadResponse"></a>

```
1. HTTP/1.1 303 Redirect
2. x-amz-request-id: 1AEE782442F35865
3. x-amz-id-2: cxzFLJRatFHy+NGtaDFRR8YvI9BHmgLxjvJzNiGGICARZ/mVXHj7T+qQKhdpzHFh
4. Content-Type: application/xml
5. Date: Wed, 14 Nov 2007 21:21:33 GMT
6. Connection: close
7. Location: https://awsexamplebucket1.s3.us-west-1.amazonaws.com/successful_upload.html?bucket=awsexamplebucket1&key=user/eric/MyPicture.jpg&etag=&quot;39d459dfbc0faabbb5e179358dfb94c3&quot;
8. Server: AmazonS3
```

## Text area upload<a name="HTTPPOSTExamplesTextArea"></a>

**Topics**
+ [Policy and form construction](#HTTPPOSTExamplesTextAreaPolicy)
+ [Sample request](#HTTPPOSTExamplesTextAreaRequest)
+ [Sample response](#HTTPPOSTExamplesTextAreaResponse)

The following example shows the complete process for constructing a policy and form to upload a text area\. Uploading a text area is useful for submitting user\-created content, such as blog postings\.

### Policy and form construction<a name="HTTPPOSTExamplesTextAreaPolicy"></a>

The following policy supports text area uploads to Amazon S3 for the awsexamplebucket1 bucket\.

```
 1. { "expiration": "2007-12-01T12:00:00.000Z",
 2.   "conditions": [
 3.     {"bucket": "awsexamplebucket1"},
 4.     ["starts-with", "$key", "user/eric/"],
 5.     {"acl": "public-read"},
 6.     {"success_action_redirect": "https://awsexamplebucket1.s3.us-west-1.amazonaws.com/new_post.html"},
 7.     ["eq", "$Content-Type", "text/html"],
 8.     {"x-amz-meta-uuid": "14365123651274"},
 9.     ["starts-with", "$x-amz-meta-tag", ""]
10.   ]
11. }
```

This policy requires the following:
+ The upload must occur before 12:00 GMT on 2007\-12\-01\.
+ The content must be uploaded to the awsexamplebucket1 bucket\.
+ The key must start with "user/eric/"\.
+ The ACL is set to public\-read\.
+ The success\_action\_redirect is set to https://awsexamplebucket1\.s3\.us\-west\-1\.amazonaws\.com/new\_post\.html\.
+ The object is HTML text\.
+ The x\-amz\-meta\-uuid tag must be set to 14365123651274\. 
+ The x\-amz\-meta\-tag can contain any value\.

Following is a Base64\-encoded version of this policy\.

```
1. eyAiZXhwaXJhdGlvbiI6ICIyMDA3LTEyLTAxVDEyOjAwOjAwLjAwMFoiLAogICJjb25kaXR
2. pb25zIjogWwogICAgeyJidWNrZXQiOiAiam9obnNtaXRoIn0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiRrZXkiLCAidXNlci9lcmljLyJd
3. LAogICAgeyJhY2wiOiAicHVibGljLXJlYWQifSwKICAgIHsic3VjY2Vzc19hY3Rpb25fcmVkaXJlY3QiOiAiaHR0cDovL2pvaG5zbWl0a
4. C5zMy5hbWF6b25hd3MuY29tL25ld19wb3N0Lmh0bWwifSwKICAgIFsiZXEiLCAiJENvbnRlbnQtVHlwZSIsICJ0ZXh0L2h0bWwiXSwKI
5. CAgIHsieC1hbXotbWV0YS11dWlkIjogIjE0MzY1MTIzNjUxMjc0In0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiR4LWFtei1tZXRhLXRhZy
6. IsICIiXQogIF0KfQo=
```

Using your credentials, create a signature\. For example, `qA7FWXKq6VvU68lI9KdveT1cWgF=` is the signature for the preceding policy document\.

The following form supports a POST request to the awsexamplebucket1\.net bucket that uses this policy\.

```
 1. <html>
 2.   <head>
 3.     ...
 4.     <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
 5.     ...
 6.   </head>
 7.   <body>
 8.   ...
 9.   <form action="https://awsexamplebucket1.s3.us-west-1.amazonaws.com/" method="post" enctype="multipart/form-data">
10.     Key to upload: <input type="input" name="key" value="user/eric/" /><br />
11.     <input type="hidden" name="acl" value="public-read" />
12.     <input type="hidden" name="success_action_redirect" value="https://awsexamplebucket1.s3.us-west-1.amazonaws.com/new_post.html" />
13.     <input type="hidden" name="Content-Type" value="text/html" />
14.     <input type="hidden" name="x-amz-meta-uuid" value="14365123651274" />
15.     Tags for File: <input type="input" name="x-amz-meta-tag" value="" /><br />
16.     <input type="hidden" name="AWSAccessKeyId" value="AKIAIOSFODNN7EXAMPLE" />
17.     <input type="hidden" name="Policy" value="POLICY" />
18.     <input type="hidden" name="Signature" value="SIGNATURE" />
19.     Entry: <textarea name="file" cols="60" rows="10">
20. 
21. Your blog post goes here.
22. 
23.     </textarea><br />
24.     <!-- The elements after this will be ignored -->
25.     <input type="submit" name="submit" value="Upload to Amazon S3" />
26.   </form>
27.   ...
28. </html>
```

### Sample request<a name="HTTPPOSTExamplesTextAreaRequest"></a>

This request assumes that the image uploaded is 117,108 bytes; the image data is not included\.

```
 1. POST / HTTP/1.1
 2. Host: awsexamplebucket1.s3.us-west-1.amazonaws.com
 3. User-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.10) Gecko/20071115 Firefox/2.0.0.10
 4. Accept: text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5
 5. Accept-Language: en-us,en;q=0.5
 6. Accept-Encoding: gzip,deflate
 7. Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7
 8. Keep-Alive: 300
 9. Connection: keep-alive
10. Content-Type: multipart/form-data; boundary=178521717625888
11. Content-Length: 118635
12. 
13. -178521717625888
14. Content-Disposition: form-data; name="key"
15. 
16. ser/eric/NewEntry.html
17. --178521717625888
18. Content-Disposition: form-data; name="acl"
19. 
20. public-read
21. --178521717625888
22. Content-Disposition: form-data; name="success_action_redirect"
23. 
24. https://awsexamplebucket1.s3.us-west-1.amazonaws.com/new_post.html
25. --178521717625888
26. Content-Disposition: form-data; name="Content-Type"
27. 
28. text/html
29. --178521717625888
30. Content-Disposition: form-data; name="x-amz-meta-uuid"
31. 
32. 14365123651274
33. --178521717625888
34. Content-Disposition: form-data; name="x-amz-meta-tag"
35. 
36. Interesting Post
37. --178521717625888
38. Content-Disposition: form-data; name="AWSAccessKeyId"
39. 
40. AKIAIOSFODNN7EXAMPLE
41. --178521717625888
42. Content-Disposition: form-data; name="Policy"
43. eyAiZXhwaXJhdGlvbiI6ICIyMDA3LTEyLTAxVDEyOjAwOjAwLjAwMFoiLAogICJjb25kaXRpb25zIjogWwogICAgeyJidWNrZXQiOiAiam9obnNtaXRoIn0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiRrZXkiLCAidXNlci9lcmljLyJdLAogICAgeyJhY2wiOiAicHVibGljLXJlYWQifSwKICAgIHsic3VjY2Vzc19hY3Rpb25fcmVkaXJlY3QiOiAiaHR0cDovL2pvaG5zbWl0aC5zMy5hbWF6b25hd3MuY29tL25ld19wb3N0Lmh0bWwifSwKICAgIFsiZXEiLCAiJENvbnRlbnQtVHlwZSIsICJ0ZXh0L2h0bWwiXSwKICAgIHsieC1hbXotbWV0YS11dWlkIjogIjE0MzY1MTIzNjUxMjc0In0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiR4LWFtei1tZXRhLXRhZyIsICIiXQogIF0KfQo=
44. 
45. --178521717625888
46. Content-Disposition: form-data; name="Signature"
47. 
48. qA7FWXKq6VvU68lI9KdveT1cWgF=
49. --178521717625888
50. Content-Disposition: form-data; name="file"
51. 
52. ...content goes here...
53. --178521717625888
54. Content-Disposition: form-data; name="submit"
55. 
56. Upload to Amazon S3
57. --178521717625888--
```

### Sample response<a name="HTTPPOSTExamplesTextAreaResponse"></a>

```
1. HTTP/1.1 303 Redirect
2. x-amz-request-id: 1AEE782442F35865
3. x-amz-id-2: cxzFLJRatFHy+NGtaDFRR8YvI9BHmgLxjvJzNiGGICARZ/mVXHj7T+qQKhdpzHFh
4. Content-Type: application/xml
5. Date: Wed, 14 Nov 2007 21:21:33 GMT
6. Connection: close
7. Location: https://awsexamplebucket1.s3.us-west-1.amazonaws.com/new_post.html?bucket=awsexamplebucket1&key=user/eric/NewEntry.html&etag=40c3271af26b7f1672e41b8a274d28d4
8. Server: AmazonS3
```


# Programmatically configuring a bucket as a static website<a name="ManagingBucketWebsiteConfig"></a>

To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket\. You can also use the AWS SDKs to create, update, and delete the website configuration programmatically\. The SDKs provide wrapper classes around the Amazon S3 REST API\. If your application requires it, you can send REST API requests directly from your application\. 

For more information about configuring your bucket for static website hosting using the AWS Management Console, see [Configuring a bucket as a static website using the AWS Management Console](HowDoIWebsiteConfiguration.md)\.

For more information about using the AWS CLI to configure an S3 bucket as a static website, see [website](https://docs.aws.amazon.com/cli/latest/reference/s3/website.html) in the *AWS CLI Command Reference*\. For more information about programmatically configuring an S3 bucket as a static website, see the following topics\. 

**Topics**
+ [Managing websites with the AWS SDK for Java](ConfigWebSiteJava.md)
+ [Managing websites with the AWS SDK for \.NET](ConfigWebSiteDotNet.md)
+ [Managing websites with the AWS SDK for PHP](ConfigWebSitePHP.md)
+ [Managing websites with the REST API](ConfigWebSiteREST.md)


# Amazon S3 Storage Lens examples using the SDK for Java<a name="S3LensJavaExamples"></a>

Amazon S3 Storage Lens aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format\. You can use the dashboard to visualize insights and trends, flag outliers, and provides recommendations for optimizing storage costs and applying data protection best practices\. You can use S3 Storage Lens through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\.\. For more information, see [Assessing storage activity and usage with Amazon S3 Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens.html)\. 

The following examples show how you can use S3 Storage Lens with the AWS SDK for Java\.

## Using Amazon S3 Storage Lens configurations using the SDK for Java<a name="S3LensConfigurationsJava"></a>

You can use the SDK for Java to list, create, get and update your S3 Storage Lens configurations\. The following examples use the helper json files for key inputs\.

**Topics**
+ [Create and update an S3 Storage Lens configuration](#S3CreateandUpdateStorageLensConfigurationJava)
+ [Delete an S3 Storage Lens configuration](#S3DeleteStorageLensConfigurationJava)
+ [Gets an S3 Storage Lens configuration](#S3GetStorageLensConfigurationJava)
+ [Lists S3 Storage Lens configurations](#S3ListStorageLensConfigurationsJava)
+ [Put tags to an S3 Storage Lens configuration](#S3PutStorageLensConfigurationTaggingJava)
+ [Get tags for an S3 Storage Lens configuration](#S3GetStorageLensConfigurationTaggingJava)
+ [Delete tags for an S3 Storage Lens configuration](#S3DeleteStorageLensConfigurationTaggingJava)
+ [Update default S3 Storage Lens configuration with Advanced Metrics and Recommendations](#S3UpdateStorageLensConfigurationAdvancedJava)

### Create and update an S3 Storage Lens configuration<a name="S3CreateandUpdateStorageLensConfigurationJava"></a>

**Example Create and update an S3 Storage Lens configuration**  

```
package aws.example.s3control;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.AccountLevel;
import com.amazonaws.services.s3control.model.ActivityMetrics;
import com.amazonaws.services.s3control.model.BucketLevel;
import com.amazonaws.services.s3control.model.Format;
import com.amazonaws.services.s3control.model.Include;
import com.amazonaws.services.s3control.model.OutputSchemaVersion;
import com.amazonaws.services.s3control.model.PrefixLevel;
import com.amazonaws.services.s3control.model.PrefixLevelStorageMetrics;
import com.amazonaws.services.s3control.model.PutStorageLensConfigurationRequest;
import com.amazonaws.services.s3control.model.S3BucketDestination;
import com.amazonaws.services.s3control.model.SSES3;
import com.amazonaws.services.s3control.model.SelectionCriteria;
import com.amazonaws.services.s3control.model.StorageLensAwsOrg;
import com.amazonaws.services.s3control.model.StorageLensConfiguration;
import com.amazonaws.services.s3control.model.StorageLensDataExport;
import com.amazonaws.services.s3control.model.StorageLensDataExportEncryption;
import com.amazonaws.services.s3control.model.StorageLensTag;

import java.util.Arrays;
import java.util.List;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class CreateAndUpdateDashboard {

    public static void main(String[] args) {
        String configurationId = "ConfigurationId";
        String sourceAccountId = "Source Account ID";
        String exportAccountId = "Destination Account ID";
        String exportBucketArn = "arn:aws:s3:::destBucketName";
        String awsOrgARN = "arn:aws:organizations::222222222222:organization/o-abcdefgh";
        Format exportFormat = Format.CSV;

        try {
            SelectionCriteria selectionCriteria = new SelectionCriteria()
                    .withDelimiter("/")
                    .withMaxDepth(5)
                    .withMinStorageBytesPercentage(10.0);
            PrefixLevelStorageMetrics prefixStorageMetrics = new PrefixLevelStorageMetrics()
                    .withIsEnabled(true)
                    .withSelectionCriteria(selectionCriteria);
            BucketLevel bucketLevel = new BucketLevel()
                    .withActivityMetrics(new ActivityMetrics().withIsEnabled(true))
                    .withPrefixLevel(new PrefixLevel().withStorageMetrics(prefixStorageMetrics));
            AccountLevel accountLevel = new AccountLevel()
                    .withActivityMetrics(new ActivityMetrics().withIsEnabled(true))
                    .withBucketLevel(bucketLevel);

            Include include = new Include()
                    .withBuckets(Arrays.asList("arn:aws:s3:::bucketName"))
                    .withRegions(Arrays.asList("us-west-2"));

            StorageLensDataExportEncryption exportEncryption = new StorageLensDataExportEncryption()
                    .withSSES3(new SSES3());
            S3BucketDestination s3BucketDestination = new S3BucketDestination()
                    .withAccountId(exportAccountId)
                    .withArn(exportBucketArn)
                    .withEncryption(exportEncryption)
                    .withFormat(exportFormat)
                    .withOutputSchemaVersion(OutputSchemaVersion.V_1)
                    .withPrefix("Prefix");
            StorageLensDataExport dataExport = new StorageLensDataExport()
                    .withS3BucketDestination(s3BucketDestination);

            StorageLensAwsOrg awsOrg = new StorageLensAwsOrg()
                    .withArn(awsOrgARN);

            StorageLensConfiguration configuration = new StorageLensConfiguration()
                    .withId(configurationId)
                    .withAccountLevel(accountLevel)
                    .withInclude(include)
                    .withDataExport(dataExport)
                    .withAwsOrg(awsOrg)
                    .withIsEnabled(true);

            List<StorageLensTag> tags = Arrays.asList(
                    new StorageLensTag().withKey("key-1").withValue("value-1"),
                    new StorageLensTag().withKey("key-2").withValue("value-2")
            );

            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            s3ControlClient.putStorageLensConfiguration(new PutStorageLensConfigurationRequest()
                    .withAccountId(sourceAccountId)
                    .withConfigId(configurationId)
                    .withStorageLensConfiguration(configuration)
                    .withTags(tags)
            );
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

### Delete an S3 Storage Lens configuration<a name="S3DeleteStorageLensConfigurationJava"></a>

**Example Delete an S3 Storage Lens configuration\.**  

```
package aws.example.s3control;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.DeleteStorageLensConfigurationRequest;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class DeleteDashboard {

    public static void main(String[] args) {
        String configurationId = "ConfigurationId";
        String sourceAccountId = "Source Account ID";
        try {
            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            s3ControlClient.deleteStorageLensConfiguration(new DeleteStorageLensConfigurationRequest()
                    .withAccountId(sourceAccountId)
                    .withConfigId(configurationId)
            );
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

### Gets an S3 Storage Lens configuration<a name="S3GetStorageLensConfigurationJava"></a>

**Example Get an S3 Storage Lens configuration**  

```
package aws.example.s3control;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.GetStorageLensConfigurationRequest;
import com.amazonaws.services.s3control.model.GetStorageLensConfigurationResult;
import com.amazonaws.services.s3control.model.StorageLensConfiguration;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class GetDashboard {

    public static void main(String[] args) {
        String configurationId = "ConfigurationId";
        String sourceAccountId = "Source Account ID";

        try {
            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            final StorageLensConfiguration configuration =
                    s3ControlClient.getStorageLensConfiguration(new GetStorageLensConfigurationRequest()
                            .withAccountId(sourceAccountId)
                            .withConfigId(configurationId)
                    ).getStorageLensConfiguration();

            System.out.println(configuration.toString());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

### Lists S3 Storage Lens configurations<a name="S3ListStorageLensConfigurationsJava"></a>

**Example Lists S3 Storage Lens configurations**  

```
package aws.example.s3control;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.ListStorageLensConfigurationEntry;
import com.amazonaws.services.s3control.model.ListStorageLensConfigurationsRequest;

import java.util.List;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class ListDashboard {

    public static void main(String[] args) {
        String sourceAccountId = "Source Account ID";
        String nextToken = "nextToken";

        try {
            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            final List<ListStorageLensConfigurationEntry> configurations =
                    s3ControlClient.listStorageLensConfigurations(new ListStorageLensConfigurationsRequest()
                            .withAccountId(sourceAccountId)
                            .withNextToken(nextToken)
                    ).getStorageLensConfigurationList();

            System.out.println(configurations.toString());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

### Put tags to an S3 Storage Lens configuration<a name="S3PutStorageLensConfigurationTaggingJava"></a>

**Example Put tags to an S3 Storage Lens configuration**  

```
package aws.example.s3control;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.PutStorageLensConfigurationTaggingRequest;
import com.amazonaws.services.s3control.model.StorageLensTag;

import java.util.Arrays;
import java.util.List;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class PutDashboardTagging {

    public static void main(String[] args) {
        String configurationId = "ConfigurationId";
        String sourceAccountId = "Source Account ID";

        try {
            List<StorageLensTag> tags = Arrays.asList(
                    new StorageLensTag().withKey("key-1").withValue("value-1"),
                    new StorageLensTag().withKey("key-2").withValue("value-2")
            );

            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            s3ControlClient.putStorageLensConfigurationTagging(new PutStorageLensConfigurationTaggingRequest()
                    .withAccountId(sourceAccountId)
                    .withConfigId(configurationId)
                    .withTags(tags)
            );
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

### Get tags for an S3 Storage Lens configuration<a name="S3GetStorageLensConfigurationTaggingJava"></a>

**Example Get tags for an S3 Storage Lens configuration**  

```
package aws.example.s3control;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.DeleteStorageLensConfigurationRequest;
import com.amazonaws.services.s3control.model.GetStorageLensConfigurationTaggingRequest;
import com.amazonaws.services.s3control.model.StorageLensTag;

import java.util.List;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class GetDashboardTagging {

    public static void main(String[] args) {
        String configurationId = "ConfigurationId";
        String sourceAccountId = "Source Account ID";
        try {
            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            final List<StorageLensTag> s3Tags = s3ControlClient
                    .getStorageLensConfigurationTagging(new GetStorageLensConfigurationTaggingRequest()
                            .withAccountId(sourceAccountId)
                            .withConfigId(configurationId)
                    ).getTags();

            System.out.println(s3Tags.toString());
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

### Delete tags for an S3 Storage Lens configuration<a name="S3DeleteStorageLensConfigurationTaggingJava"></a>

**Example Delete tags for an S3 Storage Lens configuration**  

```
package aws.example.s3control;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.DeleteStorageLensConfigurationTaggingRequest;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class DeleteDashboardTagging {

    public static void main(String[] args) {
        String configurationId = "ConfigurationId";
        String sourceAccountId = "Source Account ID";
        try {
            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            s3ControlClient.deleteStorageLensConfigurationTagging(new DeleteStorageLensConfigurationTaggingRequest()
                    .withAccountId(sourceAccountId)
                    .withConfigId(configurationId)
            );
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

### Update default S3 Storage Lens configuration with Advanced Metrics and Recommendations<a name="S3UpdateStorageLensConfigurationAdvancedJava"></a>

**Example Update default S3 Storage Lens configuration with Advanced Metrics and Recommendations**  

```
package aws.example.s3control;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3control.AWSS3Control;
import com.amazonaws.services.s3control.AWSS3ControlClient;
import com.amazonaws.services.s3control.model.AccountLevel;
import com.amazonaws.services.s3control.model.ActivityMetrics;
import com.amazonaws.services.s3control.model.BucketLevel;
import com.amazonaws.services.s3control.model.Format;
import com.amazonaws.services.s3control.model.Include;
import com.amazonaws.services.s3control.model.OutputSchemaVersion;
import com.amazonaws.services.s3control.model.PrefixLevel;
import com.amazonaws.services.s3control.model.PrefixLevelStorageMetrics;
import com.amazonaws.services.s3control.model.PutStorageLensConfigurationRequest;
import com.amazonaws.services.s3control.model.S3BucketDestination;
import com.amazonaws.services.s3control.model.SSES3;
import com.amazonaws.services.s3control.model.SelectionCriteria;
import com.amazonaws.services.s3control.model.StorageLensAwsOrg;
import com.amazonaws.services.s3control.model.StorageLensConfiguration;
import com.amazonaws.services.s3control.model.StorageLensDataExport;
import com.amazonaws.services.s3control.model.StorageLensDataExportEncryption;
import com.amazonaws.services.s3control.model.StorageLensTag;

import java.util.Arrays;
import java.util.List;

import static com.amazonaws.regions.Regions.US_WEST_2;

public class UpdateDefaultConfigWithPaidFeatures {

    public static void main(String[] args) {
        String configurationId = "default-account-dashboard"; // This configuration ID cannot be modified
        String sourceAccountId = "Source Account ID";

        try {
            SelectionCriteria selectionCriteria = new SelectionCriteria()
                    .withDelimiter("/")
                    .withMaxDepth(5)
                    .withMinStorageBytesPercentage(10.0);
            PrefixLevelStorageMetrics prefixStorageMetrics = new PrefixLevelStorageMetrics()
                    .withIsEnabled(true)
                    .withSelectionCriteria(selectionCriteria);
            BucketLevel bucketLevel = new BucketLevel()
                    .withActivityMetrics(new ActivityMetrics().withIsEnabled(true))
                    .withPrefixLevel(new PrefixLevel().withStorageMetrics(prefixStorageMetrics));
            AccountLevel accountLevel = new AccountLevel()
                    .withActivityMetrics(new ActivityMetrics().withIsEnabled(true))
                    .withBucketLevel(bucketLevel);

            StorageLensConfiguration configuration = new StorageLensConfiguration()
                    .withId(configurationId)
                    .withAccountLevel(accountLevel)
                    .withIsEnabled(true);

            AWSS3Control s3ControlClient = AWSS3ControlClient.builder()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(US_WEST_2)
                    .build();

            s3ControlClient.putStorageLensConfiguration(new PutStorageLensConfigurationRequest()
                    .withAccountId(sourceAccountId)
                    .withConfigId(configurationId)
                    .withStorageLensConfiguration(configuration)
            );

        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process
            // it and returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

**Note**  
Additional charges apply for Advanced Metrics and Recommendations\. For more information, see [Advanced Metrics and Recommendations](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens_basics_metrics_recommendations.html#storage_lens_basics_metrics_selection)\.


# Making requests using the REST API<a name="RESTAPI"></a>

This section contains information on how to make requests to Amazon S3 endpoints by using the REST API\. For a list of Amazon S3 endpoints, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\.

**Topics**
+ [Constructing S3 hostnames for REST API requests](#constructing-hostname-rest-api-requests)
+ [Virtual hosted‐style and path‐style requests](#virtual-hosted-path-style-requests)
+ [Making requests to dual\-stack endpoints by using the REST API](#rest-api-dual-stack)
+ [Virtual hosting of buckets](VirtualHosting.md)
+ [Request redirection and the REST API](RESTRedirect.md)

## Constructing S3 hostnames for REST API requests<a name="constructing-hostname-rest-api-requests"></a>

Amazon S3 endpoints follow the structure shown below:

```
s3.Region.amazonaws.com
```

Amazon S3 Access Points endpoints and dual\-stack endpoints also follow the standard structure:
+ **Amazon S3 Access Points** ‐`s3-accesspoint.Region.amazonaws.com`
+ **Dual\-stack** ‐ `s3.dualstack.Region.amazonaws.com` 

For a complete list of Amazon S3 Regions and endpoints, see [Amazon S3 Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\.

## Virtual hosted‐style and path‐style requests<a name="virtual-hosted-path-style-requests"></a>

When making requests by using the REST API, you can use virtual hosted–style or path\-style URIs for the Amazon S3 endpoints\. For more information, see [Virtual hosting of buckets](VirtualHosting.md)\.

**Example Virtual hosted–Style request**  
Following is an example of a virtual hosted–style request to delete the `puppy.jpg` file from the bucket named `examplebucket` in the US West \(Oregon\) Region\. For more information about virtual hosted\-style requests, see [Virtual Hosted\-Style Requests](VirtualHosting.md#virtual-hosted-style-access)\.  

```
1. DELETE /puppy.jpg HTTP/1.1
2. Host: examplebucket.s3.us-west-2.amazonaws.com
3. Date: Mon, 11 Apr 2016 12:00:00 GMT
4. x-amz-date: Mon, 11 Apr 2016 12:00:00 GMT
5. Authorization: authorization string
```

**Example Path\-style request**  
Following is an example of a path\-style version of the same request\.  

```
1. DELETE /examplebucket/puppy.jpg HTTP/1.1
2. Host: s3.us-west-2.amazonaws.com
3. Date: Mon, 11 Apr 2016 12:00:00 GMT
4. x-amz-date: Mon, 11 Apr 2016 12:00:00 GMT
5. Authorization: authorization string
```
Currently Amazon S3 supports virtual hosted\-style and path\-style access in all Regions, but this will be changing \(see the following **Important** note\.  
For more information about path\-style requests, see [Path\-Style Requests](VirtualHosting.md#path-style-access)\.  
Update \(September 23, 2020\) – We have decided to delay the deprecation of path\-style URLs to ensure that customers have the time that they need to transition to virtual hosted\-style URLs\. For more information, see [Amazon S3 Path Deprecation Plan – The Rest of the Story](https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/)\.

## Making requests to dual\-stack endpoints by using the REST API<a name="rest-api-dual-stack"></a>

When using the REST API, you can directly access a dual\-stack endpoint by using a virtual hosted–style or a path style endpoint name \(URI\)\. All Amazon S3 dual\-stack endpoint names include the region in the name\. Unlike the standard IPv4\-only endpoints, both virtual hosted–style and a path\-style endpoints use region\-specific endpoint names\. 

**Example Virtual hosted–Style dual\-stack endpoint request**  
You can use a virtual hosted–style endpoint in your REST request as shown in the following example that retrieves the `puppy.jpg` object from the bucket named `examplebucket` in the US West \(Oregon\) Region\.  

```
1. GET /puppy.jpg HTTP/1.1
2. Host: examplebucket.s3.dualstack.us-west-2.amazonaws.com
3. Date: Mon, 11 Apr 2016 12:00:00 GMT
4. x-amz-date: Mon, 11 Apr 2016 12:00:00 GMT
5. Authorization: authorization string
```

**Example Path\-style dual\-stack endpoint request**  
Or you can use a path\-style endpoint in your request as shown in the following example\.  

```
1. GET /examplebucket/puppy.jpg HTTP/1.1
2. Host: s3.dualstack.us-west-2.amazonaws.com
3. Date: Mon, 11 Apr 2016 12:00:00 GMT
4. x-amz-date: Mon, 11 Apr 2016 12:00:00 GMT
5. Authorization: authorization string
```

For more information about dual\-stack endpoints, see [Using Amazon S3 dual\-stack endpoints](dual-stack-endpoints.md)\.


# Stop multipart uploads to an S3 Bucket using the AWS SDK for \.NET \(high\-level API\)<a name="HLAbortDotNet"></a>

To stop in\-progress  multipart uploads, use the `TransferUtility` class from the AWS SDK for \.NET\. You provide a `DateTime`value\. The API then stops all of the multipart uploads that were initiated before the specified date and time and remove the uploaded parts\. An upload is considered to be in\-progress after you initiate it and it completes or you stop it\. 

Because you are billed for all storage associated with uploaded parts, it's important that you either complete the multipart upload to finish creating the object or stop it to remove uploaded parts\. For more information about Amazon S3 multipart uploads, see [Multipart upload overview](mpuoverview.md)\. For information about pricing, see [Multipart upload and pricing](mpuoverview.md#mpuploadpricing)\.

The following C\# example stop all in\-progress multipart uploads that were initiated on a specific bucket over a week ago\. For information about the example's compatibility with a specific version of the AWS SDK for \.NET and instructions on creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Transfer;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class AbortMPUUsingHighLevelAPITest
    {
        private const string bucketName = "*** provide bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            AbortMPUAsync().Wait();
        }

        private static async Task AbortMPUAsync()
        {
            try
            {
                var transferUtility = new TransferUtility(s3Client);

                // Abort all in-progress uploads initiated before the specified date.
                await transferUtility.AbortMultipartUploadsAsync(
                    bucketName, DateTime.Now.AddDays(-7));
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        } 
    }
}
```

**Note**  
You can also stop a specific multipart upload\. For more information, see [List multipart uploads to an S3 Bucket using the AWS SDK for \.NET \(low\-level\)List multipart uploads ](LLlistMPuploadsDotNet.md)\. 

## More info<a name="HLAbortDotNet-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# Logging Amazon S3 API calls using AWS CloudTrail<a name="cloudtrail-logging"></a>

Amazon S3 is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon S3\. CloudTrail captures a subset of API calls for Amazon S3 as events, including calls from the Amazon S3 console and from code calls to the Amazon S3 APIs\. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon S3\. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in **Event history**\. Using the information collected by CloudTrail, you can determine the request that was made to Amazon S3, the IP address from which the request was made, who made the request, when it was made, and additional details\. 

To learn more about CloudTrail, including how to configure and enable it, see the [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/)\.

## Amazon S3 information in CloudTrail<a name="cloudtrail-logging-s3-info"></a>

CloudTrail is enabled on your AWS account when you create the account\. When supported event activity occurs in Amazon S3, that activity is recorded in a CloudTrail event along with other AWS service events in **Event history**\. You can view, search, and download recent events in your AWS account\. For more information, see [Viewing Events with CloudTrail Event History](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events.html)\. 

For an ongoing record of events in your AWS account, including events for Amazon S3, create a trail\. A trail enables CloudTrail to deliver log files to an Amazon S3 bucket\. By default, when you create a trail in the console, the trail applies to all Regions\. The trail logs events from all Regions in the AWS partition and delivers the log files to the Amazon S3 bucket that you specify\. Additionally, you can configure other AWS services to further analyze and act upon the event data collected in CloudTrail logs\. For more information, see the following: 
+ [Overview for Creating a Trail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail.html)
+ [CloudTrail Supported Services and Integrations](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-aws-service-specific-topics.html#cloudtrail-aws-service-specific-topics-integrations)
+ [Configuring Amazon SNS Notifications for CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/getting_notifications_top_level.html)
+ [Receiving CloudTrail Log Files from Multiple Regions](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/receive-cloudtrail-log-files-from-multiple-regions.html) and [Receiving CloudTrail Log Files from Multiple Accounts](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-receive-logs-from-multiple-accounts.html)

Every event or log entry contains information about who generated the request\. The identity information helps you determine the following: 
+ Whether the request was made with root or IAM user credentials\.
+ Whether the request was made with temporary security credentials for a role or federated user\.
+ Whether the request was made by another AWS service\.

For more information, see the [CloudTrail userIdentity Element](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference-user-identity.html)\.

You can store your log files in your bucket for as long as you want, but you can also define Amazon S3 lifecycle rules to archive or delete log files automatically\. By default, your log files are encrypted by using Amazon S3 server\-side encryption \(SSE\)\.

### Amazon S3 account\-level actions tracked by CloudTrail logging<a name="cloudtrail-account-level-tracking"></a>

CloudTrail logs account\-level actions\. Amazon S3 records are written together with other AWS service records in a log file\. CloudTrail determines when to create and write to a new file based on a time period and file size\. 

The tables in this section list the Amazon S3 account\-level actions that are supported for logging by CloudTrail\.

**Amazon S3 account\-level API actions tracked by CloudTrail logging will show up as the following event names:**
+ [ DeletePublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_DeletePublicAccessBlock.html)
+ [ GetPublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_GetPublicAccessBlock.html)
+ [ PutPublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_PutPublicAccessBlock.html)

### Amazon S3 bucket\-level actions tracked by CloudTrail logging<a name="cloudtrail-bucket-level-tracking"></a>

By default, CloudTrail logs bucket\-level actions\. Amazon S3 records are written together with other AWS service records in a log file\. CloudTrail determines when to create and write to a new file based on a time period and file size\. 

The tables in this section list the Amazon S3 bucket\-level actions that are supported for logging by CloudTrail\.

**Amazon S3 bucket\-level API actions tracked by CloudTrail logging will show up as the following event names:**
+ [CreateBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html) 
+ [DeleteBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETE.html)
+ [DeleteBucketCors](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEcors.html)
+ [DeleteBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEencryption.html)
+ [DeleteBucketLifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETElifecycle.html)
+ [DeleteBucketPolicy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEpolicy.html)
+ [DeleteBucketReplication ](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEreplication.html)
+ [DeleteBucketTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEtagging.html)
+ [DeleteBucketWebsite](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEwebsite.html)
+ [ DeletePublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeletePublicAccessBlock.html)
+ [GetBucketCors](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETcors.html)
+ [GetBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETencryption.html)
+ [GetBucketLifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETencryption.html) 
+ [GetBucketLocation](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlocation.html) 
+ [GetBucketLogging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlogging.html) 
+ [GetBucketNotification](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETnotification.html)
+ [GetBucketPolicy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETpolicy.html)
+ [GetBucketReplication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETreplication.html)
+ [GetBucketRequestPayment](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTrequestPaymentGET.html)
+ [GetBucketTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETtagging.html)
+ [GetBucketVersioning](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETversioningStatus.html) 
+ [GetBucketWebsite](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETwebsite.html) 
+ [ GetPublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_GetPublicAccessBlock.html) 
+ [ListBuckets](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTServiceGET.html) 
+ [PutBucketAcl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTacl.html) 
+ [PutBucketCors](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTcors.html) 
+ [PutBucketEncryption](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTencryption.html)
+ [PutBucketLifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html) 
+ [PutBucketLogging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlogging.html)
+ [PutBucketNotification ](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTnotification.html)
+ [PutBucketPolicy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTpolicy.html) 
+ [PutBucketReplication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTreplication.html) 
+ [PutBucketRequestPayment](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTrequestPaymentPUT.html)
+ [PutBucketTagging ](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTtagging.html) 
+ [PutBucketVersioning](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTVersioningStatus.html)
+ [PutBucketWebsite](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTwebsite.html) 
+ [ PutPublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_PutPublicAccessBlock.html) 



In addition to these API operations, you can also use the [OPTIONS object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTOPTIONSobject.html) object\-level action\. This action is treated like a bucket\-level action in CloudTrail logging because the action checks the cors configuration of a bucket\.

### Amazon S3 object\-level actions tracked by CloudTrail logging<a name="cloudtrail-object-level-tracking"></a>

You can also get CloudTrail logs for object\-level Amazon S3 actions\. To do this, specify the Amazon S3 object for your trail\. When an object\-level action occurs in your account, CloudTrail evaluates your trail settings\. If the event matches the object that you specified in a trail, the event is logged\. For more information, see [How Do I Enable Object\-Level Logging for an S3 Bucket with AWS CloudTrail Data Events?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html) in the *Amazon Simple Storage Service Console User Guide* and [Logging Data Events for Trails](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html) in the *AWS CloudTrail User Guide*\. 

The following table lists the object\-level API actions that are logged as CloudTrail events:
+ [AbortMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadAbort.html)
+ [CompleteMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html)
+ [DeleteObjects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html)
+ [DeleteObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETE.html)
+ [GetObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html)
+ [GetObjectAcl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETacl.html)
+ [GetObjectTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtagging.html)
+ [GetObjectTorrent](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtorrent.html)
+ [HeadObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html)
+ [CreateMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)
+ [ListParts](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html)
+ [PostObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)
+ [RestoreObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html)
+ [PutObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)
+ [PutObjectAcl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUTacl.html)
+ [PutObjectTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUTtagging.html)
+ [CopyObject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)
+ [UploadPart](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html)
+ [UploadPartCopy](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html)

In addition to these operations, you can use the following bucket\-level operations to get CloudTrail logs as object\-level Amazon S3 actions under certain conditions:
+ [GET Bucket \(List Objects\) Version 2](https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html) – Select a prefix specified in the trail\.
+ [GET Bucket Object versions](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETVersion.html) – Select a prefix specified in the trail\.
+ [HEAD Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketHEAD.html) – Specify a bucket and an empty prefix\.
+ [Delete Multiple Objects](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html) – Specify a bucket and an empty prefix\. 
**Note**  
CloudTrail does not log key names for the keys that are deleted using the Delete Multiple Objects operation\.

#### Object\-level actions in cross\-account scenarios<a name="cloudtrail-object-level-crossaccount"></a>

The following are special use cases involving the object\-level API calls in cross\-account scenarios and how CloudTrail logs are reported\. CloudTrail always delivers logs to the requester \(who made the API call\)\. When setting up cross\-account access, consider the examples in this section\.

**Note**  
The examples assume that CloudTrail logs are appropriately configured\. 

##### Example 1: CloudTrail delivers access logs to the bucket owner<a name="cloudtrail-crossaccount-example1"></a>

CloudTrail delivers access logs to the bucket owner only if the bucket owner has permissions for the same object API\. Consider the following cross\-account scenario:
+ Account\-A owns the bucket\.
+ Account\-B \(the requester\) tries to access an object in that bucket\.

CloudTrail always delivers object\-level API access logs to the requester\. In addition, CloudTrail also delivers the same logs to the bucket owner only if the bucket owner has permissions for the same API actions on that object\. 

**Note**  
If the bucket owner is also the object owner, the bucket owner gets the object access logs\. Otherwise, the bucket owner must get permissions, through the object ACL, for the same object API to get the same object\-access API logs\.

##### Example 2: CloudTrail does not proliferate email addresses used in setting object ACLs<a name="cloudtrail-crossaccount-example2"></a>

Consider the following cross\-account scenario:
+ Account\-A owns the bucket\.
+  Account\-B \(the requester\) sends a request to set an object ACL grant using an email address\. For information about ACLs, see [Access Control List \(ACL\) Overview](acl-overview.md)\.

The request gets the logs along with the email information\. However, the bucket owner—if they are eligible to receive logs, as in example 1—gets the CloudTrail log reporting the event\. However, the bucket owner doesn't get the ACL configuration information, specifically the grantee email and the grant\. The only information that the log tells the bucket owner is that an ACL API call was made by Account\-B\.

### CloudTrail tracking with Amazon S3 SOAP API calls<a name="cloudtrail-s3-soap"></a>

CloudTrail tracks Amazon S3 SOAP API calls\. Amazon S3 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. For more information about Amazon S3 SOAP support, see [Appendix a: Using the SOAP API](SOAPAPI3.md)\. 

**Important**  
Newer Amazon S3 features are not supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\.


**Amazon S3 SOAP actions tracked by CloudTrail logging**  

| SOAP API name | API event name used in CloudTrail log | 
| --- | --- | 
|  [ListAllMyBuckets](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPListAllMyBuckets.html)  | ListBuckets | 
|  [CreateBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPCreateBucket.html)  | CreateBucket | 
|  [DeleteBucket](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPDeleteBucket.html)  | DeleteBucket | 
|  [GetBucketAccessControlPolicy](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPGetBucketAccessControlPolicy.html)  | GetBucketAcl | 
|  [SetBucketAccessControlPolicy](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPSetBucketAccessControlPolicy.html)  | PutBucketAcl | 
|  [GetBucketLoggingStatus](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPGetBucketLoggingStatus.html)  | GetBucketLogging | 
|  [SetBucketLoggingStatus](https://docs.aws.amazon.com/AmazonS3/latest/API/SOAPSetBucketLoggingStatus.html)  | PutBucketLogging | 

## Using CloudTrail logs with Amazon S3 server access logs and CloudWatch Logs<a name="cloudtrail-logging-vs-server-logs"></a>

AWS CloudTrail logs provide a record of actions taken by a user, role, or an AWS service in Amazon S3, while Amazon S3 server access logs provide detailed records for the requests that are made to an S3 bucket\. For more information about how the different logs work, and their properties, performance and costs, see [Logging with Amazon S3](logging-with-S3.md)\. 

You can use AWS CloudTrail logs together with server access logs for Amazon S3\. CloudTrail logs provide you with detailed API tracking for Amazon S3 bucket\-level and object\-level operations\. Server access logs for Amazon S3 provide you visibility into object\-level operations on your data in Amazon S3\. For more information about server access logs, see [Amazon S3 server access logging](ServerLogs.md)\.

You can also use CloudTrail logs together with CloudWatch for Amazon S3\. CloudTrail integration with CloudWatch Logs delivers S3 bucket\-level API activity captured by CloudTrail to a CloudWatch log stream in the CloudWatch log group that you specify\. You can create CloudWatch alarms for monitoring specific API activity and receive email notifications when the specific API activity occurs\. For more information about CloudWatch alarms for monitoring specific API activity, see the [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/)\. For more information about using CloudWatch with Amazon S3, see [Monitoring metrics with Amazon CloudWatch](cloudwatch-monitoring.md)\.

## Example: Amazon S3 log file entries<a name="cloudtrail-logging-understanding-s3-entries"></a>

 A trail is a configuration that enables delivery of events as log files to an Amazon S3 bucket that you specify\. CloudTrail log files contain one or more log entries\. An event represents a single request from any source\. It includes information about the requested action, the date and time of the action, request parameters, and so on\. CloudTrail log files are not an ordered stack trace of the public API calls, so they do not appear in any specific order\.

The following example shows a CloudTrail log entry that demonstrates the [GET Service](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTServiceGET.html), [PUT Bucket acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTacl.html), and [GET Bucket versioning](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETversioningStatus.html) actions\.

```
{
    "Records": [
    {
        "eventVersion": "1.03",
        "userIdentity": {
            "type": "IAMUser",
            "principalId": "111122223333",
            "arn": "arn:aws:iam::111122223333:user/myUserName",
            "accountId": "111122223333",
            "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
            "userName": "myUserName"
        },
        "eventTime": "2019-02-01T03:18:19Z",
        "eventSource": "s3.amazonaws.com",
        "eventName": "ListBuckets",
        "awsRegion": "us-west-2",
        "sourceIPAddress": "127.0.0.1",
        "userAgent": "[]",
        "requestParameters": {
            "host": [
                "s3.us-west-2.amazonaws.com"
            ]
        },
        "responseElements": null,
        "additionalEventData": {
            "SignatureVersion": "SigV2",
            "AuthenticationMethod": "QueryString"
    },
        "requestID": "47B8E8D397DCE7A6",
        "eventID": "cdc4b7ed-e171-4cef-975a-ad829d4123e8",
        "eventType": "AwsApiCall",
        "recipientAccountId": "111122223333"
    },
    {
       "eventVersion": "1.03",
       "userIdentity": {
            "type": "IAMUser",
            "principalId": "111122223333",
            "arn": "arn:aws:iam::111122223333:user/myUserName",
            "accountId": "111122223333",
            "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
            "userName": "myUserName"
        },
      "eventTime": "2019-02-01T03:22:33Z",
      "eventSource": "s3.amazonaws.com",
      "eventName": "PutBucketAcl",
      "awsRegion": "us-west-2",
      "sourceIPAddress": "",
      "userAgent": "[]",
      "requestParameters": {
          "bucketName": "",
          "AccessControlPolicy": {
              "AccessControlList": {
                  "Grant": {
                      "Grantee": {
                          "xsi:type": "CanonicalUser",
                          "xmlns:xsi": "http://www.w3.org/2001/XMLSchema-instance",
                          "ID": "d25639fbe9c19cd30a4c0f43fbf00e2d3f96400a9aa8dabfbbebe1906Example"
                       },
                      "Permission": "FULL_CONTROL"
                   }
              },
              "xmlns": "http://s3.amazonaws.com/doc/2006-03-01/",
              "Owner": {
                  "ID": "d25639fbe9c19cd30a4c0f43fbf00e2d3f96400a9aa8dabfbbebe1906Example"
              }
          }
          "host": [
              "s3.us-west-2.amazonaws.com"
          ],
          "acl": [
              ""
          ]
      },
      "responseElements": null,
      "additionalEventData": {
          "SignatureVersion": "SigV4",
          "CipherSuite": "ECDHE-RSA-AES128-SHA",
          "AuthenticationMethod": "AuthHeader"
      },
      "requestID": "BD8798EACDD16751",
      "eventID": "607b9532-1423-41c7-b048-ec2641693c47",
      "eventType": "AwsApiCall",
      "recipientAccountId": "111122223333"
    },
    {
      "eventVersion": "1.03",
      "userIdentity": {
          "type": "IAMUser",
          "principalId": "111122223333",
          "arn": "arn:aws:iam::111122223333:user/myUserName",
          "accountId": "111122223333",
          "accessKeyId": "AKIAIOSFODNN7EXAMPLE",
          "userName": "myUserName"
        },
      "eventTime": "2019-02-01T03:26:37Z",
      "eventSource": "s3.amazonaws.com",
      "eventName": "GetBucketVersioning",
      "awsRegion": "us-west-2",
      "sourceIPAddress": "",
      "userAgent": "[]",
      "requestParameters": {
          "host": [
              "s3.us-west-2.amazonaws.com"
          ],
          "bucketName": "DOC-EXAMPLE-BUCKET1",
          "versioning": [
              ""
          ]
      },
      "responseElements": null,
      "additionalEventData": {
          "SignatureVersion": "SigV4",
          "CipherSuite": "ECDHE-RSA-AES128-SHA",
          "AuthenticationMethod": "AuthHeader",
    },
      "requestID": "07D681279BD94AED",
      "eventID": "f2b287f3-0df1-4961-a2f4-c4bdfed47657",
      "eventType": "AwsApiCall",
      "recipientAccountId": "111122223333"
    }
  ]
}
```

## Related resources<a name="cloudtrail-logging-related-resources"></a>
+ [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/)
+ [CloudTrail Event Reference](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-event-reference.html)
+ [ Using AWS CloudTrail to identify Amazon S3 requests](cloudtrail-request-identification.md)


# Get an object Using the AWS SDK for \.NET<a name="RetrievingObjectUsingNetSDK"></a>

When you download an object, you get all of the object's metadata and a stream from which to read the contents\. You should read the content of the stream as quickly as possible because the data is streamed directly from Amazon S3 and your network connection will remain open until you read all the data or close the input stream\. You do the following to get an object:
+ Run the `getObject` method by providing bucket name and object key in the request\.
+ Run one of the `GetObjectResponse` methods to process the stream\.

The following are some variations you might use:
+ Instead of reading the entire object, you can read only the portion of the object data by specifying the byte range in the request, as shown in the following C\# example:  
**Example**  

  ```
  1. GetObjectRequest request = new GetObjectRequest 
  2. {
  3.     BucketName = bucketName,
  4.     Key = keyName,
  5.     ByteRange = new ByteRange(0, 10)
  6. };
  ```
+ When retrieving an object, you can optionally override the response header values \(see [Getting objects](GettingObjectsUsingAPIs.md)\) by using the `ResponseHeaderOverrides` object and setting the corresponding request property\. The following C\# code example shows how to do this\. For example, you can use this feature to indicate that the object should be downloaded into a file with a different file name than the object key name\.   
**Example**  

  ```
   1. GetObjectRequest request = new GetObjectRequest 
   2. {
   3.     BucketName = bucketName,
   4.     Key = keyName
   5. };
   6. 
   7. ResponseHeaderOverrides responseHeaders = new ResponseHeaderOverrides();
   8. responseHeaders.CacheControl = "No-cache";
   9. responseHeaders.ContentDisposition = "attachment; filename=testing.txt";
  10. 
  11. request.ResponseHeaderOverrides = responseHeaders;
  ```



**Example**  
The following C\# code example retrieves an object from an Amazon S3 bucket\. From the response, the example reads the object data using the `GetObjectResponse.ResponseStream` property\. The example also shows how you can use the `GetObjectResponse.Metadata` collection to read object metadata\. If the object you retrieve has the `x-amz-meta-title` metadata, the code prints the metadata value\.  
For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.IO;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class GetObjectTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string keyName = "*** object key ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            ReadObjectDataAsync().Wait();
        }

        static async Task ReadObjectDataAsync()
        {
            string responseBody = "";
            try
            {
                GetObjectRequest request = new GetObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName
                };
                using (GetObjectResponse response = await client.GetObjectAsync(request))
                using (Stream responseStream = response.ResponseStream)
                using (StreamReader reader = new StreamReader(responseStream))
                {
                    string title = response.Metadata["x-amz-meta-title"]; // Assume you have "title" as medata added to the object.
                    string contentType = response.Headers["Content-Type"];
                    Console.WriteLine("Object metadata, Title: {0}", title);
                    Console.WriteLine("Content type: {0}", contentType);

                    responseBody = reader.ReadToEnd(); // Now you process the response body.
                }
            }
            catch (AmazonS3Exception e)
            {
                // If bucket or object does not exist
                Console.WriteLine("Error encountered ***. Message:'{0}' when reading object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when reading object", e.Message);
            }
        }
    }
}
```


# Initiate restore object<a name="batch-ops-initiate-restore-object"></a>

You can use S3 Batch Operations to perform large\-scale Batch Operations on Amazon S3 objects\. S3 Batch Operations can run a single operation on a list of Amazon S3 objects that you specify including initiating object restores from Amazon S3 Glacier\. For more information, see [Performing S3 Batch Operations](batch-ops.md)\. 

Objects that you archive to the S3 Glacier or S3 Glacier Deep Archive storage classes are not accessible in real time\. Using the `InitiateRestore` operation in your S3 Batch Operations sends a restore request to S3 Glacier for each object that is specified in the manifest\. To create an Initiate Restore Object job, you must include two elements with your request:
+ **ExpirationInDays**
  + When you restore an object from S3 Glacier, the restored object is only a temporary copy, which Amazon S3 deletes after a fixed period of time\. This element specifies how long the temporary copy will remain available in Amazon S3\. After the temporary copy expires, you can only retrieve the object by restoring it from S3 Glacier again\. For more information about object restoration, see [Restoring archived objects](restoring-objects.md)\.
+ **GlacierJobTier**
  + Amazon S3 can restore objects from S3 Glacier according to three different retrieval tiers: Expedited, Standard, and Bulk\. S3 Batch Operations support only the Standard and Bulk tiers\. For more information about S3 Glacier retrieval tiers, see [Archive retrieval options](restoring-objects.md#restoring-objects-retrieval-options)\. For more information about pricing for each tier, see the "Retrieval pricing" section at [Amazon S3 Glacier pricing](https://aws.amazon.com/glacier/pricing/)\.

**Important**  
The Initiate Restore Object job only initiates the request to restore objects\. S3 Batch Operations report the job as complete for each object after the request has been initiated for that object\. Amazon S3 doesn't update the job or otherwise notify you when the objects have been restored\. However, you can use event notifications to receive notifications when the objects are available in Amazon S3\. For more information, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.



## Overlapping restores<a name="batch-ops-initiate-restore-object-in-progress"></a>

If your Initiate Restore Object job tries to restore an object that is already in the process of being restored, S3 Batch Operations will behave as follows:

The restore operation succeeds for the object if either of the following conditions are true:
+ Compared to the restoration request already in progress, this job's `ExpirationInDays` is the same and `GlacierJobTier` is faster\.
+ The previous restoration request has already completed and the object is currently available\. In this case, S3 Batch Operations update the expiration date of the restored object to match the `ExpirationInDays` specified in this job\.

The restore operation fails for the object if any of the following conditions are true:
+ The restoration request already in progress has not yet completed and the restoration duration for this job \(specified by `ExpirationInDays`\) is different than the restoration duration that is specified in the in\-progress restoration request\.
+ The restoration tier for this job \(specified by `GlacierJobTier`\) is the same or slower than the restoration tier that is specified in the in\-progress restoration request\.

## Limitations<a name="batch-ops-initiate-restore-object-limitations"></a>

Initiate Restore Object jobs have the following limitations:
+ You must create an Initiate Restore Object job in the same Region as the archived objects\.
+ S3 Batch Operations do not support S3 Glacier SELECT\.
+ S3 Batch Operations do not support the Expedited retrieval tier\.


# Configuring Amazon S3 event notifications<a name="NotificationHowTo"></a>

The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket\. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications\. You store this configuration in the *notification* subresource that is associated with a bucket\. For more information, see [Bucket configuration options](UsingBucket.md#bucket-config-options-intro)\. Amazon S3 provides an API for you to manage this subresource\. 

**Important**  
Amazon S3 event notifications are designed to be delivered at least once\. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer\.  
If two writes are made to a single non\-versioned object at the same time, it is possible that only a single event notification will be sent\. If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket\. With versioning, every successful write will create a new version of your object and will also send an event notification\.

**Topics**
+ [Overview of notifications](#notification-how-to-overview)
+ [How to enable event notifications](#how-to-enable-disable-notification-intro)
+ [Event notification types and destinations](#notification-how-to-event-types-and-destinations)
+ [Configuring notifications with object key name filtering](#notification-how-to-filtering)
+ [Granting permissions to publish event notification messages to a destination](#grant-destinations-permissions-to-s3)
+ [Walkthrough: Configure a bucket for notifications \(SNS topic or SQS queue\)](ways-to-add-notification-config-to-bucket.md)
+ [Event message structure](notification-content-structure.md)

## Overview of notifications<a name="notification-how-to-overview"></a>

 Currently, Amazon S3 can publish notifications for the following events:
+ **New object created events** — Amazon S3 supports multiple APIs to create objects\. You can request notification when only a specific API is used \(for example, `s3:ObjectCreated:Put`\), or you can use a wildcard \(for example, `s3:ObjectCreated:*`\) to request notification when an object is created regardless of the API used\. 
+ **Object removal events** — Amazon S3 supports deletes of versioned and unversioned objects\. For information about object versioning, see [Object Versioning](ObjectVersioning.md) and [Using versioning](Versioning.md)\. 

  You can request notification when a non\-versioned object is deleted or a versioned object is permanently deleted by using the `s3:ObjectRemoved:Delete` event type\. Or you can request notification when a delete marker is created for a versioned object by using `s3:ObjectRemoved:DeleteMarkerCreated`\. You can also use a wildcard `s3:ObjectRemoved:*` to request notification anytime an object is deleted\. For information about deleting versioned objects, see [Deleting object versions](DeletingObjectVersions.md)\. 
+ **Restore object events **— Amazon S3 supports the restoration of objects archived to the S3 Glacier storage classes\. You request to be notified of object restoration completion by using `s3:ObjectRestore:Completed`\. You use `s3:ObjectRestore:Post` to request notification of the initiation of a restore\.
+ **Reduced Redundancy Storage \(RRS\) object lost events** — Amazon S3 sends a notification message when it detects that an object of the RRS storage class has been lost\. 
+ **Replication events** — Amazon S3 sends event notifications for replication configurations that have S3 Replication Time Control \(S3 RTC\) enabled\. It sends these notifications when an object fails replication, when an object exceeds the 15\-minute threshold, when an object is replicated after the 15\-minute threshold, and when an object is no longer tracked by replication metrics\. It publishes a second event when that object replicates to the destination Region\.

For a list of supported event types, see [Supported event types](#supported-notification-event-types)\. 

Amazon S3 supports the following destinations where it can publish events:
+ **Amazon Simple Notification Service \(Amazon SNS\) topic**

  Amazon SNS is a flexible, fully managed push messaging service\. Using this service, you can push messages to mobile devices or distributed services\. With SNS you can publish a message once, and deliver it one or more times\. For more information about SNS, see the [Amazon SNS](https://aws.amazon.com/sns/) product detail page\.
+ **Amazon Simple Queue Service \(Amazon SQS\) queue**

  Amazon SQS is a scalable and fully managed message queuing service\. You can use SQS to transmit any volume of data without requiring other services to be always available\. In your notification configuration, you can request that Amazon S3 publish events to an SQS queue\. 

  Currently, Standard SQS queue is only allowed as an Amazon S3 event notification destination, whereas FIFO SQS queue is not allowed\. For more information about Amazon SQS, see the [Amazon SQS](https://aws.amazon.com/sqs/) product detail page\.
+ **AWS Lambda**

  AWS Lambda is a compute service that makes it easy for you to build applications that respond quickly to new information\. AWS Lambda runs your code in response to events such as image uploads, in\-app activity, website clicks, or outputs from connected devices\. 

  You can use AWS Lambda to extend other AWS services with custom logic, or create your own backend that operates at AWS scale, performance, and security\. With AWS Lambda, you can easily create discrete, event\-driven applications that run only when needed and scale automatically from a few requests per day to thousands per second\. 

  AWS Lambda can run custom code in response to Amazon S3 bucket events\. You upload your custom code to AWS Lambda and create what is called a Lambda function\. When Amazon S3 detects an event of a specific type \(for example, an object created event\), it can publish the event to AWS Lambda and invoke your function in Lambda\. In response, AWS Lambda runs your function\. 

**Warning**  
If your notification ends up writing to the bucket that triggers the notification, this could cause an execution loop\. For example, if the bucket triggers a Lambda function each time an object is uploaded, and the function uploads an object to the bucket, then the function indirectly triggers itself\. To avoid this, use two buckets, or configure the trigger to only apply to a prefix used for incoming objects\.  
For more information and an example of using Amazon S3 notifications with AWS Lambda, see [Using AWS Lambda with Amazon S3](https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html) in the *AWS Lambda Developer Guide*\. 

## How to enable event notifications<a name="how-to-enable-disable-notification-intro"></a>

Enabling notifications is a bucket\-level operation; that is, you store notification configuration information in the *notification* subresource associated with a bucket\. After creating or changing the bucket notification configuration, typically you need to wait 5 minutes for the changes to take effect\. You can use any of the following methods to manage notification configuration:
+ **Using the Amazon S3 console**

  The console UI enables you to set a notification configuration on a bucket without having to write any code\. For more information, see [How Do I Enable and Configure Event Notifications for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-event-notifications.html) in the *Amazon Simple Storage Service Console User Guide*\.
+ **Programmatically using the AWS SDKs**
**Note**  
If you need to, you can also make the Amazon S3 REST API calls directly from your code\. However, this can be cumbersome because it requires you to write code to authenticate your requests\. 

  Internally, both the console and the SDKs call the Amazon S3 REST API to manage *notification* subresources associated with the bucket\. For notification configuration using AWS SDK examples, see [Walkthrough: Configure a bucket for notifications \(SNS topic or SQS queue\)](ways-to-add-notification-config-to-bucket.md)\.

  Regardless of the method that you use, Amazon S3 stores the notification configuration as XML in the *notification* subresource associated with a bucket\. For information about bucket subresources, see [Bucket configuration options](UsingBucket.md#bucket-config-options-intro)\. 

  By default, notifications are not enabled for any type of event\. Therefore, initially the *notification* subresource stores an empty configuration\.

  ```
  <NotificationConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/"> 
  </NotificationConfiguration>
  ```

  To enable notifications for events of specific types, you replace the XML with the appropriate configuration that identifies the event types you want Amazon S3 to publish and the destination where you want the events published\. For each destination, you add a corresponding XML configuration\. For example: 
  + **Publish event messages to an SQS queue** — To set an SQS queue as the notification destination for one or more event types, you add the `QueueConfiguration`\.

    ```
    <NotificationConfiguration>
      <QueueConfiguration>
        <Id>optional-id-string</Id>
        <Queue>sqs-queue-arn</Queue>
        <Event>event-type</Event>
        <Event>event-type</Event>
         ...
      </QueueConfiguration>
       ...
    </NotificationConfiguration>
    ```
  + **Publish event messages to an SNS topic** — To set an SNS topic as the notification destination for specific event types, you add the `TopicConfiguration`\.

    ```
    <NotificationConfiguration>
      <TopicConfiguration>
         <Id>optional-id-string</Id>
         <Topic>sns-topic-arn</Topic>
         <Event>event-type</Event>
         <Event>event-type</Event>
          ...
      </TopicConfiguration>
       ...
    </NotificationConfiguration>
    ```
  + **Invoke the AWS Lambda function and provide an event message as an argument **— To set a Lambda function as the notification destination for specific event types, you add the `CloudFunctionConfiguration`\.

    ```
    <NotificationConfiguration>
      <CloudFunctionConfiguration>   
         <Id>optional-id-string</Id>   
         <CloudFunction>cloud-function-arn</CloudFunction>        
         <Event>event-type</Event>      
         <Event>event-type</Event>      
          ...  
      </CloudFunctionConfiguration>
       ...
    </NotificationConfiguration>
    ```

  To remove all notifications configured on a bucket, you save an empty `<NotificationConfiguration/>` element in the *notification* subresource\. 

  When Amazon S3 detects an event of the specific type, it publishes a message with the event information\. For more information, see [Event message structure](notification-content-structure.md)\. 

## Event notification types and destinations<a name="notification-how-to-event-types-and-destinations"></a>

This section describes the event notification types that are supported by Amazon S3 and the type of destinations where the notifications can be published\.

### Supported event types<a name="supported-notification-event-types"></a>

Amazon S3 can publish events of the following types\. You specify these event types in the notification configuration\.


|  Event types |  Description  | 
| --- | --- | 
|  *s3:ObjectCreated:\** *s3:ObjectCreated:Put* *s3:ObjectCreated:Post* *s3:ObjectCreated:Copy* *s3:ObjectCreated:CompleteMultipartUpload*  | Amazon S3 APIs such as PUT, POST, and COPY can create an object\. Using these event types, you can enable notification when an object is created using a specific API, or you can use the *s3:ObjectCreated:\** event type to request notification regardless of the API that was used to create an object\.  You do not receive event notifications from failed operations\.  | 
|  *s3:ObjectRemoved:\** *s3:ObjectRemoved:Delete* *s3:ObjectRemoved:DeleteMarkerCreated*  | By using the *ObjectRemoved* event types, you can enable notification when an object or a batch of objects is removed from a bucket\. You can request notification when an object is deleted or a versioned object is permanently deleted by using the *s3:ObjectRemoved:Delete* event type\. Or you can request notification when a delete marker is created for a versioned object by using *s3:ObjectRemoved:DeleteMarkerCreated*\. For information about deleting versioned objects, see [Deleting object versions](DeletingObjectVersions.md)\. You can also use a wildcard `s3:ObjectRemoved:*` to request notification anytime an object is deleted\.  You do not receive event notifications from automatic deletes from lifecycle policies or from failed operations\.  | 
|  *s3:ObjectRestore:Post* *s3:ObjectRestore:Completed*  |  Using restore object event types you can receive notifications for initiation and completion when restoring objects from the S3 Glacier storage class\. You use `s3:ObjectRestore:Post` to request notification of object restoration initiation\. You use `s3:ObjectRestore:Completed` to request notification of restoration completion\.   | 
| s3:ReducedRedundancyLostObject | You can use this event type to request Amazon S3 to send a notification message when Amazon S3 detects that an object of the RRS storage class is lost\. | 
| s3:Replication:OperationFailedReplication | You receive this notification event when an object that was eligible for replication using Amazon S3 Replication Time Control failed to replicate\. | 
| s3:Replication:OperationMissedThreshold | You receive this notification event when an object that was eligible for replication using Amazon S3 Replication Time Control exceeded the 15\-minute threshold for replication\. | 
| s3:Replication:OperationReplicatedAfterThreshold | You receive this notification event for an object that was eligible for replication using the Amazon S3 Replication Time Control feature replicated after the 15\-minute threshold\. | 
| s3:Replication:OperationNotTracked | You receive this notification event for an object that was eligible for replication using Amazon S3 Replication Time Control but is no longer tracked by replication metrics\. | 

### Supported destinations<a name="supported-notification-destinations"></a>

Amazon S3 can send event notification messages to the following destinations\. You specify the ARN value of these destinations in the notification configuration\.
+ Publish event messages to an Amazon Simple Notification Service \(Amazon SNS\) topic
+ Publish event messages to an Amazon Simple Queue Service \(Amazon SQS\) queue
**Note**  
If the destination queue or topic is SSE enabled, Amazon S3 will need access to the associated AWS Key Management Service \(AWS KMS\) customer master key \(CMK\) to enable message encryption\.
+ Publish event messages to AWS Lambda by invoking a Lambda function and providing the event message as an argument

You must grant Amazon S3 permissions to post messages to an Amazon SNS topic or an Amazon SQS queue\. You must also grant Amazon S3 permission to invoke an AWS Lambda function on your behalf\. For information about granting these permissions, see [Granting permissions to publish event notification messages to a destination](#grant-destinations-permissions-to-s3)\. 

## Configuring notifications with object key name filtering<a name="notification-how-to-filtering"></a>

You can configure notifications to be filtered by the prefix and suffix of the key name of objects\. For example, you can set up a configuration so that you are sent a notification only when image files with a "`.jpg`" file name extension are added to a bucket\. Or, you can have a configuration that delivers a notification to an Amazon SNS topic when an object with the prefix "`images/`" is added to the bucket, while having notifications for objects with a "`logs/`" prefix in the same bucket delivered to an AWS Lambda function\. 

You can set up notification configurations that use object key name filtering in the Amazon S3 console and by using Amazon S3 APIs through the AWS SDKs or the REST APIs directly\. For information about using the console UI to set a notification configuration on a bucket, see [ How Do I Enable and Configure Event Notifications for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-event-notifications.html) in the *Amazon Simple Storage Service Console User Guide*\. 

Amazon S3 stores the notification configuration as XML in the *notification* subresource associated with a bucket as described in [How to enable event notifications ](#how-to-enable-disable-notification-intro)\. You use the `Filter` XML structure to define the rules for notifications to be filtered by the prefix and/or suffix of an object key name\. For information about the details of the `Filter` XML structure, see [PUT Bucket notification](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTnotification.html) in the *Amazon Simple Storage Service API Reference*\. 

Notification configurations that use `Filter` cannot define filtering rules with overlapping prefixes, overlapping suffixes, or prefix and suffix overlapping\. The following sections have examples of valid notification configurations with object key name filtering\. They also contain examples of notification configurations that are invalid because of prefix/suffix overlapping\. 

### Examples of valid notification configurations with object key name filtering<a name="notification-how-to-filtering-example-valid"></a>

The following notification configuration contains a queue configuration identifying an Amazon SQS queue for Amazon S3 to publish events to of the `s3:ObjectCreated:Put` type\. The events will be published whenever an object that has a prefix of `images/` and a `jpg` suffix is PUT to a bucket\. 

```
<NotificationConfiguration>
  <QueueConfiguration>
      <Id>1</Id>
      <Filter>
          <S3Key>
              <FilterRule>
                  <Name>prefix</Name>
                  <Value>images/</Value>
              </FilterRule>
              <FilterRule>
                  <Name>suffix</Name>
                  <Value>jpg</Value>
              </FilterRule>
          </S3Key>
     </Filter>
     <Queue>arn:aws:sqs:us-west-2:444455556666:s3notificationqueue</Queue>
     <Event>s3:ObjectCreated:Put</Event>
  </QueueConfiguration>
</NotificationConfiguration>
```

The following notification configuration has multiple non\-overlapping prefixes\. The configuration defines that notifications for PUT requests in the `images/` folder go to queue\-A, while notifications for PUT requests in the `logs/` folder go to queue\-B\.

```
<NotificationConfiguration>
  <QueueConfiguration>
     <Id>1</Id>
     <Filter>
            <S3Key>
                <FilterRule>
                    <Name>prefix</Name>
                    <Value>images/</Value>
                </FilterRule>
            </S3Key>
     </Filter>
     <Queue>arn:aws:sqs:us-west-2:444455556666:sqs-queue-A</Queue>
     <Event>s3:ObjectCreated:Put</Event>
  </QueueConfiguration>
  <QueueConfiguration>
     <Id>2</Id>
     <Filter>
            <S3Key>
                <FilterRule>
                    <Name>prefix</Name>
                    <Value>logs/</Value>
                </FilterRule>
            </S3Key>
     </Filter>
     <Queue>arn:aws:sqs:us-west-2:444455556666:sqs-queue-B</Queue>
     <Event>s3:ObjectCreated:Put</Event>
  </QueueConfiguration>
</NotificationConfiguration>
```

The following notification configuration has multiple non\-overlapping suffixes\. The configuration defines that all `.jpg` images newly added to the bucket are processed by Lambda cloud\-function\-A, and all newly added `.png` images are processed by cloud\-function\-B\. The `.png` and `.jpg` suffixes are not overlapping even though they have the same last letter\. Two suffixes are considered overlapping if a given string can end with both suffixes\. A string cannot end with both `.png` and `.jpg`, so the suffixes in the example configuration are not overlapping suffixes\. 

```
<NotificationConfiguration>
  <CloudFunctionConfiguration>
     <Id>1</Id>
     <Filter>
            <S3Key>
                <FilterRule>
                    <Name>suffix</Name>
                    <Value>.jpg</Value>
                </FilterRule>
            </S3Key>
     </Filter>
     <CloudFunction>arn:aws:lambda:us-west-2:444455556666:cloud-function-A</CloudFunction>
     <Event>s3:ObjectCreated:Put</Event>
  </CloudFunctionConfiguration>
  <CloudFunctionConfiguration>
     <Id>2</Id>
     <Filter>
            <S3Key>
                <FilterRule>
                    <Name>suffix</Name>
                    <Value>.png</Value>
                </FilterRule>
            </S3Key>
     </Filter>
     <CloudFunction>arn:aws:lambda:us-west-2:444455556666:cloud-function-B</CloudFunction>
     <Event>s3:ObjectCreated:Put</Event>
  </CloudFunctionConfiguration>
</NotificationConfiguration>
```

Your notification configurations that use `Filter` cannot define filtering rules with overlapping prefixes for the same event types, unless the overlapping prefixes are used with suffixes that do not overlap\. The following example configuration shows how objects created with a common prefix but non\-overlapping suffixes can be delivered to different destinations\.

```
<NotificationConfiguration>
  <CloudFunctionConfiguration>
     <Id>1</Id>
     <Filter>
            <S3Key>
                <FilterRule>
                    <Name>prefix</Name>
                    <Value>images</Value>
                </FilterRule>
                <FilterRule>
                    <Name>suffix</Name>
                    <Value>.jpg</Value>
                </FilterRule>
            </S3Key>
     </Filter>
     <CloudFunction>arn:aws:lambda:us-west-2:444455556666:cloud-function-A</CloudFunction>
     <Event>s3:ObjectCreated:Put</Event>
  </CloudFunctionConfiguration>
  <CloudFunctionConfiguration>
     <Id>2</Id>
     <Filter>
            <S3Key>
                <FilterRule>
                    <Name>prefix</Name>
                    <Value>images</Value>
                </FilterRule>
                <FilterRule>
                    <Name>suffix</Name>
                    <Value>.png</Value>
                </FilterRule>
            </S3Key>
     </Filter>
     <CloudFunction>arn:aws:lambda:us-west-2:444455556666:cloud-function-B</CloudFunction>
     <Event>s3:ObjectCreated:Put</Event>
  </CloudFunctionConfiguration>
</NotificationConfiguration>
```

### Examples of notification configurations with invalid Prefix/Suffix overlapping<a name="notification-how-to-filtering-examples-invalid"></a>

For the most part, your notification configurations that use `Filter` cannot define filtering rules with overlapping prefixes, overlapping suffixes, or overlapping combinations of prefixes and suffixes for the same event types\. \(You can have overlapping prefixes as long as the suffixes do not overlap\. For an example, see [Configuring notifications with object key name filtering](#notification-how-to-filtering)\.\)

You can use overlapping object key name filters with different event types\. For example, you could create a notification configuration that uses the prefix `image/` for the `ObjectCreated:Put` event type and the prefix `image/` for the `ObjectRemoved:*` event type\. 

You get an error if you try to save a notification configuration that has invalid overlapping name filters for the same event types when using the Amazon S3 console or API\. This section shows examples of notification configurations that are not valid because of overlapping name filters\. 

Any existing notification configuration rule is assumed to have a default prefix and suffix that match any other prefix and suffix respectively\. The following notification configuration is not valid because it has overlapping prefixes, where the root prefix overlaps with any other prefix\. \(The same thing would be true if you were using a suffix instead of a prefix in this example\. The root suffix overlaps with any other suffix\.\)

```
<NotificationConfiguration>
     <TopicConfiguration>
         <Topic>arn:aws:sns:us-west-2:444455556666:sns-notification-one</Topic>
         <Event>s3:ObjectCreated:*</Event>
    </TopicConfiguration>
    <TopicConfiguration>
         <Topic>arn:aws:sns:us-west-2:444455556666:sns-notification-two</Topic>
         <Event>s3:ObjectCreated:*</Event>
         <Filter>
             <S3Key>
                 <FilterRule>
                     <Name>prefix</Name>
                     <Value>images</Value>
                 </FilterRule>
            </S3Key>
        </Filter>
    </TopicConfiguration>             
</NotificationConfiguration>
```

The following notification configuration is not valid because it has overlapping suffixes\. Two suffixes are considered overlapping if a given string can end with both suffixes\. A string can end with `jpg` and `pg`, so the suffixes are overlapping\. \(The same is true for prefixes; two prefixes are considered overlapping if a given string can begin with both prefixes\.\)

```
 <NotificationConfiguration>
     <TopicConfiguration>
         <Topic>arn:aws:sns:us-west-2:444455556666:sns-topic-one</Topic>
         <Event>s3:ObjectCreated:*</Event>
         <Filter>
             <S3Key>
                 <FilterRule>
                     <Name>suffix</Name>
                     <Value>jpg</Value>
                 </FilterRule>
            </S3Key>
        </Filter>
    </TopicConfiguration>
    <TopicConfiguration>
         <Topic>arn:aws:sns:us-west-2:444455556666:sns-topic-two</Topic>
         <Event>s3:ObjectCreated:Put</Event>
         <Filter>
             <S3Key>
                 <FilterRule>
                     <Name>suffix</Name>
                     <Value>pg</Value>
                 </FilterRule>
            </S3Key>
        </Filter>
    </TopicConfiguration>
</NotificationConfiguration
```

The following notification configuration is not valid because it has overlapping prefixes and suffixes\. 

```
<NotificationConfiguration>
     <TopicConfiguration>
         <Topic>arn:aws:sns:us-west-2:444455556666:sns-topic-one</Topic>
         <Event>s3:ObjectCreated:*</Event>
         <Filter>
             <S3Key>
                 <FilterRule>
                     <Name>prefix</Name>
                     <Value>images</Value>
                 </FilterRule>
                 <FilterRule>
                     <Name>suffix</Name>
                     <Value>jpg</Value>
                 </FilterRule>
            </S3Key>
        </Filter>
    </TopicConfiguration>
    <TopicConfiguration>
         <Topic>arn:aws:sns:us-west-2:444455556666:sns-topic-two</Topic>
         <Event>s3:ObjectCreated:Put</Event>
         <Filter>
             <S3Key>
                 <FilterRule>
                     <Name>suffix</Name>
                     <Value>jpg</Value>
                 </FilterRule>
            </S3Key>
        </Filter>
    </TopicConfiguration>
</NotificationConfiguration>
```

## Granting permissions to publish event notification messages to a destination<a name="grant-destinations-permissions-to-s3"></a>

Before Amazon S3 can publish messages to a destination, you must grant the Amazon S3 principal the necessary permissions to call the relevant API to publish messages to an SNS topic, an SQS queue, or a Lambda function\. 

### Granting permissions to invoke an AWS Lambda function<a name="grant-lambda-invoke-permission-to-s3"></a>

Amazon S3 publishes event messages to AWS Lambda by invoking a Lambda function and providing the event message as an argument\.

When you use the Amazon S3 console to configure event notifications on an Amazon S3 bucket for a Lambda function, the console sets up the necessary permissions on the Lambda function so that Amazon S3 has permissions to invoke the function from the bucket\. For more information, see [How Do I Enable and Configure Event Notifications for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-event-notifications.html) in the *Amazon Simple Storage Service Console User Guide*\. 

You can also grant Amazon S3 permissions from AWS Lambda to invoke your Lambda function\. For more information, see [Tutorial: Using AWS Lambda with Amazon S3](https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html) in the *AWS Lambda Developer Guide*\.

### Granting permissions to publish messages to an SNS topic or an SQS queue<a name="grant-sns-sqs-permission-for-s3"></a>

To grant Amazon S3 permissions to publish messages to the SNS topic or SQS queue, you attach an AWS Identity and Access Management \(IAM\) policy to the destination SNS topic or SQS queue\. 

For an example of how to attach a policy to an SNS topic or an SQS queue, see [Walkthrough: Configure a bucket for notifications \(SNS topic or SQS queue\)](ways-to-add-notification-config-to-bucket.md)\. For more information about permissions, see the following topics:
+ [Example Cases for Amazon SNS Access Control](https://docs.aws.amazon.com/sns/latest/dg/AccessPolicyLanguage_UseCases_Sns.html) in the *Amazon Simple Notification Service Developer Guide*
+ [Access Control Using AWS Identity and Access Management \(IAM\)](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/UsingIAM.html) in the *Amazon Simple Queue Service Developer Guide*

#### IAM policy for a destination SNS topic<a name="sns-topic-policy"></a>

The following is an example of an IAM policy that you attach to the destination SNS topic\.

```
{
 "Version": "2012-10-17",
 "Id": "example-ID",
 "Statement": [
  {
   "Sid": "example-statement-ID",
   "Effect": "Allow",
   "Principal": {
     "Service": "s3.amazonaws.com"  
   },
   "Action": [
    "SNS:Publish"
   ],
   "Resource": "arn:aws:sns:Region:account-id:topic-name",
   "Condition": {
      "ArnLike": { "aws:SourceArn": "arn:aws:s3:::awsexamplebucket1" },
      "StringEquals": { "aws:SourceAccount": "bucket-owner-account-id" }
   }
  }
 ]
}
```

#### IAM policy for a destination SQS queue<a name="sqs-queue-policy"></a>

The following is an example of an IAM policy that you attach to the destination SQS queue\.

```
{
 "Version": "2012-10-17",
 "Id": "example-ID",
 "Statement": [
  {
   "Sid": "example-statement-ID",
   "Effect": "Allow",
   "Principal": {
     "Service": "s3.amazonaws.com"  
   },
   "Action": [
    "SQS:SendMessage"
   ],
   "Resource": "arn:aws:sqs:Region:account-id:queue-name",
   "Condition": {
      "ArnLike": { "aws:SourceArn": "arn:aws:s3:*:*:awsexamplebucket1" },
      "StringEquals": { "aws:SourceAccount": "bucket-owner-account-id" }
   }
  }
 ]
}
```

Note that for both the Amazon SNS and Amazon SQS IAM policies, you can specify the `StringLike` condition in the policy, instead of the `ArnLike` condition\.

```
"Condition": {         
  "StringLike": { "aws:SourceArn": "arn:aws:s3:*:*:bucket-name" }
  }
```

#### AWS KMS key policy<a name="key-policy-sns-sqs"></a>

If the SQS queue or SNS topics are encrypted with an AWS Key Management Service \(AWS KMS\) customer managed customer master key \(CMK\), you must grant the Amazon S3 service principal permission to work with the encrypted topics and or queue\. To grant the Amazon S3 service principal permission, add the following statement to the key policy for the customer managed CMK:

```
{
    "Version": "2012-10-17",
    "Id": "example-ID",
    "Statement": [
        {
            "Sid": "example-statement-ID",
            "Effect": "Allow",
            "Principal": {
                "Service": "s3.amazonaws.com"
            },
            "Action": [
                "kms:GenerateDataKey",
                "kms:Decrypt"
            ],
            "Resource": "*"
        }
    ]
}
```

For more information about AWS KMS key policies, see [Using Key Policies in AWS KMS](https://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html) in the *AWS Key Management Service Developer Guide*\. For more information about using server\-side encryption with AWS KMS for Amazon SQS and Amazon SNS, see the following:
+ [Configuring AWS KMS Permissions for Amazon SNS](https://docs.aws.amazon.com/sns/latest/dg/sns-key-management.html) in the *Amazon Simple Notification Service Developer Guide*\.
+ [Configuring AWS KMS Permissions for Amazon SQS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-key-management.html) in the *Amazon Simple Queue Service Developer Guide*\.
+ [Encrypting messages published to Amazon SNS with AWS KMS](http://aws.amazon.com/blogs/compute/encrypting-messages-published-to-amazon-sns-with-aws-kms/) in the *AWS Compute Blog*\.


# Requester Pays buckets<a name="RequesterPaysBuckets"></a>

**Topics**
+ [Configure Requester Pays by using the Amazon S3 console](configure-requester-pays-console.md)
+ [Configure Requester Pays with the REST API](configure-requester-pays-rest.md)
+ [Charge Details](#ChargeDetails)

In general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their bucket\. A bucket owner, however, can configure a bucket to be a Requester Pays bucket\. With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket\. The bucket owner always pays the cost of storing data\. 

Typically, you configure buckets to be Requester Pays when you want to share data but not incur charges associated with others accessing the data\. You might, for example, use Requester Pays buckets when making available large datasets, such as zip code directories, reference data, geospatial information, or web crawling data\. 

**Important**  
If you enable Requester Pays on a bucket, anonymous access to that bucket is not allowed\.

You must authenticate all requests involving Requester Pays buckets\. The request authentication enables Amazon S3 to identify and charge the requester for their use of the Requester Pays bucket\. 

When the requester assumes an AWS Identity and Access Management \(IAM\) role prior to making their request, the account to which the role belongs is charged for the request\. For more information about IAM roles, see [IAM Roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html) in the *IAM User Guide*\. 

After you configure a bucket to be a Requester Pays bucket, requesters must include `x-amz-request-payer` in their requests either in the header, for POST, GET and HEAD requests, or as a parameter in a REST request to show that they understand that they will be charged for the request and the data download\.

Requester Pays buckets do not support the following\.
+ Anonymous requests
+ BitTorrent
+ SOAP requests
+ You cannot use a Requester Pays bucket as the target bucket for end user logging, or vice versa; however, you can turn on end user logging on a Requester Pays bucket where the target bucket is not a Requester Pays bucket\. 

## Charge Details<a name="ChargeDetails"></a>

The charge for successful Requester Pays requests is straightforward: the requester pays for the data transfer and the request; the bucket owner pays for the data storage\. However, the bucket owner is charged for the request under the following conditions:
+ The requester doesn't include the parameter `x-amz-request-payer` in the header \(GET, HEAD, or POST\) or as a parameter \(REST\) in the request \(HTTP code 403\)\.
+ Request authentication fails \(HTTP code 403\)\.
+ The request is anonymous \(HTTP code 403\)\.
+ The request is a SOAP request\.

For information about pricing, see [Amazon S3 pricing\.](http://aws.amazon.com/s3/pricing/)


# Object tagging<a name="object-tagging"></a>

Use object tagging to categorize storage\. Each tag is a key\-value pair\. Consider the following tagging examples:
+ Suppose that an object contains protected health information \(PHI\) data\. You might tag the object using the following key\-value pair\.

  ```
  PHI=True
  ```

  or

  ```
  Classification=PHI
  ```
+ Suppose that you store project files in your S3 bucket\. You might tag these objects with a key named `Project` and a value, as shown following\.

  ```
  Project=Blue
  ```
+ You can add multiple tags to an object, as shown following\.

  ```
  Project=x
  Classification=confidential
  ```

You can add tags to new objects when you upload them, or you can add them to existing objects\. Note the following:
+ You can associate up to 10 tags with an object\. Tags that are associated with an object must have unique tag keys\.
+ A tag key can be up to 128 Unicode characters in length, and tag values can be up to 256 Unicode characters in length\.
+ The key and values are case sensitive\.
+ For more information about tag restrictions, see [User\-Defined Tag Restrictions](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocation-tag-restrictions.html)\.

Object key name prefixes also enable you to categorize storage\. However, prefix\-based categorization is one\-dimensional\. Consider the following object key names:

```
photos/photo1.jpg
project/projectx/document.pdf
project/projecty/document2.pdf
```

These key names have the prefixes `photos/`, `project/projectx/`, and `project/projecty/`\. These prefixes enable one\-dimensional categorization\. That is, everything under a prefix is one category\. For example, the prefix `project/projectx` identifies all documents related to project x\.

With tagging, you now have another dimension\. If you want photo1 in project x category, you can tag the object accordingly\. In addition to data classification, tagging offers benefits such as the following:
+ Object tags enable fine\-grained access control of permissions\. For example, you could grant an IAM user permissions to read\-only objects with specific tags\.
+ Object tags enable fine\-grained object lifecycle management in which you can specify a tag\-based filter, in addition to a key name prefix, in a lifecycle rule\.
+ When using Amazon S3 analytics, you can configure filters to group objects together for analysis by object tags, by key name prefix, or by both prefix and tags\.
+ You can also customize Amazon CloudWatch metrics to display information by specific tag filters\. The following sections provide details\.

**Important**  
It is acceptable to use tags to label objects containing confidential data, such as personally identifiable information \(PII\) or protected health information \(PHI\)\. However, the tags themselves shouldn't contain any confidential information\. 

To add object tag sets to more than one Amazon S3 object with a single request, you can use S3 Batch Operations\. You provide S3 Batch Operations with a list of objects to operate on\. S3 Batch Operations calls the respective API to perform the specified operation\. A single Batch Operations job can perform the specified operation on billions of objects containing exabytes of data\. 

The S3 Batch Operations feature tracks progress, sends notifications, and stores a detailed completion report of all actions, providing a fully managed, auditable, serverless experience\. You can use S3 Batch Operations through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\. For more information, see [S3 Batch Operations basics](batch-ops-basics.md)\.



## API operations related to object tagging<a name="tagging-apis"></a>

Amazon S3 supports the following API operations that are specifically for object tagging:

**Object API Operations**
+  [PUT Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUTtagging.html) – Replaces tags on an object\. You specify tags in the request body\.  There are two distinct scenarios of object tag management using this API\.
  + Object has no tags – Using this API you can add a set of tags to an object \(the object has no prior tags\)\.
  + Object has a set of existing tags – To modify the existing tag set, you must first retrieve the existing tag set, modify it on the client side, and then use this API to replace the tag set\.
**Note**  
 If you send this request with an empty tag set, Amazon S3 deletes the existing tag set on the object\. If you use this method, you will be charged for a Tier 1 Request \(PUT\)\. For more information, see [Amazon S3 Pricing](https://d0.awsstatic.com/whitepapers/aws_pricing_overview.pdf)\.  
The [DELETE Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETEtagging.html) request is preferred because it achieves the same result without incurring charges\. 
+  [GET Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtagging.html) – Returns the tag set associated with an object\. Amazon S3 returns object tags in the response body\.
+ [DELETE Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectDELETEtagging.html) – Deletes the tag set associated with an object\. 



**Other API Operations That Support Tagging**
+  [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) and [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)– You can specify tags when you create objects\. You specify tags using the `x-amz-tagging` request header\. 
+  [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) – Instead of returning the tag set, Amazon S3 returns the object tag count in the `x-amz-tag-count` header \(only if the requester has permissions to read tags\) because the header response size is limited to 8 K bytes\. If you want to view the tags, you make another request for the [GET Object tagging](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETtagging.html) API operation\.
+ [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html) – You can specify tags in your POST request\. 

  As long as the tags in your request don't exceed the 8 K byte HTTP request header size limit, you can use the `PUT Object `API to create objects with tags\. If the tags you specify exceed the header size limit, you can use this POST method in which you include the tags in the body\. 

   [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) – You can specify the `x-amz-tagging-directive` in your request to direct Amazon S3 to either copy \(default behavior\) the tags or replace tags by a new set of tags provided in the request\. 

Note the following:
+ S3 Object Tagging is strongly consistent\. For more information, see [Amazon S3 data consistency model](Introduction.md#ConsistencyModel)\. 

## Object tagging and additional information<a name="tagging-other-configs"></a>

This section explains how object tagging relates to other configurations\.

### Object tagging and lifecycle management<a name="tagging-and-lifecycle"></a>

In bucket lifecycle configuration, you can specify a filter to select a subset of objects to which the rule applies\. You can specify a filter based on the key name prefixes, object tags, or both\. 

Suppose that you store photos \(raw and the finished format\) in your Amazon S3 bucket\. You might tag these objects as shown following\. 

```
phototype=raw
or
phototype=finished
```

You might consider archiving the raw photos to S3 Glacier sometime after they are created\. You can configure a lifecycle rule with a filter that identifies the subset of objects with the key name prefix \(`photos/`\) that have a specific tag \(`phototype=raw`\)\. 

For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\. 

### Object tagging and replication<a name="tagging-and-replication"></a>

If you configured Replication on your bucket, Amazon S3 replicates tags, provided you grant Amazon S3 permission to read the tags\. For more information, see [Overview of setting up replication](replication-how-setup.md)\.

### Object tagging and access control policies<a name="tagging-and-policies"></a>

You can also use permissions policies \(bucket and user policies\) to manage permissions related to object tagging\. For policy actions see the following topics: 
+  [Example — Object Operations](using-with-s3-actions.md#using-with-s3-actions-related-to-objects) 
+  [Example — Bucket Operations](using-with-s3-actions.md#using-with-s3-actions-related-to-buckets)

Object tags enable fine\-grained access control for managing permissions\. You can grant conditional permissions based on object tags\. Amazon S3 supports the following condition keys that you can use to grant conditional permissions based on object tags:
+ `s3:ExistingObjectTag/<tag-key>` – Use this condition key to verify that an existing object tag has the specific tag key and value\. 
**Note**  
When granting permissions for the `PUT Object` and `DELETE Object` operations, this condition key is not supported\. That is, you cannot create a policy to grant or deny a user permissions to delete or overwrite an object based on its existing tags\. 
+ `s3:RequestObjectTagKeys` – Use this condition key to restrict the tag keys that you want to allow on objects\. This is useful when adding tags to objects using the PutObjectTagging and PutObject, and POST object requests\.
+ `s3:RequestObjectTag/<tag-key>` – Use this condition key to restrict the tag keys and values that you want to allow on objects\. This is useful when adding tags to objects using the PutObjectTagging and PutObject, and POST Bucket requests\.

For a complete list of Amazon S3 service\-specific condition keys, see [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)\. The following permissions policies illustrate how object tagging enables fine grained access permissions management\.

**Example 1: Allow a user to read only the objects that have a specific tag**  
The following permissions policy grants a user permission to read objects, but the condition limits the read permission to only objects that have the following specific tag key and value\.  

```
security : public
```
Note that the policy uses the Amazon S3 condition key, `s3:ExistingObjectTag/<tag-key>` to specify the key and value\.  

```
 1. {
 2.   "Version": "2012-10-17",
 3.   "Statement": [
 4.     {
 5.       "Effect":     "Allow",
 6.       "Action":     "s3:GetObject",
 7.       "Resource":    "arn:aws:s3:::awsexamplebucket1/*",
 8.       "Principal":   "*",
 9.       "Condition": {  "StringEquals": {"s3:ExistingObjectTag/security": "public" } }
10.     }
11.   ]
12. }
```

**Example 2: Allow a user to add object tags with restrictions on the allowed tag keys**  
The following permissions policy grants a user permissions to perform the `s3:PutObjectTagging` action, which allows user to add tags to an existing object\. The condition limits the tag keys that the user is allowed to use\. The condition uses the `s3:RequestObjectTagKeys` condition key to specify the set of tag keys\.  

```
 1. {
 2.   "Version": "2012-10-17",
 3.   "Statement": [
 4.     {
 5.       "Effect": "Allow",
 6.       "Action": [
 7.         "s3:PutObjectTagging"
 8.       ],
 9.       "Resource": [
10.         "arn:aws:s3:::awsexamplebucket1/*"
11.       ],
12.       "Principal":{
13.         "CanonicalUser":[
14.             "64-digit-alphanumeric-value"
15.          ]
16.        },
17.       "Condition": {
18.         "ForAllValues:StringLike": {
19.           "s3:RequestObjectTagKeys": [
20.             "Owner",
21.             "CreationDate"
22.           ]
23.         }
24.       }
25.     }
26.   ]
27. }
```
The policy ensures that the tag set, if specified in the request, has the specified keys\. A user might send an empty tag set in `PutObjectTagging`, which is allowed by this policy \(an empty tag set in the request removes any existing tags on the object\)\. If you want to prevent a user from removing the tag set, you can add another condition to ensure that the user provides at least one value\. The `ForAnyValue` in the condition ensures at least one of the specified values must be present in the request\.  

```
 1. {
 2. 
 3.   "Version": "2012-10-17",
 4.   "Statement": [
 5.     {
 6.       "Effect": "Allow",
 7.       "Action": [
 8.         "s3:PutObjectTagging"
 9.       ],
10.       "Resource": [
11.         "arn:aws:s3:::awsexamplebucket1/*"
12.       ],
13.       "Principal":{
14.         "AWS":[
15.             "arn:aws:iam::account-number-without-hyphens:user/username"
16.          ]
17.        },
18.       "Condition": {
19.         "ForAllValues:StringLike": {
20.           "s3:RequestObjectTagKeys": [
21.             "Owner",
22.             "CreationDate"
23.           ]
24.         },
25.         "ForAnyValue:StringLike": {
26.           "s3:RequestObjectTagKeys": [
27.             "Owner",
28.             "CreationDate"
29.           ]
30.         }
31.       }
32.     }
33.   ]
34. }
```
For more information, see [Creating a Condition That Tests Multiple Key Values \(Set Operations\)](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_multi-value-conditions.html) in the *IAM User Guide*\.

**Example 3: Allow a user to add object tags that include a specific tag key and value**  
The following user policy grants a user permissions to perform the `s3:PutObjectTagging` action, which allows user to add tags on an existing object\. The condition requires the user to include a specific tag \(`Project`\) with value set to `X`\.   

```
 1. {
 2.   "Version": "2012-10-17",
 3.   "Statement": [
 4.     {
 5.       "Effect": "Allow",
 6.       "Action": [
 7.         "s3:PutObjectTagging"
 8.       ],
 9.       "Resource": [
10.         "arn:aws:s3:::awsexamplebucket1/*"
11.       ],
12.       "Principal":{
13.         "AWS":[
14.             "arn:aws:iam::account-number-without-hyphens:user/username"
15.          ]
16.        },
17.       "Condition": {
18.         "StringEquals": {
19.           "s3:RequestObjectTag/Project": "X"
20.         }
21.       }
22.     }
23.   ]
24. }
```




**Related Topics**  
[Managing object tags](tagging-managing.md)


# Conversion Functions<a name="s3-glacier-select-sql-reference-conversion"></a>

Amazon S3 Select and S3 Glacier Select support the following conversion functions\.

**Topics**
+ [CAST](#s3-glacier-select-sql-reference-cast)

## CAST<a name="s3-glacier-select-sql-reference-cast"></a>

The `CAST` function converts an entity, such as an expression that evaluates to a single value, from one type to another\. 

### Syntax<a name="s3-glacier-select-sql-reference-cast-syntax"></a>

```
CAST ( expression AS data_type )
```

### Parameters<a name="s3-glacier-select-sql-reference-cast-parameters"></a>

 *expression*   
A combination of one or more values, operators, and SQL functions that evaluate to a value\.

 *data\_type*   
The target data type, such as `INT`, to cast the expression to\. For a list of supported data types, see [Data Types](s3-glacier-select-sql-reference-data-types.md)\.

### Examples<a name="s3-glacier-select-sql-reference-cast-examples"></a>

```
CAST('2007-04-05T14:30Z' AS TIMESTAMP)
CAST(0.456 AS FLOAT)
```


# Manage an object's lifecycle using the Amazon S3 console<a name="manage-lifecycle-using-console"></a>

You can specify S3 Lifecycle rules on a bucket using the Amazon S3 console\. 

For instructions on how to set up S3 Lifecycle rules using the AWS Management Console, see [ How Do I Create a Lifecycle Policy for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html) in the *Amazon Simple Storage Service Console User Guide*\.


# Restore an archived object using the REST API<a name="restoring-objects-rest"></a>

Amazon S3 provides an API for you to initiate an archive restoration\. For more information, see [POST Object restore](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOSTrestore.html) in the *Amazon Simple Storage Service API Reference*\.


# Managing websites with the AWS SDK for PHP<a name="ConfigWebSitePHP"></a>

This topic explains how to use classes from the AWS SDK for PHP to configure and manage an Amazon S3 bucket for website hosting\. It assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\. For more information about the Amazon S3 website feature, see [Hosting a static website using Amazon S3](WebsiteHosting.md)\.





The following PHP example adds a website configuration to the specified bucket\. The `create_website_config` method explicitly provides the index document and error document names\. The example also retrieves the website configuration and prints the response\. For more information about the Amazon S3 website feature, see [Hosting a static website using Amazon S3](WebsiteHosting.md)\.

 For instructions on creating and testing a working sample, see [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md)\. 

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
                
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

         
// Add the website configuration.
$s3->putBucketWebsite([
    'Bucket'                => $bucket,
    'WebsiteConfiguration'  => [
        'IndexDocument' => ['Suffix' => 'index.html'],
        'ErrorDocument' => ['Key' => 'error.html']
    ]
]);
        
// Retrieve the website configuration.
$result = $s3->getBucketWebsite([
    'Bucket' => $bucket
]);
echo $result->getPath('IndexDocument/Suffix');
        
// Delete the website configuration.
$s3->deleteBucketWebsite([
    'Bucket' => $bucket
]);
```

## Related resources<a name="RelatedResources-"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Guidelines for using the available access policy options<a name="access-policy-alternatives-guidelines"></a>

Amazon S3 supports resource\-based policies and user policies to manage access to your Amazon S3 resources \(see [Managing access to resources](access-control-overview.md#access-control-resources-manage-permissions-basics)\)\. Resource\-based policies include bucket policies, bucket ACLs, and object ACLs\. This section describes specific scenarios for using resource\-based access policies to manage access to your Amazon S3 resources\. 

## When to use an ACL\-based access policy \(bucket and object ACLs\)<a name="when-to-use-acl"></a>

Both buckets and objects have associated ACLs that you can use to grant permissions\. The following sections describe scenarios for using object ACLs and bucket ACLs\.

### When to use an object ACL<a name="when-to-use-object-acl"></a>

In addition to an object ACL, there are other ways an object owner can manage object permissions\. For example:
+ If the AWS account that owns the object also owns the bucket, then it can write a bucket policy to manage the object permissions\.
+ If the AWS account that owns the object wants to grant permission to a user in its account, it can use a user policy\.

So when do you use object ACLs to manage object permissions? The following are the scenarios when you use object ACLs to manage object permissions\.
+ **An object ACL is the only way to manage access to objects not owned by the bucket owner** – An AWS account that owns the bucket can grant another AWS account permission to upload objects\. The bucket owner does not own these objects\. The AWS account that created the object must grant permissions using object ACLs\. 
**Note**  
A bucket owner cannot grant permissions on objects it does not own\. For example, a bucket policy granting object permissions applies only to objects owned by the bucket owner\. However, the bucket owner, who pays the bills, can write a bucket policy to deny access to any objects in the bucket, regardless of who owns it\. The bucket owner can also delete any objects in the bucket\. 
+ **Permissions vary by object and you need to manage permissions at the object level** – You can write a single policy statement granting an AWS account read permission on millions of objects with a specific [key name prefix](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix)\. For example, grant read permission on objects starting with key name prefix "logs"\. However, if your access permissions vary by object, granting permissions to individual objects using a bucket policy may not be practical\. Also the bucket policies are limited to 20 KB in size\. 

  In this case, you may find using object ACLs a suitable alternative\. Although, even an object ACL is also limited to a maximum of 100 grants \(see [Access Control List \(ACL\) Overview](acl-overview.md)\)\. 
+ **Object ACLs control only object\-level permissions** –  There is a single bucket policy for the entire bucket, but object ACLs are specified per object\.

  An AWS account that owns a bucket can grant another AWS account permission to manage access policy\. It allows that account to change anything in the policy\. To better manage permissions, you may choose not to give such a broad permission, and instead grant only the READ\-ACP and WRITE\-ACP permissions on a subset of objects\. This limits the account to manage permissions only on specific objects by updating individual object ACLs\.

### When to use a bucket ACL<a name="when-to-use-bucket-acl"></a>

The only recommended use case for the bucket ACL is to grant write permission to the Amazon S3 Log Delivery group to write access log objects to your bucket \(see [Amazon S3 server access logging](ServerLogs.md)\)\. If you want Amazon S3 to deliver access logs to your bucket, you will need to grant write permission on the bucket to the Log Delivery group\. The only way you can grant necessary permissions to the Log Delivery group is via a bucket ACL, as shown in the following bucket ACL fragment\.

```
<?xml version="1.0" encoding="UTF-8"?>
<AccessControlPolicy xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
  <Owner>
   ...
  </Owner>
  <AccessControlList>
    <Grant>
        ...
    </Grant>  
    <Grant>
       <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="Group">
        <URI>http://acs.amazonaws.com/groups/s3/LogDelivery</URI>
       </Grantee>
       <Permission>WRITE</Permission>
    </Grant>  
  </AccessControlList>
</AccessControlPolicy>
```

## When to use a bucket policy<a name="when-to-use-bucket-policy"></a>

If an AWS account that owns a bucket wants to grant permission to users in its account, it can use either a bucket policy or a user policy\. But in the following scenarios, you will need to use a bucket policy\.
+ **You want to manage cross\-account permissions for all amazon S3 permissions** – You can use ACLs to grant cross\-account permissions to other accounts, but ACLs support only a finite set of permission \([What Permissions Can I Grant?](acl-overview.md#permissions)\), these don't include all Amazon S3 permissions\. For example, you cannot grant permissions on bucket subresources \(see [Identity and access management in Amazon S3](s3-access-control.md)\) using an ACL\. 

  Although both bucket and user policies support granting permission for all Amazon S3 operations \(see [Amazon S3 Actions](using-with-s3-actions.md)\), the user policies are for managing permissions for users in your account\. For cross\-account permissions to other AWS accounts or users in another account, you must use a bucket policy\.

## When to use a user policy<a name="when-to-use-user-policy"></a>

In general, you can use either a user policy or a bucket policy to manage permissions\. You may choose to manage permissions by creating users and managing permissions individually by attaching policies to users \(or user groups\), or you may find that resource\-based policies, such as a bucket policy, work better for your scenario\.

Note that AWS Identity and Access Management \(IAM\) enables you to create multiple users within your AWS account and manage their permissions via user policies\. An IAM user must have permissions from the parent account to which it belongs, and from the AWS account that owns the resource the user wants to access\. The permissions can be granted as follows:
+ **Permission from the parent account** – The parent account can grant permissions to its user by attaching a user policy\.
+ **Permission from the resource owner** – The resource owner can grant permission to either the IAM user \(using a bucket policy\) or the parent account \(using a bucket policy, bucket ACL, or object ACL\)\.

This is akin to a child who wants to play with a toy that belongs to someone else\. In this case, the child must get permission from a parent to play with the toy and permission from the toy owner\.

### Permission delegation<a name="permission-delegation"></a>

If an AWS account owns a resource, it can grant those permissions to another AWS account\. That account can then delegate those permissions, or a subset of them, to users in the account\. This is referred to as permission delegation\. But an account that receives permissions from another account cannot delegate permission cross\-account to another AWS account\. 

## Related topics<a name="access-control-guidelines-related-topics"></a>

We recommend you first review all introductory topics that explain how you manage access to your Amazon S3 resources and related guidelines\. For more information, see [Introduction to managing access to Amazon S3 resources](s3-access-control.md#intro-managing-access-s3-resources)\. You can then use the following topics for more information about specific access policy options\. 
+ [Using Bucket Policies and User Policies](using-iam-policies.md)
+ [Access Control List \(ACL\) Overview](acl-overview.md)
+ [Managing Access with ACLs](S3_ACLs_UsingACLs.md)
+ [Controlling ownership of uploaded objects using S3 Object Ownership](about-object-ownership.md)


# Using Amazon S3 access logs to identify requests<a name="using-s3-access-logs-to-identify-requests"></a>

You can identify Amazon S3 requests with Amazon S3 access logs\. 

**Note**  
We recommend that you use AWS CloudTrail data events instead of Amazon S3 access logs\. CloudTrail data events are easier to set up and contain more information\. For more information, see [ Using AWS CloudTrail to identify Amazon S3 requests](cloudtrail-request-identification.md)\.
Depending on how many access requests you get, it may require more resources and/or more time to analyze your logs\.

**Topics**
+ [Enabling Amazon S3 access logs for requests](#enabling-s3-access-logs-for-requests)
+ [Querying Amazon S3 access logs for requests using Amazon Athena](#querying-s3-access-logs-for-requests)
+ [Using Amazon S3 access logs to identify signature version 2 requests](#using-s3-access-logs-to-identify-sigv2-requests)
+ [Using Amazon S3 access logs to identify object access requests](#using-s3-access-logs-to-identify-objects-access)
+ [Related resources](#s3-access-logs-requests-more-info)

## Enabling Amazon S3 access logs for requests<a name="enabling-s3-access-logs-for-requests"></a>

We recommend that you create a dedicated logging bucket in each AWS Region that you have S3 buckets in\. Then have the Amazon S3 access log delivered to that S3 bucket\.

**Example — enable access logs with five buckets across two Regions**  
In this example, you have the following five buckets:  
+ `1-awsexamplebucket1-us-east-1`
+ `2-awsexamplebucket1-us-east-1`
+ `3-awsexamplebucket1-us-east-1`
+ `1-awsexamplebucket1-us-west-2`
+ `2-awsexamplebucket1-us-west-2`

1. Create two logging buckets in the following Regions:
   + `awsexamplebucket1-logs-us-east-1`
   + `awsexamplebucket1-logs-us-west-2`

1. Then enable the Amazon S3 access logs as follows:
   + `1-awsexamplebucket1-us-east-1` logs to the S3 bucket `awsexamplebucket1-logs-us-east-1` with prefix `1-awsexamplebucket1-us-east-1`
   + `2-awsexamplebucket1-us-east-1` logs to the S3 bucket `awsexamplebucket1-logs-us-east-1` with prefix `2-awsexamplebucket1-us-east-1`
   + `3-awsexamplebucket1-us-east-1` logs to the S3 bucket `awsexamplebucket1-logs-us-east-1` with prefix `3-awsexamplebucket1-us-east-1`
   + `1-awsexamplebucket1-us-west-2` logs to the S3 bucket `awsexamplebucket1-logs-us-west-2` with prefix `1-awsexamplebucket1-us-west-2`
   + `2-awsexamplebucket1-us-west-2` logs to the S3 bucket `awsexamplebucket1-logs-us-west-2` with prefix `2-awsexamplebucket1-us-west-2`

1. You can then enable the Amazon S3 access logs using the following methods:
   + Using the [AWS Management Console](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/server-access-logging.html) or,
   + [Enabling logging programmatically](enable-logging-programming.md) or, 
   + Using the [AWS CLI put\-bucket\-logging command](https://docs.aws.amazon.com/cli/latest/reference/s3api/put-bucket-logging.html) to programmatically enable access logs on a bucket using the following commands:

     1. First, grant Amazon S3 permission using `put-bucket-acl`:

        ```
        1.  aws s3api put-bucket-acl --bucket awsexamplebucket1-logs  --grant-write URI=http://acs.amazonaws.com/groups/s3/LogDelivery --grant-read-acp URI=http://acs.amazonaws.com/groups/s3/LogDelivery 
        ```

     1. Then, apply the logging policy:

        ```
        1. aws s3api put-bucket-logging --bucket awsexamplebucket1 --bucket-logging-status file://logging.json 
        ```

        `Logging.json` is a JSON document in the current folder that contains the logging policy:

        ```
          {
              "LoggingEnabled": {
                  "TargetBucket": "awsexamplebucket1-logs",
                  "TargetPrefix": "awsexamplebucket1/",
                  "TargetGrants": [
                       {
                          "Grantee": {
                              "Type": "AmazonCustomerByEmail",
                              "EmailAddress": "user@example.com"
                           },
                          "Permission": "FULL_CONTROL"
                       }
                   ]
               }
           }
        ```
**Note**  
The `put-bucket-acl` command is required to grant the Amazon S3 log delivery system the necessary permissions \(write and read\-acp permissions\)\. 

     1. Use a bash script to add access logging for all the buckets in your account:

        ```
          loggingBucket='awsexamplebucket1-logs'
          region='us-west-2'
          
          
          # Create Logging bucket
          aws s3 mb s3://$loggingBucket --region $region
          
          aws s3api put-bucket-acl --bucket $loggingBucket --grant-write URI=http://acs.amazonaws.com/groups/s3/LogDelivery --grant-read-acp URI=http://acs.amazonaws.com/groups/s3/LogDelivery
          
          # List buckets in this account
          buckets="$(aws s3 ls | awk '{print $3}')"
          
          # Put bucket logging on each bucket
          for bucket in $buckets
              do printf '{
             "LoggingEnabled": {
                 "TargetBucket": "%s",
                 "TargetPrefix": "%s/"
                  }
              }' "$loggingBucket" "$bucket"  > logging.json
              aws s3api put-bucket-logging --bucket $bucket --bucket-logging-status file://logging.json
              echo "$bucket done"
          done
          
          rm logging.json
          
          echo "Complete"
        ```
**Note**  
This only works if all your buckets are in the same Region\. If you have buckets in multiple Regions, you must adjust the script\. 

## Querying Amazon S3 access logs for requests using Amazon Athena<a name="querying-s3-access-logs-for-requests"></a>

You can identify Amazon S3 requests with Amazon S3 access logs using Amazon Athena\. 

Amazon S3 stores server access logs as objects in an S3 bucket\. It is often easier to use a tool that can analyze the logs in Amazon S3\. Athena supports analysis of S3 objects and can be used to query Amazon S3 access logs\.

**Example**  
The following example shows how you can query Amazon S3 server access logs in Amazon Athena\.   
To specify an Amazon S3 location in an Athena query, you need to format the *target* bucket name and *target* prefix where your logs are delivered as an S3 URI, as follows: `s3://DOC-EXAMPLE-BUCKET1-logs/prefix/` 

1. Open the Athena console at [https://console\.aws\.amazon\.com/athena/](https://console.aws.amazon.com/athena/home)\.

1. In the Query Editor, run a command similar to the following:

   ```
   create database s3_access_logs_db
   ```
**Note**  
It's a best practice to create the database in the same AWS Region as your S3 bucket\. 

1. In the Query Editor, run a command similar to the following to create a table schema in the database that you created in step 2\. The `STRING` and `BIGINT` data type values are the access log properties\. You can query these properties in Athena\. For `LOCATION`, enter the S3 bucket and prefix path as noted earlier\.

   ```
   CREATE EXTERNAL TABLE `s3_access_logs_db.mybucket_logs`(
     `bucketowner` STRING, 
     `bucket_name` STRING, 
     `requestdatetime` STRING, 
     `remoteip` STRING, 
     `requester` STRING, 
     `requestid` STRING, 
     `operation` STRING, 
     `key` STRING, 
     `request_uri` STRING, 
     `httpstatus` STRING, 
     `errorcode` STRING, 
     `bytessent` BIGINT, 
     `objectsize` BIGINT, 
     `totaltime` STRING, 
     `turnaroundtime` STRING, 
     `referrer` STRING, 
     `useragent` STRING, 
     `versionid` STRING, 
     `hostid` STRING, 
     `sigv` STRING, 
     `ciphersuite` STRING, 
     `authtype` STRING, 
     `endpoint` STRING, 
     `tlsversion` STRING)
   ROW FORMAT SERDE 
     'org.apache.hadoop.hive.serde2.RegexSerDe' 
   WITH SERDEPROPERTIES ( 
     'input.regex'='([^ ]*) ([^ ]*) \\[(.*?)\\] ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\"[^\"]*\"|-) (-|[0-9]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) (\"[^\"]*\"|-) ([^ ]*)(?: ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*))?.*$') 
   STORED AS INPUTFORMAT 
     'org.apache.hadoop.mapred.TextInputFormat' 
   OUTPUTFORMAT 
     'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
   LOCATION
     's3://awsexamplebucket1-logs/prefix/'
   ```

1. In the navigation pane, under **Database**, choose your database\.

1. Under **Tables**, choose **Preview table** next to your table name\.

   In the **Results** pane, you should see data from the server access logs, such as `bucketowner`, `bucket`, `requestdatetime`, and so on\. This means that you successfully created the Athena table\. You can now query the Amazon S3 server access logs\.

**Example — show who deleted an object and when \(timestamp, IP address, and IAM user\)**  

```
SELECT RequestDateTime, RemoteIP, Requester, Key 
FROM s3_access_logs_db.mybucket_logs 
WHERE key = 'images/picture.jpg' AND operation like '%DELETE%';
```

**Example — show all operations executed by an IAM user**  

```
SELECT * 
FROM s3_access_logs_db.mybucket_logs 
WHERE requester='arn:aws:iam::123456789123:user/user_name';
```

**Example — show all operations that were performed on an object in a specific time period**  

```
SELECT *
FROM s3_access_logs_db.mybucket_logs
WHERE Key='prefix/images/picture.jpg' 
    AND parse_datetime(RequestDateTime,'dd/MMM/yyyy:HH:mm:ss Z')
    BETWEEN parse_datetime('2017-02-18:07:00:00','yyyy-MM-dd:HH:mm:ss')
    AND parse_datetime('2017-02-18:08:00:00','yyyy-MM-dd:HH:mm:ss');
```

**Example — show how much data was transferred by a specific IP address in a specific time period**  

```
SELECT SUM(bytessent) AS uploadTotal,
      SUM(objectsize) AS downloadTotal,
      SUM(bytessent + objectsize) AS Total
FROM s3_access_logs_db.mybucket_logs
WHERE RemoteIP='1.2.3.4'
AND parse_datetime(RequestDateTime,'dd/MMM/yyyy:HH:mm:ss Z')
BETWEEN parse_datetime('2017-06-01','yyyy-MM-dd')
AND parse_datetime('2017-07-01','yyyy-MM-dd');
```

**Note**  
To reduce the time that you retain your log, you can [create an Amazon S3 lifecycle policy](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html) for your server access logs bucket\. Configure the lifecycle policy to remove log files periodically\. Doing so reduces the amount of data that Athena analyzes for each query\.

## Using Amazon S3 access logs to identify signature version 2 requests<a name="using-s3-access-logs-to-identify-sigv2-requests"></a>

Amazon S3 support for Signature Version 2 will be turned off \(deprecated\)\. After that, Amazon S3 will no longer accept requests that use Signature Version 2, and all requests must use *Signature Version 4* signing\. You can identify Signature Version 2 access requests using Amazon S3 access logs\. 

**Note**  
We recommend that you use AWS CloudTrail data events instead of Amazon S3 access logs\. CloudTrail data events are easier to set up and contain more information\. For more information, see [Using AWS CloudTrail to identify Amazon S3 signature version 2 requests ](cloudtrail-request-identification.md#cloudtrail-identification-sigv2-requests)\.

**Example — show all requesters that are sending signature version 2 traffic**  

```
                   SELECT requester, Sigv, Count(Sigv) as SigCount 
                   FROM s3_access_logs_db.mybucket_logs
                   GROUP BY requester, Sigv;
```

## Using Amazon S3 access logs to identify object access requests<a name="using-s3-access-logs-to-identify-objects-access"></a>

You can use queries on Amazon S3 server access logs to identify Amazon S3 object access requests, for operations such as GET, PUT, and DELETE, and discover further information about those requests\.

The following Amazon Athena query example shows how to get all PUT object requests for Amazon S3 from the server access log\. 

**Example — show all requesters that are sending PUT object requests in a certain period**  

```
SELECT Bucket, Requester, RemoteIP, Key, HTTPStatus, ErrorCode, RequestDateTime
FROM s3_access_logs_db
WHERE Operation='REST.PUT.OBJECT' AND
parse_datetime(RequestDateTime,'dd/MMM/yyyy:HH:mm:ss Z') 
BETWEEN parse_datetime('2019-07-01:00:42:42','yyyy-MM-dd:HH:mm:ss')
AND 
parse_datetime('2019-07-02:00:42:42','yyyy-MM-dd:HH:mm:ss')
```

The following Amazon Athena query example shows how to get all GET object requests for Amazon S3 from the server access log\. 

**Example — show all requesters that are sending GET object requests in a certain period**  

```
SELECT Bucket, Requester, RemoteIP, Key, HTTPStatus, ErrorCode, RequestDateTime
FROM s3_access_logs_db
WHERE Operation='REST.GET.OBJECT' AND
parse_datetime(RequestDateTime,'dd/MMM/yyyy:HH:mm:ss Z') 
BETWEEN parse_datetime('2019-07-01:00:42:42','yyyy-MM-dd:HH:mm:ss')
AND 
parse_datetime('2019-07-02:00:42:42','yyyy-MM-dd:HH:mm:ss')
```

The following Amazon Athena query example shows how to get all anonymous requests to your S3 buckets from the server access log\. 

**Example — show all anonymous requesters that are making requests to a bucket in a certain period**  

```
SELECT Bucket, Requester, RemoteIP, Key, HTTPStatus, ErrorCode, RequestDateTime
FROM s3_access_logs_db.mybucket_logs
WHERE Requester IS NULL AND
parse_datetime(RequestDateTime,'dd/MMM/yyyy:HH:mm:ss Z') 
BETWEEN parse_datetime('2019-07-01:00:42:42','yyyy-MM-dd:HH:mm:ss')
AND 
parse_datetime('2019-07-02:00:42:42','yyyy-MM-dd:HH:mm:ss')
```

**Note**  
You can modify the date range as needed to suit your needs\.
These query examples may also be useful for security monitoring\. You can review the results for `PutObject` or `GetObject` calls from unexpected or unauthorized IP addresses/requesters and for identifying any anonymous requests to your buckets\.
This query only retrieves information from the time at which logging was enabled\. 
If you are using Amazon S3 AWS CloudTrail logs, see [Using AWS CloudTrail to identify access to Amazon S3 objects](cloudtrail-request-identification.md#cloudtrail-identification-object-access)\. 

## Related resources<a name="s3-access-logs-requests-more-info"></a>
+ [Amazon S3 Server Access Log Format](LogFormat.md)
+ [Querying AWS Service Logs](https://docs.aws.amazon.com/athena/latest/ug/querying-AWS-service-logs.html)


# Working with Amazon S3 Buckets<a name="UsingBucket"></a>

To upload your data \(photos, videos, documents etc\.\) to Amazon S3, you must first create an S3 bucket in one of the AWS Regions\. You can then upload any number of objects to the bucket\. 

In terms of implementation, buckets and objects are resources, and Amazon S3 provides APIs for you to manage them\. For example, you can create a bucket and upload objects using the Amazon S3 API\. You can also use the Amazon S3 console to perform these operations\. The console uses the Amazon S3 APIs to send requests to Amazon S3\. 

This section explains how to work with buckets\. For information about working with objects, see [Working with Amazon S3 objects](UsingObjects.md)\.

An Amazon S3 bucket name is globally unique, and the namespace is shared by all AWS accounts\. This means that after a bucket is created, the name of that bucket cannot be used by another AWS account in any AWS Region until the bucket is deleted\. You should not depend on specific bucket naming conventions for availability or security verification purposes\. For bucket naming guidelines, see [Bucket restrictions and limitations](BucketRestrictions.md)\.

Amazon S3 creates buckets in a Region you specify\. To optimize latency, minimize costs, or address regulatory requirements, choose any AWS Region that is geographically close to you\. For example, if you reside in Europe, you might find it advantageous to create buckets in the Europe \(Ireland\) or Europe \(Frankfurt\) Regions\. For a list of Amazon S3 Regions, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\.

**Note**  
Objects that belong to a bucket that you create in a specific AWS Region never leave that Region, unless you explicitly transfer them to another Region\. For example, objects that are stored in the Europe \(Ireland\) Region never leave it\. 

**Topics**
+ [Creating a bucket](#create-bucket-intro)
+ [Managing public access to buckets](#block-public-access-intro)
+ [Accessing a bucket](#access-bucket-intro)
+ [Bucket configuration options](#bucket-config-options-intro)
+ [Bucket restrictions and limitations](BucketRestrictions.md)
+ [Examples of creating a bucket](create-bucket-get-location-example.md)
+ [Deleting or emptying a bucket](delete-or-empty-bucket.md)
+ [Setting default server\-side encryption behavior for Amazon S3 buckets](bucket-encryption.md)
+ [Amazon S3 Transfer Acceleration](transfer-acceleration.md)
+ [Requester Pays buckets](RequesterPaysBuckets.md)
+ [Buckets and access control](BucketAccess.md)
+ [Billing and usage reporting for S3 buckets](BucketBilling.md)

## Creating a bucket<a name="create-bucket-intro"></a>

Amazon S3 provides APIs for creating and managing buckets\. By default, you can create up to 100 buckets in each of your AWS accounts\. If you need more buckets, you can increase your account bucket limit to a maximum of 1,000 buckets by submitting a service limit increase\. To learn how to submit a bucket limit increase, see [AWS Service Limits](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) in the *AWS General Reference*\. You can store any number of objects in a bucket\. 

When you create a bucket, you provide a name and the AWS Region where you want to create the bucket\. For information about naming buckets, see [Rules for bucket naming](BucketRestrictions.md#bucketnamingrules)\.

You can use any of the methods listed below to create a bucket\. For examples, see [Examples of creating a bucket](create-bucket-get-location-example.md)\.

### Amazon S3 console<a name="create-bucket-s3-console"></a>

You can create a bucket in the Amazon S3 console\. For more information, see [Creating a bucket](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html) in the *Amazon Simple Storage Service Console User Guide*

### REST API<a name="create-bucket-rest-api"></a>

Creating a bucket using the REST API can be cumbersome because it requires you to write code to authenticate your requests\. For more information, see [PUT Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html) in the *Amazon Simple Storage Service API Reference*\. We recommend that you use the AWS Management Console or AWS SDKs instead\. 

### AWS SDK<a name="create-bucket-aws-sdk"></a>

When you use the AWS SDKs to create a bucket, you first create a client and then use the client to send a request to create a bucket\. If you don't specify a Region when you create a client or a bucket, Amazon S3 uses US East \(N\. Virginia\), the default Region\. You can also specify a specific Region\. For a list of available AWS Regions, see [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/s3.html) in the *AWS General Reference*\. For more information about enabling or disabling an AWS Region, see [Managing AWS Regions](https://docs.aws.amazon.com/general/latest/gr/rande-manage.html) in the *AWS General Reference*\. 

As a best practice, you should create your client and bucket in the same Region\. If your Region launched *after March 20, 2019*, your client and bucket must be in the same Region\. However, you can use a client in the US East \(N\. Virginia\) Region to create a bucket in any Region that launched *before March 20, 2019*\. For more information, see [Legacy Endpoints](VirtualHosting.md#s3-legacy-endpoints)\.

**Creating a client**  
When you create the client, you should specify an AWS Region, to create the client in\. If you don’t specify a Region, Amazon S3 creates the client in US East \(N\. Virginia\) by default Region\. To create a client to access a dual\-stack endpoint, you must specify an AWS Region,\. For more information, see [Dual\-stack endpoints](dual-stack-endpoints.md#dual-stack-endpoints-description)\.

When you create a client, the Region maps to the Region\-specific endpoint\. The client uses this endpoint to communicate with Amazon S3:

```
s3.<region>.amazonaws.com
```

For example, if you create a client by specifying the eu\-west\-1 Region, it maps to the following Region\-specific endpoint: 

```
s3.eu-west-1.amazonaws.com
```

**Creating a bucket**  
If you don't specify a Region when you create a bucket, Amazon S3 creates the bucket in the US East \(N\. Virginia\) Region\. Therefore, if you want to create a bucket in a specific Region, you must specify the Region when you create the bucket\.

Update \(September 23, 2020\) – We have decided to delay the deprecation of path\-style URLs to ensure that customers have the time that they need to transition to virtual hosted\-style URLs\. For more information, see [Amazon S3 Path Deprecation Plan – The Rest of the Story](https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/)\.

### About permissions<a name="about-access-permissions-create-bucket"></a>

You can use your AWS account root credentials to create a bucket and perform any other Amazon S3 operation\. However, AWS recommends not using the root credentials of your AWS account to make requests such as to create a bucket\. Instead, create an IAM user, and grant that user full access \(users by default have no permissions\)\. We refer to these users as administrator users\. You can use the administrator user credentials, instead of the root credentials of your account, to interact with AWS and perform tasks, such as create a bucket, create users, and grant them permissions\. 

For more information, see [Root Account Credentials vs\. IAM User Credentials](https://docs.aws.amazon.com/general/latest/gr/root-vs-iam.html) in the *AWS General Reference* and [IAM Best Practices](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html) in the *IAM User Guide*\.

The AWS account that creates a resource owns that resource\. For example, if you create an IAM user in your AWS account and grant the user permission to create a bucket, the user can create a bucket\. But the user does not own the bucket; the AWS account to which the user belongs owns the bucket\. The user will need additional permission from the resource owner to perform any other bucket operations\. For more information about managing permissions for your Amazon S3 resources, see [Identity and access management in Amazon S3](s3-access-control.md)\.

## Managing public access to buckets<a name="block-public-access-intro"></a>

Public access is granted to buckets and objects through access control lists \(ACLs\), bucket policies, or both\. To help you manage public access to Amazon S3 resources, Amazon S3 provides *block public access* settings\. Amazon S3 block public access settings can override ACLs and bucket policies so that you can enforce uniform limits on public access to these resources\. You can apply block public access settings to individual buckets or to all buckets in your account\.

To help ensure that all of your Amazon S3 buckets and objects have their public access blocked, we recommend that you turn on all four settings for block public access for your account\. These settings block public access for all current and future buckets\.

Before applying these settings, verify that your applications will work correctly without public access\. If you require some level of public access to your buckets or objects, for example to host a static website as described at [Hosting a static website using Amazon S3](WebsiteHosting.md), you can customize the individual settings to suit your storage use cases\. For more information, see [Using Amazon S3 block public access](access-control-block-public-access.md)\.

## Accessing a bucket<a name="access-bucket-intro"></a>

You can access your bucket using the Amazon S3 console\. Using the console UI, you can perform almost all bucket operations without having to write any code\. 

If you access a bucket programmatically, note that Amazon S3 supports RESTful architecture in which your buckets and objects are resources, each with a resource URI that uniquely identifies the resource\. 

Amazon S3 supports both virtual\-hosted–style and path\-style URLs to access a bucket\. Because buckets can be accessed using path\-style and virtual\-hosted–style URLs, we recommend that you create buckets with DNS\-compliant bucket names\. For more information, see [Bucket restrictions and limitations](BucketRestrictions.md)\.

**Note**  
Virtual hosted style and path\-style requests use the S3 dot Region endpoint structure \(`s3.Region`\), for example, `https://my-bucket.s3.us-west-2.amazonaws.com`\. However, some older Amazon S3 Regions also support S3 dash Region endpoints `s3-Region`, for example, `https://my-bucket.s3-us-west-2.amazonaws.com`\. If your bucket is in one of these Regions, you might see `s3-Region` endpoints in your server access logs or CloudTrail logs\. We recommend that you do not use this endpoint structure in your requests\. 

### Virtual hosted style access<a name="virtual-host-style-url-ex"></a>

In a virtual\-hosted–style request, the bucket name is part of the domain name in the URL\.

Amazon S3 virtual hosted style URLs follow the format shown below\.

```
https://bucket-name.s3.Region.amazonaws.com/key name
```

In this example, `my-bucket` is the bucket name, US West \(Oregon\) is the Region, and `puppy.png` is the key name:

```
https://my-bucket.s3.us-west-2.amazonaws.com/puppy.png
```

For more information about virtual hosted style access, see [Virtual Hosted\-Style Requests](VirtualHosting.md#virtual-hosted-style-access)\.

### Path\-style access<a name="path-style-url-ex"></a>

In Amazon S3, path\-style URLs follow the format shown below\.

```
https://s3.Region.amazonaws.com/bucket-name/key name
```

For example, if you create a bucket named `mybucket` in the US West \(Oregon\) Region, and you want to access the `puppy.jpg` object in that bucket, you can use the following path\-style URL:

```
https://s3.us-west-2.amazonaws.com/mybucket/puppy.jpg
```

 For more information, see [Path\-Style Requests](VirtualHosting.md#path-style-access)\.

**Important**  
Update \(September 23, 2020\) – We have decided to delay the deprecation of path\-style URLs to ensure that customers have the time that they need to transition to virtual hosted\-style URLs\. For more information, see [Amazon S3 Path Deprecation Plan – The Rest of the Story](https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/)\.

### Accessing an S3 bucket over IPv6<a name="accessing-bucket-s3-ipv6"></a>

Amazon S3 has a set of dual\-stack endpoints, which support requests to S3 buckets over both Internet Protocol version 6 \(IPv6\) and IPv4\. For more information, see [Making requests over IPv6](ipv6-access.md)\.

### Accessing a bucket through an S3 access point<a name="accessing-bucket-through-s3-access-point"></a>

In addition to accessing a bucket directly, you can access a bucket through an S3 access point\. For more information about S3 access points, see [Managing data access with Amazon S3 access points ](access-points.md)\.

S3 access points only support virtual\-host\-style addressing\. To address a bucket through an access point, use this format:

```
https://AccessPointName-AccountId.s3-accesspoint.region.amazonaws.com.
```

**Note**  
If your access point name includes dash \(\-\) characters, include the dashes in the URL and insert another dash before the account ID\. For example, to use an access point named `finance-docs` owned by account `123456789012` in Region `us-west-2`, the appropriate URL would be `https://finance-docs-123456789012.s3-accesspoint.us-west-2.amazonaws.com`\.
S3 access points don't support access by HTTP, only secure access by HTTPS\.

### Accessing a Bucket using S3://<a name="accessing-a-bucket-using-S3-format"></a>

Some AWS services require specifying an Amazon S3 bucket using `S3://bucket`\. The correct format is shown below\. Be aware that when using this format, the bucket name does not include the region\.

```
s3://bucket-name/key-name
```

For example, using the sample bucket described in the earlier path\-style section:

```
s3://mybucket/puppy.jpg
```

## Bucket configuration options<a name="bucket-config-options-intro"></a>

Amazon S3 supports various options for you to configure your bucket\. For example, you can configure your bucket for website hosting, add configuration to manage lifecycle of objects in the bucket, and configure the bucket to log all access to the bucket\. Amazon S3 supports subresources for you to store and manage the bucket configuration information\. You can use the Amazon S3 API to create and manage these subresources\. However, you can also use the console or the AWS SDKs\. 

**Note**  
There are also object\-level configurations\. For example, you can configure object\-level permissions by configuring an access control list \(ACL\) specific to that object\.

These are referred to as subresources because they exist in the context of a specific bucket or object\. The following table lists subresources that enable you to manage bucket\-specific configurations\. 


| Subresource | Description | 
| --- | --- | 
|   *cors* \(cross\-origin resource sharing\)   |   You can configure your bucket to allow cross\-origin requests\. For more information, see [Enabling Cross\-Origin Resource Sharing](https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html)\.  | 
|   *event notification*   |  You can enable your bucket to send you notifications of specified bucket events\.  For more information, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.  | 
| lifecycle |  You can define lifecycle rules for objects in your bucket that have a well\-defined lifecycle\. For example, you can define a rule to archive objects one year after creation, or delete an object 10 years after creation\.  For more information, see [Object Lifecycle Management](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html)\.   | 
|   *location*   |   When you create a bucket, you specify the AWS Region where you want Amazon S3 to create the bucket\. Amazon S3 stores this information in the location subresource and provides an API for you to retrieve this information\.   | 
|   *logging*   |  Logging enables you to track requests for access to your bucket\. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and error code, if any\. Access log information can be useful in security and access audits\. It can also help you learn about your customer base and understand your Amazon S3 bill\.   For more information, see [Amazon S3 server access logging](ServerLogs.md)\.   | 
|   *object locking*   |  To use S3 Object Lock, you must enable it for a bucket\. You can also optionally configure a default retention mode and period that applies to new objects that are placed in the bucket\.  For more information, see [Bucket configuration](object-lock-overview.md#object-lock-bucket-config)\.   | 
|   *policy* and *ACL* \(access control list\)   |  All your resources \(such as buckets and objects\) are private by default\. Amazon S3 supports both bucket policy and access control list \(ACL\) options for you to grant and manage bucket\-level permissions\. Amazon S3 stores the permission information in the *policy* and *acl* subresources\. For more information, see [Identity and access management in Amazon S3](s3-access-control.md)\.  | 
|   *replication*   |  Replication is the automatic, asynchronous copying of objects across buckets in different or the same AWS Regions\. For more information, see [Replication](replication.md)\.  | 
|   *requestPayment*   |  By default, the AWS account that creates the bucket \(the bucket owner\) pays for downloads from the bucket\. Using this subresource, the bucket owner can specify that the person requesting the download will be charged for the download\. Amazon S3 provides an API for you to manage this subresource\. For more information, see [Requester Pays buckets](RequesterPaysBuckets.md)\.  | 
|   *tagging*   |  You can add cost allocation tags to your bucket to categorize and track your AWS costs\. Amazon S3 provides the *tagging* subresource to store and manage tags on a bucket\. Using tags you apply to your bucket, AWS generates a cost allocation report with usage and costs aggregated by your tags\.  For more information, see [Billing and usage reporting for S3 buckets](BucketBilling.md)\.   | 
|   *transfer acceleration*   |  Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket\. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations\.  For more information, see [Amazon S3 Transfer Acceleration](transfer-acceleration.md)\.  | 
| versioning |  Versioning helps you recover accidental overwrites and deletes\.  We recommend versioning as a best practice to recover objects from being deleted or overwritten by mistake\.  For more information, see [Using versioning](Versioning.md)\.  | 
|  website |   You can configure your bucket for static website hosting\. Amazon S3 stores this configuration by creating a *website* subresource\. For more information, see [Hosting a Static Website on Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html)\.   | 


# Manage an object's lifecycle using the REST API<a name="manage-lifecycle-using-rest"></a>

You can use the AWS Management Console to set the S3 Lifecycle configuration on your bucket\. If your application requires it, you can also send REST requests directly\. The following sections in the *Amazon Simple Storage Service API Reference* describe the REST API related to the S3 Lifecycle configuration\. 
+ [PUT Bucket lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html)
+ [GET Bucket lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETlifecycle.html)
+ [DELETE Bucket lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETElifecycle.html)


# Walkthrough: Configure a bucket for notifications \(SNS topic or SQS queue\)<a name="ways-to-add-notification-config-to-bucket"></a>

You can receive Amazon S3 notifications using Amazon Simple Notification Service; or Amazon Simple Queue Service\. In the following walkthrough, you will add a notification configuration to your bucket using an Amazon SNS topic and Amazon SQS queue\.

**Topics**
+ [Walkthrough summary](#notification-walkthrough-summary)
+ [Step 1: Create an Amazon SQS queue](#step1-create-sqs-queue-for-notification)
+ [Step 2: Create an Amazon SNS topic](#step1-create-sns-topic-for-notification)
+ [Step 3: Add a notification configuration to your bucket](#step2-enable-notification)
+ [Step 4: Test the setup](#notification-walkthrough-1-test)

## Walkthrough summary<a name="notification-walkthrough-summary"></a>
+ Publish events of the `s3:ObjectCreated:*` type to an Amazon SQS queue\.
+ Publish events of the `s3:ReducedRedundancyLostObject` type to an Amazon SNS topic\.

For information about notification configuration, see [ Configuring Amazon S3 event notifications](NotificationHowTo.md)\.

You can do all these steps using the console, without writing any code\. In addition, code examples using AWS SDKs for Java and \.NET are also provided to help you add notification configurations programmatically\.

You can do the following with this walkthrough:

1. Create an Amazon SQS queue\.

   Using the Amazon SQS console, you create an SQS queue\. You can access any messages Amazon S3 sends to the queue programmatically\. But for this walkthrough, you verify notification messages in the console\. 

   You attach an access policy to the queue to grant Amazon S3 permission to post messages\.

1. Create an Amazon SNS topic\.

   Using the Amazon SNS console, you create an SNS topic and subscribe to the topic so that any events posted to it are delivered to you\. You specify email as the communications protocol\. After you create a topic, Amazon SNS sends an email\. You must click a link in the email to confirm the topic subscription\. 

   You attach an access policy to the topic to grant Amazon S3 permission to post messages\. 

1. Add notification configuration to a bucket\. 

## Step 1: Create an Amazon SQS queue<a name="step1-create-sqs-queue-for-notification"></a>

Follow the steps to create and subscribe to an Amazon Simple Queue Service \(Amazon SQS\) queue\.

1. Using the Amazon SQS console, create a queue\. For instructions, see [Getting Started with Amazon SQS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-getting-started.html) in the *Amazon Simple Queue Service Developer Guide*\. 

1. Replace the access policy attached to the queue with the following policy\. \(In the Amazon SQS console, select the queue, and in the **Permissions** tab, choose **Edit Policy Document \(Advanced\)**\.

   ```
   {
    "Version": "2012-10-17",
    "Id": "example-ID",
    "Statement": [
     {
      "Sid": "example-statement-ID",
      "Effect": "Allow",
      "Principal": {
       "AWS":"*"  
      },
      "Action": [
       "SQS:SendMessage"
      ],
      "Resource": "SQS-queue-ARN",
      "Condition": {
         "ArnLike": { "aws:SourceArn": "arn:aws:s3:*:*:awsexamplebucket1" },
         "StringEquals": { "aws:SourceAccount": "bucket-owner-account-id" }
      }
     }
    ]
   }
   ```

1. \(Optional\) If the Amazon SQS queue or the Amazon SNS topic are server\-side encryption enabled with AWS Key Management Service \(AWS KMS\), add the following policy to the associated symmetric customer managed AWS KMS CMK\. 

   You must add the policy to a customer managed CMK because you cannot modify the AWS managed CMK for Amazon SQS or Amazon SNS\. 

   ```
   {
       "Version": "2012-10-17",
       "Id": "example-ID",
       "Statement": [
           {
               "Sid": "example-statement-ID",
               "Effect": "Allow",
               "Principal": {
                   "Service": "s3.amazonaws.com"
               },
               "Action": [
                   "kms:GenerateDataKey",
                   "kms:Decrypt"
               ],
               "Resource": "*"
           }
       ]
   }
   ```

   For more information about using SSE for Amazon SQS and Amazon SNS with AWS KMS, see the following:
   + [Configuring AWS KMS Permissions for Amazon SNS](https://docs.aws.amazon.com/sns/latest/dg/sns-key-management.html) in the *Amazon Simple Notification Service Developer Guide*\.
   + [Configuring AWS KMS Permissions for Amazon SQS](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-key-management.html) in the *Amazon Simple Queue Service Developer Guide*\.

1. Note the queue ARN\. 

   The SQS queue you created is another resource in your AWS account, and it has a unique Amazon Resource Name \(ARN\)\. You will need this ARN in the next step\. The ARN will be of the following format:

   ```
   arn:aws:sqs:aws-region:account-id:queue-name
   ```

## Step 2: Create an Amazon SNS topic<a name="step1-create-sns-topic-for-notification"></a>

Follow the steps to create and subscribe to an Amazon Simple Notification Service \(Amazon SNS\) topic\.

1. Using Amazon SNS console create a topic\. For instructions, see [Create a Topic](https://docs.aws.amazon.com/sns/latest/dg/CreateTopic.html) in the *Amazon Simple Notification Service Developer Guide*\. 

1. Subscribe to the topic\. For this exercise, use email as the communications protocol\. For instructions, see [Subscribe to a Topic](https://docs.aws.amazon.com/sns/latest/dg/SubscribeTopic.html) in the *Amazon Simple Notification Service Developer Guide*\. 

   You will get email requesting you to confirm your subscription to the topic\. Confirm the subscription\. 

1. Replace the access policy attached to the topic with the following policy\. You must update the policy by providing your SNS topic Amazon Resource Name \(ARN\), bucket name, and bucket owner's account ID\.

   ```
   {
    "Version": "2012-10-17",
    "Id": "example-ID",
    "Statement": [
     {
      "Sid": "example-statement-ID",
      "Effect": "Allow",
      "Principal": {
       "AWS":"*"  
      },
      "Action": [
       "SNS:Publish"
      ],
      "Resource": "SNS-topic-ARN",
      "Condition": {
         "ArnLike": { "aws:SourceArn": "arn:aws:s3:*:*:bucket-name" },
         "StringEquals": { "aws:SourceAccount": "bucket-owner-account-id" }
      }
     }
    ]
   }
   ```

1. Note the topic ARN\.

   The SNS topic you created is another resource in your AWS account, and it has a unique Amazon Resource Name \(ARN\)\. You will need this ARN in the next step\. The ARN will be of the following format:

   ```
   arn:aws:sns:aws-region:account-id:topic-name
   ```

## Step 3: Add a notification configuration to your bucket<a name="step2-enable-notification"></a>

You can enable bucket notifications either by using the Amazon S3 console or programmatically by using AWS SDKs\. Choose any one of the options to configure notifications on your bucket\. This section provides code examples using the AWS SDKs for Java and \.NET\.

### Step 3 \(option a\): Enable notifications on a bucket using the console<a name="step2-enable-notification-using-console"></a>

Using the Amazon S3 console, add a notification configuration requesting Amazon S3 to:
+ Publish events of the **All object create events** type to your Amazon SQS queue\.
+ Publish events of the **Object in RRS lost** type to your Amazon SNS topic\.

After you save the notification configuration, Amazon S3 posts a test message, which you get via email\. 

For instructions, see [How Do I Enable and Configure Event Notifications for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-event-notifications.html) in the *Amazon Simple Storage Service Console User Guide*\. 

### Step 3 \(option b\): Enable notifications on a bucket using the AWS SDK for \.NET<a name="step2-enable-notification-using-awssdk-dotnet"></a>

The following C\# code example provides a complete code listing that adds a notification configuration to a bucket\. You need to update the code and provide your bucket name and SNS topic ARN\. For information about how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

**Example**  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class EnableNotificationsTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string snsTopic = "*** SNS topic ARN ***";
        private const string sqsQueue = "*** SQS topic ARN ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            EnableNotificationAsync().Wait();
        }

        static async Task EnableNotificationAsync()
        {
            try
            {
               PutBucketNotificationRequest request = new PutBucketNotificationRequest
                {
                    BucketName = bucketName
                };

                TopicConfiguration c = new TopicConfiguration
                {
                    Events = new List<EventType> { EventType.ObjectCreatedCopy },
                    Topic = snsTopic
                };
                request.TopicConfigurations = new List<TopicConfiguration>();
                request.TopicConfigurations.Add(c);
                request.QueueConfigurations = new List<QueueConfiguration>();
                request.QueueConfigurations.Add(new QueueConfiguration()
                {
                    Events = new List<EventType> { EventType.ObjectCreatedPut },
                    Queue = sqsQueue
                });
                
                PutBucketNotificationResponse response = await client.PutBucketNotificationAsync(request);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' ", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown error encountered on server. Message:'{0}' ", e.Message);
            }
        }
    }
}
```

### Step 3 \(option c\): Enable notifications on a bucket using the AWS SDK for Java<a name="step2-enable-notification-using-java"></a>

The following example shows how to add a notification configuration to a bucket\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.

**Example**  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.io.IOException;
import java.util.EnumSet;

public class EnableNotificationOnABucket {

    public static void main(String[] args) throws IOException {
        String bucketName = "*** Bucket name ***";
        Regions clientRegion = Regions.DEFAULT_REGION;
        String snsTopicARN = "*** SNS Topic ARN ***";
        String sqsQueueARN = "*** SQS Queue ARN ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();
            BucketNotificationConfiguration notificationConfiguration = new BucketNotificationConfiguration();

            // Add an SNS topic notification.
            notificationConfiguration.addConfiguration("snsTopicConfig",
                    new TopicConfiguration(snsTopicARN, EnumSet.of(S3Event.ObjectCreated)));

            // Add an SQS queue notification.
            notificationConfiguration.addConfiguration("sqsQueueConfig",
                    new QueueConfiguration(sqsQueueARN, EnumSet.of(S3Event.ObjectCreated)));

            // Create the notification configuration request and set the bucket notification configuration.
            SetBucketNotificationConfigurationRequest request = new SetBucketNotificationConfigurationRequest(
                    bucketName, notificationConfiguration);
            s3Client.setBucketNotificationConfiguration(request);
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

## Step 4: Test the setup<a name="notification-walkthrough-1-test"></a>

Now you can test the setup by uploading an object to your bucket and verifying the event notification in the Amazon SQS console\. For instructions, see [Receiving a Message](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-getting-started.htmlReceiveMessage.html) in the *Amazon Simple Queue Service Developer Guide "Getting Started" section*\. 


# Copying Objects in a Single Operation<a name="CopyingObjectsUsingAPIs"></a>

The examples in this section show how to copy objects up to 5 GB in a single operation\. For copying objects greater than 5 GB, you must use multipart upload API\. For more information, see [Copying objects using the multipart upload API](CopyingObjctsMPUapi.md)\.

**Topics**
+ [Copy an Object Using the AWS SDK for Java](CopyingObjectUsingJava.md)
+ [Copy an Amazon S3 Object in a Single Operation Using the AWS SDK for \.NET](CopyingObjectUsingNetSDK.md)
+ [Copy an Object Using the AWS SDK for PHP](CopyingObjectUsingPHP.md)
+ [Copy an Object Using the AWS SDK for Ruby](CopyingObjectUsingRuby.md)
+ [Copy an Object Using the REST API](CopyingObjectUsingREST.md)


# Examples of lifecycle configuration<a name="lifecycle-configuration-examples"></a>

This section provides examples of S3 Lifecycle configuration\. Each example shows how you can specify the XML in each of the example scenarios\.

**Topics**
+ [Example 1: Specifying a filter](#lifecycle-config-ex1)
+ [Example 2: Disabling a lifecycle rule](#lifecycle-config-conceptual-ex2)
+ [Example 3: Tiering down storage class over an object's lifetime](#lifecycle-config-conceptual-ex3)
+ [Example 4: Specifying multiple rules](#lifecycle-config-conceptual-ex4)
+ [Example 5: Overlapping filters, conflicting lifecycle actions, and what Amazon S3 does](#lifecycle-config-conceptual-ex5)
+ [Example 6: Specifying a lifecycle rule for a versioning\-enabled bucket](#lifecycle-config-conceptual-ex6)
+ [Example 7: Removing expired object delete markers](#lifecycle-config-conceptual-ex7)
+ [Example 8: Lifecycle configuration to abort multipart uploads](#lc-expire-mpu)

## Example 1: Specifying a filter<a name="lifecycle-config-ex1"></a>

Each S3 Lifecycle rule includes a filter that you can use to identify a subset of objects in your bucket to which the Lifecycle rule applies\. The following S3 Lifecycle configurations show examples of how you can specify a filter\.
+ In this Lifecycle configuration rule, the filter specifies a key prefix \(`tax/`\)\. Therefore, the rule applies to objects with key name prefix `tax/`, such as `tax/doc1.txt` and `tax/doc2.txt`\.

  The rule specifies two actions that direct Amazon S3 to do the following:
  + Transition objects to the S3 Glacier storage class 365 days \(one year\) after creation\.
  + Delete objects \(the `Expiration` action\) 3,650 days \(10 years\) after creation\.

  ```
  <LifecycleConfiguration>
    <Rule>
      <ID>Transition and Expiration Rule</ID>
      <Filter>
         <Prefix>tax/</Prefix>
      </Filter>
      <Status>Enabled</Status>
      <Transition>
        <Days>365</Days>
        <StorageClass>S3 Glacier</StorageClass>
      </Transition>
      <Expiration>
        <Days>3650</Days>
      </Expiration>
    </Rule>
  </LifecycleConfiguration>
  ```

  Instead of specifying object age in terms of days after creation, you can specify a date for each action\. However, you can't use both `Date` and `Days` in the same rule\. 
+ If you want the Lifecycle rule to apply to all objects in the bucket, specify an empty prefix\. In the following configuration, the rule specifies a `Transition` action directing Amazon S3 to transition objects to the S3 Glacier storage class 0 days after creation in which case objects are eligible for archival to Amazon S3 Glacier at midnight UTC following creation\. 

  ```
  <LifecycleConfiguration>
    <Rule>
      <ID>Archive all object same-day upon creation</ID>
      <Filter>
        <Prefix></Prefix>
      </Filter>
      <Status>Enabled</Status>
      <Transition>
        <Days>0</Days>
        <StorageClass>S3 Glacier</StorageClass>
      </Transition>
    </Rule>
  </LifecycleConfiguration>
  ```
+ You can specify zero or one key name prefix and zero or more object tags in a filter\. The following example code applies the Lifecycle rule to a subset of objects with the `tax/` key prefix and to objects that have two tags with specific key and value\. Note that when you specify more than one filter, you must include the AND as shown \(Amazon S3 applies a logical AND to combine the specified filter conditions\)\.

  

  ```
  ...
  <Filter>
     <And>
        <Prefix>tax/</Prefix>
        <Tag>
           <Key>key1</Key>
           <Value>value1</Value>
        </Tag>
        <Tag>
           <Key>key2</Key>
           <Value>value2</Value>
        </Tag>
      </And>
  </Filter>
  ...
  ```

  
+ You can filter objects based only on tags\. For example, the following Lifecycle rule applies to objects that have the two specified tags \(it does not specify any prefix\)\.

  ```
  ...
  <Filter>
     <And>
        <Tag>
           <Key>key1</Key>
           <Value>value1</Value>
        </Tag>
        <Tag>
           <Key>key2</Key>
           <Value>value2</Value>
        </Tag>
      </And>
  </Filter>
  ...
  ```

  

**Important**  
When you have multiple rules in an S3 Lifecycle configuration, an object can become eligible for multiple Lifecycle actions\. In such cases, Amazon S3 follows these general rules:  
Permanent deletion takes precedence over transition\.
Transition takes precedence over creation of delete markers\.
When an object is eligible for both a S3 Glacier and S3 Standard\-IA \(or S3 One Zone\-IA\) transition, Amazon S3 chooses the S3 Glacier transition\.
 For examples, see [Example 5: Overlapping filters, conflicting lifecycle actions, and what Amazon S3 does ](#lifecycle-config-conceptual-ex5)\. 



## Example 2: Disabling a lifecycle rule<a name="lifecycle-config-conceptual-ex2"></a>

You can temporarily disable a Lifecycle rule\. The following Lifecycle configuration specifies two rules:
+ Rule 1 directs Amazon S3 to transition objects with the `logs/` prefix to the S3 Glacier storage class soon after creation\. 
+ Rule 2 directs Amazon S3 to transition objects with the `documents/` prefix to the S3 Glacier storage class soon after creation\. 

In the policy, Rule 1 is enabled and Rule 2 is disabled\. Amazon S3 does not take any action on disabled rules\.

```
<LifecycleConfiguration>
  <Rule>
    <ID>Rule1</ID>
    <Filter>
      <Prefix>logs/</Prefix>
    </Filter>
    <Status>Enabled</Status>
    <Transition>
      <Days>0</Days>
      <StorageClass>S3 Glacier</StorageClass>
    </Transition>
  </Rule>
  <Rule>
    <ID>Rule2</ID>
    <Prefix>documents/</Prefix>
    <Status>Disabled</Status>
    <Transition>
      <Days>0</Days>
      <StorageClass>S3 Glacier</StorageClass>
    </Transition>
  </Rule>
</LifecycleConfiguration>
```

## Example 3: Tiering down storage class over an object's lifetime<a name="lifecycle-config-conceptual-ex3"></a>

In this example, you use lifecycle configuration to tier down the storage class of objects over their lifetime\. Tiering down can help reduce storage costs\. For more information about pricing, see [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\.

The following S3 Lifecycle configuration specifies a rule that applies to objects with key name prefix `logs/`\. The rule specifies the following actions:
+ Two transition actions:
  + Transition objects to the S3 Standard\-IA storage class 30 days after creation\.
  + Transition objects to the S3 Glacier storage class 90 days after creation\.
+ One expiration action that directs Amazon S3 to delete objects a year after creation\.

```
<LifecycleConfiguration>
  <Rule>
    <ID>example-id</ID>
    <Filter>
       <Prefix>logs/</Prefix>
    </Filter>
    <Status>Enabled</Status>
    <Transition>
      <Days>30</Days>
      <StorageClass>STANDARD_IA</StorageClass>
    </Transition>
    <Transition>
      <Days>90</Days>
      <StorageClass>GLACIER</StorageClass>
    </Transition>
    <Expiration>
      <Days>365</Days>
    </Expiration>
  </Rule>
</LifecycleConfiguration>
```

**Note**  
You can use one rule to describe all Lifecycle actions if all actions apply to the same set of objects \(identified by the filter\)\. Otherwise, you can add multiple rules with each specifying a different filter\.

## Example 4: Specifying multiple rules<a name="lifecycle-config-conceptual-ex4"></a>



You can specify multiple rules if you want different Lifecycle actions of different objects\. The following Lifecycle configuration has two rules:
+ Rule 1 applies to objects with the key name prefix `classA/`\. It directs Amazon S3 to transition objects to the S3 Glacier storage class one year after creation and expire these objects 10 years after creation\.
+ Rule 2 applies to objects with key name prefix `classB/`\. It directs Amazon S3 to transition objects to the S3 Standard\-IA storage class 90 days after creation and delete them one year after creation\.

```
<LifecycleConfiguration>
    <Rule>
        <ID>ClassADocRule</ID>
        <Filter>
           <Prefix>classA/</Prefix>        
        </Filter>
        <Status>Enabled</Status>
        <Transition>        
           <Days>365</Days>        
           <StorageClass>GLACIER</StorageClass>       
        </Transition>    
        <Expiration>
             <Days>3650</Days>
        </Expiration>
    </Rule>
    <Rule>
        <ID>ClassBDocRule</ID>
        <Filter>
            <Prefix>classB/</Prefix>
        </Filter>
        <Status>Enabled</Status>
        <Transition>        
           <Days>90</Days>        
           <StorageClass>STANDARD_IA</StorageClass>       
        </Transition>    
        <Expiration>
             <Days>365</Days>
        </Expiration>
    </Rule>
</LifecycleConfiguration>
```

## Example 5: Overlapping filters, conflicting lifecycle actions, and what Amazon S3 does<a name="lifecycle-config-conceptual-ex5"></a>

You might specify an S3 Lifecycle configuration in which you specify overlapping prefixes, or actions\.

Generally, Amazon S3 Lifecycle optimizes for cost\. For example, if two expiration policies overlap, the shorter expiration policy is honored so that data is not stored for longer than expected\. 

Likewise, if two transition policies overlap, S3 Lifecycle transitions your objects to the lower\-cost storage class\. In both cases, S3 Lifecycle tries to choose the path that is least expensive for you\. An exception to this general rule is with the S3 Intelligent\-Tiering storage class\. S3 Intelligent\-Tiering is favored by S3 Lifecycle over any storage class, aside from S3 Glacier and S3 Glacier Deep Archive storage classes\.

The following examples show how Amazon S3 chooses to resolve potential conflicts\.

**Example 1: Overlapping prefixes \(no conflict\)**  
The following example configuration has two rules that specify overlapping prefixes as follows:  
+ First rule specifies an empty filter, indicating all objects in the bucket\. 
+ Second rule specifies a key name prefix `logs/`, indicating only a subset of objects\.
Rule 1 requests Amazon S3 to delete all objects one year after creation\. Rule 2 requests Amazon S3 to transition a subset of objects to the S3 Standard\-IA storage class 30 days after creation\.  

```
 1. <LifecycleConfiguration>
 2.   <Rule>
 3.     <ID>Rule 1</ID>
 4.     <Filter>
 5.     </Filter>
 6.     <Status>Enabled</Status>
 7.     <Expiration>
 8.       <Days>365</Days>
 9.     </Expiration>
10.   </Rule>
11.   <Rule>
12.     <ID>Rule 2</ID>
13.     <Filter>
14.       <Prefix>logs/</Prefix>
15.     </Filter>
16.     <Status>Enabled</Status>
17.     <Transition>
18.       <StorageClass>STANDARD_IA<StorageClass>
19.       <Days>30</Days>
20.     </Transition>
21.    </Rule>
22. </LifecycleConfiguration>
```

**Example 2: Conflicting lifecycle actions**  
In this example configuration, there are two rules that direct Amazon S3 to perform two different actions on the same set of objects at the same time in object's lifetime:  
+ Both rules specify the same key name prefix, so both rules apply to the same set of objects\.
+ Both rules specify the same 365 days after object creation when the rules apply\.
+ One rule directs Amazon S3 to transition objects to the S3 Standard\-IA storage class and another rule wants Amazon S3 to expire the objects at the same time\.

```
<LifecycleConfiguration>
  <Rule>
    <ID>Rule 1</ID>
    <Filter>
      <Prefix>logs/</Prefix>
    </Filter>
    <Status>Enabled</Status>
    <Expiration>
      <Days>365</Days>
    </Expiration>        
  </Rule>
  <Rule>
    <ID>Rule 2</ID>
    <Filter>
      <Prefix>logs/</Prefix>
    </Filter>
    <Status>Enabled</Status>
    <Transition>
      <StorageClass>STANDARD_IA<StorageClass>
      <Days>365</Days>
    </Transition>
   </Rule>
</LifecycleConfiguration>
```
In this case, because you want objects to expire \(removed\), there is no point in changing the storage class, and Amazon S3 simply chooses the expiration action on these objects\.

**Example 3: Overlapping prefixes resulting in conflicting lifecycle actions**  
In this example, the configuration has two rules, which specify overlapping prefixes as follows:  
+ Rule 1 specifies an empty prefix \(indicating all objects\)\.
+ Rule 2 specifies a key name prefix \(`logs/`\) that identifies a subset of all objects\.
For the subset of objects with the `logs/` key name prefix, Lifecycle actions in both rules apply\. One rule directing Amazon S3 to transition objects 10 days after creation and another rule directing Amazon S3 to transition objects 365 days after creation\.   

```
<LifecycleConfiguration>
  <Rule>
    <ID>Rule 1</ID>
    <Filter>
      <Prefix></Prefix>
    </Filter>
    <Status>Enabled</Status>
    <Transition>
      <StorageClass>STANDARD_IA<StorageClass>
      <Days>10</Days> 
    </Transition>
  </Rule>
  <Rule>
    <ID>Rule 2</ID>
    <Filter>
      <Prefix>logs/</Prefix>
    </Filter>
    <Status>Enabled</Status>
    <Transition>
      <StorageClass>STANDARD_IA<StorageClass>
      <Days>365</Days> 
    </Transition>
   </Rule>
</LifecycleConfiguration>
```
In this case, Amazon S3 chooses to transition them 10 days after creation\. 

**Example 4: Tag\-based filtering and resulting conflicting lifecycle actions**  
Suppose that you have the following S3 Lifecycle policy that has two rules, each specifying a tag filter:  
+ Rule 1 specifies a tag\-based filter \(`tag1/value1`\)\. This rule directs Amazon S3 to transition objects to the S3 Glacier storage class 365 days after creation\.
+ Rule 2 specifies a tag\-based filter \(`tag2/value2`\)\. This rule directs Amazon S3 to expire objects 14 days after creation\.
The Lifecycle configuration is shown following\.  

```
<LifecycleConfiguration>
  <Rule>
    <ID>Rule 1</ID>
    <Filter>
      <Tag>
         <Key>tag1</Key>
         <Value>value1</Value>
      </Tag>
    </Filter>
    <Status>Enabled</Status>
    <Transition>
      <StorageClass>GLACIER<StorageClass>
      <Days>365</Days> 
    </Transition>
  </Rule>
  <Rule>
    <ID>Rule 2</ID>
    <Filter>
      <Tag>
         <Key>tag2</Key>
         <Value>value1</Value>
      </Tag>
    </Filter>
    <Status>Enabled</Status>
    <Expiration>
      <Days>14</Days> 
    </Expiration>
   </Rule>
</LifecycleConfiguration>
```
The policy is fine, but if there is an object with both tags, then S3 has to decide what to do\. That is, both rules apply to an object, and in effect you are directing Amazon S3 to perform conflicting actions\. In this case, Amazon S3 expires the object 14 days after creation\. The object is removed, and therefore the transition action does not come into play\.





## Example 6: Specifying a lifecycle rule for a versioning\-enabled bucket<a name="lifecycle-config-conceptual-ex6"></a>

Suppose that you have a versioning\-enabled bucket, which means that for each object you have a current version and zero or more noncurrent versions\. You want to maintain one year's worth of history and then delete the noncurrent versions\. For more information about S3 Versioning, see [Object Versioning](ObjectVersioning.md)\. 

Also, you want to save storage costs by moving noncurrent versions to S3 Glacier 30 days after they become noncurrent \(assuming cold data for which you don't need real\-time access\)\. In addition, you also expect frequency of access of the current versions to diminish 90 days after creation so you might choose to move these objects to the S3 Standard\-IA storage class\.

```
 1. <LifecycleConfiguration>
 2.     <Rule>
 3.         <ID>sample-rule</ID>
 4.         <Filter>
 5.            <Prefix></Prefix>
 6.         </Filter>
 7.         <Status>Enabled</Status>
 8.         <Transition>
 9.            <Days>90</Days>
10.            <StorageClass>STANDARD_IA</StorageClass>
11.         </Transition>
12.         <NoncurrentVersionTransition>      
13.             <NoncurrentDays>30</NoncurrentDays>      
14.             <StorageClass>S3 Glacier</StorageClass>   
15.         </NoncurrentVersionTransition>    
16.        <NoncurrentVersionExpiration>     
17.             <NoncurrentDays>365</NoncurrentDays>    
18.        </NoncurrentVersionExpiration> 
19.     </Rule>
20. </LifecycleConfiguration>
```

## Example 7: Removing expired object delete markers<a name="lifecycle-config-conceptual-ex7"></a>



A versioning\-enabled bucket has one current version and zero or more noncurrent versions for each object\. When you delete an object, note the following:
+ If you don't specify a version ID in your delete request, Amazon S3 adds a delete marker instead of deleting the object\. The current object version becomes noncurrent, and then the delete marker becomes the current version\. 
+ If you specify a version ID in your delete request, Amazon S3 deletes the object version permanently \(a delete marker is not created\)\.
+ A delete marker with zero noncurrent versions is referred to as the *expired object delete marker*\. 

This example shows a scenario that can create expired object delete markers in your bucket, and how you can use S3 Lifecycle configuration to direct Amazon S3 to remove the expired object delete markers\.

Suppose that you write a Lifecycle policy that specifies the `NoncurrentVersionExpiration` action to remove the noncurrent versions 30 days after they become noncurrent, as shown following\.

```
<LifecycleConfiguration>
    <Rule>
        ...
        <NoncurrentVersionExpiration>     
            <NoncurrentDays>30</NoncurrentDays>    
        </NoncurrentVersionExpiration>
    </Rule>
</LifecycleConfiguration>
```

The `NoncurrentVersionExpiration` action does not apply to the current object versions\. It only removes noncurrent versions\.

For current object versions, you have the following options to manage their lifetime depending on whether the current object versions follow a well\-defined lifecycle: 
+ **Current object versions follow a well\-defined lifecycle\.**

  In this case you can use Lifecycle policy with the `Expiration` action to direct Amazon S3 to remove current versions as shown in the following example\.

  ```
  <LifecycleConfiguration>
      <Rule>
          ...
          <Expiration>
             <Days>60</Days>
          </Expiration>
          <NoncurrentVersionExpiration>     
              <NoncurrentDays>30</NoncurrentDays>    
          </NoncurrentVersionExpiration>
      </Rule>
  </LifecycleConfiguration>
  ```

  Amazon S3 removes current versions 60 days after they are created by adding a delete marker for each of the current object versions\. This makes the current version noncurrent and the delete marker becomes the current version\. For more information, see [Using versioning](Versioning.md)\. 
**Note**  
This rule will automatically perform `ExpiredObjectDeleteMarker` cleanup in a versioned bucket making the need to include an `ExpiredObjectDeleteMarker` tag unnecessary\.

  The `NoncurrentVersionExpiration` action in the same Lifecycle configuration removes noncurrent objects 30 days after they become noncurrent\. Thus, in this example, all object versions are permanently removed 90 days after object creation\. You will have expired object delete markers, but Amazon S3 detects and removes the expired object delete markers for you\. 
+ **Current object versions don't have a well\-defined lifecycle\.** 

  In this case you might remove the objects manually when you don't need them, creating a delete marker with one or more noncurrent versions\. If Lifecycle configuration with `NoncurrentVersionExpiration` action removes all the noncurrent versions, you now have expired object delete markers\.

  Specifically for this scenario, Amazon S3 Lifecycle configuration provides an `Expiration` action where you can request Amazon S3 to remove the expired object delete markers\.

  

  ```
  <LifecycleConfiguration>
      <Rule>
         <ID>Rule 1</ID>
          <Filter>
            <Prefix>logs/</Prefix>
          </Filter>
          <Status>Enabled</Status>
          <Expiration>
             <ExpiredObjectDeleteMarker>true</ExpiredObjectDeleteMarker>
          </Expiration>
          <NoncurrentVersionExpiration>     
              <NoncurrentDays>30</NoncurrentDays>    
          </NoncurrentVersionExpiration>
      </Rule>
  </LifecycleConfiguration>
  ```

By setting the `ExpiredObjectDeleteMarker` element to true in the `Expiration` action, you direct Amazon S3 to remove expired object delete markers\.

**Note**  
When specifying the `ExpiredObjectDeleteMarker` Lifecycle action, the rule cannot specify a tag\-based filter\.

## Example 8: Lifecycle configuration to abort multipart uploads<a name="lc-expire-mpu"></a>

You can use the multipart upload API to upload large objects in parts\. For more information about multipart uploads, see [Multipart upload overview](mpuoverview.md)\. 

Using S3 Lifecycle configuration, you can direct Amazon S3 to stop incomplete multipart uploads \(identified by the key name prefix specified in the rule\) if they don't complete within a specified number of days after initiation\. When Amazon S3 aborts a multipart upload, it deletes all parts associated with the multipart upload\. This ensures that you don't have incomplete multipart uploads with parts that are stored in Amazon S3 and, therefore, you don't have to pay any storage costs for these parts\. 

**Note**  
When specifying the `AbortIncompleteMultipartUpload` Lifecycle action, the rule cannot specify a tag\-based filter\.

The following is an example S3 Lifecycle configuration that specifies a rule with the `AbortIncompleteMultipartUpload` action\. This action requests Amazon S3 to stop incomplete multipart uploads seven days after initiation\.

```
<LifecycleConfiguration>
    <Rule>
        <ID>sample-rule</ID>
        <Filter>
           <Prefix>SomeKeyPrefix/</Prefix>
        </Filter>
        <Status>rule-status</Status>
        <AbortIncompleteMultipartUpload>
          <DaysAfterInitiation>7</DaysAfterInitiation>
        </AbortIncompleteMultipartUpload>
    </Rule>
</LifecycleConfiguration>
```


# Request redirection and the REST API<a name="RESTRedirect"></a>

**Topics**
+ [Redirects and HTTP user\-agents](#RESTRedirectHTTPUserAgents)
+ [Redirects and 100\-Continue](#RESTRedirect100Continue)
+ [Redirect example](#RESTRedirectExample)

This section describes how to handle HTTP redirects by using the Amazon S3 REST API\. For general information about Amazon S3 redirects, see [Request redirection and the REST API](Redirects.md) in the Amazon Simple Storage Service API Reference\. 

## Redirects and HTTP user\-agents<a name="RESTRedirectHTTPUserAgents"></a>

Programs that use the Amazon S3 REST API should handle redirects either at the application layer or the HTTP layer\. Many HTTP client libraries and user agents can be configured to correctly handle redirects automatically; however, many others have incorrect or incomplete redirect implementations\. 

 Before you rely on a library to fulfill the redirect requirement, test the following cases: 
+ Verify all HTTP request headers are correctly included in the redirected request \(the second request after receiving a redirect\) including HTTP standards such as Authorization and Date\.
+ Verify non\-GET redirects, such as PUT and DELETE, work correctly\.
+ Verify large PUT requests follow redirects correctly\.
+ Verify PUT requests follow redirects correctly if the 100\-continue response takes a long time to arrive\.

 HTTP user\-agents that strictly conform to RFC 2616 might require explicit confirmation before following a redirect when the HTTP request method is not GET or HEAD\. It is generally safe to follow redirects generated by Amazon S3 automatically, as the system will issue redirects only to hosts within the amazonaws\.com domain and the effect of the redirected request will be the same as that of the original request\. 

## Redirects and 100\-Continue<a name="RESTRedirect100Continue"></a>

To simplify redirect handling, improve efficiencies, and avoid the costs associated with sending a redirected request body twice, configure your application to use 100\-continues for PUT operations\. When your application uses 100\-continue, it does not send the request body until it receives an acknowledgement\. If the message is rejected based on the headers, the body of the message is not sent\. For more information about 100\-continue, go to [RFC 2616 Section 8\.2\.3](http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html#sec8.2.3) 

**Note**  
According to RFC 2616, when using `Expect: Continue` with an unknown HTTP server, you should not wait an indefinite period before sending the request body\. This is because some HTTP servers do not recognize 100\-continue\. However, Amazon S3 does recognize if your request contains an `Expect: Continue` and will respond with a provisional 100\-continue status or a final status code\. Additionally, no redirect error will occur after receiving the provisional 100 continue go\-ahead\. This will help you avoid receiving a redirect response while you are still writing the request body\. 

## Redirect example<a name="RESTRedirectExample"></a>

This section provides an example of client\-server interaction using HTTP redirects and 100\-continue\. 

Following is a sample PUT to the `quotes.s3.amazonaws.com` bucket\.

```
1. PUT /nelson.txt HTTP/1.1
2. Host: quotes.s3.amazonaws.com
3. Date: Mon, 15 Oct 2007 22:18:46 +0000
4. 
5. Content-Length: 6
6. Expect: 100-continue
```

Amazon S3 returns the following:

```
 1. HTTP/1.1 307 Temporary Redirect
 2. Location: http://quotes.s3-4c25d83b.amazonaws.com/nelson.txt?rk=8d47490b
 3. Content-Type: application/xml
 4. Transfer-Encoding: chunked
 5. Date: Mon, 15 Oct 2007 22:18:46 GMT
 6. 
 7. Server: AmazonS3
 8. 
 9. <?xml version="1.0" encoding="UTF-8"?>
10. <Error>
11.   <Code>TemporaryRedirect</Code>
12.   <Message>Please re-send this request to the
13.   specified temporary endpoint. Continue to use the
14.   original request endpoint for future requests.
15.   </Message>
16.   <Endpoint>quotes.s3-4c25d83b.amazonaws.com</Endpoint>
17.   <Bucket>quotes</Bucket>
18. </Error>
```

The client follows the redirect response and issues a new request to the `quotes.s3-4c25d83b.amazonaws.com` temporary endpoint\.

```
1. PUT /nelson.txt?rk=8d47490b HTTP/1.1
2. Host: quotes.s3-4c25d83b.amazonaws.com
3. Date: Mon, 15 Oct 2007 22:18:46 +0000
4. 
5. Content-Length: 6
6. Expect: 100-continue
```

Amazon S3 returns a 100\-continue indicating the client should proceed with sending the request body\.

```
1. HTTP/1.1 100 Continue
```

The client sends the request body\.

```
1. ha ha\n
```

Amazon S3 returns the final response\.

```
1. HTTP/1.1 200 OK
2. Date: Mon, 15 Oct 2007 22:18:48 GMT
3. 
4. ETag: "a2c8d6b872054293afd41061e93bc289"
5. Content-Length: 0
6. Server: AmazonS3
```


# Deleting objects from versioning\-suspended buckets<a name="DeletingObjectsfromVersioningSuspendedBuckets"></a>

If versioning is suspended, a `DELETE` request:
+ Can only remove an object whose version ID is `null`

  Doesn't remove anything if there isn't a null version of the object in the bucket\.
+ Inserts a delete marker into the bucket\.

The following figure shows how a simple `DELETE` removes a null version and Amazon S3 inserts a delete marker in its place with a version ID of `null`\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_versioningSuspended.png)

Remember that a delete marker doesn't have content, so you lose the content of the null version when a delete marker replaces it\.

The following figure shows a bucket that doesn't have a null version\. In this case, the `DELETE` removes nothing; Amazon S3 just inserts a delete marker\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_versioningSuspendedNoNull.png)

Even in a versioning\-suspended bucket, the bucket owner can permanently delete a specified version\. The following figure shows that deleting a specified object version permanently removes that object\. Only the bucket owner can delete a specified object version\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_versioningEnabled2.png)


# Amazon S3 on Outposts examples<a name="S3OutpostsExamples"></a>

With Amazon S3 on Outposts, you can create S3 buckets on your AWS Outposts and easily store and retrieve objects on\-premises for applications that require local data access, local data processing, and data residency\. You can use S3 on Outposts through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\. For more information, see [Using Amazon S3 on Outposts](https://docs.aws.amazon.com/AmazonS3/latest/dev/S3onOutposts.html)\. 

This section contains the following examples of creating, managing Outposts buckets and performing object operations with S3 on Outposts\. In the examples, replace any variable values with those that suit your needs\.

**Topics**
+ [Amazon S3 on Outposts examples using the AWS CLI](S3OutpostsCLIExamples.md)
+ [Amazon S3 on Outposts examples using the SDK for Java](S3OutpostsJavaExamples.md)


# Uploading objects using presigned URLs<a name="PresignedUrlUploadObject"></a>

A presigned URL gives you access to the object identified in the URL, provided that the creator of the presigned URL has permissions to access that object\. That is, if you receive a presigned URL to upload an object, you can upload the object only if the creator of the presigned URL has the necessary permissions to upload that object\. 

All objects and buckets by default are private\. The presigned URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don't require them to have AWS security credentials or permissions\. 

When you create a presigned URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method \(PUT for uploading objects\), and an expiration date and time\. The presigned URLs are valid only for the specified duration\. That is, you must start the action before the expiration date and time\. If the action consists of multiple steps, such as a multipart upload, all steps must be started before the expiration, otherwise you will receive an error when Amazon S3 attempts to start a step with an expired URL\.

You can use the presigned URL multiple times, up to the expiration date and time\.

**Note**  
Anyone with valid security credentials can create a presigned URL\. However, for you to successfully upload an object, the presigned URL must be created by someone who has permission to perform the operation that the presigned URL is based upon\.

You can generate a presigned URL programmatically using the AWS SDK for Java, \.NET, Ruby, PHP, [Node\.js](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#getSignedUrl-property), and [Python](http://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.generate_presigned_url)\.

 If you are using Microsoft Visual Studio, you can also use AWS Explorer to generate a presigned object URL without writing any code\. Anyone who receives a valid presigned URL can then programmatically upload an object\. For more information, see [Using Amazon S3 from AWS Explorer](https://docs.aws.amazon.com/AWSToolkitVS/latest/UserGuide/using-s3.html)\. For instructions on how to install AWS Explorer, see [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)\.

## Limiting presigned URL capabilities<a name="PresignedUrlUploadObject-LimitCapabilities"></a>

You can use presigned URLs to generate a URL that can be used to access your S3 buckets\. When you create a presigned URL, you associate it with a specific action\. You can share the URL, and anyone with access to it can perform the action embedded in the URL as if they were the original signing user\. The URL will expire and no longer work when it reaches its expiration time\. The capabilities of the URL are limited by the permissions of the user who created the presigned URL\. 

In essence, presigned URLs are a bearer token that grants access to customers who possess them\. As such, we recommend that you protect them appropriately\.

If you want to restrict the use of presigned URLs and all S3 access to particular network paths, you can write AWS Identity and Access Management \(IAM\) policies that require a particular network path\. These policies can be set on the IAM principal that makes the call, the Amazon S3 bucket, or both\. A network\-path restriction on the principal requires the user of those credentials to make requests from the specified network\. A restriction on the bucket limits access to that bucket only to requests originating from the specified network\. Realize that these restrictions also apply outside of the presigned URL scenario\.

The IAM global condition that you use depends on the type of endpoint\. If you are using the public endpoint for Amazon S3, use `aws:SourceIp`\. If you are using a VPC endpoint to Amazon S3, use `aws:SourceVpc` or `aws:SourceVpce`\.

The following IAM policy statement requires the principal to access AWS from only the specified network range\. With this policy statement in place, all access is required to originate from that range\. This includes the case of someone using a presigned URL for S3\.

```
{
  "Sid": "NetworkRestrictionForIAMPrincipal",
  "Effect": "Deny",
  "Action": "",
  "Resource": "",
  "Condition": {
    "NotIpAddressIfExists": { "aws:SourceIp": "IP-address" },
    "BoolIfExists": { "aws:ViaAWSService": "false" }
  }
}
```

**Topics**
+ [Upload an object using a presigned URL \(AWS SDK for Java\)](PresignedUrlUploadObjectJavaSDK.md)
+ [Upload an object using a presigned URL \(AWS SDK for \.NET\)](UploadObjectPreSignedURLDotNetSDK.md)
+ [Upload an object using a presigned URL \(AWS SDK for Ruby\)](UploadObjectPreSignedURLRubySDK.md)
+ [Upload an object using a presigned URL \(AWS SDK for JavaScript\)](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/s3-example-creating-buckets.html#s3-create-presigendurl)


# Restore an archived object using the Amazon S3 console<a name="restoring-objects-console"></a>

You can use the Amazon S3 console to restore a copy of an object that has been archived\. For instructions on how to restore an archive using the AWS Management Console, see [ How do I restore an S3 object that has been archived?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/restore-archived-objects.html) in the *Amazon Simple Storage Service Console User Guide*\.


# Controlling ownership of uploaded objects using S3 Object Ownership<a name="about-object-ownership"></a>

 S3 Object Ownership is an Amazon S3 bucket setting that you can use to control ownership of new objects that are uploaded to your buckets\. By default, when other AWS accounts upload objects to your bucket, the objects remain owned by the uploading account\. With S3 Object Ownership, any new objects that are written by other accounts with the `bucket-owner-full-control` canned access control list \(ACL\) automatically become owned by the bucket owner, who then has full control of the objects\. 

You can create shared data stores that multiple users and teams in different accounts can write to and read from, and standardize ownership of new objects in your bucket\. As the bucket owner, you can then share and manage access to these objects via resource\-based policies, such as a bucket policy\. S3 Object Ownership does not affect existing objects\. 

S3 Object Ownership has two settings:
+ **Object writer** – The uploading account will own the object\.
+ **Bucket owner preferred** – The bucket owner will own the object if the object is uploaded with the `bucket-owner-full-control` canned ACL\. Without this setting and canned ACL, the object is uploaded and remains owned by the uploading account\. For information about enforcing object ownership, see [How do I ensure that I take ownership of new objects?](#ensure-object-ownership) 

**Topics**
+ [How do I ensure that I take ownership of new objects?](#ensure-object-ownership)
+ [Using S3 Object Ownership with Amazon S3 Replication](#object-ownership-replication)
+ [Setting S3 Object Ownership using the console](#enable-object-ownership)

## How do I ensure that I take ownership of new objects?<a name="ensure-object-ownership"></a>

After setting S3 Object Ownership to *bucket owner preferred*, you can add a bucket policy to require all Amazon S3 PUT operations to include the `bucket-owner-full-control` canned ACL\. This ACL grants the bucket owner full control of new objects\. With the S3 Object Ownership setting, it transfers object ownership to the bucket owner\. If the uploader doesn't meet the ACL requirement in their upload, the request fails\. This enables bucket owners to enforce uniform object ownership across all newly uploaded objects in their buckets\.

The following bucket policy specifies that account *`111122223333`* can upload objects to *`DOC-EXAMPLE-BUCKET`* only when the object's ACL is set to `bucket-owner-full-control`\. Be sure to replace *`111122223333`* with a real account and *`DOC-EXAMPLE-BUCKET`* with the name of a real bucket\.

```
{
   "Version": "2012-10-17",
   "Statement": [
      {
         "Sid": "Only allow writes to my bucket with bucket owner full control",
         "Effect": "Allow",
         "Principal": {
            "AWS": [
               "arn:aws:iam::111122223333:user/ExampleUser"
            ]
         },
         "Action": [
            "s3:PutObject"
         ],
         "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
         "Condition": {
            "StringEquals": {
               "s3:x-amz-acl": "bucket-owner-full-control"
            }
         }
      }
   ]
}
```

The following is an example copy operation that includes the `bucket-owner-full-control` canned ACL using the AWS Command Line Interface \(AWS CLI\)\.

```
aws s3 cp file.txt s3://DOC-EXAMPLE-BUCKET --acl bucket-owner-full-control
```

If the client does not include the `bucket-owner-full-control` canned ACL, the operation fails, and the uploader receives the following error: 

An error occurred \(AccessDenied\) when calling the PutObject operation: Access Denied\.

**Note**  
If clients need access to objects after uploading, you must grant additional permissions to the uploading account\. For information about granting accounts access to your resources, see [Example walkthroughs: Managing access to your Amazon S3 resources ](example-walkthroughs-managing-access.md)\.

## Using S3 Object Ownership with Amazon S3 Replication<a name="object-ownership-replication"></a>

S3 Object Ownership does not change the behavior of Amazon S3 Replication\. In replication, the owner of the source object also owns the replica by default\. When the source and destination buckets are owned by different AWS accounts, you can add optional configuration settings to change replica ownership\. 

To transfer ownership of replicated objects to the destination bucket owner, you can use the Amazon S3 Replication owner override option\. For more information about transferring ownership of replicas, see [Changing the replica owner](replication-change-owner.md)\.

## Setting S3 Object Ownership using the console<a name="enable-object-ownership"></a>

For information about setting S3 Object Ownership on a bucket using the Amazon S3 console, see [Setting S3 Object Ownership](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/add-object-ownership.html) in the *Amazon Simple Storage Service Console User Guide*\.


# Selecting content from objects<a name="selecting-content-from-objects"></a>

With Amazon S3 Select, you can use simple structured query language \(SQL\) statements to filter the contents of Amazon S3 objects and retrieve just the subset of data that you need\. By using Amazon S3 Select to filter this data, you can reduce the amount of data that Amazon S3 transfers, which reduces the cost and latency to retrieve this data\.

Amazon S3 Select works on objects stored in CSV, JSON, or Apache Parquet format\. It also works with objects that are compressed with GZIP or BZIP2 \(for CSV and JSON objects only\), and server\-side encrypted objects\. You can specify the format of the results as either CSV or JSON, and you can determine how the records in the result are delimited\.

You pass SQL expressions to Amazon S3 in the request\. Amazon S3 Select supports a subset of SQL\. For more information about the SQL elements that are supported by Amazon S3 Select, see [SQL Reference for Amazon S3 Select and S3 Glacier Select](s3-glacier-select-sql-reference.md)\.

You can perform SQL queries using AWS SDKs, the SELECT Object Content REST API, the AWS Command Line Interface \(AWS CLI\), or the Amazon S3 console\. The Amazon S3 console limits the amount of data returned to 40 MB\. To retrieve more data, use the AWS CLI or the API\.

## Requirements and limits<a name="selecting-content-from-objects-requirements-and-limits"></a>

The following are requirements for using Amazon S3 Select:
+ You must have `s3:GetObject` permission for the object you are querying\.
+ If the object you are querying is encrypted with a customer\-provided encryption key \(SSE\-C\), you must use `https`, and you must provide the encryption key in the request\.

The following limits apply when using Amazon S3 Select:
+ The maximum length of a SQL expression is 256 KB\.
+ The maximum length of a record in the input or result is 1 MB\.
+ Amazon S3 Select can only emit nested data using the JSON output format\.

Additional limitations apply when using Amazon S3 Select with Parquet objects:
+ Amazon S3 Select supports only columnar compression using GZIP or Snappy\. Amazon S3 Select doesn't support whole\-object compression for Parquet objects\.
+ Amazon S3 Select doesn't support Parquet output\. You must specify the output format as CSV or JSON\.
+ The maximum uncompressed row group size is 256 MB\.
+ You must use the data types specified in the object's schema\.
+ Selecting on a repeated field returns only the last value\.

## Constructing a request<a name="selecting-content-from-objects-contructing-request"></a>

When you construct a request, you provide details of the object that is being queried using an `InputSerialization` object\. You provide details of how the results are to be returned using an `OutputSerialization` object\. You also include the SQL expression that Amazon S3 uses to filter the request\.

For more information about constructing an Amazon S3 Select request, see [ SELECTObjectContent](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) in the *Amazon Simple Storage Service API Reference*\. You can also see one of the SDK code examples in the following sections\.

### Requests using scan ranges<a name="selecting-content-from-objects-using-byte-range"></a>

With Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query\. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non\-overlapping scan ranges\. Scan ranges don't need to be aligned with record boundaries\. An Amazon S3 Select scan range request runs across the byte range that you specify\. A record that starts within the scan range specified but extends beyond the scan range will be processed by the query\. For example; the following shows an Amazon S3 object containing a series of records in a line\-delimited CSV format:

```
A,B
C,D
D,E
E,F
G,H
I,J
```

 Use the Amazon S3 Select `ScanRange` parameter and *Start* at \(Byte\) 1 and *End* at \(Byte\) 4\. So the scan range would start at **","** and scan till the end of record starting at **"C"** and return the result **C, D** because that is the end of the record\. 

 Amazon S3 Select scan range requests support Parquet, CSV \(without quoted delimiters\), and JSON objects \(in LINES mode only\)\. CSV and JSON objects must be uncompressed\. For line\-based CSV and JSON objects, when a scan range is specified as part of the Amazon S3 Select request, all records that start within the scan range are processed\. For Parquet objects, all of the row groups that start within the scan range requested are processed\. 

Amazon S3 Select scan range requests are available to use on the Amazon S3 CLI, API and SDK\. You can use the `ScanRange` parameter in the Amazon S3 Select request for this feature\. For more information, see the [ Amazon S3 SELECT Object Content](https://docs.aws.amazon.com/AmazonS3/latest/API/API_SelectObjectContent.html) in the *Amazon Simple Storage Service API Reference*\.

## Errors<a name="selecting-content-from-objects-errors"></a>

Amazon S3 Select returns an error code and associated error message when an issue is encountered while attempting to run a query\. For a list of error codes and descriptions, see the [List of SELECT Object Content Error Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#SelectObjectContentErrorCodeList) section of the *Error Responses* page in the *Amazon Simple Storage Service API Reference*\.

**Topics**
+ [Requirements and limits](#selecting-content-from-objects-requirements-and-limits)
+ [Constructing a request](#selecting-content-from-objects-contructing-request)
+ [Errors](#selecting-content-from-objects-errors)
+ [Related resources](#RelatedResources014)
+ [Selecting content from objects using the SDK for Java](SelectObjectContentUsingJava.md)
+ [Selecting content from objects using the REST API](SelectObjectContentUsingRestApi.md)
+ [Selecting content from objects using other SDKs](SelectObjectContentUsingOtherSDKs.md)

## Related resources<a name="RelatedResources014"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Publishing content using Amazon S3 and BitTorrent<a name="S3TorrentPublish"></a>

Every anonymously readable object stored in Amazon S3 is automatically available for download using BitTorrent\. The process for changing the ACL on an object to allow anonymous `READ` operations is described in [Identity and access management in Amazon S3](s3-access-control.md)\.

You can direct your clients to your BitTorrent accessible objects by giving them the \.torrent file directly or by publishing a link to the ?torrent URL of your object, as described by [GetObjectTorrent](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObjectTorrent.html) in the *Amazon Simple Storage Service API Reference*\. One important thing to note is that the \.torrent file describing an Amazon S3 object is generated on demand the first time it is requested \(via the REST ?torrent resource\)\. Generating the \.torrent for an object takes time proportional to the size of that object\. For large objects, this time can be significant\. Therefore, before publishing a ?torrent link, we suggest making the first request for it yourself\. Amazon S3 might take several minutes to respond to this first request, as it generates the \.torrent file\. Unless you update the object in question, subsequent requests for the \.torrent will be fast\. Following this procedure before distributing a ?torrent link will ensure a smooth BitTorrent downloading experience for your customers\.

To stop distributing a file using BitTorrent, simply remove anonymous access to it\. This can be accomplished by either deleting the file from Amazon S3, or modifying your access control policy to prohibit anonymous reads\. After doing so, Amazon S3 will no longer act as a "seeder" in the BitTorrent network for your file, and will no longer serve the \.torrent file via the ?torrent REST API\. However, after a \.torrent for your file is published, this action might not stop public downloads of your object that happen exclusively using the BitTorrent peer to peer network\.


# Deleting multiple objects per request<a name="DeletingMultipleObjects"></a>

**Topics**
+ [Deleting multiple objects using the AWS SDK for Java](DeletingMultipleObjectsUsingJava.md)
+ [Deleting multiple objects using the AWS SDK for \.NET](DeletingMultipleObjectsUsingNetSDK.md)
+ [Deleting multiple objects using the AWS SDK for PHP](DeletingMultipleObjectsUsingPHPSDK.md)
+ [Deleting multiple objects using the REST API](DeletingMultipleObjectsUsingREST.md)

Amazon S3 provides the Multi\-Object Delete API \(see [Delete \- Multi\-Object Delete](https://docs.aws.amazon.com/AmazonS3/latest/API/multiobjectdeleteapi.html)\), which enables you to delete multiple objects in a single request\. The API supports two modes for the response: verbose and quiet\. By default, the operation uses verbose mode\. In verbose mode, the response includes the result of the deletion of each key that is specified in your request\. In quiet mode, the response includes only keys for which the delete operation encountered an error\. If all keys are successfully deleted when you're using quiet mode, Amazon S3 returns an empty response\.

To learn more about object deletion, see [Deleting objects](DeletingObjects.md)\. 

You can use the REST API directly or use the AWS SDKs\. 


# How Amazon S3 Authorizes a Request for a Bucket Operation<a name="access-control-auth-workflow-bucket-operation"></a>

When Amazon S3 receives a request for a bucket operation, Amazon S3 converts all the relevant permissions—resource\-based  permissions \(bucket policy, bucket access control list \(ACL\)\) and IAM user policy if the request is from an IAM principal—into a set of policies to evaluate at run time\. It then evaluates the resulting set of policies in a series of steps according to a specific context—user context or bucket context\. 

1. **User context** – If the requester is an IAM principal, the principal must have permission from the parent AWS account to which it belongs\. In this step, Amazon S3 evaluates a subset of policies owned by the parent account \(also referred to as the context authority\)\. This subset of policies includes the user policy that the parent account attaches to the principal\. If the parent also owns the resource in the request \(in this case, the bucket\), Amazon S3 also evaluates the corresponding resource policies \(bucket policy and bucket ACL\) at the same time\. Whenever a request for a bucket operation is made, the server access logs record the canonical ID of the requester\. For more information, see [Amazon S3 server access logging](ServerLogs.md)\.

1. **Bucket context** – The requester must have permissions from the bucket owner to perform a specific bucket operation\. In this step, Amazon S3 evaluates a subset of policies owned by the AWS account that owns the bucket\. 

   The bucket owner can grant permission by using a bucket policy or bucket ACL\. Note that, if the AWS account that owns the bucket is also the parent account of an IAM principal, then it can configure bucket permissions in a user policy\. 

 The following is a graphical illustration of the context\-based evaluation for bucket operation\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/AccessControlAuthorizationFlowBucketResource.png)

The following examples illustrate the evaluation logic\. 

## Example 1: Bucket Operation Requested by Bucket Owner<a name="example1-policy-eval-logic"></a>

 In this example, the bucket owner sends a request for a bucket operation using the root credentials of the AWS account\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/example10-policy-eval-logic.png)

 Amazon S3 performs the context evaluation as follows:

1.  Because the request is made by using root credentials of an AWS account, the user context is not evaluated \.

1.  In the bucket context, Amazon S3 reviews the bucket policy to determine if the requester has permission to perform the operation\. Amazon S3 authorizes the request\. 

## Example 2: Bucket Operation Requested by an AWS Account That Is Not the Bucket Owner<a name="example2-policy-eval-logic"></a>

In this example, a request is made using root credentials of AWS account 1111\-1111\-1111 for a bucket operation owned by AWS account 2222\-2222\-2222\. No IAM users are involved in this request\.

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/example20-policy-eval-logic.png)

In this case, Amazon S3 evaluates the context as follows:

1.  Because the request is made using root credentials of an AWS account, the user context is not evaluated\.

1. In the bucket context, Amazon S3 examines the bucket policy\. If the bucket owner \(AWS account 2222\-2222\-2222\) has not authorized AWS account 1111\-1111\-1111 to perform the requested operation, Amazon S3 denies the request\. Otherwise, Amazon S3 grants the request and performs the operation\.

## Example 3: Bucket Operation Requested by an IAM Principal Whose Parent AWS Account Is Also the Bucket Owner<a name="example3-policy-eval-logic"></a>

 In the example, the request is sent by Jill, an IAM user in AWS account 1111\-1111\-1111, which also owns the bucket\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/example30-policy-eval-logic.png)

 Amazon S3 performs the following context evaluation:

1.  Because the request is from an IAM principal, in the user context, Amazon S3 evaluates all policies that belong to the parent AWS account to determine if Jill has permission to perform the operation\. 

    In this example, parent AWS account 1111\-1111\-1111, to which the principal belongs, is also the bucket owner\. As a result, in addition to the user policy, Amazon S3 also evaluates the bucket policy and bucket ACL in the same context, because they belong to the same account\.

1. Because Amazon S3 evaluated the bucket policy and bucket ACL as part of the user context, it does not evaluate the bucket context\.

## Example 4: Bucket Operation Requested by an IAM Principal Whose Parent AWS Account Is Not the Bucket Owner<a name="example4-policy-eval-logic"></a>

In this example, the request is sent by Jill, an IAM user whose parent AWS account is 1111\-1111\-1111, but the bucket is owned by another AWS account, 2222\-2222\-2222\. 

![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/example40-policy-eval-logic.png)

 Jill will need permissions from both the parent AWS account and the bucket owner\. Amazon S3 evaluates the context as follows:

1. Because the request is from an IAM principal, Amazon S3 evaluates the user context by reviewing the policies authored by the account to verify that Jill has the necessary permissions\. If Jill has permission, then Amazon S3 moves on to evaluate the bucket context; if not, it denies the request\.

1.  In the bucket context, Amazon S3 verifies that bucket owner 2222\-2222\-2222 has granted Jill \(or her parent AWS account\) permission to perform the requested operation\. If she has that permission, Amazon S3 grants the request and performs the operation; otherwise, Amazon S3 denies the request\. 


# Upload a file in multiple parts using the PHP SDK low\-level API<a name="LLuploadFilePHP"></a>

This topic guides shows how to use the low\-level `uploadPart` method from version 3 of the AWS SDK for PHP to upload a file in multiple parts\. It assumes that you are already following the instructions for [Using the AWS SDK for PHP and Running PHP Examples](UsingTheMPphpAPI.md) and have the AWS SDK for PHP properly installed\.

The following PHP example uploads a file to an Amazon S3 bucket using the low\-level PHP API multipart upload\. For information about running the PHP examples in this guide, see [Running PHP Examples](UsingTheMPphpAPI.md#running-php-samples)\. 

```
 require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
$filename = '*** Path to and Name of the File to Upload ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

$result = $s3->createMultipartUpload([
    'Bucket'       => $bucket,
    'Key'          => $keyname,
    'StorageClass' => 'REDUCED_REDUNDANCY',
    'Metadata'     => [
        'param1' => 'value 1',
        'param2' => 'value 2',
        'param3' => 'value 3'
    ]
]);
$uploadId = $result['UploadId'];

// Upload the file in parts.
try {
    $file = fopen($filename, 'r');
    $partNumber = 1;
    while (!feof($file)) {
        $result = $s3->uploadPart([
            'Bucket'     => $bucket,
            'Key'        => $keyname,
            'UploadId'   => $uploadId,
            'PartNumber' => $partNumber,
            'Body'       => fread($file, 5 * 1024 * 1024),
        ]);
        $parts['Parts'][$partNumber] = [
            'PartNumber' => $partNumber,
            'ETag' => $result['ETag'],
        ];
        $partNumber++;

        echo "Uploading part {$partNumber} of {$filename}." . PHP_EOL;
    }
    fclose($file);
} catch (S3Exception $e) {
    $result = $s3->abortMultipartUpload([
        'Bucket'   => $bucket,
        'Key'      => $keyname,
        'UploadId' => $uploadId
    ]);

    echo "Upload of {$filename} failed." . PHP_EOL;
}

// Complete the multipart upload.
$result = $s3->completeMultipartUpload([
    'Bucket'   => $bucket,
    'Key'      => $keyname,
    'UploadId' => $uploadId,
    'MultipartUpload'    => $parts,
]);
$url = $result['Location'];

echo "Uploaded {$filename} to {$url}." . PHP_EOL;
```

## Related resources<a name="RelatedResources-LLuploadFilePHP"></a>
+ [ AWS SDK for PHP for Amazon S3 Aws\\S3\\S3Client Class](https://docs.aws.amazon.com/aws-sdk-php/v3/api/class-Aws.S3.S3Client.html) 
+ [ Amazon S3 Multipart Uploads](https://docs.aws.amazon.com/aws-sdk-php/v3/guide/service/s3-multipart-upload.html) 
+ [AWS SDK for PHP Documentation](http://aws.amazon.com/documentation/sdk-for-php/)


# Example: Copying objects across AWS accounts using S3 Batch Operations<a name="batch-ops-examples-xcopy"></a>

You can use S3 Batch Operations to create a PUT copy job to copy objects to a different AWS account \(the *destination account*\)\. The following sections explain how to store and use a manifest that is in a different AWS account\. In the first section, you can use Amazon S3 Inventory to deliver the inventory report to the destination account for use during job creation or, you can use a comma\-separated values \(CSV\) manifest in the source or destination account as shown in the second section\. 

**Topics**
+ [Using an inventory report delivered to the destination account to copy objects across AWS accounts](#specify-batchjob-manifest-xaccount-inventory)
+ [Using a CSV manifest stored in the source account to copy objects across AWS accounts](#specify-batchjob-manifest-xaccount-csv)

## Using an inventory report delivered to the destination account to copy objects across AWS accounts<a name="specify-batchjob-manifest-xaccount-inventory"></a>



You can use Amazon S3 Inventory to deliver the inventory report to the destination account for use during job creation\. For using a CSV manifest in the source or destination account, see [Using a CSV manifest stored in the source account to copy objects across AWS accounts](#specify-batchjob-manifest-xaccount-csv)\.

Amazon S3 Inventory generates inventories of the objects in a bucket\. The resulting list is published to an output file\. The bucket that is inventoried is called the *source bucket*, and the bucket where the inventory report file is stored is called the *destination bucket*\. 

The Amazon S3 inventory report can be configured to be delivered to another AWS account\. This allows S3 Batch Operations to read the inventory report when the job is created in the destination AWS account\. 

For more information about Amazon S3 Inventory source and destination buckets, see [How do I set up Amazon S3 inventory?](storage-inventory.md#storage-inventory-how-to-set-up)\. 

The easiest way to set up an inventory is by using the AWS Management Console, but you can also use the REST API, AWS Command Line Interface \(AWS CLI\), or AWS SDKs\.

The following console procedure contains the high\-level steps for setting up permissions for an S3 Batch Operations job\. In this procedure, you copy objects from a source account to a destination account, with the inventory report stored in the destination AWS account\.

**To set up Amazon S3 inventory for source and destination buckets owned by different accounts**

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. Choose a destination bucket to store the inventory report in\.

   Decide on a destination manifest bucket for storing the inventory report\. In this procedure, the *destination account* is the account that owns both the destination manifest bucket and the bucket that the objects are copied to\. 

1. Configure an inventory to list the objects in a source bucket and publish the list to the destination manifest bucket\.

   Configure an inventory list for a source bucket\. When you do this, you specify the destination bucket where you want the list to be stored\. The inventory report for the source bucket is published to the destination bucket\. In this procedure, the *source account* is the account that owns the source bucket\. 

   For information about how to use the console to configure an inventory, see [How Do I Configure Amazon S3 Inventory?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/configure-inventory.html) in the *Amazon Simple Storage Service Console User Guide*\.

   Choose **CSV** for the output format\. 

   When you enter information for the destination bucket, choose **Buckets in another account**\. Then enter the name of the destination manifest bucket\. Optionally, you can enter the account ID of the destination account\.

   After the inventory configuration is saved, the console displays a message similar to the following: 

   Amazon S3 could not create a bucket policy on the destination bucket\. Ask the destination bucket owner to add the following bucket policy to allow Amazon S3 to place data in that bucket\.

   The console then displays a bucket policy that you can use for the destination bucket\.

1. Copy the destination bucket policy that appears on the console\.

1. In the destination account, add the copied bucket policy to the destination manifest bucket where the inventory report is stored\.

1. Create a role in the destination account that is based on the S3 Batch Operations trust policy\. For more information about the trust policy, see [Trust policy](batch-ops-iam-role-policies.md#batch-ops-iam-role-policies-trust)\. 

   For more information about creating a role, see [ Creating a Role to Delegate Permissions to an AWS Service ](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html) in the *IAM User Guide*\.

   

   Enter a name for the role \(the example role uses the name `BatchOperationsDestinationRoleCOPY`\)\. Choose the **S3** service, and then choose the **S3 bucket Batch Operations** use case, which applies the trust policy to the role\. 

   Then choose **Create policy** to attach the following policy to the role\.

   ```
   {
           "Version": "2012-10-17",
           "Statement": [
               {
                   "Sid": "AllowBatchOperationsDestinationObjectCOPY",
                   "Effect": "Allow",
                   "Action": [
                       "s3:PutObject",
                       "s3:PutObjectVersionAcl",
                       "s3:PutObjectAcl",
                       "s3:PutObjectVersionTagging",
                       "s3:PutObjectTagging",
                       "s3:GetObject",
                       "s3:GetObjectVersion",
                       "s3:GetObjectAcl",
                       "s3:GetObjectTagging",
                       "s3:GetObjectVersionAcl",
                       "s3:GetObjectVersionTagging"
                   ],
                   "Resource": [
                       "arn:aws:s3:::ObjectDestinationBucket/*",
                       "arn:aws:s3:::ObjectSourceBucket/*",
                       "arn:aws:s3:::ObjectDestinationManifestBucket/*"
                   ]
               }
           ]
   }
   ```

   The role uses the policy to grant `batchoperations.s3.amazonaws.com` permission to read the manifest in the destination bucket\. It also grants permissions to GET objects, access control lists \(ACLs\), tags, and versions in the source object bucket\. And it grants permissions to PUT objects, ACLs, tags, and versions into the destination object bucket\.

1. In the source account, create a bucket policy for the source bucket that grants the role that you created in the previous step to GET objects, ACLs, tags, and versions in the source bucket\. This step allows S3 Batch Operations to get objects from the source bucket through the trusted role\.

   The following is an example of the bucket policy for the source account\.

   ```
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Sid": "AllowBatchOperationsSourceObjectCOPY",
               "Effect": "Allow",
               "Principal": {
                   "AWS": "arn:aws:iam::DestinationAccountNumber:role/BatchOperationsDestinationRoleCOPY"
               },
               "Action": [
                   "s3:GetObject",
                   "s3:GetObjectVersion",
                   "s3:GetObjectAcl",
                   "s3:GetObjectTagging",
                   "s3:GetObjectVersionAcl",
                   "s3:GetObjectVersionTagging"
               ],
               "Resource": "arn:aws:s3:::ObjectSourceBucket/*"
           }
       ]
   }
   ```

1. After the inventory report is available, create an S3 Batch Operations PUT object copy job in the destination account, choosing the inventory report from the destination manifest bucket\. You need the ARN for the role that you created in the destination account\. 

   For general information about creating a job, see [Creating an S3 Batch Operations job](batch-ops-create-job.md)\. 

   For information about creating a job using the console, see [ Creating an S3 Batch Operations Job](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/batch-ops-create-job.html) in the *Amazon Simple Storage Service Console User Guide*\. 

## Using a CSV manifest stored in the source account to copy objects across AWS accounts<a name="specify-batchjob-manifest-xaccount-csv"></a>

You can use a CSV file that is stored in a different AWS account as a manifest for an S3 Batch Operations job\. For using an S3 Inventory Report, see [Using an inventory report delivered to the destination account to copy objects across AWS accounts](#specify-batchjob-manifest-xaccount-inventory)\.

The following procedure shows how to set up permissions when using an S3 Batch Operations job to copy objects from a source account to a destination account with the CSV manifest file stored in the source account\.

**To set up a CSV manifest stored in a different AWS account**

1. Create a role in the destination account that is based on the S3 Batch Operations trust policy\. In this procedure, the *destination account* is the account that the objects are being copied to\. 

   For more information about the trust policy, see [Trust policy](batch-ops-iam-role-policies.md#batch-ops-iam-role-policies-trust)\. 

   For more information about creating a role, see [Creating a Role to Delegate Permissions to an AWS Service ](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html) in the *IAM User Guide*\.

   If you create the role using the console, enter a name for the role \(the example role uses the name `BatchOperationsDestinationRoleCOPY`\)\. Choose the **S3** service, and then choose the **S3 bucket Batch Operations** use case, which applies the trust policy to the role\. 

   Then choose **Create policy** to attach the following policy to the role\.

   ```
   {
           "Version": "2012-10-17",
           "Statement": [
               {
                   "Sid": "AllowBatchOperationsDestinationObjectCOPY",
                   "Effect": "Allow",
                   "Action": [
                       "s3:PutObject",
                       "s3:PutObjectVersionAcl",
                       "s3:PutObjectAcl",
                       "s3:PutObjectVersionTagging",
                       "s3:PutObjectTagging",
                       "s3:GetObject",
                       "s3:GetObjectVersion",
                       "s3:GetObjectAcl",
                       "s3:GetObjectTagging",
                       "s3:GetObjectVersionAcl",
                       "s3:GetObjectVersionTagging"
                   ],
                   "Resource": [
                       "arn:aws:s3:::ObjectDestinationBucket/*",
                       "arn:aws:s3:::ObjectSourceBucket/*",
                       "arn:aws:s3:::ObjectSourceManifestBucket/*"
                   ]
               }
           ]
   }
   ```

   Using the policy, the role grants `batchoperations.s3.amazonaws.com` permission to read the manifest in the source manifest bucket\. It grants permissions to GET objects, ACLs, tags, and versions in the source object bucket\. It also grants permissions to PUT objects, ACLs, tags, and versions into the destination object bucket\.

1. In the source account, create a bucket policy for the bucket that contains the manifest to grant the role that you created in the previous step to GET objects and versions in the source manifest bucket\. 

   This step allows S3 Batch Operations to read the manifest using the trusted role\. Apply the bucket policy to the bucket that contains the manifest\. 

   The following is an example of the bucket policy to apply to the source manifest bucket\.

   ```
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Sid": "AllowBatchOperationsSourceManfiestRead",
               "Effect": "Allow",
               "Principal": {
                   "AWS": [
                       "arn:aws:iam::DestinationAccountNumber:user/ConsoleUserCreatingJob",
                       "arn:aws:iam::DestinationAccountNumber:role/BatchOperationsDestinationRoleCOPY"
                   ]
               },
               "Action": [
                   "s3:GetObject",
                   "s3:GetObjectVersion"
               ],
               "Resource": "arn:aws:s3:::ObjectSourceManifestBucket/*"
           }
       ]
   }
   ```

   This policy also grants permissions to allow a console user who is creating a job in the destination account the same permissions in the source manifest bucket through the same bucket policy\.

1. In the source account, create a bucket policy for the source bucket that grants the role you created to GET objects, ACLs, tags, and versions in the source object bucket\. S3 Batch Operations can then get objects from the source bucket through the trusted role\.

   The following is an example of the bucket policy for the bucket that contains the source objects\.

   ```
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Sid": "AllowBatchOperationsSourceObjectCOPY",
               "Effect": "Allow",
               "Principal": {
                   "AWS": "arn:aws:iam::DestinationAccountNumber:role/BatchOperationsDestinationRoleCOPY"
               },
               "Action": [
                   "s3:GetObject",
                   "s3:GetObjectVersion",
                   "s3:GetObjectAcl",
                   "s3:GetObjectTagging",
                   "s3:GetObjectVersionAcl",
                   "s3:GetObjectVersionTagging"
               ],
               "Resource": "arn:aws:s3:::ObjectSourceBucket/*"
           }
       ]
   }
   ```

1. Create an S3 Batch Operations job in the destination account\. You need the Amazon Resource Name \(ARN\) for the role that you created in the destination account\. 

   For general information about creating a job, see [Creating an S3 Batch Operations job](batch-ops-create-job.md)\. 

   For information about creating a job using the console, see [ Creating an S3 Batch Operations Job](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/batch-ops-create-job.html) in the *Amazon Simple Storage Service Console User Guide*\. 


# Specifying AWS KMS in Amazon S3 using the AWS SDKs<a name="kms-using-sdks"></a>

When using AWS SDKs, you can request Amazon S3 to use AWS Key Management Service \(AWS KMS\) customer master keys \(CMKs\)\. This section provides examples of using the AWS SDKs for Java and \.NET\. For information about other SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code)\.

**Important**  
When you use an AWS KMS CMK for server\-side encryption in Amazon S3, you must choose a symmetric CMK\. Amazon S3 only supports symmetric CMKs and not asymmetric CMKs\. For more information, see [Using Symmetric and Asymmetric Keys](https://docs.aws.amazon.com/kms/latest/developerguide/symmetric-asymmetric.html) in the *AWS Key Management Service Developer Guide*\.

**Topics**
+ [AWS SDK for Java](#kms-using-sdks-java)
+ [AWS SDK for \.NET](#kms-using-sdks-dotnet)

## AWS SDK for Java<a name="kms-using-sdks-java"></a>

This section explains various Amazon S3 operations using the AWS SDK for Java and how you use the AWS KMS CMKs\.

### Put operation<a name="kms-using-sdks-java-put"></a>

When uploading an object using the AWS SDK for Java, you can request Amazon S3 to use an AWS KMS CMK by adding the `SSEAwsKeyManagementParams` property as shown in the following request\.

```
PutObjectRequest putRequest = new PutObjectRequest(bucketName,
   keyName, file).withSSEAwsKeyManagementParams(new SSEAwsKeyManagementParams());
```

In this case, Amazon S3 uses the AWS managed CMK \(see [Protecting data with server\-side encryption using AWS KMS CMKs](UsingKMSEncryption.md)\)\. You can optionally create a symmetric customer managed CMK and specify that in the request\.

```
PutObjectRequest putRequest = new PutObjectRequest(bucketName,
   keyName, file).withSSEAwsKeyManagementParams(new SSEAwsKeyManagementParams(keyID));
```

For more information about creating customer managed CMKs, see [Programming the AWS KMS API](https://docs.aws.amazon.com/kms/latest/developerguide/programming-top.html) in the *AWS Key Management Service Developer Guide*\.

For working code examples of uploading an object, see the following topics\. You will need to update those code examples and provide encryption information as shown in the preceding code fragment\.
+ For uploading an object in a single operation, see [Upload an object Using the AWS SDK for Java](UploadObjSingleOpJava.md)\.
+ For a multipart upload, see the following topics:
  + Using high\-level multipart upload API, see [Upload a file](HLuploadFileJava.md)\. 
  + If you are using the low\-level multipart upload API, see [Upload a file](llJavaUploadFile.md)\.

### Copy operation<a name="kms-using-sdks-java-copy"></a>

When copying objects, you add the same request properties \(`ServerSideEncryptionMethod` and `ServerSideEncryptionKeyManagementServiceKeyId`\) to request Amazon S3 to use an AWS KMS CMK\. For more information about copying objects, see [Copying objects](CopyingObjectsExamples.md)\.

### Presigned URLs<a name="kms-using-sdks-java-presigned-url"></a>

When creating a presigned URL for an object encrypted using an AWS KMS CMK, you must explicitly specify Signature Version 4\.

```
ClientConfiguration clientConfiguration = new ClientConfiguration();
clientConfiguration.setSignerOverride("AWSS3V4SignerType");
AmazonS3Client s3client = new AmazonS3Client(
        new ProfileCredentialsProvider(), clientConfiguration);
...
```

For a code example, see [Generate a presigned object URL using the AWS SDK for Java](ShareObjectPreSignedURLJavaSDK.md)\. 

## AWS SDK for \.NET<a name="kms-using-sdks-dotnet"></a>

This section explains various Amazon S3 operations using the AWS SDK for \.NET and how you use the AWS KMS CMKs\.

### Put operation<a name="kms-using-sdks-dotnet-put"></a>

When uploading an object using the AWS SDK for \.NET, you can request Amazon S3 to use an AWS KMS CMK by adding the `ServerSideEncryptionMethod` property as shown in the following request\.

```
PutObjectRequest putRequest = new PutObjectRequest
   {
       BucketName = bucketName,
       Key = keyName,
       // other properties.
       ServerSideEncryptionMethod = ServerSideEncryptionMethod.AWSKMS
   };
```

In this case, Amazon S3 uses the AWS managed CMK\. For more information, see [Protecting data with server\-side encryption using AWS KMS CMKs \(SSE\-KMS\)](UsingKMSEncryption.md)\. You can optionally create your own symmetric customer managed CMK and specify that in the request\. 

```
PutObjectRequest putRequest1 = new PutObjectRequest
{
    BucketName = bucketName,
    Key = keyName,
    // other properties.
    ServerSideEncryptionMethod = ServerSideEncryptionMethod.AWSKMS,
    ServerSideEncryptionKeyManagementServiceKeyId = keyId
};
```

For more information about creating customer managed CMKs, see [Programming the AWS KMS API](https://docs.aws.amazon.com/kms/latest/developerguide/programming-top.html) in the *AWS Key Management Service Developer Guide*\. 

For working code examples of uploading an object, see the following topics\. You will need to update these code examples and provide encryption information as shown in the preceding code fragment\.
+ For uploading an object in a single operation, see [Upload an object using the AWS SDK for \.NET](UploadObjSingleOpNET.md)\.
+ For multipart upload see the following topics:
  + Using high\-level multipart upload API, see [Upload a file to an S3 bucket using the AWS SDK for \.NET \(high\-level API\)](HLuploadFileDotNet.md)\. 
  + Using low\-level multipart upload API, see [Upload a file to an S3 Bucket using the AWS SDK for \.NET \(low\-level API\)](LLuploadFileDotNet.md)\.

### Copy operation<a name="kms-using-sdks-dotnet-copy"></a>

When copying objects, you add the same request properties \(`ServerSideEncryptionMethod` and `ServerSideEncryptionKeyManagementServiceKeyId`\) to request Amazon S3 to use an AWS KMS CMK\. For more information about copying objects, see [Copying objects](CopyingObjectsExamples.md)\.

### Presigned URLs<a name="kms-using-sdks-dotnet-presigned-url"></a>

When creating a presigned URL for an object encrypted using an AWS KMS CMK, you must explicitly specify Signature Version 4\.

```
AWSConfigs.S3Config.UseSignatureVersion4 = true;
```

For a code example, see [Generate a presigned object URL using AWS SDK for \.NET](ShareObjectPreSignedURLDotNetSDK.md)\.


# Using the AWS SDK for Ruby for Multipart Upload<a name="uploadobjusingmpu-ruby-sdk"></a>

The AWS SDK for Ruby version 3 supports Amazon S3 multipart uploads in two ways\. For the first option, you can use managed file uploads\. For more information, see [Uploading Files to Amazon S3](https://aws.amazon.com/blogs/developer/uploading-files-to-amazon-s3/) in the *AWS Developer Blog*\. Managed file uploads are the recommended method for uploading files to a bucket\. They provides the following benefits:
+ Manages multipart uploads for objects larger than 15MB\.
+ Correctly opens files in binary mode to avoid encoding issues\.
+ Uses multiple threads for uploading parts of large objects in parallel\.

Alternatively, you can use the following multipart upload client operations directly:
+ [create\_multipart\_upload](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/S3/Client.html#create_multipart_upload-instance_method) – Initiates a multipart upload and returns an upload ID\.
+ [upload\_part](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/S3/Client.html#upload_part-instance_method) – Uploads a part in a multipart upload\.
+ [upload\_part\_copy](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/S3/Client.html#upload_part_copy-instance_method) – Uploads a part by copying data from an existing object as data source\.
+ [complete\_multipart\_upload](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/S3/Client.html#complete_multipart_upload-instance_method) – Completes a multipart upload by assembling previously uploaded parts\.
+ [abort\_multipart\_upload](https://docs.aws.amazon.com/sdk-for-ruby/v3/api/Aws/S3/Client.html#abort_multipart_upload-instance_method) – Stops a multipart upload\.

For more information, see [Using the AWS SDK for Ruby \- Version 3](UsingTheMPRubyAPI.md)\.


# Authenticating SOAP requests<a name="SOAPAuthentication"></a>

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

Every non\-anonymous request must contain authentication information to establish the identity of the principal making the request\. In SOAP, the authentication information is put into the following elements of the SOAP request:
+ Your AWS Access Key ID
**Note**  
When making authenticated SOAP requests, temporary security credentials are not supported\. For more information about types of credentials, see [Making requests](MakingRequests.md)\.
+ `Timestamp:` This must be a dateTime \(go to [http://www\.w3\.org/TR/xmlschema\-2/\#dateTime](http://www.w3.org/TR/xmlschema-2/#dateTime)\) in the Coordinated Universal Time \(Greenwich Mean Time\) time zone, such as `2009-01-01T12:00:00.000Z`\. Authorization will fail if this timestamp is more than 15 minutes away from the clock on Amazon S3 servers\.
+ `Signature:` The RFC 2104 HMAC\-SHA1 digest \(go to [http://www\.ietf\.org/rfc/rfc2104\.txt](http://www.ietf.org/rfc/rfc2104.txt)\) of the concatenation of "AmazonS3" \+ OPERATION \+ Timestamp, using your AWS Secret Access Key as the key\. For example, in the following CreateBucket sample request, the signature element would contain the HMAC\-SHA1 digest of the value "AmazonS3CreateBucket2009\-01\-01T12:00:00\.000Z":

For example, in the following CreateBucket sample request, the signature element would contain the HMAC\-SHA1 digest of the value "AmazonS3CreateBucket2009\-01\-01T12:00:00\.000Z":

**Example**  

```
1. <CreateBucket xmlns="https://doc.s3.amazonaws.com/2006-03-01">
2.   <Bucket>quotes</Bucket>
3.   <Acl>private</Acl>
4.   <AWSAccessKeyId>AKIAIOSFODNN7EXAMPLE</AWSAccessKeyId>
5.   <Timestamp>2009-01-01T12:00:00.000Z</Timestamp>
6.   <Signature>Iuyz3d3P0aTou39dzbqaEXAMPLE=</Signature>
7. </CreateBucket>
```

**Note**  
SOAP requests, both authenticated and anonymous, must be sent to Amazon S3 using SSL\. Amazon S3 returns an error when you send a SOAP request over HTTP\. 

**Important**  
Due to different interpretations regarding how extra time precision should be dropped, \.NET users should take care not to send Amazon S3 overly specific time stamps\. This can be accomplished by manually constructing `DateTime` objects with only millisecond precision\.


# Using BitTorrent to retrieve objects stored in Amazon S3<a name="S3TorrentRetrieve"></a>

Any object in Amazon S3 that can be read anonymously can also be downloaded via BitTorrent\. Doing so requires use of a BitTorrent client application\. Amazon does not distribute a BitTorrent client application, but there are many free clients available\. The Amazon S3BitTorrent implementation has been tested to work with the official BitTorrent client \(go to [http://www\.bittorrent\.com/](http://www.bittorrent.com/)\)\.

The starting point for a BitTorrent download is a \.torrent file\. This small file describes for BitTorrent clients both the data to be downloaded and where to get started finding that data\. A \.torrent file is a small fraction of the size of the actual object to be downloaded\. Once you feed your BitTorrent client application an Amazon S3 generated \.torrent file, it should start downloading immediately from Amazon S3 and from any "peer" BitTorrent clients\.

Retrieving a \.torrent file for any publicly available object is easy\. Simply add a "?torrent" query string parameter at the end of the REST GET request for the object\. No authentication is required\. Once you have a BitTorrent client installed, downloading an object using BitTorrent download might be as easy as opening this URL in your web browser\.

There is no mechanism to fetch the \.torrent for an Amazon S3 object using the SOAP API\.

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

**Example**  
This example retrieves the Torrent file for the "Nelson" object in the "quotes" bucket\.  
`Sample Request`  

```
1. GET /quotes/Nelson?torrent HTTP/1.0
2. Date: Wed, 25 Nov 2009 12:00:00 GMT
```
`Sample Response`  

```
1. HTTP/1.1 200 OK
2. x-amz-request-id: 7CD745EBB7AB5ED9
3. Date: Wed, 25 Nov 2009 12:00:00 GMT
4. Content-Disposition: attachment; filename=Nelson.torrent;
5. Content-Type: application/x-bittorrent
6. Content-Length: 537
7. Server: AmazonS3
8. 
9. <body: a Bencoded dictionary as defined by the BitTorrent specification>
```


# Reserved Keywords<a name="s3-glacier-select-sql-reference-keyword-list"></a>

Below is the list of reserved keywords for Amazon S3 Select and S3 Glacier Select\. These include function names, data types, operators, etc\., that needed to run the SQL expressions used to query object content\.

```
absolute
action
add
all
allocate
alter
and
any
are
as
asc
assertion
at
authorization
avg
bag
begin
between
bit
bit_length
blob
bool
boolean
both
by
cascade
cascaded
case
cast
catalog
char
char_length
character
character_length
check
clob
close
coalesce
collate
collation
column
commit
connect
connection
constraint
constraints
continue
convert
corresponding
count
create
cross
current
current_date
current_time
current_timestamp
current_user
cursor
date
day
deallocate
dec
decimal
declare
default
deferrable
deferred
delete
desc
describe
descriptor
diagnostics
disconnect
distinct
domain
double
drop
else
end
end-exec
escape
except
exception
exec
execute
exists
external
extract
false
fetch
first
float
for
foreign
found
from
full
get
global
go
goto
grant
group
having
hour
identity
immediate
in
indicator
initially
inner
input
insensitive
insert
int
integer
intersect
interval
into
is
isolation
join
key
language
last
leading
left
level
like
limit
list
local
lower
match
max
min
minute
missing
module
month
names
national
natural
nchar
next
no
not
null
nullif
numeric
octet_length
of
on
only
open
option
or
order
outer
output
overlaps
pad
partial
pivot
position
precision
prepare
preserve
primary
prior
privileges
procedure
public
read
real
references
relative
restrict
revoke
right
rollback
rows
schema
scroll
second
section
select
session
session_user
set
sexp
size
smallint
some
space
sql
sqlcode
sqlerror
sqlstate
string
struct
substring
sum
symbol
system_user
table
temporary
then
time
timestamp
timezone_hour
timezone_minute
to
trailing
transaction
translate
translation
trim
true
tuple
union
unique
unknown
unpivot
update
upper
usage
user
using
value
values
varchar
varying
view
when
whenever
where
with
work
write
year
zone
```


# Making requests using federated user temporary credentials<a name="AuthUsingTempFederationToken"></a>

You can request temporary security credentials and provide them to your federated users or applications who need to access your AWS resources\. This section provides examples of how you can use the AWS SDK to obtain temporary security credentials for your federated users or applications and send authenticated requests to Amazon S3 using those credentials\. For a list of available AWS SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code/)\. 

**Note**  
Both the AWS account and an IAM user can request temporary security credentials for federated users\. However, for added security, only an IAM user with the necessary permissions should request these temporary credentials to ensure that the federated user gets at most the permissions of the requesting IAM user\. In some applications, you might find it suitable to create an IAM user with specific permissions for the sole purpose of granting temporary security credentials to your federated users and applications\.


# Actions, resources, and condition keys for Amazon S3<a name="list_amazons3"></a>

Amazon S3 \(service prefix: `s3`\) provides the following service\-specific resources, actions, and condition context keys for use in IAM permission policies\.

References:
+ Learn how to [configure this service](https://docs.aws.amazon.com/AmazonS3/latest/dev/)\.
+ View a list of the [API operations available for this service](https://docs.aws.amazon.com/AmazonS3/latest/API/)\.
+ Learn how to secure this service and its resources by [using IAM](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-overview.html) permission policies\.

**Topics**
+ [Actions defined by Amazon S3](#amazons3-actions-as-permissions)
+ [Resource types defined by Amazon S3](#amazons3-resources-for-iam-policies)
+ [Condition keys for Amazon S3](#amazons3-policy-keys)

## Actions defined by Amazon S3<a name="amazons3-actions-as-permissions"></a>

You can specify the following actions in the `Action` element of an IAM policy statement\. Use policies to grant permissions to perform an operation in AWS\. When you use an action in a policy, you usually allow or deny access to the API operation or CLI command with the same name\. However, in some cases, a single action controls access to more than one operation\. Alternatively, some operations require several different actions\.

The **Resource types** column indicates whether each action supports resource\-level permissions\. If there is no value for this column, you must specify all resources \("\*"\) in the `Resource` element of your policy statement\. If the column includes a resource type, then you can specify an ARN of that type in a statement with that action\. Required resources are indicated in the table with an asterisk \(\*\)\. If you specify a resource\-level permission ARN in a statement using this action, then it must be of this type\. Some actions support multiple resource types\. If the resource type is optional \(not indicated as required\), then you can choose to use one but not the other\.


****  
[\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/list_amazons3.html)

## Resource types defined by Amazon S3<a name="amazons3-resources-for-iam-policies"></a>

The following resource types are defined by this service and can be used in the `Resource` element of IAM permission policy statements\. Each action in the [Actions table](#amazons3-actions-as-permissions) identifies the resource types that can be specified with that action\. A resource type can also define which condition keys you can include in a policy\. These keys are displayed in the last column of the table\.


****  

| Resource types | ARN | Condition keys | 
| --- | --- | --- | 
|   [ accesspoint ](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-points.html)  |  arn:$\{Partition\}:s3:$\{Region\}:$\{Account\}:accesspoint/$\{AccessPointName\}  |  | 
|   [ bucket ](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html)  |  arn:$\{Partition\}:s3:::$\{BucketName\}  |  | 
|   [ object ](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html)  |  arn:$\{Partition\}:s3:::$\{BucketName\}/$\{ObjectName\}  |  | 
|   [ job ](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-managing-jobs.html)  |  arn:$\{Partition\}:s3:$\{Region\}:$\{Account\}:job/$\{JobId\}  |  | 
|   [ storagelensconfiguration ](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens.html)  |  arn:$\{Partition\}:s3:$\{Region\}:$\{Account\}:storage\-lens/$\{ConfigId\}  |   [ aws:ResourceTag/$\{TagKey\} ](#amazons3-aws_ResourceTag___TagKey_)   | 

## Condition keys for Amazon S3<a name="amazons3-policy-keys"></a>

Amazon S3 defines the following condition keys that can be used in the `Condition` element of an IAM policy\. You can use these keys to further refine the conditions under which the policy statement applies\.

To view the global condition keys that are available to all services, see [Available global condition keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#AvailableKeys)\.


****  

| Condition keys | Description | Type | 
| --- | --- | --- | 
|   [ aws:RequestTag/$\{TagKey\} ](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-requesttag)  | Filters actions based on the tags that are passed in the request | String | 
|   [ aws:ResourceTag/$\{TagKey\} ](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-resourcetag)  | Filters actions based on the tags associated with the resource | String | 
|   [ aws:TagKeys ](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-tagkeys)  | Filters actions based on the tag keys that are passed in the request | String | 
|   [ s3:AccessPointNetworkOrigin ](https://docs.aws.amazon.com/AmazonS3/latest/dev/creating-access-points.html#access-points-policies)  | Filters access by the network origin \(Internet or VPC\) | String | 
|   [ s3:DataAccessPointAccount ](https://docs.aws.amazon.com/AmazonS3/latest/dev/creating-access-points.html#access-points-policies)  | Filters access by the AWS Account ID that owns the access point | String | 
|   s3:DataAccessPointArn  | Filters access by an access point Amazon Resource Name \(ARN\) | String | 
|   [ s3:ExistingJobOperation ](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-job-tags-examples.html)  | Filters access to updating the job priority by operation | String | 
|   [ s3:ExistingJobPriority ](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-job-tags-examples.html)  | Filters access to cancelling existing jobs by priority range | Numeric | 
|   [ s3:ExistingObjectTag/<key> ](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-tagging.html#tagging-and-policies)  | Filters access by existing object tag key and value | String | 
|   [ s3:JobSuspendedCause ](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-job-tags-examples.html)  | Filters access to cancelling suspended jobs by a specific job suspended cause \(for example, AWAITING\_CONFIRMATION\) | String | 
|   [ s3:LocationConstraint ](https://docs.aws.amazon.com/AmazonS3/latest/dev/amazon-s3-policy-keys.html#condition-key-bucket-ops-1)  | Filters access by a specific Region | String | 
|   [ s3:RequestJobOperation ](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-job-tags-examples.html)  | Filters access to creating jobs by operation | String | 
|   [ s3:RequestJobPriority ](https://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-job-tags-examples.html)  | Filters access to creating new jobs by priority range | Numeric | 
|   [ s3:RequestObjectTag/<key> ](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-tagging.html#tagging-and-policies)  | Filters access by the tag keys and values to be added to objects | String | 
|   [ s3:RequestObjectTagKeys ](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-tagging.html#tagging-and-policies)  | Filters access by the tag keys to be added to objects | String | 
|   [ s3:VersionId ](https://docs.aws.amazon.com/AmazonS3/latest/dev/amazon-s3-policy-keys.html#getobjectversion-limit-access-to-specific-version-3)  | Filters access by a specific object version | String | 
|   [ s3:authType ](https://docs.aws.amazon.com/AmazonS3/latest/API/bucket-policy-s3-sigv4-conditions.html)  | Filters access by authentication method | String | 
|   [ s3:delimiter ](https://docs.aws.amazon.com/AmazonS3/latest/dev/walkthrough1.html)  | Filters access by delimiter parameter | String | 
|   [ s3:locationconstraint ](https://docs.aws.amazon.com/AmazonS3/latest/dev/amazon-s3-policy-keys.html#condition-key-bucket-ops-1)  | Filters access by a specific Region | String | 
|   [ s3:max\-keys ](https://docs.aws.amazon.com/AmazonS3/latest/dev/amazon-s3-policy-keys.html#example-numeric-condition-operators)  | Filters access by maximum number of keys returned in a ListBucket request | Numeric | 
|   [ s3:object\-lock\-legal\-hold ](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html#object-lock-legal-holds)  | Filters access by object legal hold status | String | 
|   s3:object\-lock\-mode  | Filters access by object retention mode \(COMPLIANCE or GOVERNANCE\) | String | 
|   [ s3:object\-lock\-remaining\-retention\-days ](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-managing.html#object-lock-managing-retention-limits)  | Filters access by remaining object retention days | String | 
|   [ s3:object\-lock\-retain\-until\-date ](https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock-overview.html#object-lock-retention-periods)  | Filters access by object retain\-until date | String | 
|   [ s3:prefix ](https://docs.aws.amazon.com/AmazonS3/latest/dev/amazon-s3-policy-keys.html#condition-key-bucket-ops-2)  | Filters access by key name prefix | String | 
|   [ s3:signatureAge ](https://docs.aws.amazon.com/AmazonS3/latest/API/bucket-policy-s3-sigv4-conditions.html)  | Filters access by the age in milliseconds of the request signature | Numeric | 
|   [ s3:signatureversion ](https://docs.aws.amazon.com/AmazonS3/latest/API/bucket-policy-s3-sigv4-conditions.html)  | Filters access by the version of AWS Signature used on the request | String | 
|   [ s3:versionid ](https://docs.aws.amazon.com/AmazonS3/latest/dev/amazon-s3-policy-keys.html#getobjectversion-limit-access-to-specific-version-3)  | Filters access by a specific object version | String | 
|   [ s3:x\-amz\-acl ](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#permissions)  | Filters access by canned ACL in the request's x\-amz\-acl header | String | 
|   [ s3:x\-amz\-content\-sha256 ](https://docs.aws.amazon.com/AmazonS3/latest/API/bucket-policy-s3-sigv4-conditions.html)  | Filters access to unsigned content in your bucket | String | 
|   [ s3:x\-amz\-copy\-source ](https://docs.aws.amazon.com/AmazonS3/latest/dev/amazon-s3-policy-keys.html#putobject-limit-copy-source-3)  | Filters access to requests with a specific bucket, prefix, or object as the copy source | String | 
|   [ s3:x\-amz\-grant\-full\-control ](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#permissions)  | Filters access to requests with the x\-amz\-grant\-full\-control \(full control\) header | String | 
|   [ s3:x\-amz\-grant\-read ](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#permissions)  | Filters access to requests with the x\-amz\-grant\-read \(read access\) header | String | 
|   [ s3:x\-amz\-grant\-read\-acp ](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#permissions)  | Filters access to requests with the x\-amz\-grant\-read\-acp \(read permissions for the ACL\) header | String | 
|   [ s3:x\-amz\-grant\-write ](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#permissions)  | Filters access to requests with the x\-amz\-grant\-write \(write access\) header | String | 
|   [ s3:x\-amz\-grant\-write\-acp ](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#permissions)  | Filters access to requests with the x\-amz\-grant\-write\-acp \(write permissions for the ACL\) header | String | 
|   [ s3:x\-amz\-metadata\-directive ](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CopyObject.html)  | Filters access by object metadata behavior \(COPY or REPLACE\) when objects are copied | String | 
|   [ s3:x\-amz\-server\-side\-encryption ](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html)  | Filters access by server\-side encryption | String | 
|   [ s3:x\-amz\-server\-side\-encryption\-aws\-kms\-key\-id ](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html#require-sse-kms)  | Filters access by AWS KMS customer managed CMK for server\-side encryption | String | 
|   [ s3:x\-amz\-storage\-class ](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html#sc-howtoset)  | Filters access by storage class | String | 
|   [ s3:x\-amz\-website\-redirect\-location ](https://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html#page-redirect-using-rest-api)  | Filters access by a specific website redirect location for buckets that are configured as static websites | String | 


# Infrastructure security in Amazon S3<a name="network-isolation"></a>

As a managed service, Amazon S3 is protected by the AWS global network security procedures that are described in the [Amazon Web Services: Overview of Security Processes](https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf)\.

Access to Amazon S3 via the network is through AWS published APIs\. Clients must support Transport Layer Security \(TLS\) 1\.0\. We recommend TLS 1\.2\. Clients must also support cipher suites with Perfect Forward Secrecy \(PFS\) such as Ephemeral Diffie\-Hellman \(DHE\) or Elliptic Curve Diffie\-Hellman Ephemeral \(ECDHE\)\. Additionally, requests must be signed using AWS Signature V4 or AWS Signature V2, requiring valid credentials to be provided\.

These APIs are callable from any network location\. However, Amazon S3 does support resource\-based access policies, which can include restrictions based on the source IP address\. You can also use Amazon S3 bucket policies to control access to buckets from specific virtual private cloud \(VPC\) \(VPC\) endpoints, or specific VPCs\. Effectively, this isolates network access to a given Amazon S3 bucket from only the specific VPC within the AWS network\. For more information, see [Example Bucket Policies for VPC Endpoints for Amazon S3](example-bucket-policies-vpc-endpoint.md)\.

The following security best practices also address infrastructure security in Amazon S3:
+ [Consider VPC endpoints for Amazon S3 access](security-best-practices.md#end-points)
+ [Identify and audit all your Amazon S3 buckets](security-best-practices.md#audit)


# Copy an Amazon S3 object using the AWS SDK for \.NET multipart upload API<a name="CopyingObjctsUsingLLNetMPUapi"></a>

The following C\# example shows how to use the AWS SDK for \.NET to copy an Amazon S3 object that is larger than 5 GB from one source location to another, such as from one bucket to another\. To copy objects that are smaller than 5 GB, use the single\-operation copy procedure described in [Copy an Amazon S3 Object in a Single Operation Using the AWS SDK for \.NET](CopyingObjectUsingNetSDK.md)\. For more information about Amazon S3 multipart uploads, see [Multipart upload overview](mpuoverview.md)\.

This example shows how to copy an Amazon S3 object that is larger than 5 GB from one S3 bucket to another using the AWS SDK for \.NET multipart upload API\. For information about SDK compatibility and instructions for creating and testing a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CopyObjectUsingMPUapiTest
    {
        private const string sourceBucket = "*** provide the name of the bucket with source object ***";
        private const string targetBucket = "*** provide the name of the bucket to copy the object to ***";
        private const string sourceObjectKey = "*** provide the name of object to copy ***";
        private const string targetObjectKey = "*** provide the name of the object copy ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2; 
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            Console.WriteLine("Copying an object");
            MPUCopyObjectAsync().Wait();
        }
        private static async Task MPUCopyObjectAsync()
        {
            // Create a list to store the upload part responses.
            List<UploadPartResponse> uploadResponses = new List<UploadPartResponse>();
            List<CopyPartResponse> copyResponses = new List<CopyPartResponse>();

            // Setup information required to initiate the multipart upload.
            InitiateMultipartUploadRequest initiateRequest =
                new InitiateMultipartUploadRequest
                {
                    BucketName = targetBucket,
                    Key = targetObjectKey
                };

            // Initiate the upload.
            InitiateMultipartUploadResponse initResponse =
                await s3Client.InitiateMultipartUploadAsync(initiateRequest);

            // Save the upload ID.
            String uploadId = initResponse.UploadId;

            try
            {
                // Get the size of the object.
                GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest
                {
                    BucketName = sourceBucket,
                    Key = sourceObjectKey
                };

                GetObjectMetadataResponse metadataResponse =
                    await s3Client.GetObjectMetadataAsync(metadataRequest);
                long objectSize = metadataResponse.ContentLength; // Length in bytes.

                // Copy the parts.
                long partSize = 5 * (long)Math.Pow(2, 20); // Part size is 5 MB.

                long bytePosition = 0;
                for (int i = 1; bytePosition < objectSize; i++)
                {
                    CopyPartRequest copyRequest = new CopyPartRequest
                    {
                        DestinationBucket = targetBucket,
                        DestinationKey = targetObjectKey,
                        SourceBucket = sourceBucket,
                        SourceKey = sourceObjectKey,
                        UploadId = uploadId,
                        FirstByte = bytePosition,
                        LastByte = bytePosition + partSize - 1 >= objectSize ? objectSize - 1 : bytePosition + partSize - 1,
                        PartNumber = i
                    };

                    copyResponses.Add(await s3Client.CopyPartAsync(copyRequest));

                    bytePosition += partSize;
                }

                // Set up to complete the copy.
                CompleteMultipartUploadRequest completeRequest =
                new CompleteMultipartUploadRequest
                {
                    BucketName = targetBucket,
                    Key = targetObjectKey,
                    UploadId = initResponse.UploadId
                };
                completeRequest.AddPartETags(copyResponses);

                // Complete the copy.
                CompleteMultipartUploadResponse completeUploadResponse = 
                    await s3Client.CompleteMultipartUploadAsync(completeRequest);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}
```

## More info<a name="CopyingObjctsUsingLLNetMPUapi-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# Using access points<a name="using-access-points"></a>

You can access the objects in an Amazon S3 bucket with an *access point* using the AWS Management Console, AWS CLI, AWS SDKs, or the S3 REST APIs\.

Access points have Amazon Resource Names \(ARNs\)\. Access point ARNs are similar to bucket ARNs, but they are explicitly typed and encode the access point's Region and the AWS account ID of the access point's owner\. For more information about ARNs, see [Amazon Resource Names \(ARNs\)](https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html) in the *AWS General Reference*\.

Access point ARNs use the format `arn:aws:s3:region:account-id:accesspoint/resource`\. For example:
+ **arn:aws:s3:us\-west\-2:123456789012:accesspoint/test** represents the access point named `test`, owned by account `123456789012` in Region `us-west-2`\.
+ **arn:aws:s3:us\-west\-2:123456789012:accesspoint/\*** represents all access points under account `123456789012` in Region `us-west-2`\.

ARNs for objects accessed through an access point use the format `arn:aws:s3:region:account-id:accesspoint/access-point-name/object/resource`\. For example:
+ **arn:aws:s3:us\-west\-2:123456789012:accesspoint/test/object/unit\-01** represents the object `unit-01`, accessed through the access point named `test`, owned by account `123456789012` in Region `us-west-2`\.
+ **arn:aws:s3:us\-west\-2:123456789012:accesspoint/test/object/\*** represents all objects for access point `test`, in account `123456789012` in Region `us-west-2`\.
+ **arn:aws:s3:us\-west\-2:123456789012:accesspoint/test/object/unit\-01/finance/\*** represents all objects under prefix `unit-01/finance/` for access point `test`, in account `123456789012` in Region `us-west-2`\.

## Access point compatibility with S3 operations and AWS services<a name="access-points-service-api-support"></a>

Access points in Amazon S3 are compatible with a subset of S3 operations and other AWS services\. The following sections list the compatible services and S3 operations\.

**AWS Services**

You can use S3 Access Points with AWS CloudFormation\.

For more information about AWS CloudFormation, see [What is AWS CloudFormation?](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html) in the *AWS CloudFormation User Guide*\.

**S3 operations**

You can use access points to access a bucket using the following subset of Amazon S3 APIs:
+ `[AbortMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_AbortMultipartUpload.html)`
+ `[CompleteMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CompleteMultipartUpload.html)`
+ `[CopyObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CopyObject.html)` \(same\-region copies only\)
+ `[CreateMultipartUpload](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateMultipartUpload.html)`
+ `[DeleteObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteObject.html)`
+ `[DeleteObjectTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteObjectTagging.html)`
+ `[GetObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html)`
+ `[GetObjectAcl](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObjectAcl.html)`
+ `[GetObjectLegalHold](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObjectLegalHold.html)`
+ `[GetObjectRetention](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObjectRetention.html)`
+ `[GetObjectTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObjectTagging.html)`
+ `[HeadObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_HeadObject.html)`
+ `[ListMultipartUploads](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListMultipartUploads.html)`
+ `[ListObjectsV2](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html)`
+ `[ListParts](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListParts.html)`
+ `[PutObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html)`
+ `[PutObjectLegalHold](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObjectLegalHold.html)`
+ `[PutObjectRetention](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObjectRetention.html)`
+ `[PutObjectAcl](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObjectAcl.html)`
+ `[PutObjectTagging](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObjectTagging.html)`
+ `[RestoreObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_RestoreObject.html)`
+ `[UploadPart](https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPart.html)`
+ `[UploadPartCopy](https://docs.aws.amazon.com/AmazonS3/latest/API/API_UploadPartCopy.html)` \(same\-region copies only\)

## Monitoring and logging<a name="access-points-monitoring-logging"></a>

Amazon S3 logs requests made through access points and requests made to the APIs that manage access points, such as `CreateAccessPoint` and `GetAccessPointPolicy`\.

Requests made to Amazon S3 through an access point appear in your S3 server access logs and AWS CloudTrail logs with the access point's hostname\. An access point's hostname takes the form `access_point_name-account_id.s3-accesspoint.Region.amazonaws.com`\. For example, suppose that you have the following bucket and access point configuration:
+ A bucket named `my-bucket` in Region `us-west-2` that contains object `my-image.jpg`
+ An access point named `my-bucket-ap` that is associated with `my-bucket`
+ Your AWS account ID is `123456789012`

A request made to retrieve `my-image.jpg` directly through the bucket appears in your logs with a hostname of `my-bucket.s3.us-west-2.amazonaws.com`\. If you make the request through the access point instead, Amazon S3 retrieves the same object but logs the request with a hostname of `my-bucket-ap-123456789012.s3-accesspoint.us-west-2.amazonaws.com`\.

For more information about S3 Server Access Logs, see [Amazon S3 server access logging](ServerLogs.md)\. For more information about AWS CloudTrail, see [What is AWS CloudTrail?](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html) in the *AWS CloudTrail User Guide*\.

**Note**  
S3 access points aren't currently compatible with Amazon CloudWatch metrics\.

## Examples<a name="access-points-usage-examples"></a>

The following examples demonstrate how to use access points with compatible operations in Amazon S3\.

**Example**  
***Example: Request an object through an access point***  
The following example requests the object `my-image.jpg` through the access point `prod` owned by account ID `123456789012` in Region `us-west-2`, and saves the downloaded file as `download.jpg`\.  

```
aws s3api get-object --key my-image.jpg --bucket arn:aws:s3:us-west-2:123456789012:accesspoint/prod download.jpg
```

**Example**  
***Example: Upload an object through an access point***  
The following example uploads the object `my-image.jpg` through the access point `prod` owned by account ID `123456789012` in Region `us-west-2`\.  

```
aws s3api put-object --bucket arn:aws:s3:us-west-2:123456789012:accesspoint/prod --key my-image.jpg --body my-image.jpg
```

**Example**  
***Example: Delete an object through an access point***  
The following example deletes the object `my-image.jpg` through the access point `prod` owned by account ID `123456789012` in Region `us-west-2`\.  

```
aws s3api delete-object --bucket arn:aws:s3:us-west-2:123456789012:accesspoint/prod --key my-image.jpg
```

**Example**  
***Example: List objects through an access point***  
The following example lists objects through the access point `prod` owned by account ID `123456789012` in Region `us-west-2`\.  

```
aws s3api list-objects-v2 --bucket arn:aws:s3:us-west-2:123456789012:accesspoint/prod
```

**Example**  
***Example: Add a tag set to an object through an access point***  
The following example adds a tag set to the existing object `my-image.jpg` through the access point `prod` owned by account ID `123456789012` in Region `us-west-2`\.  

```
aws s3api put-object-tagging --bucket arn:aws:s3:us-west-2:123456789012:accesspoint/prod --key my-image.jpg --tagging TagSet=[{Key="finance",Value="true"}]
```

**Example**  
***Example: Grant access permissions through an access point using an ACL***  
The following example applies an ACL to an existing object `my-image.jpg` through the access point `prod` owned by account ID `123456789012` in Region `us-west-2`\.  

```
aws s3api put-object-acl --bucket arn:aws:s3:us-west-2:123456789012:accesspoint/prod --key my-image.jpg --acl private
```


# Best Practices Design Patterns: Optimizing Amazon S3 Performance<a name="optimizing-performance"></a>

Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3\. Amazon S3 automatically scales to high request rates\. For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per [prefix](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html#keyprefix) in a bucket\. There are no limits to the number of prefixes in a bucket\. You can increase your read or write performance by parallelizing reads\. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second\.

Some data lake applications on Amazon S3 scan millions or billions of objects for queries that run over petabytes of data\. These data lake applications achieve single\-instance transfer rates that maximize the network interface use for their [Amazon EC2](https://docs.aws.amazon.com/ec2/index.html) instance, which can be up to 100 Gb/s on a single instance\. These applications then aggregate throughput across multiple instances to get multiple terabits per second\. 

Other applications are sensitive to latency, such as social media messaging applications\. These applications can achieve consistent small object latencies \(and first\-byte\-out latencies for larger objects\) of roughly 100–200 milliseconds\.

Other AWS services can also help accelerate performance for different application architectures\. For example, if you want higher transfer rates over a single HTTP connection or single\-digit millisecond latencies, use [Amazon CloudFront](https://docs.aws.amazon.com/cloudfront/index.html) or [Amazon ElastiCache](https://docs.aws.amazon.com/elasticache/index.html) for caching with Amazon S3\.

Additionally, if you want fast data transport over long distances between a client and an S3 bucket, use [Amazon S3 Transfer Acceleration](transfer-acceleration.md)\. Transfer Acceleration uses the globally distributed edge locations in CloudFront to accelerate data transport over geographical distances\. If your Amazon S3 workload uses server\-side encryption with AWS Key Management Service \(SSE\-KMS\), see [AWS KMS Limits](https://docs.aws.amazon.com/kms/latest/developerguide/limits.html) in the AWS Key Management Service Developer Guide for information about the request rates supported for your use case\. 

The following topics describe best practice guidelines and design patterns for optimizing performance for applications that use Amazon S3\. This guidance supersedes any previous guidance on optimizing performance for Amazon S3\. For example, previously Amazon S3 performance guidelines recommended randomizing prefix naming with hashed characters to optimize performance for frequent data retrievals\. You no longer have to randomize prefix naming for performance, and can use sequential date\-based naming for your prefixes\. Refer to the [Performance Guidelines for Amazon S3](optimizing-performance-guidelines.md) and [Performance Design Patterns for Amazon S3](optimizing-performance-design-patterns.md) for the most current information about performance optimization for Amazon S3\. 

**Topics**
+ [Performance Guidelines](optimizing-performance-guidelines.md)
+ [Performance Design Patterns](optimizing-performance-design-patterns.md)


# Deleting object versions<a name="DeletingObjectVersions"></a>

You can delete object versions whenever you want\. In addition, you can also define lifecycle configuration rules for objects that have a well\-defined lifecycle to request Amazon S3 to expire current object versions or permanently remove noncurrent object versions\. When your bucket is version\-enabled or versioning is suspended, the lifecycle configuration actions work as follows:
+ The `Expiration` action applies to the current object version and instead of deleting the current object version, Amazon S3 retains the current version as a noncurrent version by adding a delete marker, which then becomes the current version\.
+ The `NoncurrentVersionExpiration` action applies to noncurrent object versions, and Amazon S3 permanently removes these object versions\. You cannot recover permanently removed objects\.

For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

A `DELETE` request has the following use cases:
+ When versioning is enabled, a simple `DELETE` cannot permanently delete an object\. 

  Instead, Amazon S3 inserts a delete marker in the bucket, and that marker becomes the current version of the object with a new ID\. When you try to `GET` an object whose current version is a delete marker, Amazon S3 behaves as though the object has been deleted \(even though it has not been erased\) and returns a 404 error\. 

  The following figure shows that a simple `DELETE` does not actually remove the specified object\. Instead, Amazon S3 inserts a delete marker\.  
![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_versioningEnabled.png)
+ To permanently delete versioned objects, you must use `DELETE Object versionId`\.

  The following figure shows that deleting a specified object version permanently removes that object\.  
![\[Image NOT FOUND\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/versioning_DELETE_versioningEnabled2.png)

## Using the console<a name="delete-obj-version-version-enabled-console"></a>

For instructions see, [How Do I See the Versions of an S3 Object?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/view-object-versions.html) in the Amazon Simple Storage Service Console User Guide\. 

## Using the AWS SDKs<a name="delete-obj-version-version-enabled-bucket-sdks"></a>

For examples of deleting objects using the AWS SDKs for Java, \.NET, and PHP, see [Deleting objects](DeletingObjects.md)\. The examples for deleting objects in nonversioned and versioning\-enabled buckets are the same, although in the case of versioning\-enabled buckets, Amazon S3 assigns a version number\. Otherwise, the version number is null\. 

For information about using other AWS SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code/)\. 

## Using REST<a name="delete-obj-version-enabled-bucket-rest"></a>

**To a delete a specific version of an object**
+ In a `DELETE`, specify a version ID\.

**Example Deleting a specific version**  
The following example shows how to delete version UIORUnfnd89493jJFJ of `photo.gif`\.  

```
1. DELETE /photo.gif?versionId=UIORUnfnd89493jJFJ HTTP/1.1 
2. 
3. Host: bucket.s3.amazonaws.com
4. Date: Wed, 12 Oct 2009 17:50:00 GMT
5. Authorization: AWS AKIAIOSFODNN7EXAMPLE:xQE0diMbLRepdf3YB+FIEXAMPLE=
6. Content-Type: text/plain
7. Content-Length: 0
```

## Related topics<a name="delete-obj-version-enabled-related-topics"></a>

 [Using MFA delete](UsingMFADelete.md) 

 [Working with delete markers](DeleteMarker.md) 

 [Removing delete markers](RemDelMarker.md) 

 [Using versioning](Versioning.md) 


# Managing ACLs Using the REST API<a name="acl-using-rest-api"></a>

Amazon S3 APIs enable you to set an ACL when you create a bucket or an object\. Amazon S3 also provides API to set an ACL on an existing bucket or an object\. These APIs provide the following methods to set an ACL:
+ **Set ACL using request headers—** When you send a request to create a resource \(bucket or object\), you set an ACL using the request headers\. Using these headers, you can either specify a canned ACL or specify grants explicitly \(identifying grantee and permissions explicitly\)\. 
+ **Set ACL using request body—** When you send a request to set an ACL on an existing resource, you can set the ACL either in the request header or in the body\. 

For information on the REST API support for managing ACLs, see the following sections in the *Amazon Simple Storage Service API Reference*:
+  [GET Bucket acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETacl.html) 
+  [PUT Bucket acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTacl.html) 
+  [GET Object acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGETacl.html) 
+  [PUT Object acl](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUTacl.html) 
+  [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) 
+  [PUT Bucket](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html) 
+  [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html) 
+  [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html) 

## Access Control List \(ACL\)\-Specific Request Headers<a name="acl-headers-rest-api"></a>

You can use headers to grant access control list \(ACL\)\-based permissions\. By default, all objects are private\. Only the owner has full access control\. When adding a new object, you can grant permissions to individual AWS accounts or to predefined groups defined by Amazon S3\. These permissions are then added to the Access Control List \(ACL\) on the object\. For more information, see [Using ACLs](https://docs.aws.amazon.com/AmazonS3/latest/dev/S3_ACLs_UsingACLs.html)\. 

With this operation, you can grant access permissions using one these two methods:
+ **Canned ACL \(`x-amz-acl`\)** — Amazon S3 supports a set of predefined ACLs, known as canned ACLs\. Each canned ACL has a predefined set of grantees and permissions\. For more information, see [Canned ACL](acl-overview.md#canned-acl)\.
+ **Access Permissions** — To explicitly grant access permissions to specific AWS accounts or groups, use the following headers\. Each header maps to specific permissions that Amazon S3 supports in an ACL\. For more information, see [Access Control List \(ACL\) Overview](acl-overview.md)\. In the header, you specify a list of grantees who get the specific permission\. 
  + x\-amz\-grant\-read
  + x\-amz\-grant\-write
  + x\-amz\-grant\-read\-acp
  + x\-amz\-grant\-write\-acp
  + x\-amz\-grant\-full\-control


# Deleting multiple objects using the AWS SDK for Java<a name="DeletingMultipleObjectsUsingJava"></a>

The AWS SDK for Java provides the `AmazonS3Client.deleteObjects()` method for deleting multiple objects\. For each object that you want to delete, you specify the key name\. If the bucket is versioning\-enabled, you have the following options:
+ Specify only the object's key name\. Amazon S3 will add a delete marker to the object\.
+ Specify both the object's key name and a version ID to be deleted\. Amazon S3 will delete the specified version of the object\.

**Example**  
The following example uses the Multi\-Object Delete API to delete objects from a bucket that is not version\-enabled\. The example uploads sample objects to the bucket and then uses the `AmazonS3Client.deleteObjects()` method to delete the objects in a single request\. In the `DeleteObjectsRequest`, the example specifies only the object key names because the objects do not have version IDs\.   
For more information about deleting objects, see [Deleting objects](DeletingObjects.md)\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
 
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.DeleteObjectsRequest;
import com.amazonaws.services.s3.model.DeleteObjectsRequest.KeyVersion;
import com.amazonaws.services.s3.model.DeleteObjectsResult;

import java.io.IOException;
import java.util.ArrayList;

public class DeleteMultipleObjectsNonVersionedBucket {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .build();

            // Upload three sample objects.
            ArrayList<KeyVersion> keys = new ArrayList<KeyVersion>();
            for (int i = 0; i < 3; i++) {
                String keyName = "delete object example " + i;
                s3Client.putObject(bucketName, keyName, "Object number " + i + " to be deleted.");
                keys.add(new KeyVersion(keyName));
            }
            System.out.println(keys.size() + " objects successfully created.");

            // Delete the sample objects.
            DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest(bucketName)
                    .withKeys(keys)
                    .withQuiet(false);

            // Verify that the objects were deleted successfully.
            DeleteObjectsResult delObjRes = s3Client.deleteObjects(multiObjectDeleteRequest);
            int successfulDeletes = delObjRes.getDeletedObjects().size();
            System.out.println(successfulDeletes + " objects successfully deleted.");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}
```

**Example**  
The following example uses the Multi\-Object Delete API to delete objects from a version\-enabled bucket\. It does the following:   

1. Creates sample objects and then deletes them, specifying the key name and version ID for each object to delete\. The operation deletes only the specified object versions\.

1. Creates sample objects and then deletes them by specifying only the key names\. Because the example doesn't specify version IDs, the operation adds a delete marker to each object, without deleting any specific object versions\. After the delete markers are added, these objects will not appear in the AWS Management Console\.

1. Remove the delete markers by specifying the object keys and version IDs of the delete markers\. The operation deletes the delete markers, which results in the objects reappearing in the AWS Management Console\.

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketVersioningConfiguration;
import com.amazonaws.services.s3.model.DeleteObjectsRequest;
import com.amazonaws.services.s3.model.DeleteObjectsRequest.KeyVersion;
import com.amazonaws.services.s3.model.DeleteObjectsResult;
import com.amazonaws.services.s3.model.DeleteObjectsResult.DeletedObject;
import com.amazonaws.services.s3.model.PutObjectResult;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class DeleteMultipleObjectsVersionEnabledBucket {
    private static AmazonS3 S3_CLIENT;
    private static String VERSIONED_BUCKET_NAME;

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        VERSIONED_BUCKET_NAME = "*** Bucket name ***";

        try {
            S3_CLIENT = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Check to make sure that the bucket is versioning-enabled.
            String bucketVersionStatus = S3_CLIENT.getBucketVersioningConfiguration(VERSIONED_BUCKET_NAME).getStatus();
            if (!bucketVersionStatus.equals(BucketVersioningConfiguration.ENABLED)) {
                System.out.printf("Bucket %s is not versioning-enabled.", VERSIONED_BUCKET_NAME);
            } else {
                // Upload and delete sample objects, using specific object versions.
                uploadAndDeleteObjectsWithVersions();

                // Upload and delete sample objects without specifying version IDs.
                // Amazon S3 creates a delete marker for each object rather than deleting
                // specific versions.
                DeleteObjectsResult unversionedDeleteResult = uploadAndDeleteObjectsWithoutVersions();

                // Remove the delete markers placed on objects in the non-versioned create/delete method.
                multiObjectVersionedDeleteRemoveDeleteMarkers(unversionedDeleteResult);
            }
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void uploadAndDeleteObjectsWithVersions() {
        System.out.println("Uploading and deleting objects with versions specified.");

        // Upload three sample objects.
        ArrayList<KeyVersion> keys = new ArrayList<KeyVersion>();
        for (int i = 0; i < 3; i++) {
            String keyName = "delete object without version ID example " + i;
            PutObjectResult putResult = S3_CLIENT.putObject(VERSIONED_BUCKET_NAME, keyName,
                    "Object number " + i + " to be deleted.");
            // Gather the new object keys with version IDs.
            keys.add(new KeyVersion(keyName, putResult.getVersionId()));
        }

        // Delete the specified versions of the sample objects.
        DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest(VERSIONED_BUCKET_NAME)
                .withKeys(keys)
                .withQuiet(false);

        // Verify that the object versions were successfully deleted.
        DeleteObjectsResult delObjRes = S3_CLIENT.deleteObjects(multiObjectDeleteRequest);
        int successfulDeletes = delObjRes.getDeletedObjects().size();
        System.out.println(successfulDeletes + " objects successfully deleted");
    }

    private static DeleteObjectsResult uploadAndDeleteObjectsWithoutVersions() {
        System.out.println("Uploading and deleting objects with no versions specified.");

        // Upload three sample objects.
        ArrayList<KeyVersion> keys = new ArrayList<KeyVersion>();
        for (int i = 0; i < 3; i++) {
            String keyName = "delete object with version ID example " + i;
            S3_CLIENT.putObject(VERSIONED_BUCKET_NAME, keyName, "Object number " + i + " to be deleted.");
            // Gather the new object keys without version IDs.
            keys.add(new KeyVersion(keyName));
        }

        // Delete the sample objects without specifying versions.
        DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest(VERSIONED_BUCKET_NAME).withKeys(keys)
                .withQuiet(false);

        // Verify that delete markers were successfully added to the objects.
        DeleteObjectsResult delObjRes = S3_CLIENT.deleteObjects(multiObjectDeleteRequest);
        int successfulDeletes = delObjRes.getDeletedObjects().size();
        System.out.println(successfulDeletes + " objects successfully marked for deletion without versions.");
        return delObjRes;
    }

    private static void multiObjectVersionedDeleteRemoveDeleteMarkers(DeleteObjectsResult response) {
        List<KeyVersion> keyList = new ArrayList<KeyVersion>();
        for (DeletedObject deletedObject : response.getDeletedObjects()) {
            // Note that the specified version ID is the version ID for the delete marker.
            keyList.add(new KeyVersion(deletedObject.getKey(), deletedObject.getDeleteMarkerVersionId()));
        }
        // Create a request to delete the delete markers.
        DeleteObjectsRequest deleteRequest = new DeleteObjectsRequest(VERSIONED_BUCKET_NAME).withKeys(keyList);

        // Delete the delete markers, leaving the objects intact in the bucket.
        DeleteObjectsResult delObjRes = S3_CLIENT.deleteObjects(deleteRequest);
        int successfulDeletes = delObjRes.getDeletedObjects().size();
        System.out.println(successfulDeletes + " delete markers successfully deleted");
    }
}
```


# Signing and authenticating REST requests<a name="RESTAuthentication"></a>

**Topics**
+ [Using temporary security credentials](#UsingTemporarySecurityCredentials)
+ [The authentication header](#ConstructingTheAuthenticationHeader)
+ [Request canonicalization for signing](#RESTAuthenticationRequestCanonicalization)
+ [Constructing the CanonicalizedResource element](#ConstructingTheCanonicalizedResourceElement)
+ [Constructing the CanonicalizedAmzHeaders element](#RESTAuthenticationConstructingCanonicalizedAmzHeaders)
+ [Positional versus named HTTP header StringToSign elements](#RESTAuthenticationStringToSign)
+ [Time stamp requirement](#RESTAuthenticationTimeStamp)
+ [Authentication examples](#RESTAuthenticationExamples)
+ [REST request signing problems](#RESTAuthenticationDebugging)
+ [Query string request authentication alternative](#RESTAuthenticationQueryStringAuth)

**Note**  
This topic explains authenticating requests using Signature Version 2\. Amazon S3 now supports the latest Signature Version 4\. This latest signature version is supported in all regions and any new regions after January 30, 2014 will support only Signature Version 4\. For more information, go to [Authenticating Requests \(AWS Signature Version 4\)](https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html) in the *Amazon Simple Storage Service API Reference*\.

 Authentication is the process of proving your identity to the system\. Identity is an important factor in Amazon S3 access control decisions\. Requests are allowed or denied in part based on the identity of the requester\. For example, the right to create buckets is reserved for registered developers and \(by default\) the right to create objects in a bucket is reserved for the owner of the bucket in question\. As a developer, you'll be making requests that invoke these privileges, so you'll need to prove your identity to the system by authenticating your requests\. This section shows you how\. 

**Note**  
 The content in this section does not apply to HTTP POST\. For more information, see [Browser\-based uploads using POST \(AWS signature version 2\)](UsingHTTPPOST.md)\. 

 The Amazon S3 REST API uses a custom HTTP scheme based on a keyed\-HMAC \(Hash Message Authentication Code\) for authentication\. To authenticate a request, you first concatenate selected elements of the request to form a string\. You then use your AWS secret access key to calculate the HMAC of that string\. Informally, we call this process "signing the request," and we call the output of the HMAC algorithm the signature, because it simulates the security properties of a real signature\. Finally, you add this signature as a parameter of the request by using the syntax described in this section\. 

 When the system receives an authenticated request, it fetches the AWS secret access key that you claim to have and uses it in the same way to compute a signature for the message it received\. It then compares the signature it calculated against the signature presented by the requester\. If the two signatures match, the system concludes that the requester must have access to the AWS secret access key and therefore acts with the authority of the principal to whom the key was issued\. If the two signatures do not match, the request is dropped and the system responds with an error message\. 

**Example Authenticated Amazon S3 REST request**  

```
1. GET /photos/puppy.jpg HTTP/1.1
2. Host: awsexamplebucket1.us-west-1.s3.amazonaws.com
3. Date: Tue, 27 Mar 2007 19:36:42 +0000
4. 
5. Authorization: AWS AKIAIOSFODNN7EXAMPLE:
6. qgk2+6Sv9/oM7G3qLEjTH1a1l1g=
```

## Using temporary security credentials<a name="UsingTemporarySecurityCredentials"></a>

If you are signing your request using temporary security credentials \(see [Making requests](MakingRequests.md)\), you must include the corresponding security token in your request by adding the `x-amz-security-token` header\. 

When you obtain temporary security credentials using the AWS Security Token Service API, the response includes temporary security credentials and a session token\. You provide the session token value in the `x-amz-security-token` header when you send requests to Amazon S3\. For information about the AWS Security Token Service API provided by IAM, go to [Action](https://docs.aws.amazon.com/STS/latest/APIReference/API_Operations.html) in the *AWS Security Token Service API Reference Guide *\.

## The authentication header<a name="ConstructingTheAuthenticationHeader"></a>

The Amazon S3 REST API uses the standard HTTP `Authorization` header to pass authentication information\. \(The name of the standard header is unfortunate because it carries authentication information, not authorization\.\) Under the Amazon S3 authentication scheme, the Authorization header has the following form:

```
1. Authorization: AWS AWSAccessKeyId:Signature
```

Developers are issued an AWS access key ID and AWS secret access key when they register\. For request authentication, the `AWSAccessKeyId` element identifies the access key ID that was used to compute the signature and, indirectly, the developer making the request\.

The `Signature` element is the RFC 2104 HMAC\-SHA1 of selected elements from the request, and so the `Signature` part of the Authorization header will vary from request to request\. If the request signature calculated by the system matches the `Signature` included with the request, the requester will have demonstrated possession of the AWS secret access key\. The request will then be processed under the identity, and with the authority, of the developer to whom the key was issued\.

Following is pseudogrammar that illustrates the construction of the `Authorization` request header\. \(In the example, `\n` means the Unicode code point `U+000A`, commonly called newline\)\. 

```
 1. Authorization = "AWS" + " " + AWSAccessKeyId + ":" + Signature;
 2. 
 3. Signature = Base64( HMAC-SHA1( UTF-8-Encoding-Of(YourSecretAccessKey), UTF-8-Encoding-Of( StringToSign ) ) );
 4. 
 5. StringToSign = HTTP-Verb + "\n" +
 6. 	Content-MD5 + "\n" +
 7. 	Content-Type + "\n" +
 8. 	Date + "\n" +
 9. 	CanonicalizedAmzHeaders +
10. 	CanonicalizedResource;
11. 
12. CanonicalizedResource = [ "/" + Bucket ] +
13. 	<HTTP-Request-URI, from the protocol name up to the query string> +
14. 	[ subresource, if present. For example "?acl", "?location", "?logging", or "?torrent"];
15. 
16. CanonicalizedAmzHeaders = <described below>
```

 HMAC\-SHA1 is an algorithm defined by [ RFC 2104 \- Keyed\-Hashing for Message Authentication ](http://www.ietf.org/rfc/rfc2104.txt)\. The algorithm takes as input two byte\-strings, a key and a message\. For Amazon S3 request authentication, use your AWS secret access key \(`YourSecretAccessKey`\) as the key, and the UTF\-8 encoding of the `StringToSign` as the message\. The output of HMAC\-SHA1 is also a byte string, called the digest\. The `Signature` request parameter is constructed by Base64 encoding this digest\. 

## Request canonicalization for signing<a name="RESTAuthenticationRequestCanonicalization"></a>

 Recall that when the system receives an authenticated request, it compares the computed request signature with the signature provided in the request in `StringToSign`\. For that reason, you must compute the signature by using the same method used by Amazon S3\. We call the process of putting a request in an agreed\-upon form for signing *canonicalization*\. 

## Constructing the CanonicalizedResource element<a name="ConstructingTheCanonicalizedResourceElement"></a>

 `CanonicalizedResource` represents the Amazon S3 resource targeted by the request\. Construct it for a REST request as follows: 


**Launch process**  

|  |  | 
| --- |--- |
|  1  |  Start with an empty string \(`""`\)\.  | 
|  2  |  If the request specifies a bucket using the HTTP Host header \(virtual hosted\-style\), append the bucket name preceded by a `"/"` \(e\.g\., "/bucketname"\)\. For path\-style requests and requests that don't address a bucket, do nothing\. For more information about virtual hosted\-style requests, see [Virtual hosting of buckets](VirtualHosting.md)\.  For a virtual hosted\-style request "https://awsexamplebucket1\.s3\.us\-west\-1\.amazonaws\.com/photos/puppy\.jpg", the `CanonicalizedResource` is "/awsexamplebucket1"\.  For the path\-style request, "https://s3\.us\-west\-1\.amazonaws\.com/awsexamplebucket1/photos/puppy\.jpg", the `CanonicalizedResource` is ""\.  | 
|  3  |  Append the path part of the un\-decoded HTTP Request\-URI, up\-to but not including the query string\. For a virtual hosted\-style request "https://awsexamplebucket1\.s3\.us\-west\-1\.amazonaws\.com/photos/puppy\.jpg", the `CanonicalizedResource` is "/awsexamplebucket1/photos/puppy\.jpg"\. For a path\-style request, "https://s3\.us\-west\-1\.amazonaws\.com/awsexamplebucket1/photos/puppy\.jpg", the `CanonicalizedResource` is "/awsexamplebucket1/photos/puppy\.jpg"\. At this point, the `CanonicalizedResource` is the same for both the virtual hosted\-style and path\-style request\. For a request that does not address a bucket, such as [GET Service](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTServiceGET.html), append "/"\.  | 
|  4  |  If the request addresses a subresource, such as `?versioning`, `?location`, `?acl`, `?torrent`, `?lifecycle`, or `?versionid`, append the subresource, its value if it has one, and the question mark\. Note that in case of multiple subresources, subresources must be lexicographically sorted by subresource name and separated by '&', e\.g\., ?acl&versionId=*value*\.  The subresources that must be included when constructing the CanonicalizedResource Element are acl, lifecycle, location, logging, notification, partNumber, policy, requestPayment, torrent, uploadId, uploads, versionId, versioning, versions, and website\.  If the request specifies query string parameters overriding the response header values \(see [Get Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html)\), append the query string parameters and their values\. When signing, you do not encode these values; however, when making the request, you must encode these parameter values\. The query string parameters in a GET request include `response-content-type`, `response-content-language`, `response-expires`, `response-cache-control`, `response-content-disposition`, and `response-content-encoding`\. The `delete` query string parameter must be included when you create the CanonicalizedResource for a multi\-object Delete request\.  | 

Elements of the CanonicalizedResource that come from the HTTP Request\-URI should be signed literally as they appear in the HTTP request, including URL\-Encoding meta characters\. 

The `CanonicalizedResource` might be different than the HTTP Request\-URI\. In particular, if your request uses the HTTP `Host` header to specify a bucket, the bucket does not appear in the HTTP Request\-URI\. However, the `CanonicalizedResource` continues to include the bucket\. Query string parameters might also appear in the Request\-URI but are not included in `CanonicalizedResource`\. For more information, see [Virtual hosting of buckets](VirtualHosting.md)\. 

## Constructing the CanonicalizedAmzHeaders element<a name="RESTAuthenticationConstructingCanonicalizedAmzHeaders"></a>

To construct the CanonicalizedAmzHeaders part of `StringToSign`, select all HTTP request headers that start with 'x\-amz\-' \(using a case\-insensitive comparison\), and use the following process\. 


**CanonicalizedAmzHeaders process**  

|  |  | 
| --- |--- |
| 1 | Convert each HTTP header name to lowercase\. For example, 'X\-Amz\-Date' becomes 'x\-amz\-date'\. | 
| 2 | Sort the collection of headers lexicographically by header name\. | 
| 3 | Combine header fields with the same name into one "header\-name:comma\-separated\-value\-list" pair as prescribed by RFC 2616, section 4\.2, without any spaces between values\. For example, the two metadata headers 'x\-amz\-meta\-username: fred' and 'x\-amz\-meta\-username: barney' would be combined into the single header 'x\-amz\-meta\-username: fred,barney'\. | 
| 4 | "Unfold" long headers that span multiple lines \(as allowed by RFC 2616, section 4\.2\) by replacing the folding spaces \(including new\-line\) by a single space\. | 
| 5 | Trim any spaces around the colon in the header\. For example, the header 'x\-amz\-meta\-username: fred,barney' would become 'x\-amz\-meta\-username:fred,barney'  | 
| 6 |  Finally, append a newline character \(U\+000A\) to each canonicalized header in the resulting list\. Construct the CanonicalizedResource element by concatenating all headers in this list into a single string\. | 

## Positional versus named HTTP header StringToSign elements<a name="RESTAuthenticationStringToSign"></a>

 The first few header elements of `StringToSign` \(Content\-Type, Date, and Content\-MD5\) are positional in nature\. `StringToSign` does not include the names of these headers, only their values from the request\. In contrast, the '`x-amz-`' elements are named\. Both the header names and the header values appear in `StringToSign`\. 

 If a positional header called for in the definition of `StringToSign` is not present in your request \(for example, `Content-Type` or `Content-MD5` are optional for PUT requests and meaningless for GET requests\), substitute the empty string \(""\) for that position\. 

## Time stamp requirement<a name="RESTAuthenticationTimeStamp"></a>

A valid time stamp \(using either the HTTP `Date` header or an `x-amz-date` alternative\) is mandatory for authenticated requests\. Furthermore, the client timestamp included with an authenticated request must be within 15 minutes of the Amazon S3 system time when the request is received\. If not, the request will fail with the `RequestTimeTooSkewed` error code\. The intention of these restrictions is to limit the possibility that intercepted requests could be replayed by an adversary\. For stronger protection against eavesdropping, use the HTTPS transport for authenticated requests\. 

**Note**  
The validation constraint on request date applies only to authenticated requests that do not use query string authentication\. For more information, see [Query string request authentication alternative](#RESTAuthenticationQueryStringAuth)\.

Some HTTP client libraries do not expose the ability to set the `Date` header for a request\. If you have trouble including the value of the 'Date' header in the canonicalized headers, you can set the timestamp for the request by using an '`x-amz-date`' header instead\. The value of the `x-amz-date` header must be in one of the RFC 2616 formats \([http://www\.ietf\.org/rfc/rfc2616\.txt](http://www.ietf.org/rfc/rfc2616.txt)\)\. When an `x-amz-date` header is present in a request, the system will ignore any `Date` header when computing the request signature\. Therefore, if you include the `x-amz-date` header, use the empty string for the `Date` when constructing the `StringToSign`\. See the next section for an example\. 

## Authentication examples<a name="RESTAuthenticationExamples"></a>

 The examples in this section use the \(non\-working\) credentials in the following table\. 


| Parameter | Value | 
| --- | --- | 
| AWSAccessKeyId | AKIAIOSFODNN7EXAMPLE | 
| AWSSecretAccessKey | wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY | 

In the example `StringToSign`s, formatting is not significant, and `\n` means the Unicode code point `U+000A`, commonly called newline\. Also, the examples use "\+0000" to designate the time zone\. You can use "GMT" to designate timezone instead, but the signatures shown in the examples will be different\.

### Object GET<a name="RESTAuthenticationExamples-1"></a>

This example gets an object from the awsexamplebucket1 bucket\.


| Request | StringToSign | 
| --- | --- | 
|  <pre>GET /photos/puppy.jpg HTTP/1.1<br />Host: awsexamplebucket1.us-west-1.s3.amazonaws.com<br />Date: Tue, 27 Mar 2007 19:36:42 +0000<br /><br />Authorization: AWS AKIAIOSFODNN7EXAMPLE:<br />qgk2+6Sv9/oM7G3qLEjTH1a1l1g=</pre>  |  <pre>GET\n<br />\n<br />\n<br />Tue, 27 Mar 2007 19:36:42 +0000\n<br />/awsexamplebucket1/photos/puppy.jpg</pre>  | 

 Note that the CanonicalizedResource includes the bucket name, but the HTTP Request\-URI does not\. \(The bucket is specified by the Host header\.\) 

**Note**  
The following Python script calculates the preceeding signature, using the provided parameters\. You can use this script to construct your own signatures, replacing the keys and StringToSign as appropriate\.  

```
 1. import base64
 2. import hmac
 3. from hashlib import sha1
 4. 
 5. access_key = 'AKIAIOSFODNN7EXAMPLE'.encode("UTF-8")
 6. secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'.encode("UTF-8")
 7. 
 8. string_to_sign = 'GET\n\n\nTue, 27 Mar 2007 19:36:42 +0000\n/awsexamplebucket1/photos/puppy.jpg'.encode("UTF-8")
 9. signature = base64.encodestring(
10.                                 hmac.new(
11.                                          secret_key, string_to_sign, sha1
12.                                          ).digest()
13.                                 ).strip()
14. 
15. 
16. print(f"AWS {access_key.decode()}:{signature.decode()}")
```

### Object PUT<a name="RESTAuthenticationExamples-2"></a>

This example puts an object into the awsexamplebucket1 bucket\.


| Request | StringToSign | 
| --- | --- | 
|  <pre>PUT /photos/puppy.jpg HTTP/1.1<br />Content-Type: image/jpeg<br />Content-Length: 94328<br />Host: awsexamplebucket1.s3.us-west-1.amazonaws.com<br />Date: Tue, 27 Mar 2007 21:15:45 +0000<br /><br />Authorization: AWS AKIAIOSFODNN7EXAMPLE:<br />iqRzw+ileNPu1fhspnRs8nOjjIA=<br /></pre>  |  <pre>PUT\n<br />\n<br />image/jpeg\n<br />Tue, 27 Mar 2007 21:15:45 +0000\n<br />/awsexamplebucket1/photos/puppy.jpg</pre>  | 

 Note the Content\-Type header in the request and in the StringToSign\. Also note that the Content\-MD5 is left blank in the StringToSign, because it is not present in the request\. 

### List<a name="RESTAuthenticationExamples-3"></a>



This example lists the content of the awsexamplebucket1 bucket\.


| Request | StringToSign | 
| --- | --- | 
|  <pre>GET /?prefix=photos&max-keys=50&marker=puppy HTTP/1.1<br />User-Agent: Mozilla/5.0<br />Host: awsexamplebucket1.s3.us-west-1.amazonaws.com<br />Date: Tue, 27 Mar 2007 19:42:41 +0000<br /><br />Authorization: AWS AKIAIOSFODNN7EXAMPLE:<br />m0WP8eCtspQl5Ahe6L1SozdX9YA=</pre>  |  <pre>GET\n<br />\n<br />\n<br />Tue, 27 Mar 2007 19:42:41 +0000\n<br />/awsexamplebucket1/</pre>  | 

 Note the trailing slash on the CanonicalizedResource and the absence of query string parameters\. 

### Fetch<a name="RESTAuthenticationExamples-4"></a>

This example fetches the access control policy subresource for the 'awsexamplebucket1' bucket\.


| Request | StringToSign | 
| --- | --- | 
|  <pre>GET /?acl HTTP/1.1<br />Host: awsexamplebucket1.s3.us-west-1.amazonaws.com<br />Date: Tue, 27 Mar 2007 19:44:46 +0000<br /><br />Authorization: AWS AKIAIOSFODNN7EXAMPLE:<br />82ZHiFIjc+WbcwFKGUVEQspPn+0=</pre>  |  <pre>GET\n<br />\n<br />\n<br />Tue, 27 Mar 2007 19:44:46 +0000\n<br />/awsexamplebucket1/?acl</pre>  | 

 Notice how the subresource query string parameter is included in the CanonicalizedResource\. 

### Delete<a name="RESTAuthenticationExamples-5"></a>

This example deletes an object from the 'awsexamplebucket1' bucket using the path\-style and Date alternative\.


| Request | StringToSign | 
| --- | --- | 
|  <pre>DELETE /awsexamplebucket1/photos/puppy.jpg HTTP/1.1<br />User-Agent: dotnet<br />Host: s3.us-west-1.amazonaws.com<br />Date: Tue, 27 Mar 2007 21:20:27 +0000<br /><br />x-amz-date: Tue, 27 Mar 2007 21:20:26 +0000<br />Authorization: AWS AKIAIOSFODNN7EXAMPLE:XbyTlbQdu9Xw5o8P4iMwPktxQd8=</pre>  |  <pre>DELETE\n<br />\n<br />\n<br />Tue, 27 Mar 2007 21:20:26 +0000\n<br />/awsexamplebucket1/photos/puppy.jpg</pre>  | 

 Note how we used the alternate 'x\-amz\-date' method of specifying the date \(because our client library prevented us from setting the date, say\)\. In this case, the `x-amz-date` takes precedence over the `Date` header\. Therefore, date entry in the signature must contain the value of the `x-amz-date` header\. 

### Upload<a name="RESTAuthenticationExamples-6"></a>

This example uploads an object to a CNAME style virtual hosted bucket with metadata\.


| Request | StringToSign | 
| --- | --- | 
|  <pre>PUT /db-backup.dat.gz HTTP/1.1<br />User-Agent: curl/7.15.5<br />Host: static.awsexamplebucket1.net:8080<br />Date: Tue, 27 Mar 2007 21:06:08 +0000<br /><br />x-amz-acl: public-read<br />content-type: application/x-download<br />Content-MD5: 4gJE4saaMU4BqNR0kLY+lw==<br />X-Amz-Meta-ReviewedBy: joe@awsexamplebucket1.net<br />X-Amz-Meta-ReviewedBy: jane@awsexamplebucket1.net<br />X-Amz-Meta-FileChecksum: 0x02661779<br />X-Amz-Meta-ChecksumAlgorithm: crc32<br />Content-Disposition: attachment; filename=database.dat<br />Content-Encoding: gzip<br />Content-Length: 5913339<br /><br />Authorization: AWS AKIAIOSFODNN7EXAMPLE:<br />dKZcB+bz2EPXgSdXZp9ozGeOM4I=</pre>  |  <pre>PUT\n<br />4gJE4saaMU4BqNR0kLY+lw==\n<br />application/x-download\n<br />Tue, 27 Mar 2007 21:06:08 +0000\n<br /><br />x-amz-acl:public-read\n<br />x-amz-meta-checksumalgorithm:crc32\n<br />x-amz-meta-filechecksum:0x02661779\n<br />x-amz-meta-reviewedby:<br />joe@awsexamplebucket1.net,jane@awsexamplebucket1.net\n<br />/static.awsexamplebucket1.net/db-backup.dat.gz</pre>  | 

 Notice how the 'x\-amz\-' headers are sorted, trimmed of extra spaces, and converted to lowercase\. Note also that multiple headers with the same name have been joined using commas to separate values\. 

 Note how only the `Content-Type` and `Content-MD5` HTTP entity headers appear in the `StringToSign`\. The other `Content-*` entity headers do not\. 

 Again, note that the `CanonicalizedResource` includes the bucket name, but the HTTP Request\-URI does not\. \(The bucket is specified by the Host header\.\) 

### List all my buckets<a name="RESTAuthenticationExamples-7"></a>


| Request | StringToSign | 
| --- | --- | 
|  <pre>GET / HTTP/1.1<br />Host: s3.us-west-1.amazonaws.com<br />Date: Wed, 28 Mar 2007 01:29:59 +0000<br /><br />Authorization: AWS AKIAIOSFODNN7EXAMPLE:qGdzdERIC03wnaRNKh6OqZehG9s=</pre>  |  <pre>GET\n<br />\n<br />\n<br />Wed, 28 Mar 2007 01:29:59 +0000\n<br />/</pre>  | 

### Unicode keys<a name="RESTAuthenticationExamples-8"></a>


| Request | StringToSign | 
| --- | --- | 
|  <pre>GET /dictionary/fran%C3%A7ais/pr%c3%a9f%c3%a8re HTTP/1.1<br />Host: s3.us-west-1.amazonaws.com<br />Date: Wed, 28 Mar 2007 01:49:49 +0000<br />Authorization: AWS AKIAIOSFODNN7EXAMPLE:DNEZGsoieTZ92F3bUfSPQcbGmlM=</pre>  |  <pre>GET\n<br />\n<br />\n<br />Wed, 28 Mar 2007 01:49:49 +0000\n<br />/dictionary/fran%C3%A7ais/pr%c3%a9f%c3%a8re</pre>  | 

**Note**  
The elements in `StringToSign` that were derived from the Request\-URI are taken literally, including URL\-Encoding and capitalization\. 

## REST request signing problems<a name="RESTAuthenticationDebugging"></a>

 When REST request authentication fails, the system responds to the request with an XML error document\. The information contained in this error document is meant to help developers diagnose the problem\. In particular, the `StringToSign` element of the `SignatureDoesNotMatch` error document tells you exactly what request canonicalization the system is using\. 

Some toolkits silently insert headers that you do not know about beforehand, such as adding the header `Content-Type` during a PUT\. In most of these cases, the value of the inserted header remains constant, allowing you to discover the missing headers by using tools such as Ethereal or tcpmon\. 

## Query string request authentication alternative<a name="RESTAuthenticationQueryStringAuth"></a>

You can authenticate certain types of requests by passing the required information as query\-string parameters instead of using the `Authorization` HTTP header\. This is useful for enabling direct third\-party browser access to your private Amazon S3 data without proxying the request\. The idea is to construct a "presigned" request and encode it as a URL that an end\-user's browser can retrieve\. Additionally, you can limit a presigned request by specifying an expiration time\. 

For more information on using query parameters to authenticate requests , see [Authenticating Requests: Using Query Parameters \(AWS Signature Version 4\)](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html) in the *Amazon Simple Storage Service API Reference*\. For examples of using the AWS SDKs to generating presigned URLs, see [Share an object with others](ShareObjectPreSignedURL.md)\. 

### Creating a signature<a name="CreatingASignature"></a>

Following is an example query string authenticated Amazon S3 REST request\.

```
1. GET /photos/puppy.jpg
2. ?AWSAccessKeyId=AKIAIOSFODNN7EXAMPLE&Expires=1141889120&Signature=vjbyPxybdZaNmGa%2ByT272YEAiv4%3D HTTP/1.1
3. Host: awsexamplebucket1.s3.us-west-1.amazonaws.com
4. Date: Mon, 26 Mar 2007 19:37:58 +0000
```

The query string request authentication method doesn't require any special HTTP headers\. Instead, the required authentication elements are specified as query string parameters: 


| Query string parameter name | Example value | Description | 
| --- | --- | --- | 
| AWSAccessKeyId | AKIAIOSFODNN7EXAMPLE | Your AWS access key ID\. Specifies the AWS secret access key used to sign the request and, indirectly, the identity of the developer making the request\. | 
| Expires | 1141889120 | The time when the signature expires, specified as the number of seconds since the epoch \(00:00:00 UTC on January 1, 1970\)\. A request received after this time \(according to the server\) will be rejected\.  | 
| Signature | vjbyPxybdZaNmGa%2ByT272YEAiv4%3D | The URL encoding of the Base64 encoding of the HMAC\-SHA1 of StringToSign\. | 

The query string request authentication method differs slightly from the ordinary method but only in the format of the `Signature` request parameter and the `StringToSign` element\. Following is pseudo\-grammar that illustrates the query string request authentication method\. 

```
1. Signature = URL-Encode( Base64( HMAC-SHA1( YourSecretAccessKey, UTF-8-Encoding-Of( StringToSign ) ) ) );
2. 
3. StringToSign = HTTP-VERB + "\n" +
4.     Content-MD5 + "\n" +
5.     Content-Type + "\n" +
6.     Expires + "\n" +
7.     CanonicalizedAmzHeaders +
8.     CanonicalizedResource;
```

`YourSecretAccessKey` is the AWS secret access key ID that Amazon assigns to you when you sign up to be an Amazon Web Service developer\. Notice how the `Signature` is URL\-Encoded to make it suitable for placement in the query string\. Note also that in `StringToSign`, the HTTP `Date` positional element has been replaced with `Expires`\. The `CanonicalizedAmzHeaders` and `CanonicalizedResource` are the same\. 

**Note**  
In the query string authentication method, you do not use the `Date` or the `x-amz-date request` header when calculating the string to sign\.

#### Query string request authentication<a name="query-str-auth-ex"></a>


| Request | StringToSign | 
| --- | --- | 
|  <pre>GET /photos/puppy.jpg?AWSAccessKeyId=AKIAIOSFODNN7EXAMPLE&<br />    Signature=NpgCjnDzrM%2BWFzoENXmpNDUsSn8%3D&<br />    Expires=1175139620 HTTP/1.1<br /><br />Host: awsexamplebucket1.s3.us-west-1.amazonaws.com</pre>  |  <pre>GET\n<br />\n<br />\n<br />1175139620\n<br /><br />/awsexamplebucket1/photos/puppy.jpg</pre>  | 

We assume that when a browser makes the GET request, it won't provide a Content\-MD5 or a Content\-Type header, nor will it set any x\-amz\- headers, so those parts of the `StringToSign` are left blank\. 

#### Using Base64 encoding<a name="S3_Authentication_Base64"></a>

HMAC request signatures must be Base64 encoded\. Base64 encoding converts the signature into a simple ASCII string that can be attached to the request\. Characters that could appear in the signature string like plus \(\+\), forward slash \(/\), and equals \(=\) must be encoded if used in a URI\. For example, if the authentication code includes a plus \(\+\) sign, encode it as %2B in the request\. Encode a forward slash as %2F and equals as %3D\.

For examples of Base64 encoding, refer to the Amazon S3 [Authentication examples](#RESTAuthenticationExamples)\.


# Viewing Amazon S3 Storage Lens metrics using a data export<a name="storage_lens_view_metrics_export"></a>

Amazon S3 Storage Lens metrics are generated daily in CSV or Apache Parquet\-formatted metrics export files and placed in an S3 bucket in your account\. From there, you can ingest the metrics export into the analytics tools of your choice, such as Amazon QuickSight and Amazon Athena, where you can analyze storage usage and activity trends\.

**Topics**
+ [Using an AWS KMS CMK to encrypt your metrics exports](storage_lens_encrypt_permissions.md)
+ [What is an S3 Storage Lens export manifest?](storage_lens_whatis_metrics_export_manifest.md)
+ [Understanding the Amazon S3 Storage Lens export schema](storage_lens_understanding_metrics_export_schema.md)


# How you are charged for BitTorrent delivery<a name="S3TorrentCharge"></a>

There is no extra charge for use of BitTorrent with Amazon S3\. Data transfer via the BitTorrent protocol is metered at the same rate as client/server delivery\. To be precise, whenever a downloading BitTorrent client requests a "piece" of an object from the Amazon S3 "seeder," charges accrue just as if an anonymous request for that piece had been made using the REST or SOAP protocol\. These charges will appear on your Amazon S3 bill and usage reports in the same way\. The difference is that if a lot of clients are requesting the same object simultaneously via BitTorrent, then the amount of data Amazon S3 must serve to satisfy those clients will be lower than with client/server delivery\. This is because the BitTorrent clients are simultaneously uploading and downloading amongst themselves\.

**Note**  
 SOAP support over HTTP is deprecated, but it is still available over HTTPS\. New Amazon S3 features will not be supported for SOAP\. We recommend that you use either the REST API or the AWS SDKs\. 

The data transfer savings achieved from use of BitTorrent can vary widely depending on how popular your object is\. Less popular objects require heavier use of the "seeder" to serve clients, and thus the difference between BitTorrent distribution costs and client/server distribution costs might be small for such objects\. In particular, if only one client is ever downloading a particular object at a time, the cost of BitTorrent delivery will be the same as direct download\.


# Bucket Policy Examples<a name="example-bucket-policies"></a>

This section presents a few examples of typical use cases for bucket policies\. The policies use *bucket* and *examplebucket* strings in the resource value\. To test these policies, replace these strings with your bucket name\. For information about access policy language, see [Policies and Permissions in Amazon S3](access-policy-language-overview.md)\.

**Note**  
Bucket policies are limited to 20 KB in size\.

You can use the [AWS Policy Generator](http://aws.amazon.com/blogs/aws/aws-policy-generator/) to create a bucket policy for your Amazon S3 bucket\. You can then use the generated document to set your bucket policy by using the [Amazon S3 console](https://console.aws.amazon.com/s3/home), through several third\-party tools, or via your application\. 

**Important**  
When testing permissions using the Amazon S3 console, you will need to grant additional permissions that the console requires—`s3:ListAllMyBuckets`, `s3:GetBucketLocation`, and `s3:ListBucket` permissions\. For an example walkthrough that grants permissions to users and tests them using the console, see [Walkthrough: Controlling access to a bucket with user policies](walkthrough1.md)\.

**Topics**
+ [Granting Permissions to Multiple Accounts with Added Conditions](#example-bucket-policies-use-case-1)
+ [Granting Read\-Only Permission to an Anonymous User](#example-bucket-policies-use-case-2)
+ [Limiting Access to Specific IP Addresses](#example-bucket-policies-use-case-3)
+ [Restricting Access to a Specific HTTP Referer](#example-bucket-policies-use-case-4)
+ [Granting Permission to an Amazon CloudFront OAI](#example-bucket-policies-cloudfront)
+ [Adding a Bucket Policy to Require MFA](#example-bucket-policies-use-case-7)
+ [Granting Cross\-Account Permissions to Upload Objects While Ensuring the Bucket Owner Has Full Control](#example-bucket-policies-use-case-8)
+ [Granting Permissions for Amazon S3 Inventory and Amazon S3 Analytics](#example-bucket-policies-use-case-9)
+ [Granting Permissions for Amazon S3 Storage Lens](#example-bucket-policies-lens)
+ [Example Bucket Policies for VPC Endpoints for Amazon S3](example-bucket-policies-vpc-endpoint.md)

## Granting Permissions to Multiple Accounts with Added Conditions<a name="example-bucket-policies-use-case-1"></a>

The following example policy grants the `s3:PutObject` and `s3:PutObjectAcl` permissions to multiple AWS accounts and requires that any request for these operations include the `public-read` canned access control list \(ACL\)\. For more information, see [Amazon S3 Actions](using-with-s3-actions.md) and [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)\.

```
 1. {
 2.   "Version":"2012-10-17",
 3.   "Statement":[
 4.     {
 5.       "Sid":"AddCannedAcl",
 6.       "Effect":"Allow",
 7.     "Principal": {"AWS": ["arn:aws:iam::111122223333:root","arn:aws:iam::444455556666:root"]},
 8.       "Action":["s3:PutObject","s3:PutObjectAcl"],
 9.       "Resource":"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
10.       "Condition":{"StringEquals":{"s3:x-amz-acl":["public-read"]}}
11.     }
12.   ]
13. }
```

## Granting Read\-Only Permission to an Anonymous User<a name="example-bucket-policies-use-case-2"></a>

The following example policy grants the `s3:GetObject` permission to any public anonymous users\. \(For a list of permissions and the operations that they allow, see [Amazon S3 Actions](using-with-s3-actions.md)\.\) This permission allows anyone to read the object data, which is useful for when you configure your bucket as a website and want everyone to be able to read objects in the bucket\. Before you use a bucket policy to grant read\-only permission to an anonymous user, you must disable block public access settings for your bucket\. For more information, see [Setting permissions for website access](WebsiteAccessPermissionsReqd.md)\.

**Warning**  
Use caution when granting anonymous access to your Amazon S3 bucket or disabling block public access settings\. When you grant anonymous access, anyone in the world can access your bucket\. We recommend that you never grant anonymous access to your Amazon S3 bucket unless you specifically need to, such as with [static website hosting](WebsiteHosting.md)\.

```
 1. {
 2.   "Version":"2012-10-17",
 3.   "Statement":[
 4.     {
 5.       "Sid":"PublicRead",
 6.       "Effect":"Allow",
 7.       "Principal": "*",
 8.       "Action":["s3:GetObject","s3:GetObjectVersion"],
 9.       "Resource":["arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"]
10.     }
11.   ]
12. }
```

## Limiting Access to Specific IP Addresses<a name="example-bucket-policies-use-case-3"></a>

The following example denies permissions to any user to perform any Amazon S3 operations on objects in the specified S3 bucket unless the request originates from the range of IP addresses specified in the condition\. 

This statement identifies the 54\.240\.143\.0/24 as the range of allowed Internet Protocol version 4 \(IPv4\) IP addresses\. 

The `Condition` block uses the `NotIpAddress` condition and the `aws:SourceIp` condition key, which is an AWS\-wide condition key\. For more information about these condition keys, see [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)\. The `aws:SourceIp` IPv4 values use the standard CIDR notation\. For more information, see [IAM JSON Policy Elements Reference](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Conditions_IPAddress) in the *IAM User Guide*\. 

**Important**  
Replace the IP address range in this example with an appropriate value for your use case before using this policy\. Otherwise, you will lose the ability to access your bucket\.

```
 1. {
 2.   "Version": "2012-10-17",
 3.   "Id": "S3PolicyId1",
 4.   "Statement": [
 5.     {
 6.       "Sid": "IPAllow",
 7.       "Effect": "Deny",
 8.       "Principal": "*",
 9.       "Action": "s3:*",
10.       "Resource": [
11. 	       "arn:aws:s3:::DOC-EXAMPLE-BUCKET",
12.          "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"
13.       ],
14.       "Condition": {
15. 	 "NotIpAddress": {"aws:SourceIp": "54.240.143.0/24"}
16.       }
17.     }
18.   ]
19. }
```

### Allowing IPv4 and IPv6 Addresses<a name="example-bucket-policies-use-case-ipv6"></a>

When you start using IPv6 addresses, we recommend that you update all of your organization's policies with your IPv6 address ranges in addition to your existing IPv4 ranges to ensure that the policies continue to work as you make the transition to IPv6\.

The following example bucket policy shows how to mix IPv4 and IPv6 address ranges to cover all of your organization's valid IP addresses\. The example policy would allow access to the example IP addresses `54.240.143.1` and `2001:DB8:1234:5678::1` and would deny access to the addresses `54.240.143.129` and `2001:DB8:1234:5678:ABCD::1`\.

The IPv6 values for `aws:SourceIp` must be in standard CIDR format\. For IPv6, we support using `::` to represent a range of 0s \(for example, `2032001:DB8:1234:5678::/64`\)\. For more information, see [ IP Address Condition Operators](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition_operators.html#Conditions_IPAddress) in the *IAM User Guide*\.

**Important**  
Replace the IP address ranges in this example with appropriate values for your use case before using this policy\. Otherwise, you might lose the ability to access your bucket\.

```
 1. {
 2.   "Id":"PolicyId2",
 3.   "Version":"2012-10-17",
 4.   "Statement":[
 5.     {
 6.       "Sid":"AllowIPmix",
 7.       "Effect":"Allow",
 8.       "Principal":"*",
 9.       "Action":"s3:*",
10.       "Resource": [
11. 	       "arn:aws:s3:::DOC-EXAMPLE-BUCKET",
12.          "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"
13.       ],
14.       "Condition": {
15.         "IpAddress": {
16.           "aws:SourceIp": [
17.             "54.240.143.0/24",
18. 	    "2001:DB8:1234:5678::/64"
19.           ]
20.         },
21.         "NotIpAddress": {
22.           "aws:SourceIp": [
23. 	     "54.240.143.128/30",
24. 	     "2001:DB8:1234:5678:ABCD::/80"
25.           ]
26.         }
27.       }
28.     }
29.   ]
30. }
```

## Restricting Access to a Specific HTTP Referer<a name="example-bucket-policies-use-case-4"></a>

Suppose that you have a website with a domain name \(`www.example.com` or `example.com`\) with links to photos and videos stored in your Amazon S3 bucket, `DOC-EXAMPLE-BUCKET`\. By default, all the Amazon S3 resources are private, so only the AWS account that created the resources can access them\. To allow read access to these objects from your website, you can add a bucket policy that allows `s3:GetObject` permission with a condition, using the `aws:Referer` key, that the get request must originate from specific webpages\. The following policy specifies the `StringLike` condition with the `aws:Referer` condition key\.

```
 1. {
 2.   "Version":"2012-10-17",
 3.   "Id":"http referer policy example",
 4.   "Statement":[
 5.     {
 6.       "Sid":"Allow get requests originating from www.example.com and example.com.",
 7.       "Effect":"Allow",
 8.       "Principal":"*",
 9.       "Action":["s3:GetObject","s3:GetObjectVersion"],
10.       "Resource":"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
11.       "Condition":{
12.         "StringLike":{"aws:Referer":["http://www.example.com/*","http://example.com/*"]}
13.       }
14.     }
15.   ]
16. }
```

Make sure the browsers you use include the HTTP `referer` header in the request\.

## Granting Permission to an Amazon CloudFront OAI<a name="example-bucket-policies-cloudfront"></a>

The following example bucket policy grants a CloudFront origin access identity \(OAI\) permission to get \(read\) all objects in your Amazon S3 bucket\. You can use a CloudFront OAI to allow users to access objects in your bucket through CloudFront but not directly through Amazon S3\. For more information, see [Restricting Access to Amazon S3 Content by Using an Origin Access Identity](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html) in the *Amazon CloudFront Developer Guide*\.

The following policy uses the OAI’s ID as the policy’s `Principal`\. For more information about using S3 bucket policies to grant access to a CloudFront OAI, see [Using Amazon S3 Bucket Policies](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#private-content-updating-s3-bucket-policies) in the *Amazon CloudFront Developer Guide*\.

To use this example:
+ Replace *EH1HDMB1FH2TC* with the OAI’s ID\. To find the OAI’s ID, see the [Origin Access Identity page](https://console.aws.amazon.com/cloudfront/home?region=us-east-1#oai:) on the CloudFront console, or use [ListCloudFrontOriginAccessIdentities](https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_ListCloudFrontOriginAccessIdentities.html) in the CloudFront API\.
+ Replace *DOC\-EXAMPLE\-BUCKET* with the name of your Amazon S3 bucket\.

```
 1. {
 2.     "Version": "2012-10-17",
 3.     "Id": "PolicyForCloudFrontPrivateContent",
 4.     "Statement": [
 5.         {
 6.             "Effect": "Allow",
 7.             "Principal": {
 8.                 "AWS": "arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity EH1HDMB1FH2TC"
 9.             },
10.             "Action": "s3:GetObject",
11.             "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"
12.         }
13.     ]
14. }
```

## Adding a Bucket Policy to Require MFA<a name="example-bucket-policies-use-case-7"></a>

Amazon S3 supports MFA\-protected API access, a feature that can enforce multi\-factor authentication \(MFA\) for access to your Amazon S3 resources\. Multi\-factor authentication provides an extra level of security that you can apply to your AWS environment\. It is a security feature that requires users to prove physical possession of an MFA device by providing a valid MFA code\. For more information, see [AWS Multi\-Factor Authentication](https://aws.amazon.com/mfa/)\. You can require MFA for any requests to access your Amazon S3 resources\. 

You can enforce the MFA requirement using the `aws:MultiFactorAuthAge` key in a bucket policy\. AWS Identity and Access Management \(IAM\) users can access Amazon S3 resources by using temporary credentials issued by the AWS Security Token Service \(AWS STS\)\. You provide the MFA code at the time of the AWS STS request\. 

When Amazon S3 receives a request with multi\-factor authentication, the `aws:MultiFactorAuthAge` key provides a numeric value indicating how long ago \(in seconds\) the temporary credential was created\. If the temporary credential provided in the request was not created using an MFA device, this key value is null \(absent\)\. In a bucket policy, you can add a condition to check this value, as shown in the following example bucket policy\. The policy denies any Amazon S3 operation on the `/taxdocuments` folder in the `DOC-EXAMPLE-BUCKET` bucket if the request is not authenticated using MFA\. To learn more about MFA, see [Using Multi\-Factor Authentication \(MFA\) in AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html) in the *IAM User Guide*\.

```
 1. {
 2.     "Version": "2012-10-17",
 3.     "Id": "123",
 4.     "Statement": [
 5.       {
 6.         "Sid": "",
 7.         "Effect": "Deny",
 8.         "Principal": "*",
 9.         "Action": "s3:*",
10.         "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/taxdocuments/*",
11.         "Condition": { "Null": { "aws:MultiFactorAuthAge": true }}
12.       }
13.     ]
14.  }
```

The `Null` condition in the `Condition` block evaluates to true if the `aws:MultiFactorAuthAge` key value is null, indicating that the temporary security credentials in the request were created without the MFA key\. 

The following bucket policy is an extension of the preceding bucket policy\. It includes two policy statements\. One statement allows the `s3:GetObject` permission on a bucket \(`DOC-EXAMPLE-BUCKET`\) to everyone\. Another statement further restricts access to the `DOC-EXAMPLE-BUCKET/taxdocuments` folder in the bucket by requiring MFA\. 

```
 1. {
 2.     "Version": "2012-10-17",
 3.     "Id": "123",
 4.     "Statement": [
 5.       {
 6.         "Sid": "",
 7.         "Effect": "Deny",
 8.         "Principal": "*",
 9.         "Action": "s3:*",
10.         "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/taxdocuments/*",
11.         "Condition": { "Null": { "aws:MultiFactorAuthAge": true } }
12.       },
13.       {
14.         "Sid": "",
15.         "Effect": "Allow",
16.         "Principal": "*",
17.         "Action": ["s3:GetObject"],
18.         "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"
19.       }
20.     ]
21.  }
```

You can optionally use a numeric condition to limit the duration for which the `aws:MultiFactorAuthAge` key is valid, independent of the lifetime of the temporary security credential used in authenticating the request\. For example, the following bucket policy, in addition to requiring MFA authentication, also checks how long ago the temporary session was created\. The policy denies any operation if the `aws:MultiFactorAuthAge` key value indicates that the temporary session was created more than an hour ago \(3,600 seconds\)\. 

```
 1. {
 2.     "Version": "2012-10-17",
 3.     "Id": "123",
 4.     "Statement": [
 5.       {
 6.         "Sid": "",
 7.         "Effect": "Deny",
 8.         "Principal": "*",
 9.         "Action": "s3:*",
10.         "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/taxdocuments/*",
11.         "Condition": {"Null": {"aws:MultiFactorAuthAge": true }}
12.       },
13.       {
14.         "Sid": "",
15.         "Effect": "Deny",
16.         "Principal": "*",
17.         "Action": "s3:*",
18.         "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/taxdocuments/*",
19.         "Condition": {"NumericGreaterThan": {"aws:MultiFactorAuthAge": 3600 }}
20.        },
21.        {
22.          "Sid": "",
23.          "Effect": "Allow",
24.          "Principal": "*",
25.          "Action": ["s3:GetObject"],
26.          "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*"
27.        }
28.     ]
29.  }
```

## Granting Cross\-Account Permissions to Upload Objects While Ensuring the Bucket Owner Has Full Control<a name="example-bucket-policies-use-case-8"></a>

The following example shows how to allow another AWS account to upload objects to your bucket while taking full control of the uploaded objects\. This policy enforces that a specific AWS account \(`123456789012`\) be granted the ability to upload objects only if that account includes the bucket\-owner\-full\-control canned ACL on upload\. The StringEquals condition in the policy specifies the s3:x\-amz\-acl condition key to express the requirement \(see [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)\)\. 

```
 1. {
 2.    "Version":"2012-10-17",
 3.    "Statement":[
 4.      {
 5.        "Sid":"PolicyForAllowUploadWithACL",
 6.        "Effect":"Allow",
 7.        "Principal":{"AWS":"123456789012"},
 8.        "Action":"s3:PutObject",
 9.        "Resource":"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
10.        "Condition": {
11.          "StringEquals": {"s3:x-amz-acl":"bucket-owner-full-control"}
12.        }
13.      }
14.    ]
15. }
```

## Granting Permissions for Amazon S3 Inventory and Amazon S3 Analytics<a name="example-bucket-policies-use-case-9"></a>

Amazon S3 inventory creates lists of the objects in an Amazon S3 bucket, and Amazon S3 analytics export creates output files of the data used in the analysis\. The bucket that the inventory lists the objects for is called the *source bucket*\. The bucket where the inventory file is written and the bucket where the analytics export file is written is called a *destination bucket*\. You must create a bucket policy for the destination bucket when setting up inventory for an Amazon S3 bucket and when setting up the analytics export\. For more information, see [ Amazon S3 inventory](storage-inventory.md) and [Amazon S3 analytics – Storage Class Analysis](analytics-storage-class.md)\.

The following example bucket policy grants Amazon S3 permission to write objects \(PUTs\) from the account for the source bucket to the destination bucket\. You use a bucket policy like this on the destination bucket when setting up Amazon S3 inventory and Amazon S3 analytics export\.

```
 1. {
 2.   "Version":"2012-10-17",
 3.   "Statement":[
 4.     {
 5.       "Sid":"InventoryAndAnalyticsExamplePolicy",
 6.       "Effect":"Allow",
 7.       "Principal": {"Service": "s3.amazonaws.com"},
 8.       "Action":"s3:PutObject",
 9.       "Resource":["arn:aws:s3:::destinationbucket/*"],
10.       "Condition": {
11.           "ArnLike": {
12.               "aws:SourceArn": "arn:aws:s3:::sourcebucket"
13.            },
14.          "StringEquals": {
15.              "aws:SourceAccount": "123456789012",
16.              "s3:x-amz-acl": "bucket-owner-full-control"
17.           }
18.        }
19.     }
20.   ]
21. }
```

## Granting Permissions for Amazon S3 Storage Lens<a name="example-bucket-policies-lens"></a>

Amazon S3 Storage Lens aggregates your usage and activity metrics and displays the information in an interactive dashboard on the Amazon S3 console or through a metrics data export that can be downloaded in CSV or Parquet format\. You can use the dashboard to visualize insights and trends, flag outliers, and provides recommendations for optimizing storage costs and applying data protection best practices\. You can use S3 Storage Lens through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\.

S3 Storage Lens can aggregate your storage usage to metrics exports in an Amazon S3 bucket for further analysis\. The bucket that S3 Storage Lens places its metrics exports is known as the *destination bucket*\. You must have a bucket policy for the *destination bucket* when when setting up your S3 Storage Lens metrics export\. For more information, see [Amazon S3 Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage_lens.html)\.

The following example bucket policy grants Amazon S3 permission to write objects \(PUTs\) to a *destination bucket*\. You use a bucket policy like this on the *destination bucket* when setting up an S3 Storage Lens metrics export\.

```
 1. {
 2.     "Version": "2012-10-17",
 3.     "Statement": [
 4.         {
 5.             "Sid": "S3StorageLensExamplePolicy",
 6.             "Effect": "Allow",
 7.             "Principal": {
 8.                 "Service": [
 9.                     "storage-lens.s3.amazonaws.com"
10.                 ]
11.             },
12.             "Action": "s3:PutObject",
13.             "Resource": "arn:aws:s3:::destination-bucket/destination-prefix/StorageLens/111122223333/*",
14.             "Condition": {
15.                 "StringEquals": {
16.                     "s3:x-amz-acl": "bucket-owner-full-control"
17.                 },
18.                 "StringEquals": {
19.                     "aws:SourceAccount": "111122223333"
20.                 },
21.                 "StringEquals": {
22.                     "aws:SourceArn": "arn:aws:s3:your-region:111122223333:storage-lens/your-dashboard-configuration-id"
23.                 }
24.             }
25.         }
26.     ]
27. }
```

The following modification to the previous bucket policy `"Action": "s3:PutObject"` resource when setting up an S3 Storage Lens organization\-level metrics export\.

```
1. {
2.             "Action": "s3:PutObject",
3.             "Resource": "arn:aws:s3:::destination-bucket/destination-prefix/StorageLens/your-organization-id/*",
```


# Copy an object using the AWS SDK for Java multipart upload API<a name="CopyingObjctsUsingLLJavaMPUapi"></a>

To copy an Amazon S3 object that is larger than 5 GB with the AWS SDK for Java, use the low\-level Java API \. For objects smaller than 5 GB, use the single\-operation copy described in [Copy an Object Using the AWS SDK for Java](CopyingObjectUsingJava.md)\. 

To copy an object using the low\-level Java API, do the following:
+ Initiate a multipart upload by executing the `AmazonS3Client.initiateMultipartUpload()` method\.
+ Save the upload ID from the response object that the `AmazonS3Client.initiateMultipartUpload()` method returns\. You provide this upload ID for each part\-upload operation\.
+ Copy all of the parts\. For each part that you need to copy, create a new instance of the `CopyPartRequest` class\. Provide the part information, including the source and destination bucket names, source and destination object keys, upload ID, locations of the first and last bytes of the part, and part number\. 
+ Save the responses of the `AmazonS3Client.copyPart()` method calls\. Each response includes the `ETag` value and part number for the uploaded part\. You need this information to complete the multipart upload\. 
+ Call the `AmazonS3Client.completeMultipartUpload()` method to complete the copy operation\. 

**Example**  
The following example shows how to use the Amazon S3 low\-level Java API to perform a multipart copy\. For instructions on creating and testing a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.  

```
import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class LowLevelMultipartCopy {

    public static void main(String[] args) throws IOException {
        Regions clientRegion = Regions.DEFAULT_REGION;
        String sourceBucketName = "*** Source bucket name ***";
        String sourceObjectKey = "*** Source object key ***";
        String destBucketName = "*** Target bucket name ***";
        String destObjectKey = "*** Target object key ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Initiate the multipart upload.
            InitiateMultipartUploadRequest initRequest = new InitiateMultipartUploadRequest(destBucketName, destObjectKey);
            InitiateMultipartUploadResult initResult = s3Client.initiateMultipartUpload(initRequest);

            // Get the object size to track the end of the copy operation.
            GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest(sourceBucketName, sourceObjectKey);
            ObjectMetadata metadataResult = s3Client.getObjectMetadata(metadataRequest);
            long objectSize = metadataResult.getContentLength();

            // Copy the object using 5 MB parts.
            long partSize = 5 * 1024 * 1024;
            long bytePosition = 0;
            int partNum = 1;
            List<CopyPartResult> copyResponses = new ArrayList<CopyPartResult>();
            while (bytePosition < objectSize) {
                // The last part might be smaller than partSize, so check to make sure
                // that lastByte isn't beyond the end of the object.
                long lastByte = Math.min(bytePosition + partSize - 1, objectSize - 1);

                // Copy this part.
                CopyPartRequest copyRequest = new CopyPartRequest()
                        .withSourceBucketName(sourceBucketName)
                        .withSourceKey(sourceObjectKey)
                        .withDestinationBucketName(destBucketName)
                        .withDestinationKey(destObjectKey)
                        .withUploadId(initResult.getUploadId())
                        .withFirstByte(bytePosition)
                        .withLastByte(lastByte)
                        .withPartNumber(partNum++);
                copyResponses.add(s3Client.copyPart(copyRequest));
                bytePosition += partSize;
            }

            // Complete the upload request to concatenate all uploaded parts and make the copied object available.
            CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest(
                    destBucketName,
                    destObjectKey,
                    initResult.getUploadId(),
                    getETags(copyResponses));
            s3Client.completeMultipartUpload(completeRequest);
            System.out.println("Multipart copy complete.");
        } catch (AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        } catch (SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    // This is a helper function to construct a list of ETags.
    private static List<PartETag> getETags(List<CopyPartResult> responses) {
        List<PartETag> etags = new ArrayList<PartETag>();
        for (CopyPartResult response : responses) {
            etags.add(new PartETag(response.getPartNumber(), response.getETag()));
        }
        return etags;
    }
}
```


# Specifying AWS KMS in Amazon S3 using the REST API<a name="KMSUsingRESTAPI"></a>

When you create an object—that is, when you upload a new object or copy an existing object—you can specify the use of server\-side encryption with AWS Key Management Service \(AWS KMS\) customer master keys \(CMKs\) to encrypt your data\. To do this, add the `x-amz-server-side-encryption` header to the request\. Set the value of the header to the encryption algorithm `aws:kms`\. Amazon S3 confirms that your object is stored using SSE\-KMS by returning the response header `x-amz-server-side-encryption`\. 

If you specify the `x-amz-server-side-encryption` header with a value of `aws:kms`, you can also use the following request headers:
+ `x-amz-server-side-encryption-aws-kms-key-id`
+ `x-amz-server-side-encryption-context`

**Topics**
+ [Amazon S3 REST APIs that support SSE\-KMS](#sse-request-headers-kms)
+ [Encryption context \(x\-amz\-server\-side\-encryption\-context\)](#s3-kms-encryption-context)
+ [AWS KMS key ID \(x\-amz\-server\-side\-encryption\-aws\-kms\-key\-id\)](#s3-kms-key-id-api)
+ [S3 Bucket Keys \(x\-amz\-server\-side\-encryption\-aws\-bucket\-key\-enabled\)](#bucket-key-api)

## Amazon S3 REST APIs that support SSE\-KMS<a name="sse-request-headers-kms"></a>

The following REST APIs accept the `x-amz-server-side-encryption`, `x-amz-server-side-encryption-aws-kms-key-id`, and `x-amz-server-side-encryption-context` request headers\.
+ [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html) — When you upload data using the PUT API, you can specify these request headers\. 
+ [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)— When you copy an object, you have both a source object and a target object\. When you pass SSE\-KMS headers with the COPY operation, they are applied only to the target object\. When copying an existing object, regardless of whether the source object is encrypted or not, the destination object is not encrypted unless you explicitly request server\-side encryption\.
+ [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)— When you use a POST operation to upload an object, instead of the request headers, you provide the same information in the form fields\.
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)— When you upload large objects using the multipart upload API, you can specify these headers\. You specify these headers in the initiate request\.

The response headers of the following REST APIs return the `x-amz-server-side-encryption` header when an object is stored using server\-side encryption\.
+ [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)
+ [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)
+ [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)
+ [Upload Part](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html)
+ [Upload Part \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html)
+ [Complete Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html)
+ [Get Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html)
+ [Head Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html)

**Important**  
All GET and PUT requests for an object protected by AWS KMS will fail if you don't make them using Secure Sockets Language \(SSL\) or Signature Version 4\.
If your object uses SSE\-KMS, don't send encryption request headers for `GET` requests and `HEAD` requests, or you’ll get an HTTP 400 BadRequest error\.

## Encryption context \(x\-amz\-server\-side\-encryption\-context\)<a name="s3-kms-encryption-context"></a>

If you specify `x-amz-server-side-encryption:aws:kms`, the Amazon S3 API supports an encryption context with the `x-amz-server-side-encryption-context` header\. An encryption context is an optional set of key\-value pairs that can contain additional contextual information about the data\. For more information about encryption context, see [AWS Key Management Service Concepts \- Encryption Context](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context) in the *AWS Key Management Service Developer Guide*\. 

The encryption context can be any value that you want, provided that the header adheres to the Base64\-encoded JSON format\. However, because the encryption context is not encrypted and because it is logged if AWS CloudTrail logging is turned on, the encryption context should not include sensitive information\. We further recommend that your context describe the data being encrypted or decrypted so that you can better understand the CloudTrail events produced by AWS KMS\.

In Amazon S3, the object or bucket Amazon Resource Name \(ARN\) is commonly used as an encryption context\. If you use SSE\-KMS without enabling an S3 Bucket Key, you use the object ARN as your encryption context, for example, `arn:aws:s3:::object_ARN`\. However, if you use SSE\-KMS and enable an S3 Bucket Key, you use the bucket ARN for your encryption context, for example, `arn:aws:s3:::bucket_ARN`\. For more information about S3 Bucket Keys, see [Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys](bucket-key.md)\.

If the key `aws:s3:arn` is not already in the encryption context, Amazon S3 can append a predefined key of `aws:s3:arn` to the encryption context that you provide\. Amazon S3 appends this predefined key when it processes your requests\. If you use SSE\-KMS without an S3 Bucket Key, the value is equal to the object ARN\. If you use SSE\-KMS with an S3 Bucket Key enabled, the value is equal to the bucket ARN\. 

You can use this predefined key to track relevant requests in CloudTrail\. So you can always see which Amazon S3 ARN was used with which encryption key\. You can use CloudTrail logs to ensure that the encryption context is not identical between different Amazon S3 objects and buckets, which provides additional security\. Your full encryption context will be validated to have the value equal to the object or bucket ARN\. 

## AWS KMS key ID \(x\-amz\-server\-side\-encryption\-aws\-kms\-key\-id\)<a name="s3-kms-key-id-api"></a>

You can use the `x-amz-server-side-encryption-aws-kms-key-id` header to specify the ID of the customer managed CMK used to protect the data\. If you specify `x-amz-server-side-encryption:aws:kms`, but don't provide `x-amz-server-side-encryption-aws-kms-key-id`, Amazon S3 uses the AWS managed CMK in AWS KMS to protect the data\. If you want to use a customer managed AWS KMS CMK, you must provide the `x-amz-server-side-encryption-aws-kms-key-id` of the customer managed CMK\.

**Important**  
When you use an AWS KMS CMK for server\-side encryption in Amazon S3, you must choose a symmetric CMK\. Amazon S3 only supports symmetric CMKs and not asymmetric CMKs\. For more information, see [Using Symmetric and Asymmetric Keys](https://docs.aws.amazon.com/kms/latest/developerguide/symmetric-asymmetric.html) in the *AWS Key Management Service Developer Guide*\.

## S3 Bucket Keys \(x\-amz\-server\-side\-encryption\-aws\-bucket\-key\-enabled\)<a name="bucket-key-api"></a>

You can use the `x-amz-server-side-encryption-aws-bucket-key-enabled` request header to enable or disable an S3 Bucket Key at the object\-level\. S3 Bucket Keys can reduce your AWS KMS request costs by decreasing the request traffic from Amazon S3 to AWS KMS\. For more information, see [Reducing the cost of SSE\-KMS with Amazon S3 Bucket Keys](bucket-key.md)\.

If you specify `x-amz-server-side-encryption:aws:kms`, but don't provide `x-amz-server-side-encryption-aws-bucket-key-enabled`, your object uses the S3 Bucket Key settings for the destination bucket to encrypt your object\. For more information, see [Configuring an S3 Bucket Key at the object level using the REST API, AWS SDKs, or AWS CLI](configuring-bucket-key-object.md)\.


# Policies and Permissions in Amazon S3<a name="access-policy-language-overview"></a>

This page provides an overview of bucket and user policies in Amazon S3 and describes the basic elements of a policy\. Each listed element links to more details about that element and examples of how to use it\. 

For a complete list of Amazon S3 actions, resources, and conditions, see [Actions, resources, and condition keys for Amazon S3](list_amazons3.md)\.

## Policy Language Overview<a name="policy-elements-overview"></a>

In its most basic sense, a policy contains the following elements:
+ [Resources](s3-arn-format.md) – Buckets, objects, access points, and jobs are the Amazon S3 resources for which you can allow or deny permissions\. In a policy, you use the Amazon Resource Name \(ARN\) to identify the resource\. For more information, see [Amazon S3 Resources](s3-arn-format.md)\.
+ [Actions](using-with-s3-actions.md) – For each resource, Amazon S3 supports a set of operations\. You identify resource operations that you will allow \(or deny\) by using action keywords\. 

  For example, the `s3:ListBucket` permission allows the user to use the Amazon S3 [GET Bucket \(List Objects\)](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGET.html) operation\. For more information, see [Amazon S3 Actions](using-with-s3-actions.md)\.
+ [Effect](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_effect.html) – What the effect will be when the user requests the specific action—this can be either *allow* or *deny*\. 

  If you do not explicitly grant access to \(allow\) a resource, access is implicitly denied\. You can also explicitly deny access to a resource\. You might do this to make sure that a user can't access the resource, even if a different policy grants access\. For more information, see [IAM JSON Policy Elements: Effect](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_effect.html)\.
+ [Principal](s3-bucket-user-policy-specifying-principal-intro.md) – The account or user who is allowed access to the actions and resources in the statement\. In a bucket policy, the principal is the user, account, service, or other entity that is the recipient of this permission\. For more information, see [Principals](s3-bucket-user-policy-specifying-principal-intro.md)\.
+ [Condition](amazon-s3-policy-keys.md) – Conditions for when a policy is in effect\. You can use AWS‐wide keys and Amazon S3‐specific keys to specify conditions in an Amazon S3 access policy\. For more information, see [Amazon S3 Condition Keys](amazon-s3-policy-keys.md)\.

The following example bucket policy shows the preceding policy elements\. The policy allows Dave, a user in account *Account\-ID*, `s3:GetObject`, `s3:GetBucketLocation`, and `s3:ListBucket` Amazon S3 permissions on the `awsexamplebucket1` bucket\.

```
{
    "Version": "2012-10-17",
    "Id": "ExamplePolicy01",
    "Statement": [
        {
            "Sid": "ExampleStatement01",
            "Effect": "Allow",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:user/Dave"
            },
            "Action": [
                "s3:GetObject",
                "s3:GetBucketLocation",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::awsexamplebucket1/*",
                "arn:aws:s3:::awsexamplebucket1"
            ]
        }
    ]
}
```

For complete policy language information, see [Policies and Permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html) and [IAM JSON Policy Reference](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies.html) in the *IAM User Guide*\.


# Using the AWS Mobile SDKs for iOS and Android<a name="using-mobile-sdks"></a>

You can use the AWS Mobile SDKs for [Android](https://aws-amplify.github.io/docs/android/storage) and [iOS](https://aws-amplify.github.io/docs/ios/storage) to quickly and easily integrate robust cloud backends into your existing mobile apps\. You can configure and use features like user sign\-in, databases, push notifications, and more, without being an AWS expert\.

The AWS Mobile SDKs provide easy access to Amazon S3 and many other AWS services\. To get started using the AWS Mobile SDKs, see [Getting Started with the AWS Mobile SDKs](https://docs.aws.amazon.com/aws-mobile/latest/developerguide/getting-started.html)\.

## More Info<a name="using-mobile-sdks-moreinfo"></a>

[Using the AWS Amplify JavaScript Library ](using-aws-amplify.md)


# Upload a directory<a name="HLuploadDirDotNet"></a>

You can use the `TransferUtility` class to upload an entire directory\. By default, the API uploads only the files at the root of the specified directory\. You can, however, specify recursively uploading files in all of the subdirectories\. 

To select files in the specified directory based on filtering criteria, specify filtering expressions\. For example, to upload only the \.pdf files from a directory, specify the `"*.pdf"` filter expression\. 

When uploading files from a directory, you don't specify the key names for the resulting objects\. Amazon S3 constructs the key names using the original file path\. For example, assume that you have a directory called `c:\myfolder` with the following structure:

**Example**  

```
1. C:\myfolder
2.       \a.txt
3.       \b.pdf
4.       \media\               
5.              An.mp3
```

When you upload this directory, Amazon S3 uses the following key names:

**Example**  

```
1. a.txt
2. b.pdf
3. media/An.mp3
```

**Example**  
The following C\# example uploads a directory to an Amazon S3 bucket\. It shows how to use various `TransferUtility.UploadDirectory` overloads to upload the directory\. Each successive call to upload replaces the previous upload\. For instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.  

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Transfer;
using System;
using System.IO;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class UploadDirMPUHighLevelAPITest
    {
        private const string existingBucketName = "*** bucket name ***";
        private const string directoryPath = @"*** directory path ***";
        // The example uploads only .txt files.
        private const string wildCard = "*.txt";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            UploadDirAsync().Wait();
        }

        private static async Task UploadDirAsync()
        {
            try
            {
                var directoryTransferUtility =
                    new TransferUtility(s3Client);

                // 1. Upload a directory.
                await directoryTransferUtility.UploadDirectoryAsync(directoryPath,
                    existingBucketName);
                Console.WriteLine("Upload statement 1 completed");

                // 2. Upload only the .txt files from a directory 
                //    and search recursively. 
                await directoryTransferUtility.UploadDirectoryAsync(
                                               directoryPath,
                                               existingBucketName,
                                               wildCard,
                                               SearchOption.AllDirectories);
                Console.WriteLine("Upload statement 2 completed");

                // 3. The same as Step 2 and some optional configuration. 
                //    Search recursively for .txt files to upload.
                var request = new TransferUtilityUploadDirectoryRequest
                {
                    BucketName = existingBucketName,
                    Directory = directoryPath,
                    SearchOption = SearchOption.AllDirectories,
                    SearchPattern = wildCard
                };

                await directoryTransferUtility.UploadDirectoryAsync(request);
                Console.WriteLine("Upload statement 3 completed");
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine(
                        "Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine(
                    "Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}
```


# Hosting a static website using Amazon S3<a name="WebsiteHosting"></a>

You can use Amazon S3 to host a static website\. On a *static* website, individual webpages include static content\. They might also contain client\-side scripts\.

By contrast, a *dynamic* website relies on server\-side processing, including server\-side scripts such as PHP, JSP, or ASP\.NET\. Amazon S3 does not support server\-side scripting, but AWS has other resources for hosting dynamic websites\. To learn more about website hosting on AWS, see [Web Hosting](https://aws.amazon.com/websites/)\. 

**Note**  
You can use the AWS Amplify Console to host a single page web app\. The AWS Amplify Console supports single page apps built with single page app frameworks \(for example, React JS, Vue JS, Angular JS, and Nuxt\) and static site generators \(for example, Gatsby JS, React\-static, Jekyll, and Hugo\)\. For more information, see [Getting Started](https://docs.aws.amazon.com/amplify/latest/userguide/getting-started.html) in the *AWS Amplify Console User Guide*\.

To configure your bucket for static website hosting, you can use the AWS Management Console without writing any code\. You can also create, update, and delete the website configuration *programmatically* by using the AWS SDKs\. The SDKs provide wrapper classes around the Amazon S3 REST API\. If your application requires it, you can send REST API requests directly from your application\.

To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket\. When you configure a bucket as a static website, you must [enable website hosting](EnableWebsiteHosting.md), [set permissions](WebsiteAccessPermissionsReqd.md), and [create and add an index document](IndexDocumentSupport.md)\. Depending on your website requirements, you can also configure [redirects](how-to-page-redirect.md), [web traffic logging](LoggingWebsiteTraffic.md), and a [custom error document](CustomErrorDocSupport.md)\.

After you configure your bucket as a static website, you can access the bucket through the AWS Region\-specific Amazon S3 website endpoints for your bucket\. Website endpoints are different from the endpoints where you send REST API requests\. For more information, see [Website endpoints](WebsiteEndpoints.md)\. Amazon S3 doesn't support HTTPS access for website endpoints\. If you want to use HTTPS, you can use CloudFront to serve a static website hosted on Amazon S3\. For more information, see [Speeding up your website with Amazon CloudFront](website-hosting-cloudfront-walkthrough.md)\.

For more information about hosting a static website on Amazon S3, including instructions and step\-by\-step walkthroughs, see the following topics:

**Topics**
+ [Website endpoints](WebsiteEndpoints.md)
+ [Configuring a bucket as a static website using the AWS Management Console](HowDoIWebsiteConfiguration.md)
+ [Programmatically configuring a bucket as a static website](ManagingBucketWebsiteConfig.md)
+ [Example walkthroughs \- Hosting websites on Amazon S3](hosting-websites-on-s3-examples.md)


# AWS usage report for Amazon S3<a name="aws-usage-report"></a>

For more detail about your Amazon S3 storage usage, download dynamically generated AWS usage reports\. You can choose which usage type, operation, and time period to include\. You can also choose how the data is aggregated\. 

When you download a usage report, you can choose to aggregate usage data by hour, day, or month\. The Amazon S3 usage report lists operations by usage type and AWS Region, for example, the amount of data transferred out of the Asia Pacific \(Sydney\) Region\.

The Amazon S3 usage report includes the following information:
+ **Service** – `Amazon Simple Storage Service`
+ **Operation** – The operation performed on your bucket or object\. For a detailed explanation of Amazon S3 operations, see [Tracking Operations in Your Usage Reports](aws-usage-report-understand.md#aws-usage-report-understand-operations)\.
+ **UsageType** – One of the following values:
  + A code that identifies the type of storage
  + A code that identifies the type of request
  + A code that identifies the type of retrieval
  + A code that identifies the type of data transfer
  + A code that identifies early deletions from INTELLIGENT\_TIERING, STANDARD\_IA, ONEZONE\_IA, S3 Glacier, or S3 Glacier Deep Archive storage
  + `StorageObjectCount` – The count of objects stored within a given bucket

  For a detailed explanation of Amazon S3 usage types, see [Understanding your AWS billing and usage reports for Amazon S3](aws-usage-report-understand.md)\.
+ **Resource** – The name of the bucket associated with the listed usage\.
+ **StartTime** – Start time of the day that the usage applies to, in Coordinated Universal Time \(UTC\)\.
+ **EndTime ** – End time of the day that the usage applies to, in Coordinated Universal Time \(UTC\)\. 
+ **UsageValue** – One of the following volume values:
  + The number of requests during the specified time period
  + The amount of data transferred, in bytes
  + The amount of data stored, in byte\-hours, which is the number of bytes stored in a given hour
  + The amount of data associated with restorations from S3 Glacier Deep Archive, S3 Glacier, STANDARD\_IA, or ONEZONE\_IA storage, in bytes

**Tip**  
For detailed information about every request that Amazon S3 receives for your objects, turn on server access logging for your buckets\. For more information, see [Amazon S3 server access logging](ServerLogs.md)\. 

You can download a usage report as an XML or a comma\-separated values \(CSV\) file\. The following is an example CSV usage report opened in a spreadsheet application\.

![\[Screenshot of a CSV usage report in a spreadsheet application.\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/s3-usage-report.png)

For information on understanding the usage report, see [Understanding your AWS billing and usage reports for Amazon S3](aws-usage-report-understand.md)\.

## Downloading the AWS Usage Report<a name="aws-usage-report-download"></a>

You can download a usage report as an \.xml or a \.csv file\.

**To download the usage report**

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the title bar, choose your AWS Identity and Access Management \(IAM\) user name, and then choose **My Billing Dashboard**\. 

1. In the navigation pane, choose **AWS Cost and Usage Reports**\.

1. In the **Other Reports** section, choose **AWS Usage Report**\.

1. For **Services:**, choose **Amazon Simple Storage Service**\.

1. For **Download Usage Report**, choose the following settings:
   + ****Usage Types **** – For a detailed explanation of Amazon S3 usage types, see [Understanding your AWS billing and usage reports for Amazon S3](aws-usage-report-understand.md)\.
   + ****Operation **** – For a detailed explanation of Amazon S3 operations, see [Tracking Operations in Your Usage Reports](aws-usage-report-understand.md#aws-usage-report-understand-operations)\.
   + ****Time Period **** – The time period that you want the report to cover\. 
   + ****Report Granularity**** – Whether you want the report to include subtotals by the hour, by the day, or by the month\.

1. To choose the format for the report, choose the **Download** for that format, and then follow the prompts to see or save the report\.

## More Info<a name="aws-usage-report-more-info"></a>
+ [Understanding your AWS billing and usage reports for Amazon S3](aws-usage-report-understand.md)
+ [AWS Billing reports for Amazon S3](aws-billing-reports.md)


# Copying objects<a name="CopyingObjectsExamples"></a>

The copy operation creates a copy of an object that is already stored in Amazon S3\. You can create a copy of your object up to 5 GB in a single atomic operation\. However, for copying an object that is greater than 5 GB, you must use the multipart upload API\. Using the `copy` operation, you can:
+ Create additional copies of objects 
+  Rename objects by copying them and deleting the original ones 
+  Move objects across Amazon S3 locations \(e\.g\., us\-west\-1 and Europe\) 
+ Change object metadata

  Each Amazon S3 object has metadata\. It is a set of name\-value pairs\. You can set object metadata at the time you upload it\. After you upload the object, you cannot modify object metadata\. The only way to modify object metadata is to make a copy of the object and set the metadata\. In the copy operation you set the same object as the source and target\. 

Each object has metadata\. Some of it is system metadata and other user\-defined\. Users control some of the system metadata such as storage class configuration to use for the object, and configure server\-side encryption\. When you copy an object, user\-controlled system metadata and user\-defined metadata are also copied\. Amazon S3 resets the system\-controlled metadata\. For example, when you copy an object, Amazon S3 resets the creation date of the copied object\. You don't need to set any of these values in your copy request\. 

When copying an object, you might decide to update some of the metadata values\. For example, if your source object is configured to use standard storage, you might choose to use reduced redundancy storage for the object copy\. You might also decide to alter some of the user\-defined metadata values present on the source object\. Note that if you choose to update any of the object's user\-configurable metadata \(system or user\-defined\) during the copy, then you must explicitly specify all of the user\-configurable metadata present on the source object in your request, even if you are only changing only one of the metadata values\.

For more information about the object metadata, see [Object key and metadata](UsingMetadata.md)\.

**Note**  
Copying objects across locations incurs bandwidth charges\. 
 If the source object is archived in `S3 Glacier` or `S3 Glacier Deep Archive`, you must first restore a temporary copy before you can copy the object to another bucket\. For information about archiving objects, see [Transitioning to the S3 Glacier and S3 Glacier Deep Archive storage classes \(object archival\)](lifecycle-transition-general-considerations.md#before-deciding-to-archive-objects)\. 

When copying objects, you can request Amazon S3 to save the target object encrypted with an AWS Key Management Service \(AWS KMS\) customer master key \(CMK\), an Amazon S3\-managed encryption key, or a customer\-provided encryption key\. Accordingly, you must specify encryption information in your request\. If the copy source is an object that is stored in Amazon S3 using server\-side encryption with customer provided key, you will need to provide encryption information in your request so Amazon S3 can decrypt the object for copying\. For more information, see [Protecting data using encryption](UsingEncryption.md)\.

To copy more than one Amazon S3 object with a single request, you can use Amazon S3 batch operations\. You provide S3 Batch Operations with a list of objects to operate on\. S3 Batch Operations calls the respective API to perform the specified operation\. A single Batch Operations job can perform the specified operation on billions of objects containing exabytes of data\. 

The S3 Batch Operations feature tracks progress, sends notifications, and stores a detailed completion report of all actions, providing a fully managed, auditable, serverless experience\. You can use S3 Batch Operations through the AWS Management Console, AWS CLI, AWS SDKs, or REST API\. For more information, see [S3 Batch Operations basics](batch-ops-basics.md)\.



## Related Resources<a name="RelatedResources015"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# Replicating metadata changes with Amazon S3 replica modification sync<a name="replication-for-metadata-changes"></a>

Amazon S3 replica modification sync can help you keep object metadata such as tags, ACLs, and Object Lock settings replicated between replicas and source objects\. By default, Amazon S3 replicates metadata from the source objects to the replicas only\. When replica modification sync is enabled, Amazon S3 replicates metadata changes made to the replica copies back to the source object, making the replication bidirectional\.

## Enabling replica modification sync<a name="enabling-replication-for-metadata-changes"></a>

You can use Amazon S3 replica modification sync with new or existing replication rules\. You can apply it to an entire S3 bucket or to Amazon S3 objects that have a specific prefix\.

To enable replica modification sync using the Amazon S3 console, see [How do I add a replication rule to an S3 bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html) in the *Amazon Simple Storage Service Console User Guide*\. This topic provides instructions for enabling replica modification sync in your replication configuration when buckets are owned by the same or different AWS accounts\.

To enable replica modification sync using the AWS Command Line Interface \(AWS CLI\), you must add a replication configuration to the bucket containing the replicas with `ReplicaModifications` enabled\. To make replication bidirectional enable replica modification sync on the bucket containing the replicas and the bucket containing the source objects\. 

In the following example configuration, Amazon S3 replicates metadata changes under the prefix *Tax* to the bucket *DOC\-EXAMPLE\-BUCKET*, which would contain the source objects\.

```
{
    "Rules": [
        {
            "Status": "Enabled",
            "Filter": {
                "Prefix": "Tax"
            },
            "SourceSelectionCriteria": {
                "ReplicaModifications":{
                    "Status": "Enabled"
                }
            },
            "Destination": {
                "Bucket": "arn:aws:s3:::DOC-EXAMPLE-BUCKET"
            },
            "Priority": 1
        }
    ],
    "Role": "IAM-Role-ARN"
}
```

For full instructions on creating replication rules using the AWS CLI, see [Example 1: Configuring replication when the source and destination buckets are owned by the same account](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-walkthrough1.html) in the [Replication walkthroughs](https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-example-walkthroughs.html) section\.


# Using the AWS SDK for \.NET for multipart upload \(high\-level API\)<a name="usingHLmpuDotNet"></a>

**Topics**
+ [Upload a file to an S3 bucket using the AWS SDK for \.NET \(high\-level API\)](HLuploadFileDotNet.md)
+ [Upload a directory](HLuploadDirDotNet.md)
+ [Stop multipart uploads to an S3 Bucket using the AWS SDK for \.NET \(high\-level API\)](HLAbortDotNet.md)
+ [Track the progress of a multipart upload to an S3 Bucket using the AWS SDK for \.NET \(high\-level API\)](HLTrackProgressMPUDotNet.md)

The AWS SDK for \.NET exposes a high\-level API that simplifies multipart uploads \(see [Uploading objects using multipart upload API](uploadobjusingmpu.md)\)\. You can upload data from a file, a directory, or a stream\. For more information about Amazon S3 multipart uploads, see [Multipart upload overview](mpuoverview.md)\.

The `TransferUtility` class provides a methods for uploading files and directories, tracking upload progress, and stopping multipart uploads\.


# \(Optional\) Configuring a custom error document<a name="CustomErrorDocSupport"></a>

After you configure your bucket as a static website, when an error occurs, Amazon S3 returns an HTML error document\. You can optionally configure your bucket with a custom error document so that Amazon S3 returns that document when an error occurs\. 

**Note**  
Some browsers display their own error message when an error occurs, ignoring the error document that Amazon S3 returns\. For example, when an HTTP 404 Not Found error occurs, Google Chrome might ignore the error document that Amazon S3 returns and display its own error\.

**Topics**
+ [Amazon S3 HTTP response codes](#s3-http-error-codes)
+ [Configuring a custom error document](#custom-error-document)

## Amazon S3 HTTP response codes<a name="s3-http-error-codes"></a>

The following table lists the subset of HTTP response codes that Amazon S3 returns when an error occurs\. 


| HTTP error code | Description | 
| --- | --- | 
| 301 Moved Permanently | When a user sends a request directly to the Amazon S3 website endpoint \(http://s3\-website\.Region\.amazonaws\.com/\), Amazon S3 returns a 301 Moved Permanently response and redirects those requests to https://aws\.amazon\.com/s3/\. | 
| 302 Found |  When Amazon S3 receives a request for a key `x`, `http://bucket-name.s3-website.Region.amazonaws.com/x`, without a trailing slash, it first looks for the object with the key name `x`\. If the object is not found, Amazon S3 determines that the request is for subfolder `x` and redirects the request by adding a slash at the end, and returns **302 Found**\.   | 
| 304 Not Modified |  Amazon S3 users request headers `If-Modified-Since`, `If-Unmodified-Since`, `If-Match` and/or `If-None-Match` to determine whether the requested object is same as the cached copy held by the client\. If the object is the same, the website endpoint returns a **304 Not Modified** response\.  | 
| 400 Malformed Request |  The website endpoint responds with a **400 Malformed Request** when a user attempts to access a bucket through the incorrect regional endpoint\.   | 
| 403 Forbidden |  The website endpoint responds with a **403 Forbidden** when a user request translates to an object that is not publicly readable\. The object owner must make the object publicly readable using a bucket policy or an ACL\.   | 
| 404 Not Found |  The website endpoint responds with **404 Not Found** for the following reasons: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/CustomErrorDocSupport.html) You can create a custom document that is returned for **404 Not Found**\. Make sure that the document is uploaded to the bucket configured as a website, and that the website hosting configuration is set to use the document\. For information on how Amazon S3 interprets the URL as a request for an object or an index document, see [Configuring an index document](IndexDocumentSupport.md)\.   | 
| 500 Service Error |  The website endpoint responds with a **500 Service Error** when an internal server error occurs\.  | 
| 503 Service Unavailable |  The website endpoint responds with a **503 Service Unavailable** when Amazon S3 determines that you need to reduce your request rate\.   | 

 For each of these errors, Amazon S3 returns a predefined HTML message\. The following is an example HTML message that is returned for a **403 Forbidden** response\.

![\[403 Forbidden error message example\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/images/WebsiteErrorExample403.png)

## Configuring a custom error document<a name="custom-error-document"></a>

When you configure your bucket as a static website, you can optionally provide a custom error document that contains a user\-friendly error message and additional help\. Amazon S3 returns your custom error document for only the HTTP 4XX class of error codes\. 

To configure a custom error document using the S3 console, follow the steps below\. You can also configure an error document using the REST API, the AWS SDKs, the AWS CLI, or AWS CloudFormation\. For more information, see the following:
+ [Configuring a static website using the REST API and AWS SDKs](https://docs.aws.amazon.com/AmazonS3/latest/dev/ManagingBucketWebsiteConfig.html)
+ [AWS::S3::Bucket WebsiteConfiguration](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-websiteconfiguration.html) in the *AWS CloudFormation User Guide*
+ [put\-bucket\-website](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/put-bucket-website.html) in the *AWS CLI Command Reference*

**To configure a custom error document**

1. Sign in to the AWS Management Console and open the Amazon S3 console at [https://console\.aws\.amazon\.com/s3/](https://console.aws.amazon.com/s3/)\.

1. In the **Buckets** list, choose your bucket name\.

1. Choose **Properties**\.

1. Under **Static website hosting**, choose **Edit**\.

   If your bucket is already configured as a static website, you can follow the next step to update or add error document information\. If you have not configured your bucket as a static website, you must first set up the required configuration\. For more information, see [Enabling website hosting](EnableWebsiteHosting.md)\.

1. In the **Error document** box, enter your error document name\.

   The error document name is case sensitive and must exactly match the file name of the HTML error document that you plan to upload to your S3 bucket\. If you don't specify a custom error document and an error occurs, Amazon S3 returns a default HTML error document\.

1. Choose **Save changes**\.

For more information about using the REST API to configure your bucket as a static website with a custom error document, see [PutBucketWebsite](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketWebsite.html) in the *Amazon Simple Storage Service API Reference*\.


# Making requests using IAM user temporary credentials \- AWS SDK for \.NET<a name="AuthUsingTempSessionTokenDotNet"></a>

An IAM user or an AWS account can request temporary security credentials using the AWS SDK for \.NET and use them to access Amazon S3\. These credentials expire after the session duration\. 

By default, the session duration is one hour\. If you use IAM user credentials, you can specify the duration when requesting the temporary security credentials from 15 minutes to the maximum session duration for the role\. For more information about temporary security credentials, see [Temporary Security Credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html) in the *IAM User Guide*\. For more information about making requests, see [Making requests](MakingRequests.md)\.

**To get temporary security credentials and access Amazon S3**

1. Create an instance of the AWS Security Token Service client, `AmazonSecurityTokenServiceClient`\. For information about providing credentials, see [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)\.

1. Start a session by calling the `GetSessionToken` method of the STS client you created in the preceding step\. You provide session information to this method using a `GetSessionTokenRequest` object\. 

   The method returns your temporary security credentials\.

1. Package the temporary security credentials in an instance of the `SessionAWSCredentials` object\. You use this object to provide the temporary security credentials to your Amazon S3 client\.

1. Create an instance of the `AmazonS3Client` class by passing in the temporary security credentials\. You send requests to Amazon S3 using this client\. If you send requests using expired credentials, Amazon S3 returns an error\.

**Note**  
If you obtain temporary security credentials using your AWS account security credentials, those credentials are valid for only one hour\. You can specify a session duration only if you use IAM user credentials to request a session\.

The following C\# example lists object keys in the specified bucket\. For illustration, the example obtains temporary security credentials for a default one\-hour session and uses them to send authenticated request to Amazon S3\. 

If you want to test the sample using IAM user credentials, you need to create an IAM user under your AWS account\. For more information about how to create an IAM user, see [Creating Your First IAM User and Administrators Group](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html) in the *IAM User Guide*\. For more information about making requests, see [Making requests](MakingRequests.md)\.

 For instructions on creating and testing a working example, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

```
using Amazon;
using Amazon.Runtime;
using Amazon.S3;
using Amazon.S3.Model;
using Amazon.SecurityToken;
using Amazon.SecurityToken.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class TempCredExplicitSessionStartTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        public static void Main()
        {
            ListObjectsAsync().Wait();
        }

        private static async Task ListObjectsAsync()
        {
            try
            {
                // Credentials use the default AWS SDK for .NET credential search chain. 
                // On local development machines, this is your default profile.
                Console.WriteLine("Listing objects stored in a bucket");
                SessionAWSCredentials tempCredentials = await GetTemporaryCredentialsAsync();

                // Create a client by providing temporary security credentials.
                using (s3Client = new AmazonS3Client(tempCredentials, bucketRegion))
                {
                    var listObjectRequest = new ListObjectsRequest
                    {
                        BucketName = bucketName
                    };
                    // Send request to Amazon S3.
                    ListObjectsResponse response = await s3Client.ListObjectsAsync(listObjectRequest);
                    List<S3Object> objects = response.S3Objects;
                    Console.WriteLine("Object count = {0}", objects.Count);
                }
            }
            catch (AmazonS3Exception s3Exception)
            {
                Console.WriteLine(s3Exception.Message, s3Exception.InnerException);
            }
            catch (AmazonSecurityTokenServiceException stsException)
            {
                Console.WriteLine(stsException.Message, stsException.InnerException);
            }
        }

        private static async Task<SessionAWSCredentials> GetTemporaryCredentialsAsync()
        {
            using (var stsClient = new AmazonSecurityTokenServiceClient())
            {
                var getSessionTokenRequest = new GetSessionTokenRequest
                {
                    DurationSeconds = 7200 // seconds
                };

                GetSessionTokenResponse sessionTokenResponse =
                              await stsClient.GetSessionTokenAsync(getSessionTokenRequest);

                Credentials credentials = sessionTokenResponse.Credentials;

                var sessionCredentials =
                    new SessionAWSCredentials(credentials.AccessKeyId,
                                              credentials.SecretAccessKey,
                                              credentials.SessionToken);
                return sessionCredentials;
            }
        }
    }
}
```

## Related resources<a name="RelatedResources009"></a>
+ [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)


# What is Amazon S3?<a name="Welcome"></a>

Amazon Simple Storage Service is storage for the Internet\. It is designed to make web\-scale computing easier for developers\.

Amazon S3 has a simple web services interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web\. It gives any developer access to the same highly scalable, reliable, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites\. The service aims to maximize benefits of scale and to pass those benefits on to developers\.

This guide explains the core concepts of Amazon S3, such as buckets, access points, and objects, and how to work with these resources using the Amazon S3 application programming interface \(API\)\.

## How do I\.\.\.?<a name="HowThisGuideIsOrganized"></a>


|  Information  |  Relevant sections  | 
| --- | --- | 
|  General product overview and pricing  |  [Amazon S3](https://aws.amazon.com/s3/)  | 
|  Get a quick hands\-on introduction to Amazon S3  |  [Amazon Simple Storage Service Getting Started Guide](https://docs.aws.amazon.com/AmazonS3/latest/gsg/)  | 
|  Learn about Amazon S3 key terminology and concepts  |  [Introduction to Amazon S3](Introduction.md)  | 
|  How do I work with buckets?  |  [Working with Amazon S3 Buckets](UsingBucket.md)  | 
| How do I work with access points? | [Managing data access with Amazon S3 access points ](access-points.md) | 
|  How do I work with objects?  |  [Working with Amazon S3 objects](UsingObjects.md)  | 
|  How do I make requests?  |  [Making requests](MakingRequests.md)  | 
|  How do I manage access to my resources?  |  [Identity and access management in Amazon S3](s3-access-control.md)  | 


# Using Amazon S3 block public access<a name="access-control-block-public-access"></a>

The Amazon S3 Block Public Access feature provides settings for access points, buckets, and accounts to help you manage public access to Amazon S3 resources\. By default, new buckets, access points, and objects don't allow public access\. However, users can modify bucket policies, access point policies, or object permissions to allow public access\. S3 Block Public Access settings override these policies and permissions so that you can limit public access to these resources\. 

With S3 Block Public Access, account administrators and bucket owners can easily set up centralized controls to limit public access to their Amazon S3 resources that are enforced regardless of how the resources are created\.

When Amazon S3 receives a request to access a bucket or an object, it determines whether the bucket or the bucket owner's account has a block public access setting applied\. If the request was made through an access point, Amazon S3 also checks for block public access settings for the access point\. If there is an existing block public access setting that prohibits the requested access, Amazon S3 rejects the request\. 

Amazon S3 Block Public Access provides four settings\. These settings are independent and can be used in any combination\. Each setting can be applied to an access point, a bucket, or an entire AWS account\. If the block public access settings for the access point, bucket, or account differ, then Amazon S3 applies the most restrictive combination of the access point, bucket, and account settings\. 

When Amazon S3 evaluates whether an operation is prohibited by a block public access setting, it rejects any request that violates an access point, bucket, or account setting\.

**Warning**  
Public access is granted to buckets and objects through access control lists \(ACLs\), access point policies, bucket policies, or all\. To help ensure that all of your Amazon S3 access points, buckets, and objects have their public access blocked, we recommend that you turn on all four settings for block public access for your account\. These settings block public access for all current and future buckets and access points\.   
Before applying these settings, verify that your applications will work correctly without public access\. If you require some level of public access to your buckets or objects, for example to host a static website as described at [Hosting a static website using Amazon S3](WebsiteHosting.md), you can customize the individual settings to suit your storage use cases\.

**Note**  
You can enable block public access settings only for access points, buckets, and AWS accounts\. Amazon S3 doesn't support block public access settings on a per\-object basis\.
When you apply block public access settings to an account, the settings apply to all AWS Regions globally\. The settings might not take effect in all Regions immediately or simultaneously, but they eventually propagate to all Regions\.

**Topics**
+ [Enable block public access on the Amazon S3 console](#console-block-public-access-options)
+ [Block public access settings](#access-control-block-public-access-options)
+ [The meaning of "public"](#access-control-block-public-access-policy-status)
+ [Using Access Analyzer for S3 to review public buckets](#access-analyzer-public-info)
+ [Permissions](#access-control-block-public-access-permissions)
+ [Examples](#access-control-block-public-access-examples)

## Enable block public access on the Amazon S3 console<a name="console-block-public-access-options"></a>

Amazon S3 Block Public Access provides four settings\. You can apply these settings in any combination to individual access points, buckets, or entire AWS accounts\. For more information, see [Setting Permissions: Block Public Access](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/block-public-access.html) in the *Amazon Simple Storage Service Console User Guide*\. 

## Block public access settings<a name="access-control-block-public-access-options"></a>

S3 Block Public Access provides four settings\. You can apply these settings in any combination to individual access points, buckets, or entire AWS accounts\. If you apply a setting to an account, it applies to all buckets and access points that are owned by that account\. Similarly, if you apply a setting to a bucket, it applies to all access points associated with that bucket\.

The following table contains the available settings\.


| Name | Description | 
| --- | --- | 
| BlockPublicAcls |  Setting this option to `TRUE` causes the following behavior: [\[See the AWS documentation website for more details\]](http://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html) When this setting is set to `TRUE`, the specified operations fail \(whether made through the REST API, AWS CLI, or AWS SDKs\)\. However, existing policies and ACLs for buckets and objects are not modified\. This setting enables you to protect against public access while allowing you to audit, refine, or otherwise alter the existing policies and ACLs for your buckets and objects\.  Access points don't have ACLs associated with them\. If you apply this setting to an access point, it acts as a passthrough to the underlying bucket\. If an access point has this setting enabled, requests made through the access point behave as though the underlying bucket has this setting enabled, regardless of whether the bucket actually has this setting enabled\.   | 
| IgnorePublicAcls |  Setting this option to `TRUE` causes Amazon S3 to ignore all public ACLs on a bucket and any objects that it contains\. This setting enables you to safely block public access granted by ACLs while still allowing PUT Object calls that include a public ACL \(as opposed to `BlockPublicAcls`, which rejects PUT Object calls that include a public ACL\)\. Enabling this setting doesn't affect the persistence of any existing ACLs and doesn't prevent new public ACLs from being set\.  Access points don't have ACLs associated with them\. If you apply this setting to an access point, it acts as a passthrough to the underlying bucket\. If an access point has this setting enabled, requests made through the access point behave as though the underlying bucket has this setting enabled, regardless of whether the bucket actually has this setting enabled\.   | 
| BlockPublicPolicy |  Setting this option to `TRUE` for a bucket causes Amazon S3 to reject calls to PUT Bucket policy if the specified bucket policy allows public access, and to reject calls to PUT Access Point policy for all of the bucket's access points if the specified policy allows public access\. Setting this option to `TRUE` for an access point causes Amazon S3 to reject calls to PUT Access Point policy and PUT Bucket policy that are made through the access point if the specified policy \(for either the access point or the underlying bucket\) is public\. This setting enables you to allow users to manage access point and bucket policies without allowing them to publicly share the bucket or the objects it contains\. Enabling this setting doesn't affect existing access point or bucket policies\.  To use this setting effectively, you should apply it at the *account* level\. A bucket policy can allow users to alter a bucket's block public access settings\. Therefore, users who have permission to change a bucket policy could insert a policy that allows them to disable the block public access settings for the bucket\. If this setting is enabled for the entire account, rather than for a specific bucket, Amazon S3 blocks public policies even if a user alters the bucket policy to disable this setting\.   | 
| RestrictPublicBuckets |  Setting this option to `TRUE` restricts access to an access point or bucket with a public policy to only AWS service principals and authorized users within the bucket owner's account\. This setting blocks all cross\-account access to the access point or bucket \(except by AWS service principals\), while still allowing users within the account to manage the access point or bucket\. Enabling this setting doesn't affect existing access point or bucket policies, except that Amazon S3 blocks public and cross\-account access derived from any public access point or bucket policy, including non\-public delegation to specific accounts\.  | 

**Important**  
Calls to GET Bucket acl and GET Object acl always return the effective permissions in place for the specified bucket or object\. For example, suppose that a bucket has an ACL that grants public access, but the bucket also has the `IgnorePublicAcls` setting enabled\. In this case, GET Bucket acl returns an ACL that reflects the access permissions that Amazon S3 is enforcing, rather than the actual ACL that is associated with the bucket\.
Block public access settings don't alter existing policies or ACLs\. Therefore, removing a block public access setting causes a bucket or object with a public policy or ACL to again be publicly accessible\. 

## The meaning of "public"<a name="access-control-block-public-access-policy-status"></a>

### Buckets<a name="access-control-block-public-access-policy-status-buckets"></a>
+ **ACLs**
  + Amazon S3 considers a bucket or object ACL public if it grants any permissions to members of the predefined `AllUsers` or `AuthenticatedUsers` groups\. For more information about predefined groups, see [Amazon S3 Predefined Groups](acl-overview.md#specifying-grantee-predefined-groups)\.
+ **Policies**
  + When evaluating a bucket policy, Amazon S3 begins by assuming that the policy is public\. It then evaluates the policy to determine whether it qualifies as non\-public\. To be considered non\-public, a bucket policy must grant access only to fixed values \(values that don't contain a wildcard\) of one or more of the following:
    + A set of Classless Inter\-Domain Routings \(CIDRs\), using `aws:SourceIp`\. For more information about CIDR, see [RFC 4632](http://www.rfc-editor.org/rfc/rfc4632.txt) on the RFC Editor website\.
    + An AWS principal, user, role, or service principal \(e\.g\. `aws:PrincipalOrgID`\)
    + `aws:SourceArn`
    + `aws:SourceVpc`
    + `aws:SourceVpce`
    + `aws:SourceOwner`
    + `aws:SourceAccount`
    + `s3:x-amz-server-side-encryption-aws-kms-key-id`
    + `aws:userid`, outside the pattern "`AROLEID:*`"
    + `s3:DataAccessPointArn`
**Note**  
When used in a bucket policy, this value can contain a wildcard for the access point name without rendering the policy public, as long as the account id is fixed\. For example, allowing access to `arn:aws:s3:us-west-2:123456789012:accesspoint/*` would permit access to any access point associated with account `123456789012` in Region `us-west-2`, without rendering the bucket policy public\. Note that this behavior is different for access point policies\. For more information, see [Access points](#access-control-block-public-access-policy-status-access-points)\.
    + `s3:DataAccessPointAccount`
  + Under these rules, the following example policies are considered public\.

    ```
    { 
    		"Principal": { "Federated": "graph.facebook.com" }, 
    		"Resource": "*", 
    		"Action": "s3:PutObject", 
    		"Effect": "Allow"
    	}
    ```

    ```
    {
    		"Principal": "*", 
    		"Resource": "*", 
    		"Action": "s3:PutObject", 
    		"Effect": "Allow" 
    	}
    ```

    ```
    {
    		"Principal": "*", 
    		"Resource": "*", 
    		"Action": "s3:PutObject", 
    		"Effect": "Allow", 
    		"Condition": { "StringLike": {"aws:SourceVpc": "vpc-*"}}
    	}
    ```

    You can make these policies non\-public by including any of the condition keys listed previously, using a fixed value\. For example, you can make the last policy preceding non\-public by setting `aws:SourceVpc` to a fixed value, like the following\.

    ```
    {
    		"Principal": "*", 
    		"Resource": "*", 
    		"Action": "s3:PutObject", 
    		"Effect": "Allow", 
    		"Condition": {"StringEquals": {"aws:SourceVpc": "vpc-91237329"}}
    	}
    ```
  + For more information about bucket policies, see [Using Bucket Policies and User Policies](using-iam-policies.md)\.

#### Example<a name="access-control-block-public-access-policy-example"></a>

This example shows how Amazon S3 evaluates a bucket policy that contains both public and non\-public access grants\.

Suppose that a bucket has a policy that grants access to a set of fixed principals\. Under the previously described rules, this policy isn't public\. Thus, if you enable the `RestrictPublicBuckets` setting, the policy remains in effect as written, because `RestrictPublicBuckets` only applies to buckets that have public policies\. However, if you add a public statement to the policy, `RestrictPublicBuckets` takes effect on the bucket\. It allows only AWS service principals and authorized users of the bucket owner's account to access the bucket\.

As an example, suppose that a bucket owned by "Account\-1" has a policy that contains the following:

1. A statement that grants access to AWS CloudTrail \(which is an AWS service principal\)

1. A statement that grants access to account "Account\-2"

1. A statement that grants access to the public, for example by specifying `"Principal": "*"` with no limiting `Condition`

This policy qualifies as public because of the third statement\. With this policy in place and `RestrictPublicBuckets` enabled, Amazon S3 allows access only by CloudTrail\. Even though statement 2 isn't public, Amazon S3 disables access by "Account\-2\." This is because statement 3 renders the entire policy public, so `RestrictPublicBuckets` applies\. As a result, Amazon S3 disables cross\-account access, even though the policy delegates access to a specific account, "Account\-2\." But if you remove statement 3 from the policy, then the policy doesn't qualify as public, and `RestrictPublicBuckets` no longer applies\. Thus, "Account\-2" regains access to the bucket, even if you leave `RestrictPublicBuckets` enabled\.

### Access points<a name="access-control-block-public-access-policy-status-access-points"></a>

Amazon S3 evaluates block public access settings slightly differently for access points compared to buckets\. The rules that Amazon S3 applies to determine when an access point policy is public are generally the same for access points as for buckets, except in the following situations:
+ An access point that has a VPC network origin is always considered non\-public, regardless of the contents of its access point policy\.
+ An access point policy that grants access to a set of access points using `s3:DataAccessPointArn` is considered public\. Note that this behavior is different than for bucket policies\. For example, a bucket policy that grants access to values of `s3:DataAccessPointArn` that match `arn:aws:s3:us-west-2:123456789012:accesspoint/*` is not considered public\. However, the same statement in an access point policy would render the access point public\.

## Using Access Analyzer for S3 to review public buckets<a name="access-analyzer-public-info"></a>

You can use Access Analyzer for S3 to review buckets with bucket ACLs, bucket policies, or access point policies that grant public access\. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization\. For each public or shared bucket, you receive findings that report the source and level of public or shared access\. 

Armed with the knowledge presented in the findings, you can take immediate and precise corrective action\. In Access Analyzer for S3, you can block all public access to a bucket with a single click\. You can also drill down into bucket\-level permission settings to configure granular levels of access\. For specific and verified use cases that require public or shared access, you can acknowledge and record your intent for the bucket to remain public or shared by archiving the findings for the bucket\.

In rare events, Access Analyzer for S3 might report no findings for a bucket that an Amazon S3 block public access evaluation reports as public\. This happens because Amazon S3 block public access reviews policies for current actions and any potential actions that might be added in the future, leading to a bucket becoming public\. On the other hand, Access Analyzer for S3 only analyzes the current actions specified for the Amazon S3 service in the evaluation of access status\.

For more information about Access Analyzer for S3, see [Using Access Analyzer for S3](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html) in the *Amazon Simple Storage Service Console User Guide*\.

## Permissions<a name="access-control-block-public-access-permissions"></a>

To use Amazon S3 Block Public Access features, you must have the following permissions\.


| Operation | Required permissions | 
| --- | --- | 
| GET bucket policy status | s3:GetBucketPolicyStatus | 
| GET bucket Block Public Access settings | s3:GetBucketPublicAccessBlock | 
| PUT bucket Block Public Access settings | s3:PutBucketPublicAccessBlock | 
| DELETE bucket Block Public Access settings | s3:PutBucketPublicAccessBlock | 
| GET account Block Public Access settings | s3:GetAccountPublicAccessBlock | 
| PUT account Block Public Access settings | s3:PutAccountPublicAccessBlock | 
| DELETE account Block Public Access settings | s3:PutAccountPublicAccessBlock | 
| PUT access point Block Public Access settings | s3:PutAccessPointPublicAccessBlock | 

**Note**  
The DELETE operations require the same permissions as the PUT operations\. There are no separate permissions for the DELETE operations\.

## Examples<a name="access-control-block-public-access-examples"></a>

### Using block public access with the AWS CLI<a name="access-control-block-public-access-examples-cli"></a>

You can use Amazon S3 Block Public Access through the AWS CLI\. The command you use depends on whether you want to perform a block public access call on an access point, bucket, or account\. For more information about setting up and using the AWS CLI, see [What is the AWS Command Line Interface?](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html)
+ **Access point**
  + To perform block public access operations on an access point, use the AWS CLI service `s3control`\. Note that it isn't currently possible to change an access point's block public access settings after creating the access point\. Thus, the only way to specify block public access settings for an access point is by including them when creating the access point\.
+ **Bucket**
  + To perform block public access operations on a bucket, use the AWS CLI service `s3api`\. The bucket\-level operations that use this service are as follows:
    + PUT PublicAccessBlock \(for a bucket\)
    + GET PublicAccessBlock \(for a bucket\)
    + DELETE PublicAccessBlock \(for a bucket\)
    + GET BucketPolicyStatus
+ **Account**
  + To perform block public access operations on an account, use the AWS CLI service `s3control`\. The account\-level operations that use this service are as follows:
    + PUT PublicAccessBlock \(for an account\)
    + GET PublicAccessBlock \(for an account\)
    + DELETE PublicAccessBlock \(for an account\)

### Using block public access with the AWS SDK for Java<a name="access-control-block-public-access-examples-java"></a>

The following examples show how to use Amazon S3 Block Public Access with the AWS SDK for Java\. For instructions on how to create and test a working sample, see [Using the AWS SDK for Java](UsingTheMPJavaAPI.md)\.

#### Example 1<a name="access-control-block-public-access-examples-java-1"></a>

This example shows how to set a public access block configuration on an S3 bucket using the AWS SDK for Java\.

```
AmazonS3 client = AmazonS3ClientBuilder.standard()
	  .withCredentials(<credentials>)
	  .build();

client.setPublicAccessBlock(new SetPublicAccessBlockRequest()
		.withBucketName(<bucket-name>)
		.withPublicAccessBlockConfiguration(new PublicAccessBlockConfiguration()
				.withBlockPublicAcls(<value>)
				.withIgnorePublicAcls(<value>)
				.withBlockPublicPolicy(<value>)
				.withRestrictPublicBuckets(<value>)));
```

**Important**  
This example pertains only to bucket\-level operations, which use the `AmazonS3` client class\. For account\-level operations, see the following example\.

#### Example 2<a name="access-control-block-public-access-examples-java-2"></a>

This example shows how to put a public access block configuration on an Amazon S3 account using the AWS SDK for Java\.

```
AWSS3ControlClientBuilder controlClientBuilder = AWSS3ControlClientBuilder.standard();
controlClientBuilder.setRegion(<region>);
controlClientBuilder.setCredentials(<credentials>);
					
AWSS3Control client = controlClientBuilder.build();
client.putPublicAccessBlock(new PutPublicAccessBlockRequest()
		.withAccountId(<account-id>)
		.withPublicAccessBlockConfiguration(new PublicAccessBlockConfiguration()
				.withIgnorePublicAcls(<value>)
				.withBlockPublicAcls(<value>)
				.withBlockPublicPolicy(<value>)
				.withRestrictPublicBuckets(<value>)));
```

**Important**  
This example pertains only to account\-level operations, which use the `AWSS3Control` client class\. For bucket\-level operations, see the preceding example\.

### Using block public access with other AWS SDKs<a name="access-control-block-public-access-examples-other-sdk"></a>

For information about using the other AWS SDKs, see [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)\.

### Using block public access with the REST APIs<a name="access-control-block-public-access-examples-api"></a>

For information about using Amazon S3 Block Public Access through the REST APIs, see the following topics in the *Amazon Simple Storage Service API Reference*\.
+ Account\-level operations
  + [PUT PublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTAccountPUTPublicAccessBlock.html)
  + [GET PublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTAccountGETPublicAccessBlock.html)
  + [DELETE PublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTAccountDELETEPublicAccessBlock.html)
+ Bucket\-level operations
  + [PUT PublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTPublicAccessBlock.html)
  + [GET PublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETPublicAccessBlock.html)
  + [DELETE PublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEPublicAccessBlock.html)
  + [GET BucketPolicyStatus](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketGETPolicyStatus.html)


# AWS glossary<a name="glossary"></a>

For the latest AWS terminology, see the [AWS glossary](https://docs.aws.amazon.com/general/latest/gr/glos-chap.html) in the *AWS General Reference*\.


# Replication walkthroughs<a name="replication-example-walkthroughs"></a>

The following examples show how to configure replication for common use cases\. The examples demonstrate replication configuration using the Amazon S3 console, AWS Command Line Interface \(AWS CLI\), and AWS SDKs \(Java and \.NET SDK examples are shown\)\. For information about installing and configuring the AWS CLI, see the following topics in the *AWS Command Line Interface User Guide*\.
+  [Installing the AWS Command Line Interface](https://docs.aws.amazon.com/cli/latest/userguide/installing.html) 
+  [Configuring the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html) \- You must set up at least one profile\. If you are exploring cross\-account scenarios, set up two profiles\.

For information about AWS SDKs, see [AWS SDK for Java](https://aws.amazon.com/sdk-for-java/) and [AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)\.



**Topics**
+ [Example 1: Configuring for buckets in the same account](replication-walkthrough1.md)
+ [Example 2: Configuring for buckets in different accounts](replication-walkthrough-2.md)
+ [Example 3: Changing replica owner](replication-walkthrough-3.md)
+ [Example 4: Replicating encrypted objects](replication-walkthrough-4.md)
+ [Example 5: S3 Replication Time Control \(S3 RTC\)](replication-walkthrough-5.md)


# Stop multipart uploads to an S3 Bucket using the AWS SDK for \.NET \(low\-level\)<a name="LLAbortMPUnet"></a>

You can stop an in\-progress multipart upload by calling the `AmazonS3Client.AbortMultipartUploadAsync` method\. In addition to stopping the upload, this method deletes all parts that were uploaded to Amazon S3\. 

To stop a multipart upload, you provide the upload ID, and the bucket and key names that are used in the upload\. After you have stopped a multipart upload, you can't use the upload ID to upload additional parts\. For more information about Amazon S3 multipart uploads, see [Multipart upload overview](mpuoverview.md)\.

The following C\# example shows how to stop a multipart upload\. For a complete C\# sample that includes the following code, see [Upload a file to an S3 Bucket using the AWS SDK for \.NET \(low\-level API\)](LLuploadFileDotNet.md)\.

```
AbortMultipartUploadRequest abortMPURequest = new AbortMultipartUploadRequest
{
    BucketName = existingBucketName,
    Key = keyName,
    UploadId = initResponse.UploadId
};
await AmazonS3Client.AbortMultipartUploadAsync(abortMPURequest);
```

You can also stop all in\-progress multipart uploads that were initiated prior to a specific time\. This clean\-up operation is useful for stopping multipart uploads that didn't complete or were stopped\. For more information, see [Stop multipart uploads to an S3 Bucket using the AWS SDK for \.NET \(high\-level API\)](HLAbortDotNet.md)\.

## More info<a name="LLAbortMPUnet-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# Generate a presigned object URL using AWS explorer for Visual Studio<a name="ShareObjectPreSignedURLVSExplorer"></a>

If you are using Visual Studio, you can generate a presigned URL for an object without writing any code by using AWS Explorer for Visual Studio\. Anyone with this URL can download the object\. For more information, go to [Using Amazon S3 from AWS Explorer](https://docs.aws.amazon.com/AWSToolkitVS/latest/UserGuide/using-s3.html)\. 

For instructions about how to install the AWS Explorer, see [Using the AWS SDKs, CLI, and Explorers](UsingAWSSDK.md)\.


# Specifying Server\-Side Encryption Using the REST API<a name="SSEUsingRESTAPI"></a>

At the time of object creation—that is, when you are uploading a new object or making a copy of an existing object—you can specify if you want Amazon S3 to encrypt your data by adding the `x-amz-server-side-encryption` header to the request\. Set the value of the header to the encryption algorithm `AES256` that Amazon S3 supports\. Amazon S3 confirms that your object is stored using server\-side encryption by returning the response header `x-amz-server-side-encryption`\. 

The following REST upload APIs accept the `x-amz-server-side-encryption` request header\.
+ [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)
+ [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)
+ [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)

When uploading large objects using the multipart upload API, you can specify server\-side encryption by adding the `x-amz-server-side-encryption` header to the Initiate Multipart Upload request\. When you are copying an existing object, regardless of whether the source object is encrypted or not, the destination object is not encrypted unless you explicitly request server\-side encryption\.

The response headers of the following REST APIs return the `x-amz-server-side-encryption` header when an object is stored using server\-side encryption\. 
+ [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)
+ [PUT Object \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html)
+ [POST Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPOST.html)
+ [Initiate Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadInitiate.html)
+ [Upload Part](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPart.html)
+ [Upload Part \- Copy](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadUploadPartCopy.html)
+ [Complete Multipart Upload](https://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html)
+ [Get Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html)
+ [Head Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectHEAD.html)

**Note**  
Encryption request headers should not be sent for `GET` requests and `HEAD` requests if your object uses SSE\-S3 or you’ll get an HTTP 400 BadRequest error\.


# Uploading an object in a single operation<a name="UploadInSingleOp"></a>

**Topics**
+ [Upload an object Using the AWS SDK for Java](UploadObjSingleOpJava.md)
+ [Upload an object using the AWS SDK for \.NET](UploadObjSingleOpNET.md)
+ [Upload an object using the AWS SDK for C\+\+](UploadObjSingleCpp.md)
+ [Upload an object using the AWS SDK for PHP](UploadObjSingleOpPHP.md)
+ [Upload an object using the AWS SDK for Ruby](UploadObjSingleOpRuby.md)
+ [Upload an object using the REST API](UploadObjSingleOpREST.md)
+ [Upload an object using the CLI](UploadObjSingleOpCLI.md)

You can use the AWS SDK to upload objects\. The SDK provides wrapper libraries for you to upload data easily\. However, if your application requires it, you can use the REST API directly in your application\. You can also use the AWS Command Line Interface\.


# Examples of enabling bucket versioning<a name="manage-versioning-examples"></a>

**Topics**
+ [Using the Amazon S3 console](#manage-versioning-examples-console)
+ [Using the AWS CLI](#manage-versioning-examples-cli)
+ [Using the AWS SDK for Java](#manage-versioning-examples-java)
+ [Using the AWS SDK for \.NET](#manage-versioning-examples-dotnet)
+ [Using other AWS SDKs](#manage-versioning-examples-sdks)

 This section provides examples of enabling versioning on a bucket\. The examples first enable versioning on a bucket and then retrieve versioning status\. For an introduction, see [Using versioning](Versioning.md)\.

## Using the Amazon S3 console<a name="manage-versioning-examples-console"></a>

For more information about enabling versioning on a bucket using the Amazon S3 console, see [ How Do I Enable or Suspend Versioning for an S3 Bucket?](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-versioning.html) in the *Amazon Simple Storage Service Console User Guide*\.

## Using the AWS CLI<a name="manage-versioning-examples-cli"></a>

**Example: Enable versioning on a bucket**

```
aws s3api put-bucket-versioning --bucket DOC-EXAMPLE-BUCKET1 --versioning-configuration Status=Enabled
```

**Example: Enable versioning and multi\-factor authentication \(MFA\) delete on a bucket**

```
aws s3api put-bucket-versioning --bucket DOC-EXAMPLE-BUCKET1 --versioning-configuration Status=Enabled,MFADelete=Enabled --mfa "SERIAL 123456"
```

**Note**  
Using MFA delete requires an approved physical or virtual authentication device\. For more information about using MFA Delete on Amazon S3, see [MFA delete](Versioning.md#MultiFactorAuthenticationDelete)\.

For more information about enabling versioning using the AWS CLI, see [put\-bucket\-versioining](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/put-bucket-versioning.html) in the *AWS CLI Command Reference*\.

## Using the AWS SDK for Java<a name="manage-versioning-examples-java"></a>

**Example**  
For instructions on how to create and test a working sample, see [Testing the Amazon S3 Java Code Examples](UsingTheMPJavaAPI.md#TestingJavaSamples)\.   

```
import java.io.IOException;

import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.Region;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3Client;
import com.amazonaws.services.s3.model.AmazonS3Exception;
import com.amazonaws.services.s3.model.BucketVersioningConfiguration;
import com.amazonaws.services.s3.model.SetBucketVersioningConfigurationRequest;

public class BucketVersioningConfigurationExample {
    public static String bucketName = "*** bucket name ***"; 
    public static AmazonS3Client s3Client;

    public static void main(String[] args) throws IOException {
        s3Client = new AmazonS3Client(new ProfileCredentialsProvider());
        s3Client.setRegion(Region.getRegion(Regions.US_EAST_1));
        try {

            // 1. Enable versioning on the bucket.
        	BucketVersioningConfiguration configuration = 
        			new BucketVersioningConfiguration().withStatus("Enabled");
            
			SetBucketVersioningConfigurationRequest setBucketVersioningConfigurationRequest = 
					new SetBucketVersioningConfigurationRequest(bucketName,configuration);
			
			s3Client.setBucketVersioningConfiguration(setBucketVersioningConfigurationRequest);
			
			// 2. Get bucket versioning configuration information.
			BucketVersioningConfiguration conf = s3Client.getBucketVersioningConfiguration(bucketName);
			 System.out.println("bucket versioning configuration status:    " + conf.getStatus());

        } catch (AmazonS3Exception amazonS3Exception) {
            System.out.format("An Amazon S3 error occurred. Exception: %s", amazonS3Exception.toString());
        } catch (Exception ex) {
            System.out.format("Exception: %s", ex.toString());
        }        
    }
}
```

## Using the AWS SDK for \.NET<a name="manage-versioning-examples-dotnet"></a>

For information about how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\. 

**Example**  

```
using System;
using Amazon.S3;
using Amazon.S3.Model;

namespace s3.amazon.com.docsamples
{
    class BucketVersioningConfiguration
    {
        static string bucketName = "*** bucket name ***";

        public static void Main(string[] args)
        {
            using (var client = new AmazonS3Client(Amazon.RegionEndpoint.USEast1))
            {
                try
                {
                    EnableVersioningOnBucket(client);
                    string bucketVersioningStatus = RetrieveBucketVersioningConfiguration(client);
                }
                catch (AmazonS3Exception amazonS3Exception)
                {
                    if (amazonS3Exception.ErrorCode != null &&
                        (amazonS3Exception.ErrorCode.Equals("InvalidAccessKeyId")
                        ||
                        amazonS3Exception.ErrorCode.Equals("InvalidSecurity")))
                    {
                        Console.WriteLine("Check the provided AWS Credentials.");
                        Console.WriteLine(
                        "To sign up for service, go to http://aws.amazon.com/s3");
                    }
                    else
                    {
                        Console.WriteLine(
                         "Error occurred. Message:'{0}' when listing objects",
                         amazonS3Exception.Message);
                    }
                }
            }

            Console.WriteLine("Press any key to continue...");
            Console.ReadKey();
        }

        static void EnableVersioningOnBucket(IAmazonS3 client)
        {

                PutBucketVersioningRequest request = new PutBucketVersioningRequest
                {
                    BucketName = bucketName,
                    VersioningConfig = new S3BucketVersioningConfig 
                    {
                        Status = VersionStatus.Enabled
                    }
                };

                PutBucketVersioningResponse response = client.PutBucketVersioning(request);
        }


        static string RetrieveBucketVersioningConfiguration(IAmazonS3 client)
        {
                GetBucketVersioningRequest request = new GetBucketVersioningRequest
                {
                    BucketName = bucketName
                };
 
                GetBucketVersioningResponse response = client.GetBucketVersioning(request);
                return response.VersioningConfig.Status;
            }
    }
}
```

## Using other AWS SDKs<a name="manage-versioning-examples-sdks"></a>

For information about using other AWS SDKs, see [Sample Code and Libraries](https://aws.amazon.com/code/)\. 


# Making requests using federated user temporary credentials \- AWS SDK for Ruby<a name="AuthUsingTempFederationTokenRuby"></a>

You can provide temporary security credentials for your federated users and applications so that they can send authenticated requests to access your AWS resources\. When requesting temporary credentials from the IAM service, you must provide a user name and an IAM policy that describes the resource permissions that you want to grant\. By default, the session duration is one hour\. However, if you are requesting temporary credentials using IAM user credentials, you can explicitly set a different duration value when requesting the temporary security credentials for federated users and applications\. For information about temporary security credentials for your federated users and applications, see [Making requests](MakingRequests.md)\.

**Note**  
For added security when you request temporary security credentials for federated users and applications, you might want to use a dedicated IAM user with only the necessary access permissions\. The temporary user you create can never get more permissions than the IAM user who requested the temporary security credentials\. For more information, see [ AWS Identity and Access Management FAQs ](https://aws.amazon.com/iam/faqs/#What_are_the_best_practices_for_using_temporary_security_credentials)\.

**Example**  
The following Ruby code example allows a federated user with a limited set of permissions to lists keys in the specified bucket\.   

```
require 'aws-sdk-s3'
require 'aws-sdk-iam'
require 'json'

# Checks to see whether a user exists in IAM; otherwise,
# creates the user.
#
# @param iam [Aws::IAM::Client] An initialized IAM client.
# @param user_name [String] The user's name.
# @return [Aws::IAM::Types::User] The existing or new user.
# @example
#   iam = Aws::IAM::Client.new(region: 'us-east-1')
#   user = get_user(iam, 'my-user')
#   exit 1 unless user.user_name
#   puts "User's name: #{user.user_name}"
def get_user(iam, user_name)
  puts "Checking for a user with the name '#{user_name}'..."
  response = iam.get_user(user_name: user_name)
  puts "A user with the name '#{user_name}' already exists."
  return response.user
# If the user doesn't exist, create them.
rescue Aws::IAM::Errors::NoSuchEntity
  puts "A user with the name '#{user_name}' doesn't exist. Creating this user..."
  response = iam.create_user(user_name: user_name)
  iam.wait_until(:user_exists, user_name: user_name)
  puts "Created user with the name '#{user_name}'."
  return response.user
rescue StandardError => e
  puts "Error while accessing or creating the user named '#{user_name}': #{e.message}"
end

# Gets temporary AWS credentials for an IAM user with the specified permissions.
#
# @param sts [Aws::STS::Client] An initialized AWS STS client.
# @param duration_seconds [Integer] The number of seconds for valid credentials.
# @param user_name [String] The user's name.
# @param policy [Hash] The access policy.
# @return [Aws::STS::Types::Credentials] AWS credentials for API authentication.
# @example
#   sts = Aws::STS::Client.new(region: 'us-east-1')
#   credentials = get_temporary_credentials(sts, duration_seconds, user_name, 
#     {
#       'Version' => '2012-10-17',
#       'Statement' => [
#         'Sid' => 'Stmt1',
#         'Effect' => 'Allow',
#         'Action' => 's3:ListBucket',
#         'Resource' => 'arn:aws:s3:::doc-example-bucket'
#       ]
#     }
#   )
#   exit 1 unless credentials.access_key_id
#   puts "Access key ID: #{credentials.access_key_id}"
def get_temporary_credentials(sts, duration_seconds, user_name, policy)
  response = sts.get_federation_token(
    duration_seconds: duration_seconds,
    name: user_name,
    policy: policy.to_json
  )
  return response.credentials
rescue StandardError => e
  puts "Error while getting federation token: #{e.message}"
end

# Lists the keys and ETags for the objects in an Amazon S3 bucket.
#
# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.
# @param bucket_name [String] The bucket's name.
# @return [Boolean] true if the objects were listed; otherwise, false.
# @example
#   s3_client = Aws::S3::Client.new(region: 'us-east-1')
#   exit 1 unless list_objects_in_bucket?(s3_client, 'doc-example-bucket')
def list_objects_in_bucket?(s3_client, bucket_name)
  puts "Accessing the contents of the bucket named '#{bucket_name}'..."
  response = s3_client.list_objects_v2(
    bucket: bucket_name,
    max_keys: 50
  )

  if response.count.positive?
    puts "Contents of the bucket named '#{bucket_name}' (first 50 objects):"
    puts 'Name => ETag'
    response.contents.each do |obj|
      puts "#{obj.key} => #{obj.etag}"
    end
  else
    puts "No objects in the bucket named '#{bucket_name}'."
  end
  return true
rescue StandardError => e
  puts "Error while accessing the bucket named '#{bucket_name}': #{e.message}"
end
```


# Upload an object using the AWS SDK for Ruby<a name="UploadObjSingleOpRuby"></a>

The AWS SDK for Ruby \- Version 3 has two ways of uploading an object to Amazon S3\. The first uses a managed file uploader, which makes it easy to upload files of any size from disk\. To use the managed file uploader method:

1. Create an instance of the `Aws::S3::Resource` class\.

1. Reference the target object by bucket name and key\. Objects live in a bucket and have unique keys that identify each object\.

1. Call`#upload_file` on the object\.

**Example**  

```
require 'aws-sdk-s3'

# Uploads an object to a bucket in Amazon Simple Storage Service (Amazon S3).
#
# Prerequisites:
#
# - An S3 bucket.
# - An object to upload to the bucket.
#
# @param s3_client [Aws::S3::Resource] An initialized S3 resource.
# @param bucket_name [String] The name of the bucket.
# @param object_key [String] The name of the object.
# @param file_path [String] The path and file name of the object to upload.
# @return [Boolean] true if the object was uploaded; otherwise, false.
# @example
#   exit 1 unless object_uploaded?(
#     Aws::S3::Resource.new(region: 'us-east-1'),
#     'doc-example-bucket',
#     'my-file.txt',
#     './my-file.txt'
#   )
def object_uploaded?(s3_resource, bucket_name, object_key, file_path)
  object = s3_resource.bucket(bucket_name).object(object_key)
  object.upload_file(file_path)
  return true
rescue StandardError => e
  puts "Error uploading object: #{e.message}"
  return false
end
```

The second way that AWS SDK for Ruby \- Version 3 can upload an object uses the `#put` method of `Aws::S3::Object`\. This is useful if the object is a string or an I/O object that is not a file on disk\. To use this method:

1. Create an instance of the `Aws::S3::Resource` class\.

1. Reference the target object by bucket name and key\.

1. Call`#put`, passing in the string or I/O object\.

**Example**  

```
require 'aws-sdk-s3'

# Uploads an object to a bucket in Amazon Simple Storage Service (Amazon S3).
#
# Prerequisites:
#
# - An S3 bucket.
# - An object to upload to the bucket.
#
# @param s3_client [Aws::S3::Resource] An initialized S3 resource.
# @param bucket_name [String] The name of the bucket.
# @param object_key [String] The name of the object.
# @param file_path [String] The path and file name of the object to upload.
# @return [Boolean] true if the object was uploaded; otherwise, false.
# @example
#   exit 1 unless object_uploaded?(
#     Aws::S3::Resource.new(region: 'us-east-1'),
#     'doc-example-bucket',
#     'my-file.txt',
#     './my-file.txt'
#   )
def object_uploaded?(s3_resource, bucket_name, object_key, file_path)
  object = s3_resource.bucket(bucket_name).object(object_key)
  File.open(file_path, 'rb') do |file|
    object.put(body: file)
  end
  return true
rescue StandardError => e
  puts "Error uploading object: #{e.message}"
  return false
end
```


# Upload an object using the REST API<a name="UploadObjSingleOpREST"></a>

You can use AWS SDK to upload an object\. However, if your application requires it, you can send REST requests directly\. You can send a PUT request to upload data in a single operation\. For more information, see [PUT Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html)\.


# User Policy Examples<a name="example-policies-s3"></a>

This section shows several IAM user policies for controlling user access to Amazon S3\. For information about access policy language, see [Policies and Permissions in Amazon S3](access-policy-language-overview.md)\.

The following example policies will work if you test them programmatically\. However, to use them with the Amazon S3 console, you must grant additional permissions that are required by the console\. For information about using policies such as these with the Amazon S3 console, see [Walkthrough: Controlling access to a bucket with user policies](walkthrough1.md)\. 

**Topics**
+ [Allowing an IAM User Access to One of Your Buckets](#iam-policy-ex0)
+ [Allowing Each IAM User Access to a Folder in a Bucket](#iam-policy-ex1)
+ [Allowing a Group to Have a Shared Folder in Amazon S3](#iam-policy-ex2)
+ [Allowing All Your Users to Read Objects in a Portion of the Corporate Bucket](#iam-policy-ex3)
+ [Allowing a Partner to Drop Files into a Specific Portion of the Corporate Bucket](#iam-policy-ex4)
+ [Walkthrough: Controlling access to a bucket with user policies](walkthrough1.md)

## Allowing an IAM User Access to One of Your Buckets<a name="iam-policy-ex0"></a>

In this example, you want to grant an IAM user in your AWS account access to one of your buckets, `awsexamplebucket1`, and allow the user to add, update, and delete objects\. 

In addition to granting the `s3:PutObject`, `s3:GetObject`, and `s3:DeleteObject` permissions to the user, the policy also grants the `s3:ListAllMyBuckets`, `s3:GetBucketLocation`, and `s3:ListBucket` permissions\. These are the additional permissions required by the console\. Also, the `s3:PutObjectAcl` and the `s3:GetObjectAcl` actions are required to be able to copy, cut, and paste objects in the console\. For an example walkthrough that grants permissions to users and tests them using the console, see [Walkthrough: Controlling access to a bucket with user policies](walkthrough1.md)\. 

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Effect":"Allow",
         "Action": "s3:ListAllMyBuckets",
         "Resource":"arn:aws:s3:::*"
      },
      {
         "Effect":"Allow",
         "Action":["s3:ListBucket","s3:GetBucketLocation"],
         "Resource":"arn:aws:s3:::awsexamplebucket1"
      },
      {
         "Effect":"Allow",
         "Action":[
            "s3:PutObject",
            "s3:PutObjectAcl",
            "s3:GetObject",
            "s3:GetObjectAcl",
            "s3:DeleteObject"
         ],
         "Resource":"arn:aws:s3:::awsexamplebucket1/*"
      }
   ]
}
```

## Allowing Each IAM User Access to a Folder in a Bucket<a name="iam-policy-ex1"></a>

In this example, you want two IAM users, Alice and Bob, to have access to your bucket, `examplebucket`, so that they can add, update, and delete objects\. However, you want to restrict each user’s access to a single folder in the bucket\. You might create folders with names that match the user names\. 

```
awsexamplebucket1
   Alice/
   Bob/
```

To grant each user access only to his or her folder, you can write a policy for each user and attach it individually\. For example, you can attach the following policy to user Alice to allow her specific Amazon S3 permissions on the `awsexamplebucket1/Alice` folder\.

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Effect":"Allow",
         "Action":[
            "s3:PutObject",
            "s3:GetObject",
            "s3:GetObjectVersion",
            "s3:DeleteObject",
            "s3:DeleteObjectVersion"
         ],
         "Resource":"arn:aws:s3:::awsexamplebucket1/Alice/*"
      }
   ]
}
```

You then attach a similar policy to user `Bob`, identifying folder `Bob` in the `Resource` value\. 

Instead of attaching policies to individual users, you can write a single policy that uses a policy variable and attach the policy to a group\. First you must create a group and add both Alice and Bob to the group\. The following example policy allows a set of Amazon S3 permissions in the `awsexamplebucket1/${aws:username}` folder\. When the policy is evaluated, the policy variable `${aws:username}` is replaced by the requester's user name\. For example, if Alice sends a request to put an object, the operation is allowed only if Alice is uploading the object to the `examplebucket/Alice` folder\.

```
 1. {
 2.    "Version":"2012-10-17",
 3.    "Statement":[
 4.       {
 5.          "Effect":"Allow",
 6.          "Action":[
 7.             "s3:PutObject",
 8.             "s3:GetObject",
 9.             "s3:GetObjectVersion",
10.             "s3:DeleteObject",
11.             "s3:DeleteObjectVersion"
12.          ],
13.          "Resource":"arn:aws:s3:::awsexamplebucket1/${aws:username}/*"
14.       }
15.    ]
16. }
```

**Note**  
When using policy variables, you must explicitly specify version `2012-10-17` in the policy\. The default version of the access policy language, 2008\-10\-17, does not support policy variables\. 

 If you want to test the preceding policy on the Amazon S3 console, the console requires permission for additional Amazon S3 permissions, as shown in the following policy\. For information about how the console uses these permissions, see [Walkthrough: Controlling access to a bucket with user policies](walkthrough1.md)\. 

```
{
 "Version":"2012-10-17",
  "Statement": [
    {
      "Sid": "AllowGroupToSeeBucketListInTheConsole",
      "Action": [ 
      	"s3:ListAllMyBuckets", 
      	"s3:GetBucketLocation" 
      ],
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::*"  
    },
    {
      "Sid": "AllowRootLevelListingOfTheBucket",
      "Action": "s3:ListBucket",
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::awsexamplebucket1",
      "Condition":{ 
            "StringEquals":{
                    "s3:prefix":[""], "s3:delimiter":["/"]
                           }
                 }
    },
    {
      "Sid": "AllowListBucketOfASpecificUserPrefix",
      "Action": "s3:ListBucket",
      "Effect": "Allow",
      "Resource": "arn:aws:s3:::awsexamplebucket1",
      "Condition":{  "StringLike":{"s3:prefix":["${aws:username}/*"] }
       }
    },
      {
     "Sid": "AllowUserSpecificActionsOnlyInTheSpecificUserPrefix",
         "Effect":"Allow",
         "Action":[
            "s3:PutObject",
            "s3:GetObject",
            "s3:GetObjectVersion",
            "s3:DeleteObject",
            "s3:DeleteObjectVersion"
         ],
         "Resource":"arn:aws:s3:::awsexamplebucket1/${aws:username}/*"
      }
  ]
}
```

**Note**  
In the 2012\-10\-17 version of the policy, policy variables start with `$`\. This change in syntax can potentially create a conflict if your object key includes a `$`\. For example, to include an object key `my$file` in a policy, you specify the `$` character with `${$}`, `my${$}file`\.

Although IAM user names are friendly, human\-readable identifiers, they are not required to be globally unique\. For example, if user Bob leaves the organization and another Bob joins, then new Bob could access old Bob's information\. Instead of using user names, you could create folders based on user IDs\. Each user ID is unique\. In this case, you must modify the preceding policy to use the `${aws:userid}` policy variable\. For more information about user identifiers, see [IAM Identifiers](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html) in the *IAM User Guide*\.

```
{
   "Version":"2012-10-17",
   "Statement":[
      {
         "Effect":"Allow",
         "Action":[
            "s3:PutObject",
            "s3:GetObject",
            "s3:GetObjectVersion",
            "s3:DeleteObject",
            "s3:DeleteObjectVersion"
         ],
         "Resource":"arn:aws:s3:::mycorporatebucket/home/${aws:userid}/*"
      }
   ]
}
```

### Allowing Non\-IAM Users \(Mobile App Users\) Access to Folders in a Bucket<a name="non-iam-mobile-app-user-access"></a>

Suppose that you want to develop a mobile app, a game that stores users' data in an S3 bucket\. For each app user, you want to create a folder in your bucket\. You also want to limit each user’s access to his or her own folder\.  But you cannot create folders before someone downloads your app and starts playing the game, because you don’t have a user ID\. 

In this case, you can require users to sign in to your app by using public identity providers such as Login with Amazon, Facebook, or Google\. After users have signed in to your app through one of these providers, they have a user ID that you can use to create user\-specific folders at runtime\. 

You can then use web identity federation in AWS Security Token Service to integrate information from the identity provider with your app and to get temporary security credentials for each user\. You can then create IAM policies that allow the app to access your bucket and perform such operations as creating user\-specific folders and uploading data\. For more information about web identity federation, see [About Web Identity Federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html) in the *IAM User Guide*\.

## Allowing a Group to Have a Shared Folder in Amazon S3<a name="iam-policy-ex2"></a>

 Attaching the following policy to the group grants everybody in the group access to the following folder in Amazon S3: `mycorporatebucket/share/marketing`\. Group members are allowed to access only the specific Amazon S3 permissions shown in the policy and only for objects in the specified folder\. 

```
 1. {
 2.    "Version":"2012-10-17",
 3.    "Statement":[
 4.       {
 5.          "Effect":"Allow",
 6.          "Action":[
 7.             "s3:PutObject",
 8.             "s3:GetObject",
 9.             "s3:GetObjectVersion",
10.             "s3:DeleteObject",
11.             "s3:DeleteObjectVersion"
12.          ],
13.          "Resource":"arn:aws:s3:::mycorporatebucket/share/marketing/*"
14.       }
15.    ]
16. }
```

## Allowing All Your Users to Read Objects in a Portion of the Corporate Bucket<a name="iam-policy-ex3"></a>

 In this example, you create a group named `AllUsers`, which contains all the IAM users that are owned by the AWS account\. You then attach a policy that gives the group access to `GetObject` and `GetObjectVersion`, but only for objects in the `mycorporatebucket/readonly` folder\. 

```
 1. {
 2.    "Version":"2012-10-17",
 3.    "Statement":[
 4.       {
 5.          "Effect":"Allow",
 6.          "Action":[
 7.             "s3:GetObject",
 8.             "s3:GetObjectVersion"
 9.          ],
10.          "Resource":"arn:aws:s3:::MyCorporateBucket/readonly/*"
11.       }
12.    ]
13. }
```

## Allowing a Partner to Drop Files into a Specific Portion of the Corporate Bucket<a name="iam-policy-ex4"></a>

 In this example, you create a group called `WidgetCo` that represents a partner company\. You create an IAM user for the specific person or application at the partner company that needs access, and then you put the user in the group\. 

You then attach a policy that gives the group `PutObject` access to the following folder in the corporate bucket: `mycorporatebucket/uploads/widgetco`\. 

You want to prevent the `WidgetCo` group from doing anything else with the bucket, so you add a statement that explicitly denies permission to any Amazon S3 permissions except `PutObject` on any Amazon S3 resource in the AWS account\. This step is necessary only if there's a broad policy in use elsewhere in your AWS account that gives users wide access to Amazon S3 resources\.

```
 1. {
 2.    "Version":"2012-10-17",
 3.    "Statement":[
 4.       {
 5.          "Effect":"Allow",
 6.          "Action":"s3:PutObject",
 7.          "Resource":"arn:aws:s3:::mycorporatebucket/uploads/widgetco/*"
 8.       },
 9.       {
10.          "Effect":"Deny",
11.          "NotAction":"s3:PutObject",
12.          "Resource":"arn:aws:s3:::mycorporatebucket/uploads/widgetco/*"
13.       },
14.       {
15.          "Effect":"Deny",
16.          "Action":"s3:*",
17.          "NotResource":"arn:aws:s3:::mycorporatebucket/uploads/widgetco/*"
18.       }
19.    ]
20. }
```


# Example: Tracking an S3 Batch Operations job in Amazon EventBridge through AWS CloudTrail<a name="batch-ops-examples-event-bridge-cloud-trail"></a>

Amazon S3 Batch Operations job activity is recorded as events in AWS CloudTrail\. You can create a custom rule in Amazon EventBridge and send these events to the target notification resource of your choice, such as Amazon Simple Notification Service \(Amazon SNS\)\. 

**Note**  
Amazon EventBridge is the preferred way to manage your events\. Amazon CloudWatch Events and EventBridge are the same underlying service and API, but EventBridge provides more features\. Changes that you make in either CloudWatch or EventBridge appear in each console\. For more information, see the [Amazon EventBridge User Guide](https://docs.aws.amazon.com/eventbridge/latest/userguide/)\.

**Topics**
+ [S3 Batch Operations events emitted to CloudTrail](#batch-ops-examples-cloud-trail-events)
+ [Using an EventBridge rule for tracking S3 Batch Operations job events](#batch-ops-examples-event-bridge)

## S3 Batch Operations events emitted to CloudTrail<a name="batch-ops-examples-cloud-trail-events"></a>



When a Batch Operations job is created, it is recorded as a `JobCreated` event in CloudTrail\. As the job runs, it changes state during processing, and other `JobStatusChanged` events are recorded in CloudTrail\. You can view these events on the [CloudTrail console](https://console.aws.amazon.com/cloudtrail)\. For more information about CloudTrail, see the [AWS CloudTrail User Guide](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html)\.

**Note**  
Only S3 Batch Operations job `status-change` events are recorded in CloudTrail\.

**Example — S3 Batch Operations job completion event recorded by CloudTrail**  

```
{
    "eventVersion": "1.05",
    "userIdentity": {
        "accountId": "123456789012",
        "invokedBy": "s3.amazonaws.com"
    },
    "eventTime": "2020-02-05T18:25:30Z",
    "eventSource": "s3.amazonaws.com",
    "eventName": "JobStatusChanged",
    "awsRegion": "us-west-2",
    "sourceIPAddress": "s3.amazonaws.com",
    "userAgent": "s3.amazonaws.com",
    "requestParameters": null,
    "responseElements": null,
    "eventID": "f907577b-bf3d-4c53-b9ed-8a83a118a554",
    "readOnly": false,
    "eventType": "AwsServiceEvent",
    "recipientAccountId": "123412341234",
    "serviceEventDetails": {
        "jobId": "d6e58ec4-897a-4b6d-975f-10d7f0fb63ce",
        "jobArn": "arn:aws:s3:us-west-2:181572960644:job/d6e58ec4-897a-4b6d-975f-10d7f0fb63ce",
        "status": "Complete",
        "jobEventId": "b268784cf0a66749f1a05bce259804f5",
        "failureCodes": [],
        "statusChangeReason": []
    }
}
```

## Using an EventBridge rule for tracking S3 Batch Operations job events<a name="batch-ops-examples-event-bridge"></a>

The following example shows how to create a rule in Amazon EventBridge to capture S3 Batch Operations events recorded by AWS CloudTrail to a target of your choice\.

To do this, you create a rule by following all the steps in [Creating an EventBridge Rule That Triggers on an AWS API Call Using CloudTrail](https://docs.aws.amazon.com/eventbridge/latest/userguide/create-eventbridge-cloudtrail-rule.html)\. You paste the following S3 Batch Operations custom event pattern policy where applicable, and choose the target service of your choice\.

**S3 Batch Operations custom event pattern policy**

```
{
    "source": [
        "aws.s3"
    ],
    "detail-type": [
        "AWS Service Event via CloudTrail"
    ],
    "detail": {
        "eventSource": [
            "s3.amazonaws.com"
        ],
        "eventName": [
            "JobCreated",
            "JobStatusChanged"
        ]
    }
}
```



 The following examples are two Batch Operations events that were sent to Amazon Simple Queue Service \(Amazon SQS\) from an EventBridge event rule\. A Batch Operations job goes through many different states while processing \(`New`, `Preparing`, `Active`, etc\.\), so you can expect to receive several messages for each job\.

**Example — `JobCreated` sample event**  

```
{
    "version": "0",
    "id": "51dc8145-541c-5518-2349-56d7dffdf2d8",
    "detail-type": "AWS Service Event via CloudTrail",
    "source": "aws.s3",
    "account": "123456789012",
    "time": "2020-02-27T15:25:49Z",
    "region": "us-east-1",
    "resources": [],
    "detail": {
        "eventVersion": "1.05",
        "userIdentity": {
            "accountId": "11112223334444",
            "invokedBy": "s3.amazonaws.com"
        },
        "eventTime": "2020-02-27T15:25:49Z",
        "eventSource": "s3.amazonaws.com",
        "eventName": "JobCreated",
        "awsRegion": "us-east-1",
        "sourceIPAddress": "s3.amazonaws.com",
        "userAgent": "s3.amazonaws.com",
        "eventID": "7c38220f-f80b-4239-8b78-2ed867b7d3fa",
        "readOnly": false,
        "eventType": "AwsServiceEvent",
        "serviceEventDetails": {
            "jobId": "e849b567-5232-44be-9a0c-40988f14e80c",
            "jobArn": "arn:aws:s3:us-east-1:181572960644:job/e849b567-5232-44be-9a0c-40988f14e80c",
            "status": "New",
            "jobEventId": "f177ff24f1f097b69768e327038f30ac",
            "failureCodes": [],
            "statusChangeReason": []
        }
    }
}
```

**Example — `JobStatusChanged` sample event for when a job is complete**  

```
{
  "version": "0",
  "id": "c8791abf-2af8-c754-0435-fd869ce25233",
  "detail-type": "AWS Service Event via CloudTrail",
  "source": "aws.s3",
  "account": "123456789012",
  "time": "2020-02-27T15:26:42Z",
  "region": "us-east-1",
  "resources": [],
  "detail": {
    "eventVersion": "1.05",
    "userIdentity": {
      "accountId": "1111222233334444",
      "invokedBy": "s3.amazonaws.com"
    },
    "eventTime": "2020-02-27T15:26:42Z",
    "eventSource": "s3.amazonaws.com",
    "eventName": "JobStatusChanged",
    "awsRegion": "us-east-1",
    "sourceIPAddress": "s3.amazonaws.com",
    "userAgent": "s3.amazonaws.com",
    "eventID": "0238c1f7-c2b0-440b-8dbd-1ed5e5833afb",
    "readOnly": false,
    "eventType": "AwsServiceEvent",
    "serviceEventDetails": {
      "jobId": "e849b567-5232-44be-9a0c-40988f14e80c",
      "jobArn": "arn:aws:s3:us-east-1:181572960644:job/e849b567-5232-44be-9a0c-40988f14e80c",
      "status": "Complete",
      "jobEventId": "51f5ac17dba408301d56cd1b2c8d1e9e",
      "failureCodes": [],
      "statusChangeReason": []
    }
  }
}
```


# Copy an Amazon S3 Object in a Single Operation Using the AWS SDK for \.NET<a name="CopyingObjectUsingNetSDK"></a>

The following C\# example shows how to use the high\-level AWS SDK for \.NET to copy objects that are as big as 5 GB in a single operation\. For objects that are bigger than 5 GB, use the multipart upload copy example described in [Copy an Amazon S3 object using the AWS SDK for \.NET multipart upload API](CopyingObjctsUsingLLNetMPUapi.md)\.

This example makes a copy of an object that is a maximum of 5 GB\. For information about the example's compatibility with a specific version of the AWS SDK for \.NET and instructions on how to create and test a working sample, see [Running the Amazon S3 \.NET Code Examples](UsingTheMPDotNetAPI.md#TestingDotNetApiSamples)\.

```
using Amazon;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CopyObjectTest
    {
        private const string sourceBucket = "*** provide the name of the bucket with source object ***";
        private const string destinationBucket = "*** provide the name of the bucket to copy the object to ***";
        private const string objectKey = "*** provide the name of object to copy ***";
        private const string destObjectKey = "*** provide the destination object key name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2; 
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            Console.WriteLine("Copying an object");
            CopyingObjectAsync().Wait();
        }

        private static async Task CopyingObjectAsync()
        {
            try
            {
                CopyObjectRequest request = new CopyObjectRequest
                {
                    SourceBucket = sourceBucket,
                    SourceKey = objectKey,
                    DestinationBucket = destinationBucket,
                    DestinationKey = destObjectKey
                };
                CopyObjectResponse response = await s3Client.CopyObjectAsync(request);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}
```

## More Info<a name="CopyingObjectUsingNetSDK-more-info"></a>

[AWS SDK for \.NET](https://aws.amazon.com/sdk-for-net/)


# Amazon S3 Security<a name="security"></a>

Cloud security at AWS is the highest priority\. As an AWS customer, you benefit from a data center and network architecture that are built to meet the requirements of the most security\-sensitive organizations\.

Security is a shared responsibility between AWS and you\. The [shared responsibility model](https://aws.amazon.com/compliance/shared-responsibility-model/) describes this as security *of* the cloud and security *in* the cloud:
+ **Security of the cloud** – AWS is responsible for protecting the infrastructure that runs AWS services in the AWS Cloud\. AWS also provides you with services that you can use securely\. The effectiveness of our security is regularly tested and verified by third\-party auditors as part of the [AWS compliance programs](https://aws.amazon.com/compliance/programs/)\. To learn about the compliance programs that apply to Amazon S3, see [AWS Services in Scope by Compliance Program](https://aws.amazon.com/compliance/services-in-scope/)\.
+ **Security in the cloud** – Your responsibility is determined by the AWS service that you use\. You are also responsible for other factors including the sensitivity of your data, your organization’s requirements, and applicable laws and regulations\. 

This documentation will help you understand how to apply the shared responsibility model when using Amazon S3\. The following topics show you how to configure Amazon S3 to meet your security and compliance objectives\. You'll also learn how to use other AWS services that can help you monitor and secure your Amazon S3 resources\. 

**Topics**
+ [Data protection in Amazon S3](DataDurability.md)
+ [Identity and access management in Amazon S3](s3-access-control.md)
+ [Logging and monitoring in Amazon S3](s3-incident-response.md)
+ [Compliance Validation for Amazon S3](s3-compliance.md)
+ [Resilience in Amazon S3](disaster-recovery-resiliency.md)
+ [Infrastructure security in Amazon S3](network-isolation.md)
+ [Configuration and vulnerability analysis in Amazon S3](vulnerability-analysis-and-management.md)
+ [Bucket owner condition](bucket-owner-condition.md)
+ [Security Best Practices for Amazon S3](security-best-practices.md)


# Replication additional considerations<a name="replication-and-other-bucket-configs"></a>

Amazon S3 also supports bucket configurations for the following:
+ Versioning — For more information, see [Using versioning](Versioning.md)\.
+ Website hosting — For more information, see [Hosting a static website using Amazon S3](WebsiteHosting.md)\.
+ Bucket access through a bucket policy or access control list \(ACL\) — For more information, see [Using Bucket Policies and User Policies](using-iam-policies.md) and see [Managing Access with ACLs](S3_ACLs_UsingACLs.md)\.
+ Log storage — For more information, [Amazon S3 server access logging](ServerLogs.md)\.
+ Lifecycle management for objects in a bucket — For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

This topic explains how bucket replication configuration affects the behavior of these bucket configurations\.

**Topics**
+ [Lifecycle configuration and object replicas](#replica-and-lifecycle)
+ [Versioning configuration and replication configuration](#replication-and-versioning)
+ [Logging configuration and replication configuration](#replication-and-logging)
+ [Data transfer pricing when replicating across Regions](#replication-and-dest-region)
+ [Pausing replication](#replication-pause)
+ [Related topics](#replication-other-config-related-topics)

## Lifecycle configuration and object replicas<a name="replica-and-lifecycle"></a>

The time it takes for Amazon S3 to replicate an object depends on the size of the object\. For large objects, it can take several hours\. Although it might take a while before a replica is available in the destination, it takes the same amount of time to create the replica as it took to create the corresponding object in the source bucket\. If a lifecycle policy is enabled on a destination bucket, the lifecycle rules honor the original creation time of the object, not when the replica became available in the destination bucket\. 

Replication configuration requires the bucket to be versioning\-enabled\. When you enable versioning on a bucket, keep the following in mind:
+ If you have an object Expiration lifecycle policy, after you enable versioning, add a `NonCurrentVersionExpiration` policy to maintain the same permanent delete behavior as before you enabled versioning\.
+ If you have a Transition lifecycle policy, after you enable versioning, consider adding a `NonCurrentVersionTransition` policy\.

## Versioning configuration and replication configuration<a name="replication-and-versioning"></a>

Both the source and destination buckets must be versioning\-enabled when you configure replication on a bucket\. After you enable versioning on both the source and destination buckets and configure replication on the source bucket, you will encounter the following issues:
+ If you attempt to disable versioning on the source bucket, Amazon S3 returns an error\. You must remove the replication configuration before you can disable versioning on the source bucket\.
+ If you disable versioning on a destination bucket, replication fails\. The source object has the replication status `FAILED`\.

## Logging configuration and replication configuration<a name="replication-and-logging"></a>

If Amazon S3 delivers logs to a bucket that has replication enabled, it replicates the log objects\.

If server access logs \([Amazon S3 server access logging](ServerLogs.md)\) or AWS CloudTrail Logs \( [Logging Amazon S3 API calls using AWS CloudTrail](cloudtrail-logging.md)\) are enabled on your source or destination bucket, Amazon S3 includes replication\-related requests in the logs\. For example, Amazon S3 logs each object that it replicates\. 

## Data transfer pricing when replicating across Regions<a name="replication-and-dest-region"></a>

Amazon S3 Cross\-Region Replication \(CRR\) is used to copy objects across S3 buckets in different AWS Regions\. You might choose the Region for your destination bucket based on either your business needs or cost considerations\. For example, inter\-Region data transfer charges vary depending on the Regions that you choose\. 

Suppose that you chose US East \(N\. Virginia\) \(us\-east\-1\) as the Region for your source bucket\. If you choose US West \(Oregon\) \(us\-west\-2\) as the Region for your destination buckets, you pay more than if you choose the US East \(Ohio\) \(us\-east\-2\) Region\. For pricing information, see "Data Transfer Pricing" in [Amazon S3 pricing](https://aws.amazon.com/s3/pricing/)\. 

There are no data transfer charges associated with Same\-Region Replication \(SRR\)\.

## Pausing replication<a name="replication-pause"></a>

To temporarily pause replication, disable the relevant rule in the replication configuration\. 

If replication is enabled and you remove the IAM role that grants Amazon S3 the required permissions, replication fails\. Amazon S3 reports the replication status for affected objects as `FAILED`\.

## Related topics<a name="replication-other-config-related-topics"></a>

[Replication](replication.md)


# Resilience in Amazon S3<a name="disaster-recovery-resiliency"></a>

The AWS global infrastructure is built around Regions and Availability Zones\. AWS Regions provide multiple, physically separated and isolated Availability Zones that are connected with low latency, high throughput, and highly redundant networking\. These Availability Zones offer you an effective way to design and operate applications and databases\. They are more highly available, fault tolerant, and scalable than traditional single data center infrastructures or multi\-data center infrastructures\. If you specifically need to replicate your data over greater geographic distances, you can use [Replication](replication.md), which enables automatic, asynchronous copying of objects across buckets in different AWS Regions\.

Each AWS Region has multiple Availability Zones\. You can deploy your applications across multiple Availability Zones in the same Region for fault tolerance and low latency\. Availability Zones are connected to each other with fast, private fiber\-optic networking, enabling you to easily architect applications that automatically fail over between Availability Zones without interruption\.

For more information about AWS Regions and Availability Zones, see [AWS Global Infrastructure](https://aws.amazon.com/about-aws/global-infrastructure/)\.

In addition to the AWS global infrastructure, Amazon S3 offers several features to help support your data resiliency and backup needs\.

**Lifecycle configuration**  
A lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects\. With lifecycle configuration rules, you can tell Amazon S3 to transition objects to less expensive storage classes, archive them, or delete them\. For more information, see [Object lifecycle management](object-lifecycle-mgmt.md)\.

**Versioning**  
Versioning is a means of keeping multiple variants of an object in the same bucket\. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket\. With versioning, you can easily recover from both unintended user actions and application failures\. For more information, see [Using versioning](Versioning.md)\.

**S3 Object Lock**  
You can use S3 Object Lock to store objects using a *write once, read many* \(WORM\) model\. Using S3 Object Lock, you can prevent an object from being deleted or overwritten for a fixed amount of time or indefinitely\. S3 Object Lock enables you to meet regulatory requirements that require WORM storage or simply to add an additional layer of protection against object changes and deletion\. For more information, see [Locking objects using S3 Object Lock](object-lock.md)\.

**Storage classes**  
Amazon S3 offers a range of storage classes for the objects that you store\. Two of these storage classes \(STANDARD\_IA and ONEZONE\_IA\) are designed for long\-lived and infrequently accessed data, such as backups\. You can also use the S3 Glacier storage class to archive objects that you don't need to access in real time\. For more information, see [Amazon S3 storage classes](storage-class-intro.md)\.

The following security best practices also address resilience:
+ [Enable versioning](security-best-practices.md#versioning)
+ [Consider Amazon S3 cross-region replication](security-best-practices.md#cross-region)
+ [Identify and audit all your Amazon S3 buckets](security-best-practices.md#audit)

## Encryption of Amazon S3 backups<a name="backup-encryption"></a>

If you are storing backups using Amazon S3, the encryption of your backups depends on the configuration of those buckets\. Amazon S3 provides a way to set the default encryption behavior for an S3 bucket\. You can set default encryption on a bucket so that all objects are encrypted when they are stored in the bucket\. The default encryption supports keys stored in AWS KMS \(SSE\-KMS\)\. For more information, see [ Setting default server\-side encryption behavior for Amazon S3 buckets](bucket-encryption.md)\.


## This guide has been archived
This guide has been archived. Please see https://github.com/awsdocs/amazon-s3-userguide which combines information from the three retired Amazon S3 guides: Amazon S3 Developer Guide, Console User Guide, and Getting Started Guide. 

## Amazon S3 Devloper Guide

The open source version of the Amazon S3 Devloper Guide. You can submit feedback & requests for changes by submitting issues in this repo or by making proposed changes & submitting a pull request.

## License Summary

The documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.

The sample code within this documentation is made available under a modified MIT license. See the LICENSE-SAMPLECODE file.



Copyright ${THIS_YEAR} Amazon.com, Inc. or its affiliates. All Rights Reserved. 

The documentation is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See the LICENSE file.

The sample code within this documentation is made available under a modified MIT license. See the LICENSE-SAMPLECODE file.



Copyright ${THIS_YEAR} Amazon.com, Inc. or its affiliates. All Rights Reserved.

Permission is hereby granted, free of charge, to any person obtaining a copy of this
software and associated documentation files (the "Software"), to deal in the Software
without restriction, including without limitation the rights to use, copy, modify,
merge, publish, distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



  Copyright 2010-2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.

   This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0
   International License (the "License"). You may not use this file except in compliance with the
   License. A copy of the License is located at http://creativecommons.org/licenses/by-nc-sa/4.0/.

   This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
   either express or implied. See the License for the specific language governing permissions and
   limitations under the License.



Amazon S3 Documentation PHP Examples
==============================================


These are the PHP examples used in the [Amazon S3 developer documentation](https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html).

Prerequisites
=============

To build and run these examples, you'll need:

* [AWS SDK for PHP](https://aws.amazon.com/sdk-for-php/) (downloaded and extracted somewhere on
  your machine)
* AWS credentials, either configured in a local AWS credentials file or by setting the
  ``AWS_ACCESS_KEY_ID`` and ``AWS_SECRET_ACCESS_KEY`` environment variables.

For information about how to set AWS credentials for use with the AWS SDK for PHP,
see [Credentials for the AWS SDK for PHP Version 3 ](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_credentials.html) in the *AWS
SDK for PHP Developer Guide*.

Running the Examples
====================

To run the PHP examples, you will need to create a PHP page in your preferred development environment.
For more information, see [Getting Started](https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/getting-started_index.html). 

**IMPORTANT**

   The examples perform AWS operations for the account and region for which you've specified
   credentials, and you may incur AWS service charges by running them. Please visit the [AWS Pricing](https://aws.amazon.com/pricing/) page for details about the charges you can
   expect for a given service and operation.

   Some of these examples perform *destructive* operations on AWS resources, such as deleting an
   Amazon S3 bucket. **Be very careful** when running an operation that
   may delete or modify AWS resources in your account. It's best to create separate test-only
   resources when experimenting with these examples.

All of the examples require replacing certain configuration values in the source code. These values
are specified as String variables at the beginning of each example, and begin and end with three stars
(for example, "\*\*\* Your Bucket Name \*\*\*"). The source-code comments and developer guide provide
further information.



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';

// $filepath should be an absolute path to a file on disk.
$filepath = '*** Your File Path ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Upload a file with server-side encryption.
$result = $s3->putObject([
    'Bucket'               => $bucket,
    'Key'                  => $keyname,
    'SourceFile'           => $filepath,
    'ServerSideEncryption' => 'AES256',
]);



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

try {
    // Get the object.
    $result = $s3->getObject([
        'Bucket' => $bucket,
        'Key'    => $keyname
    ]);

    // Display the object in the browser.
    header("Content-Type: {$result['ContentType']}");
    echo $result['Body'];
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$sourceBucket = '*** Your Source Bucket Name ***';
$sourceKeyname = '*** Your Source Object Key ***';

$targetBucket = '*** Your Target Bucket Name ***';
$targetKeyname = '*** Your Target Object Key ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Copy an object and add server-side encryption.
$s3->copyObject([
    'Bucket'               => $targetBucket,
    'Key'                  => $targetKeyname,
    'CopySource'           => "{$sourceBucket}/{$sourceKeyname}",
    'ServerSideEncryption' => 'AES256',
]);



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
$filename = '*** Path to and Name of the File to Upload ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

$result = $s3->createMultipartUpload([
    'Bucket'       => $bucket,
    'Key'          => $keyname,
    'StorageClass' => 'REDUCED_REDUNDANCY',
    'ACL'          => 'public-read',
    'Metadata'     => [
        'param1' => 'value 1',
        'param2' => 'value 2',
        'param3' => 'value 3'
    ]
]);
$uploadId = $result['UploadId'];

// Upload the file in parts.
try {
    $file = fopen($filename, 'r');
    $partNumber = 1;
    while (!feof($file)) {
        $result = $s3->uploadPart([
            'Bucket'     => $bucket,
            'Key'        => $keyname,
            'UploadId'   => $uploadId,
            'PartNumber' => $partNumber,
            'Body'       => fread($file, 5 * 1024 * 1024),
        ]);
        $parts['Parts'][$partNumber] = [
            'PartNumber' => $partNumber,
            'ETag' => $result['ETag'],
        ];
        $partNumber++;

        echo "Uploading part {$partNumber} of {$filename}." . PHP_EOL;
    }
    fclose($file);
} catch (S3Exception $e) {
    $result = $s3->abortMultipartUpload([
        'Bucket'   => $bucket,
        'Key'      => $keyname,
        'UploadId' => $uploadId
    ]);

    echo "Upload of {$filename} failed." . PHP_EOL;
}

// Complete the multipart upload.
$result = $s3->completeMultipartUpload([
    'Bucket'   => $bucket,
    'Key'      => $keyname,
    'UploadId' => $uploadId,
    'MultipartUpload'    => $parts,
]);
$url = $result['Location'];

echo "Uploaded {$filename} to {$url}." . PHP_EOL;



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';

// Instantiate the client.
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Use the high-level iterators (returns ALL of your objects).
try {
    $objects = $s3->getPaginator('ListObjects', [
        'Bucket' => $bucket
    ]);

    echo "Keys retrieved!" . PHP_EOL;
    foreach ($objects as $object) {
        echo $object['Key'] . PHP_EOL;
    }
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}

// Use the plain API (returns ONLY up to 1000 of your objects).
try {
    $result = $s3->listObjects([
        'Bucket' => $bucket
    ]);

    echo "Keys retrieved!" . PHP_EOL;
    foreach ($result['Contents'] as $object) {
        echo $object['Key'] . PHP_EOL;
    }
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
$uploadId = '*** Upload ID of upload to Abort ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Abort the multipart upload.
$s3->abortMultipartUpload([
    'Bucket'   => $bucket,
    'Key'      => $keyname,
    'UploadId' => $uploadId,
]);



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// 1. Create a few objects.
for ($i = 1; $i <= 3; $i++) {
    $s3->putObject([
        'Bucket' => $bucket,
        'Key'    => "key{$i}",
        'Body'   => "content {$i}",
    ]);
}

// 2. List the objects and get the keys.
$keys = $s3->listObjects([
    'Bucket' => $bucket
]) ->getPath('Contents/*/Key');

// 3. Delete the objects.
$s3->deleteObjects([
    'Bucket'  => $bucket,
    'Delete' => [
        'Objects' => array_map(function ($key) {
            return ['Key' => $key];
        }, $keys)
    ],
]);



<?php
//// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\Sts\StsClient;
use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';

// In real applications, the following code is part of your trusted code. It has
// the security credentials that you use to obtain temporary security credentials.
$sts = new StsClient(
    [
    'version' => 'latest',
    'region' => 'us-east-1']
);

// Fetch the federated credentials.
$sessionToken = $sts->getFederationToken([
    'Name'              => 'User1',
    'DurationSeconds'    => '3600',
    'Policy'            => json_encode([
        'Statement' => [
            'Sid'              => 'randomstatementid' . time(),
            'Action'           => ['s3:ListBucket'],
            'Effect'           => 'Allow',
            'Resource'         => 'arn:aws:s3:::' . $bucket
        ]
    ])
]);

// The following will be part of your less trusted code. You provide temporary
// security credentials so the code can send authenticated requests to Amazon S3.

$s3 = new S3Client([
    'region' => 'us-east-1',
    'version' => 'latest',
    'credentials' => [
        'key'    => $sessionToken['Credentials']['AccessKeyId'],
        'secret' => $sessionToken['Credentials']['SecretAccessKey'],
        'token'  => $sessionToken['Credentials']['SessionToken']
    ]
]);

try {
    $result = $s3->listObjects([
        'Bucket' => $bucket
    ]);
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\Common\Exception\MultipartUploadException;
use Aws\S3\MultipartUploader;
use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
                        
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);
 
// Prepare the upload parameters.
$uploader = new MultipartUploader($s3, '/path/to/large/file.zip', [
    'bucket' => $bucket,
    'key'    => $keyname
]);

// Perform the upload.
try {
    $result = $uploader->upload();
    echo "Upload complete: {$result['ObjectURL']}" . PHP_EOL;
} catch (MultipartUploadException $e) {
    echo $e->getMessage() . PHP_EOL;
}



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$sourceBucket = '*** Your Source Bucket Name ***';
$sourceKeyname = '*** Your Source Object Key ***';
$targetBucket = '*** Your Target Bucket Name ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Copy an object.
$s3->copyObject([
    'Bucket'     => $targetBucket,
    'Key'        => "{$sourceKeyname}-copy",
    'CopySource' => "{$sourceBucket}/{$sourceKeyname}",
]);

// Perform a batch of CopyObject operations.
$batch = array();
for ($i = 1; $i <= 3; $i++) {
    $batch[] = $s3->getCommand('CopyObject', [
        'Bucket'     => $targetBucket,
        'Key'        => "{targetKeyname}-{$i}",
        'CopySource' => "{$sourceBucket}/{$sourceKeyname}",
    ]);
}
try {
    $succeeded = $s3->execute($batch);
    $failed = array();
} catch (CommandTransferException $e) {
    $succeeded = $e->getSuccessfulCommands();
    echo "Failed Commands:" . PHP_EOL;
    foreach ($e->getFailedCommands() as $failedCommand) {
        echo $e->getExceptionForFailedCommand($FailedCommand)->getMessage() . PHP_EOL;
    }
}



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Delete an object from the bucket.
$s3->deleteObject([
    'Bucket' => $bucket,
    'Key'    => $keyname
]);



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// 1. Enable object versioning for the bucket.
$s3->putBucketVersioning([
    'Bucket' => $bucket,
    'Status' => 'Enabled',
]);

// 2. Create a few versions of an object.
for ($i = 1; $i <= 3; $i++) {
    $s3->putObject([
        'Bucket' => $bucket,
        'Key'    => $keyname,
        'Body'   => "content {$i}",
    ]);
}

// 3. List the objects versions and get the keys and version IDs.
$versions = $s3->listObjectVersions(['Bucket' => $bucket])
    ->getPath('Versions');

// 4. Delete the object versions.
$s3->deleteObjects([
    'Bucket'  => $bucket,
    'Delete' => [
        'Objects' => array_map(function ($version) {
          return [
              'Key'       => $version['Key'],
              'VersionId' => $version['VersionId']
        }, $versions),
    ],       
]);

echo "The following objects were deleted successfully:". PHP_EOL;
foreach ($result['Deleted'] as $object) {
    echo "Key: {$object['Key']}, VersionId: {$object['VersionId']}" . PHP_EOL;
}

echo PHP_EOL . "The following objects could not be deleted:" . PHP_EOL;
foreach ($result['Errors'] as $object) {
    echo "Key: {$object['Key']}, VersionId: {$object['VersionId']}" . PHP_EOL;
}

// 5. Suspend object versioning for the bucket.
$s3->putBucketVersioning([
    'Bucket' => $bucket,
    'Status' => 'Suspended',
]);



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
            
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Check which server-side encryption algorithm is used.
$result = $s3->headObject([
    'Bucket' => $bucket,
    'Key'    => $keyname,
]);
echo $result['ServerSideEncryption'];



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$sourceBucket = '*** Your Source Bucket Name ***';
$sourceKeyname = '*** Your Source Object Key ***';

$targetBucket = '*** Your Target Bucket Name ***';
$targetKeyname = '*** Your Target Object Key ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Copy an object and add server-side encryption.
$s3->copyObject([
    'Bucket'               => $targetBucket,
    'Key'                  => $targetKeyname,
    'CopySource'           => "{$sourceBucket}/{$sourceKeyname}",
    'ServerSideEncryption' => 'AES256',
]);



<?php 
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';

$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

// Retrieve a list of the current multipart uploads.
$result = $s3->listMultipartUploads([
    'Bucket' => $bucket
]);

// Write the list of uploads to the page.
print_r($result->toArray());



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\Sts\StsClient;
use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';

$s3 = new S3Client([
    'region' => 'us-east-1',
    'version' => 'latest',
]);

// Retrieve the list of buckets.
$result = $s3->listBuckets();

try {
    // Retrieve a paginator for listing objects.
    $objects = $s3->getPaginator('ListObjects', [
        'Bucket' => $bucket
    ]);

    echo "Keys retrieved!" . PHP_EOL;
    
    // Print the list of objects to the page.
    foreach ($objects as $object) {
        echo $object['Key'] . PHP_EOL;
    }
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';
$keyname = '*** Your Object Key ***';
                        
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

try {
    // Upload data.
    $result = $s3->putObject([
        'Bucket' => $bucket,
        'Key'    => $keyname,
        'Body'   => 'Hello, world!',
        'ACL'    => 'public-read'
    ]);

    // Print the URL to the object.
    echo $result['ObjectURL'] . PHP_EOL;
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\S3\S3Client;

$bucket = '*** Your Bucket Name ***';
                
$s3 = new S3Client([
    'version' => 'latest',
    'region'  => 'us-east-1'
]);

         
// Add the website configuration.
$s3->putBucketWebsite([
    'Bucket'                => $bucket,
    'WebsiteConfiguration'  => [
        'IndexDocument' => ['Suffix' => 'index.html'],
        'ErrorDocument' => ['Key' => 'error.html']
    ]
]);
        
// Retrieve the website configuration.
$result = $s3->getBucketWebsite([
    'Bucket' => $bucket
]);
echo $result->getPath('IndexDocument/Suffix');
        
// Delete the website configuration.
$s3->deleteBucketWebsite([
    'Bucket' => $bucket
]);



<?php
// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE )

require 'vendor/autoload.php';

use Aws\Sts\StsClient;
use Aws\S3\S3Client;
use Aws\S3\Exception\S3Exception;

$bucket = '*** Your Bucket Name ***';

$sts = new StsClient([
    'version' => 'latest',
    'region' => 'us-east-1'
]);
    
$sessionToken = $sts->getSessionToken();

$s3 = new S3Client([
    'region' => 'us-east-1',
    'version' => 'latest',
    'credentials' => [
        'key'    => $sessionToken['Credentials']['AccessKeyId'],
        'secret' => $sessionToken['Credentials']['SecretAccessKey'],
        'token'  => $sessionToken['Credentials']['SessionToken']
    ]
]);

$result = $s3->listBuckets();


try {
    // Retrieve a paginator for listing objects.
    $objects = $s3->getPaginator('ListObjects', [
        'Bucket' => $bucket
    ]);
    
    echo "Keys retrieved!" . PHP_EOL;
    
    // List objects
    foreach ($objects as $object) {
        echo $object['Key'] . PHP_EOL;
    }
} catch (S3Exception $e) {
    echo $e->getMessage() . PHP_EOL;
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.util.ArrayList;
import java.util.Collection;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.AccessControlList;
import com.amazonaws.services.s3.model.CannedAccessControlList;
import com.amazonaws.services.s3.model.CanonicalGrantee;
import com.amazonaws.services.s3.model.CreateBucketRequest;
import com.amazonaws.services.s3.model.Grant;
import com.amazonaws.services.s3.model.GroupGrantee;
import com.amazonaws.services.s3.model.Permission;

public class CreateBucketWithACL {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Create a bucket with a canned ACL. This ACL will be deleted by the
            // getGrantsAsList().clear() call below. It is here for demonstration
            // purposes.
            CreateBucketRequest createBucketRequest = new CreateBucketRequest(bucketName, clientRegion)
                    .withCannedAcl(CannedAccessControlList.LogDeliveryWrite);
            s3Client.createBucket(createBucketRequest);
    
            // Create a collection of grants to add to the bucket.
            Collection<Grant> grantCollection = new ArrayList<Grant>();
            
            // Grant the account owner full control.
            Grant grant1 = new Grant(new CanonicalGrantee(s3Client.getS3AccountOwner().getId()), Permission.FullControl);
            grantCollection.add(grant1);
    
            // Grant the LogDelivery group permission to write to the bucket.
            Grant grant2 = new Grant(GroupGrantee.LogDelivery, Permission.Write);
            grantCollection.add(grant2);
    
            // Save (replace) grants by deleting all current ACL grants and replacing
            // them with the two we just created.
            AccessControlList bucketAcl = s3Client.getBucketAcl(bucketName);
            bucketAcl.getGrantsAsList().clear();
            bucketAcl.getGrantsAsList().addAll(grantCollection);
            s3Client.setBucketAcl(bucketName, bucketAcl);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it and returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.util.EnumSet;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketNotificationConfiguration;
import com.amazonaws.services.s3.model.TopicConfiguration;
import com.amazonaws.services.s3.model.QueueConfiguration;
import com.amazonaws.services.s3.model.S3Event;
import com.amazonaws.services.s3.model.SetBucketNotificationConfigurationRequest;

public class EnableNotificationOnABucket {

    public static void main(String[] args) throws IOException {
        String bucketName = "*** Bucket name ***";
        String clientRegion = "*** Client region ***";
        String snsTopicARN = "*** SNS Topic ARN ***";
        String sqsQueueARN = "*** SQS Queue ARN ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();
            BucketNotificationConfiguration notificationConfiguration = new BucketNotificationConfiguration();

            // Add an SNS topic notification.
            notificationConfiguration.addConfiguration("snsTopicConfig",
                    new TopicConfiguration(snsTopicARN, EnumSet.of(S3Event.ObjectCreated)));

            // Add an SQS queue notification.
            notificationConfiguration.addConfiguration("sqsQueueConfig",
                    new QueueConfiguration(sqsQueueARN, EnumSet.of(S3Event.ObjectCreated)));
            
            // Create the notification configuration request and set the bucket notification configuration.
            SetBucketNotificationConfigurationRequest request = new SetBucketNotificationConfigurationRequest(
                    bucketName, notificationConfiguration);
            s3Client.setBucketNotificationConfiguration(request);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.ByteArrayOutputStream;
import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.regions.RegionUtils;
import com.amazonaws.services.kms.AWSKMS;
import com.amazonaws.services.kms.AWSKMSClientBuilder;
import com.amazonaws.services.kms.model.CreateKeyResult;
import com.amazonaws.services.s3.AmazonS3Encryption;
import com.amazonaws.services.s3.AmazonS3EncryptionClientBuilder;
import com.amazonaws.services.s3.model.CryptoConfiguration;
import com.amazonaws.services.s3.model.KMSEncryptionMaterialsProvider;
import com.amazonaws.services.s3.model.S3Object;
import com.amazonaws.services.s3.model.S3ObjectInputStream;

public class UploadObjectKMSKey {

    public static void main(String[] args) throws IOException {
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Object key name ***";
        String clientRegion = "*** Client region ***";
        String kms_cmk_id = "***AWS KMS customer master key ID***";
        int readChunkSize = 4096;

        try {
            // Optional: If you don't have a KMS key (or need another one),
            // create one. This example creates a key with AWS-created
            // key material.
            AWSKMS kmsClient = AWSKMSClientBuilder.standard()
                                    .withCredentials(new ProfileCredentialsProvider())
                                    .withRegion(clientRegion)
                                    .build();
            CreateKeyResult keyResult = kmsClient.createKey();
            kms_cmk_id = keyResult.getKeyMetadata().getKeyId();
    
             // Create the encryption client.
            KMSEncryptionMaterialsProvider materialProvider = new KMSEncryptionMaterialsProvider(kms_cmk_id);
            CryptoConfiguration cryptoConfig = new CryptoConfiguration()
                    .withAwsKmsRegion(RegionUtils.getRegion(clientRegion));
            AmazonS3Encryption encryptionClient = AmazonS3EncryptionClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withEncryptionMaterials(materialProvider)
                    .withCryptoConfiguration(cryptoConfig)
                    .withRegion(clientRegion).build();
    
            // Upload an object using the encryption client.
            String origContent = "S3 Encrypted Object Using KMS-Managed Customer Master Key.";
            int origContentLength = origContent.length();
            encryptionClient.putObject(bucketName, keyName, origContent);
    
            // Download the object. The downloaded object is still encrypted.
            S3Object downloadedObject = encryptionClient.getObject(bucketName, keyName);
            S3ObjectInputStream input = downloadedObject.getObjectContent();
    
            // Decrypt and read the object and close the input stream.
            byte[] readBuffer = new byte[readChunkSize];
            ByteArrayOutputStream baos = new ByteArrayOutputStream(readChunkSize);
            int bytesRead = 0;
            int decryptedContentLength = 0;
    
            while ((bytesRead = input.read(readBuffer)) != -1) {
                baos.write(readBuffer, 0, bytesRead);
                decryptedContentLength += bytesRead;
            }
            input.close();
    
            // Verify that the original and decrypted contents are the same size.
            System.out.println("Original content length: " + origContentLength);
            System.out.println("Decrypted content length: " + decryptedContentLength);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.File;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.event.ProgressEvent;
import com.amazonaws.event.ProgressListener;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.transfer.TransferManager;
import com.amazonaws.services.s3.transfer.TransferManagerBuilder;
import com.amazonaws.services.s3.transfer.Upload;

public class HighLevelTrackMultipartUpload {

    public static void main(String[] args) throws Exception {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Object key ***";
        String filePath = "*** Path to file to upload ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();
            TransferManager tm = TransferManagerBuilder.standard()
                                .withS3Client(s3Client)
                                .build();
            PutObjectRequest request = new PutObjectRequest(bucketName, keyName, new File(filePath));
            
            // To receive notifications when bytes are transferred, add a  
            // ProgressListener to your request.
            request.setGeneralProgressListener(new ProgressListener() {
                public void progressChanged(ProgressEvent progressEvent) {
                    System.out.println("Transferred bytes: " + progressEvent.getBytesTransferred());
                    }
            });
            // TransferManager processes all transfers asynchronously,
            // so this call returns immediately.
            Upload upload = tm.upload(request);
    
            // Optionally, you can wait for the upload to finish before continuing.
            upload.waitForCompletion();
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client 
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketWebsiteConfiguration;

public class WebsiteConfiguration {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String indexDocName = "*** Index document name ***";
        String errorDocName = "*** Error document name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

            // Print the existing website configuration, if it exists.
            printWebsiteConfig(s3Client, bucketName);
    
            // Set the new website configuration.
            s3Client.setBucketWebsiteConfiguration(bucketName, new BucketWebsiteConfiguration(indexDocName, errorDocName));
    
            // Verify that the configuration was set properly by printing it.
            printWebsiteConfig(s3Client, bucketName);
    
            // Delete the website configuration.
            s3Client.deleteBucketWebsiteConfiguration(bucketName);
    
            // Verify that the website configuration was deleted by printing it.
            printWebsiteConfig(s3Client, bucketName);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void printWebsiteConfig(AmazonS3 s3Client, String bucketName) {
        System.out.println("Website configuration: ");
        BucketWebsiteConfiguration bucketWebsiteConfig = s3Client.getBucketWebsiteConfiguration(bucketName);
        if (bucketWebsiteConfig == null) {
            System.out.println("No website config.");
        } else {
            System.out.println("Index doc: " + bucketWebsiteConfig.getIndexDocumentSuffix());
            System.out.println("Error doc: " + bucketWebsiteConfig.getErrorDocument());
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.net.URL;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.HttpMethod;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.GeneratePresignedUrlRequest;

public class GeneratePresignedURL {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String objectKey = "*** Object key ***";

        try {            
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();
    
            // Set the presigned URL to expire after one hour.
            java.util.Date expiration = new java.util.Date();
            long expTimeMillis = expiration.getTime();
            expTimeMillis += 1000 * 60 * 60;
            expiration.setTime(expTimeMillis);

            // Generate the presigned URL.
            System.out.println("Generating pre-signed URL.");
            GeneratePresignedUrlRequest generatePresignedUrlRequest = 
                    new GeneratePresignedUrlRequest(bucketName, objectKey)
                    .withMethod(HttpMethod.GET)
                    .withExpiration(expiration);
            URL url = s3Client.generatePresignedUrl(generatePresignedUrlRequest);
    
            System.out.println("Pre-Signed URL: " + url.toString());
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.util.ArrayList;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.DeleteObjectsRequest;
import com.amazonaws.services.s3.model.DeleteObjectsRequest.KeyVersion;
import com.amazonaws.services.s3.model.DeleteObjectsResult;

public class DeleteMultipleObjectsNonVersionedBucket {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Upload three sample objects.
            ArrayList<KeyVersion> keys = new ArrayList<KeyVersion>();
            for (int i = 0; i < 3; i++) {
                String keyName = "delete object example " + i;
                s3Client.putObject(bucketName, keyName, "Object number " + i + " to be deleted.");
                keys.add(new KeyVersion(keyName));
            }
            System.out.println(keys.size() + " objects successfully created.");
    
            // Delete the sample objects.
            DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest(bucketName)
                                                                    .withKeys(keys)
                                                                    .withQuiet(false);
    
            // Verify that the objects were deleted successfully.
            DeleteObjectsResult delObjRes = s3Client.deleteObjects(multiObjectDeleteRequest);
            int successfulDeletes = delObjRes.getDeletedObjects().size();
            System.out.println(successfulDeletes + " objects successfully deleted.");
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicSessionCredentials;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ObjectListing;
import com.amazonaws.services.securitytoken.AWSSecurityTokenService;
import com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder;
import com.amazonaws.services.securitytoken.model.AssumeRoleRequest;
import com.amazonaws.services.securitytoken.model.Credentials;
import com.amazonaws.services.securitytoken.model.GetSessionTokenRequest;
import com.amazonaws.services.securitytoken.model.GetSessionTokenResult;

public class MakingRequestsWithIAMTempCredentials {
    public static void main(String[] args) {
        String clientRegion = "*** Client region ***";
        String roleARN = "*** ARN for role to be assumed ***";
        String roleSessionName = "*** Role session name ***";
        String bucketName = "*** Bucket name ***";

        try {
            // Creating the STS client is part of your trusted code. It has
            // the security credentials you use to obtain temporary security credentials.
            AWSSecurityTokenService stsClient = AWSSecurityTokenServiceClientBuilder.standard()
                                                    .withCredentials(new ProfileCredentialsProvider())
                                                    .withRegion(clientRegion)
                                                    .build();

            // Assume the IAM role. Note that you cannot assume the role of an AWS root account;
            // Amazon S3 will deny access. You must use credentials for an IAM user or an IAM role.
            AssumeRoleRequest roleRequest = new AssumeRoleRequest()
                                                    .withRoleArn(roleARN)
                                                    .withRoleSessionName(roleSessionName);
            stsClient.assumeRole(roleRequest);

            // Start a session.
            GetSessionTokenRequest getSessionTokenRequest = new GetSessionTokenRequest();
            // The duration can be set to more than 3600 seconds only if temporary
            // credentials are requested by an IAM user rather than an account owner.
            getSessionTokenRequest.setDurationSeconds(7200);
            GetSessionTokenResult sessionTokenResult = stsClient.getSessionToken(getSessionTokenRequest);
            Credentials sessionCredentials = sessionTokenResult.getCredentials();

            // Package the temporary security credentials as a BasicSessionCredentials object 
            // for an Amazon S3 client object to use.
            BasicSessionCredentials basicSessionCredentials = new BasicSessionCredentials(
                    sessionCredentials.getAccessKeyId(), sessionCredentials.getSecretAccessKey(),
                    sessionCredentials.getSessionToken());

            // Provide temporary security credentials so that the Amazon S3 client 
			// can send authenticated requests to Amazon S3. You create the client 
			// using the basicSessionCredentials object.
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                                    .withCredentials(new AWSStaticCredentialsProvider(basicSessionCredentials))
                                    .withRegion(clientRegion)
                                    .build();

            // Verify that assuming the role worked and the permissions are set correctly
            // by getting a set of object keys from the bucket.
            ObjectListing objects = s3Client.listObjects(bucketName);
            System.out.println("No. of Objects: " + objects.getObjectSummaries().size());
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.security.NoSuchAlgorithmException;
import java.security.SecureRandom;

import javax.crypto.KeyGenerator;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CopyObjectRequest;
import com.amazonaws.services.s3.model.GetObjectMetadataRequest;
import com.amazonaws.services.s3.model.GetObjectRequest;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.model.S3Object;
import com.amazonaws.services.s3.model.S3ObjectInputStream;
import com.amazonaws.services.s3.model.SSECustomerKey;

public class ServerSideEncryptionUsingClientSideEncryptionKey {
    private static SSECustomerKey SSE_KEY;
    private static AmazonS3 S3_CLIENT;
    private static KeyGenerator KEY_GENERATOR;

    public static void main(String[] args) throws IOException, NoSuchAlgorithmException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ***";
        String uploadFileName = "*** File path ***";
        String targetKeyName = "*** Target key name ***";
        
        // Create an encryption key.
        KEY_GENERATOR = KeyGenerator.getInstance("AES");
        KEY_GENERATOR.init(256, new SecureRandom());
        SSE_KEY = new SSECustomerKey(KEY_GENERATOR.generateKey());

        try {
            S3_CLIENT = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Upload an object.
            uploadObject(bucketName, keyName, new File(uploadFileName));
    
            // Download the object.
            downloadObject(bucketName, keyName);
    
            // Verify that the object is properly encrypted by attempting to retrieve it
            // using the encryption key.
            retrieveObjectMetadata(bucketName, keyName);
    
            // Copy the object into a new object that also uses SSE-C.
            copyObject(bucketName, keyName, targetKeyName);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void uploadObject(String bucketName, String keyName, File file) {
        PutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName, file).withSSECustomerKey(SSE_KEY);
        S3_CLIENT.putObject(putRequest);
        System.out.println("Object uploaded");
    }

    private static void downloadObject(String bucketName, String keyName) throws IOException {
        GetObjectRequest getObjectRequest = new GetObjectRequest(bucketName, keyName).withSSECustomerKey(SSE_KEY);
        S3Object object = S3_CLIENT.getObject(getObjectRequest);

        System.out.println("Object content: ");
        displayTextInputStream(object.getObjectContent());
    }

    private static void retrieveObjectMetadata(String bucketName, String keyName) {
        GetObjectMetadataRequest getMetadataRequest = new GetObjectMetadataRequest(bucketName, keyName)
                                                                .withSSECustomerKey(SSE_KEY);
        ObjectMetadata objectMetadata = S3_CLIENT.getObjectMetadata(getMetadataRequest);
        System.out.println("Metadata retrieved. Object size: " + objectMetadata.getContentLength());
    }
    
    private static void copyObject(String bucketName, String keyName, String targetKeyName)
            throws NoSuchAlgorithmException {
        // Create a new encryption key for target so that the target is saved using SSE-C.
        SSECustomerKey newSSEKey = new SSECustomerKey(KEY_GENERATOR.generateKey());

        CopyObjectRequest copyRequest = new CopyObjectRequest(bucketName, keyName, bucketName, targetKeyName)
                .withSourceSSECustomerKey(SSE_KEY)
				.withDestinationSSECustomerKey(newSSEKey);

        S3_CLIENT.copyObject(copyRequest);
        System.out.println("Object copied");
    }

    private static void displayTextInputStream(S3ObjectInputStream input) throws IOException {
        // Read one line at a time from the input stream and display each line.
        BufferedReader reader = new BufferedReader(new InputStreamReader(input));
        String line;
        while ((line = reader.readLine()) != null) {
            System.out.println(line);
        }
        System.out.println();
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketVersioningConfiguration;
import com.amazonaws.services.s3.model.DeleteVersionRequest;
import com.amazonaws.services.s3.model.PutObjectResult;

public class DeleteObjectVersionEnabledBucket {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ****";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Check to ensure that the bucket is versioning-enabled.
            String bucketVersionStatus = s3Client.getBucketVersioningConfiguration(bucketName).getStatus();
            if(!bucketVersionStatus.equals(BucketVersioningConfiguration.ENABLED)) {
                System.out.printf("Bucket %s is not versioning-enabled.", bucketName);
            }
            else {
                // Add an object.
                PutObjectResult putResult = s3Client.putObject(bucketName, keyName, "Sample content for deletion example.");
                System.out.printf("Object %s added to bucket %s\n", keyName, bucketName);
        
                // Delete the version of the object that we just created.
                System.out.println("Deleting versioned object " + keyName);
                s3Client.deleteVersion(new DeleteVersionRequest(bucketName, keyName, putResult.getVersionId()));
                System.out.printf("Object %s, version %s deleted\n", keyName, putResult.getVersionId());
            }
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketVersioningConfiguration;
import com.amazonaws.services.s3.model.DeleteObjectsRequest;
import com.amazonaws.services.s3.model.DeleteObjectsRequest.KeyVersion;
import com.amazonaws.services.s3.model.DeleteObjectsResult;
import com.amazonaws.services.s3.model.DeleteObjectsResult.DeletedObject;
import com.amazonaws.services.s3.model.PutObjectResult;

public class DeleteMultipleObjectsVersionEnabledBucket {
    private static AmazonS3 S3_CLIENT;
    private static String VERSIONED_BUCKET_NAME;

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        VERSIONED_BUCKET_NAME = "*** Bucket name ***";
        
        try {
            S3_CLIENT = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Check to make sure that the bucket is versioning-enabled.
            String bucketVersionStatus = S3_CLIENT.getBucketVersioningConfiguration(VERSIONED_BUCKET_NAME).getStatus();
            if(!bucketVersionStatus.equals(BucketVersioningConfiguration.ENABLED)) {
                System.out.printf("Bucket %s is not versioning-enabled.", VERSIONED_BUCKET_NAME);
            }
            else {
                // Upload and delete sample objects, using specific object versions.
                uploadAndDeleteObjectsWithVersions();
        
                // Upload and delete sample objects without specifying version IDs.
                // Amazon S3 creates a delete marker for each object rather than deleting
                // specific versions.
                DeleteObjectsResult unversionedDeleteResult = uploadAndDeleteObjectsWithoutVersions();
        
                // Remove the delete markers placed on objects in the non-versioned create/delete method.
                multiObjectVersionedDeleteRemoveDeleteMarkers(unversionedDeleteResult);
            }
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void uploadAndDeleteObjectsWithVersions() {
        System.out.println("Uploading and deleting objects with versions specified.");

        // Upload three sample objects.
        ArrayList<KeyVersion> keys = new ArrayList<KeyVersion>();
        for (int i = 0; i < 3; i++) {
            String keyName = "delete object without version ID example " + i;
            PutObjectResult putResult = S3_CLIENT.putObject(VERSIONED_BUCKET_NAME, keyName,
                    "Object number " + i + " to be deleted.");
            // Gather the new object keys with version IDs.
            keys.add(new KeyVersion(keyName, putResult.getVersionId()));
        }

        // Delete the specified versions of the sample objects.
        DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest(VERSIONED_BUCKET_NAME)
                .withKeys(keys)
                .withQuiet(false);

        // Verify that the object versions were successfully deleted.
        DeleteObjectsResult delObjRes = S3_CLIENT.deleteObjects(multiObjectDeleteRequest);
        int successfulDeletes = delObjRes.getDeletedObjects().size();
        System.out.println(successfulDeletes + " objects successfully deleted");
    }

    private static DeleteObjectsResult uploadAndDeleteObjectsWithoutVersions() {
        System.out.println("Uploading and deleting objects with no versions specified.");

        // Upload three sample objects.
        ArrayList<KeyVersion> keys = new ArrayList<KeyVersion>();
        for (int i = 0; i < 3; i++) {
            String keyName = "delete object with version ID example " + i;
            S3_CLIENT.putObject(VERSIONED_BUCKET_NAME, keyName, "Object number " + i + " to be deleted.");
            // Gather the new object keys without version IDs.
            keys.add(new KeyVersion(keyName));
        }

        // Delete the sample objects without specifying versions.
        DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest(VERSIONED_BUCKET_NAME).withKeys(keys)
                .withQuiet(false);

        // Verify that delete markers were successfully added to the objects.
        DeleteObjectsResult delObjRes = S3_CLIENT.deleteObjects(multiObjectDeleteRequest);
        int successfulDeletes = delObjRes.getDeletedObjects().size();
        System.out.println(successfulDeletes + " objects successfully marked for deletion without versions.");
        return delObjRes;
    }

    private static void multiObjectVersionedDeleteRemoveDeleteMarkers(DeleteObjectsResult response) {
        List<KeyVersion> keyList = new ArrayList<KeyVersion>();
        for (DeletedObject deletedObject : response.getDeletedObjects()) {
            // Note that the specified version ID is the version ID for the delete marker.
            keyList.add(new KeyVersion(deletedObject.getKey(), deletedObject.getDeleteMarkerVersionId()));
        }
        // Create a request to delete the delete markers.
        DeleteObjectsRequest deleteRequest = new DeleteObjectsRequest(VERSIONED_BUCKET_NAME).withKeys(keyList);

        // Delete the delete markers, leaving the objects intact in the bucket.
        DeleteObjectsResult delObjRes = S3_CLIENT.deleteObjects(deleteRequest);
        int successfulDeletes = delObjRes.getDeletedObjects().size();
        System.out.println(successfulDeletes + " delete markers successfully deleted");
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.util.Arrays;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketLifecycleConfiguration;
import com.amazonaws.services.s3.model.BucketLifecycleConfiguration.Transition;
import com.amazonaws.services.s3.model.StorageClass;
import com.amazonaws.services.s3.model.Tag;
import com.amazonaws.services.s3.model.lifecycle.LifecycleAndOperator;
import com.amazonaws.services.s3.model.lifecycle.LifecycleFilter;
import com.amazonaws.services.s3.model.lifecycle.LifecyclePrefixPredicate;
import com.amazonaws.services.s3.model.lifecycle.LifecycleTagPredicate;

public class LifecycleConfiguration {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        
        // Create a rule to archive objects with the "glacierobjects/" prefix to Glacier immediately.
        BucketLifecycleConfiguration.Rule rule1 = new BucketLifecycleConfiguration.Rule()
                .withId("Archive immediately rule")
                .withFilter(new LifecycleFilter(new LifecyclePrefixPredicate("glacierobjects/")))
                .addTransition(new Transition().withDays(0).withStorageClass(StorageClass.Glacier))
                .withStatus(BucketLifecycleConfiguration.ENABLED);

        // Create a rule to transition objects to the Standard-Infrequent Access storage class 
        // after 30 days, then to Glacier after 365 days. Amazon S3 will delete the objects after 3650 days.
        // The rule applies to all objects with the tag "archive" set to "true". 
        BucketLifecycleConfiguration.Rule rule2 = new BucketLifecycleConfiguration.Rule()
                .withId("Archive and then delete rule")
                .withFilter(new LifecycleFilter(new LifecycleTagPredicate(new Tag("archive", "true"))))
                .addTransition(new Transition().withDays(30).withStorageClass(StorageClass.StandardInfrequentAccess))
                .addTransition(new Transition().withDays(365).withStorageClass(StorageClass.Glacier))
                .withExpirationInDays(3650)
                .withStatus(BucketLifecycleConfiguration.ENABLED);

        // Add the rules to a new BucketLifecycleConfiguration.
        BucketLifecycleConfiguration configuration = new BucketLifecycleConfiguration()
                .withRules(Arrays.asList(rule1, rule2));

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Save the configuration.
            s3Client.setBucketLifecycleConfiguration(bucketName, configuration);
    
            // Retrieve the configuration.
            configuration = s3Client.getBucketLifecycleConfiguration(bucketName);
    
            // Add a new rule with both a prefix predicate and a tag predicate.
            configuration.getRules().add(new BucketLifecycleConfiguration.Rule().withId("NewRule")
                    .withFilter(new LifecycleFilter(new LifecycleAndOperator(
                            Arrays.asList(new LifecyclePrefixPredicate("YearlyDocuments/"),
                                          new LifecycleTagPredicate(new Tag("expire_after", "ten_years"))))))
                    .withExpirationInDays(3650)
                    .withStatus(BucketLifecycleConfiguration.ENABLED));
    
            // Save the configuration.
            s3Client.setBucketLifecycleConfiguration(bucketName, configuration);
    
            // Retrieve the configuration.
            configuration = s3Client.getBucketLifecycleConfiguration(bucketName);
    
            // Verify that the configuration now has three rules.
            configuration = s3Client.getBucketLifecycleConfiguration(bucketName);
            System.out.println("Expected # of rules = 3; found: " + configuration.getRules().size());
    
            // Delete the configuration.
            s3Client.deleteBucketLifecycleConfiguration(bucketName);
    
            // Verify that the configuration has been deleted by attempting to retrieve it.
            configuration = s3Client.getBucketLifecycleConfiguration(bucketName);
            String s = (configuration == null) ? "No configuration found." : "Configuration found.";
            System.out.println(s);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.security.KeyFactory;
import java.security.KeyPair;
import java.security.KeyPairGenerator;
import java.security.NoSuchAlgorithmException;
import java.security.PrivateKey;
import java.security.PublicKey;
import java.security.SecureRandom;
import java.security.spec.InvalidKeySpecException;
import java.security.spec.PKCS8EncodedKeySpec;
import java.security.spec.X509EncodedKeySpec;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3EncryptionClientBuilder;
import com.amazonaws.services.s3.model.EncryptionMaterials;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.model.S3Object;
import com.amazonaws.services.s3.model.StaticEncryptionMaterialsProvider;
import com.amazonaws.util.IOUtils;

public class S3ClientSideEncryptionAsymmetricMasterKey {

    public static void main(String[] args) throws Exception {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String objectKeyName = "*** Key name ***";
        String rsaKeyDir = System.getProperty("java.io.tmpdir");
        String publicKeyName = "public.key";
        String privateKeyName = "private.key";

        // Generate a 1024-bit RSA key pair.
        KeyPairGenerator keyGenerator = KeyPairGenerator.getInstance("RSA");
        keyGenerator.initialize(1024, new SecureRandom());
        KeyPair origKeyPair = keyGenerator.generateKeyPair();

        // To see how it works, save and load the key pair to and from the file system.
        saveKeyPair(rsaKeyDir, publicKeyName, privateKeyName, origKeyPair);
        KeyPair keyPair = loadKeyPair(rsaKeyDir, publicKeyName, privateKeyName, "RSA");

        try {
            // Create the encryption client.
            EncryptionMaterials encryptionMaterials = new EncryptionMaterials(keyPair);
            AmazonS3 s3EncryptionClient = AmazonS3EncryptionClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withEncryptionMaterials(new StaticEncryptionMaterialsProvider(encryptionMaterials))
                    .withRegion(clientRegion)
                    .build();
    
            // Create a new object.
            byte[] plaintext = "S3 Object Encrypted Using Client-Side Asymmetric Master Key.".getBytes();
            S3Object object = new S3Object();
            object.setKey(objectKeyName);
            object.setObjectContent(new ByteArrayInputStream(plaintext));
            ObjectMetadata metadata = new ObjectMetadata();
            metadata.setContentLength(plaintext.length);

            // Upload the object. The encryption client automatically encrypts it.
            PutObjectRequest putRequest = new PutObjectRequest(bucketName, 
                                                               object.getKey(), 
                                                               object.getObjectContent(),
                                                               metadata);
            s3EncryptionClient.putObject(putRequest);
    
            // Download and decrypt the object.
            S3Object downloadedObject = s3EncryptionClient.getObject(bucketName, object.getKey());
            byte[] decrypted = IOUtils.toByteArray(downloadedObject.getObjectContent());
    
            // Verify that the data that you downloaded is the same as the original data.
            System.out.println("Plaintext: " + new String(plaintext));
            System.out.println("Decrypted text: " + new String(decrypted));
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void saveKeyPair(String dir, 
                                   String publicKeyName, 
                                   String privateKeyName, 
                                   KeyPair keyPair) throws IOException {
        PrivateKey privateKey = keyPair.getPrivate();
        PublicKey publicKey = keyPair.getPublic();

        // Write the public key to the specified file.
        X509EncodedKeySpec x509EncodedKeySpec = new X509EncodedKeySpec(publicKey.getEncoded());
        FileOutputStream publicKeyOutputStream = new FileOutputStream(dir + File.separator + publicKeyName);
        publicKeyOutputStream.write(x509EncodedKeySpec.getEncoded());
        publicKeyOutputStream.close();

        // Write the private key to the specified file.
        PKCS8EncodedKeySpec pkcs8EncodedKeySpec = new PKCS8EncodedKeySpec(privateKey.getEncoded());
        FileOutputStream privateKeyOutputStream = new FileOutputStream(dir + File.separator + privateKeyName);
        privateKeyOutputStream.write(pkcs8EncodedKeySpec.getEncoded());
        privateKeyOutputStream.close();
    }

    private static KeyPair loadKeyPair(String dir,
                                      String publicKeyName,
                                      String privateKeyName,
                                      String algorithm)
            throws IOException, NoSuchAlgorithmException, InvalidKeySpecException {
        // Read the public key from the specified file.
        File publicKeyFile = new File(dir + File.separator + publicKeyName);
        FileInputStream publicKeyInputStream = new FileInputStream(publicKeyFile);
        byte[] encodedPublicKey = new byte[(int) publicKeyFile.length()];
        publicKeyInputStream.read(encodedPublicKey);
        publicKeyInputStream.close();

        // Read the private key from the specified file.
        File privateKeyFile = new File(dir + File.separator + privateKeyName);
        FileInputStream privateKeyInputStream = new FileInputStream(privateKeyFile);
        byte[] encodedPrivateKey = new byte[(int) privateKeyFile.length()];
        privateKeyInputStream.read(encodedPrivateKey);
        privateKeyInputStream.close();

        // Convert the keys into a key pair.
        KeyFactory keyFactory = KeyFactory.getInstance(algorithm);
        X509EncodedKeySpec publicKeySpec = new X509EncodedKeySpec(encodedPublicKey);
        PublicKey publicKey = keyFactory.generatePublic(publicKeySpec);

        PKCS8EncodedKeySpec privateKeySpec = new PKCS8EncodedKeySpec(encodedPrivateKey);
        PrivateKey privateKey = keyFactory.generatePrivate(privateKeySpec);

        return new KeyPair(publicKey, privateKey);
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.*;
import com.amazonaws.services.s3.model.*;

public class LowLevelMultipartCopy {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String sourceBucketName = "*** Source bucket name ***";
        String sourceObjectKey = "*** Source object key ***";
        String destBucketName = "*** Target bucket name ***";
        String destObjectKey = "*** Target object key ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Initiate the multipart upload.
            InitiateMultipartUploadRequest initRequest = new InitiateMultipartUploadRequest(destBucketName, destObjectKey);
            InitiateMultipartUploadResult initResult = s3Client.initiateMultipartUpload(initRequest);
    
            // Get the object size to track the end of the copy operation.
            GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest(sourceBucketName, sourceObjectKey);
            ObjectMetadata metadataResult = s3Client.getObjectMetadata(metadataRequest);
            long objectSize = metadataResult.getContentLength();
    
            // Copy the object using 5 MB parts.
            long partSize = 5 * 1024 * 1024;
            long bytePosition = 0;
            int partNum = 1;
            List<CopyPartResult> copyResponses = new ArrayList<CopyPartResult>();
            while (bytePosition < objectSize) {
                // The last part might be smaller than partSize, so check to make sure
                // that lastByte isn't beyond the end of the object.
                long lastByte = Math.min(bytePosition + partSize - 1, objectSize - 1);
                
                // Copy this part.
                CopyPartRequest copyRequest = new CopyPartRequest()
                        .withSourceBucketName(sourceBucketName)
                        .withSourceKey(sourceObjectKey)
                        .withDestinationBucketName(destBucketName)
                        .withDestinationKey(destObjectKey)
                        .withUploadId(initResult.getUploadId())
                        .withFirstByte(bytePosition)
                        .withLastByte(lastByte)
                        .withPartNumber(partNum++);
                copyResponses.add(s3Client.copyPart(copyRequest));
                bytePosition += partSize;
            }
    
            // Complete the upload request to concatenate all uploaded parts and make the copied object available.
            CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest(
                                                                        destBucketName,
                                                                        destObjectKey, 
                                                                        initResult.getUploadId(),
                                                                        getETags(copyResponses));
            s3Client.completeMultipartUpload(completeRequest);
            System.out.println("Multipart copy complete.");
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    // This is a helper function to construct a list of ETags.
    private static List<PartETag> getETags(List<CopyPartResult> responses) {
        List<PartETag> etags = new ArrayList<PartETag>();
        for (CopyPartResult response : responses) {
            etags.add(new PartETag(response.getPartNumber(), response.getETag()));
        }
        return etags;
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketAccelerateConfiguration;
import com.amazonaws.services.s3.model.BucketAccelerateStatus;
import com.amazonaws.services.s3.model.GetBucketAccelerateConfigurationRequest;
import com.amazonaws.services.s3.model.SetBucketAccelerateConfigurationRequest;

public class TransferAcceleration {
    public static void main(String[] args) {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ***";
        
        try {
            // Create an Amazon S3 client that is configured to use the accelerate endpoint.
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .enableAccelerateMode()
                    .build();

            // Enable Transfer Acceleration for the specified bucket.
            s3Client.setBucketAccelerateConfiguration(
                    new SetBucketAccelerateConfigurationRequest(bucketName,
                                                                new BucketAccelerateConfiguration(
                                                                        BucketAccelerateStatus.Enabled)));
    
            // Verify that transfer acceleration is enabled for the bucket.
            String accelerateStatus = s3Client.getBucketAccelerateConfiguration(
                                                    new GetBucketAccelerateConfigurationRequest(bucketName))
                                              .getStatus();
            System.out.println("Bucket accelerate status: " + accelerateStatus);
            
            // Upload a new object using the accelerate endpoint.
            s3Client.putObject(bucketName, keyName, "Test object for transfer acceleration");
            System.out.println("Object \"" + keyName + "\" uploaded with transfer acceleration.");
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.GetObjectRequest;
import com.amazonaws.services.s3.model.ResponseHeaderOverrides;
import com.amazonaws.services.s3.model.S3Object;

public class GetObject {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String key = "*** Object key ***";

        S3Object fullObject = null, objectPortion = null, headerOverrideObject = null;
        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

            // Get an object and print its contents.
            System.out.println("Downloading an object");
            fullObject = s3Client.getObject(new GetObjectRequest(bucketName, key));
            System.out.println("Content-Type: " + fullObject.getObjectMetadata().getContentType());
            System.out.println("Content: ");
            displayTextInputStream(fullObject.getObjectContent());
            
            // Get a range of bytes from an object and print the bytes.
            GetObjectRequest rangeObjectRequest = new GetObjectRequest(bucketName, key)
                                                        .withRange(0,9);
            objectPortion = s3Client.getObject(rangeObjectRequest);
            System.out.println("Printing bytes retrieved.");
            displayTextInputStream(objectPortion.getObjectContent());
            
            // Get an entire object, overriding the specified response headers, and print the object's content.
            ResponseHeaderOverrides headerOverrides = new ResponseHeaderOverrides()
                                                            .withCacheControl("No-cache")
                                                            .withContentDisposition("attachment; filename=example.txt");
            GetObjectRequest getObjectRequestHeaderOverride = new GetObjectRequest(bucketName, key)
                                                            .withResponseHeaders(headerOverrides);
            headerOverrideObject = s3Client.getObject(getObjectRequestHeaderOverride);
            displayTextInputStream(headerOverrideObject.getObjectContent());
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
        finally {
            // To ensure that the network connection doesn't remain open, close any open input streams.
            if(fullObject != null) {
                fullObject.close();
            }
            if(objectPortion != null) {
                objectPortion.close();
            }
            if(headerOverrideObject != null) {
                headerOverrideObject.close();
            }
        }
    }

    private static void displayTextInputStream(InputStream input) throws IOException {
        // Read the text input stream one line at a time and display each line.
        BufferedReader reader = new BufferedReader(new InputStreamReader(input));
        String line = null;
        while ((line = reader.readLine()) != null) {
            System.out.println(line);
        }
        System.out.println();
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.File;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.transfer.TransferManager;
import com.amazonaws.services.s3.transfer.TransferManagerBuilder;
import com.amazonaws.services.s3.transfer.Upload;

public class HighLevelMultipartUpload {

    public static void main(String[] args) throws Exception {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Object key ***";
        String filePath = "*** Path for file to upload ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();
            TransferManager tm = TransferManagerBuilder.standard()
                    .withS3Client(s3Client)
                    .build();
            
            // TransferManager processes all transfers asynchronously,
            // so this call returns immediately.
            Upload upload = tm.upload(bucketName, keyName, new File(filePath));
            System.out.println("Object upload started");
    
            // Optionally, wait for the upload to finish before continuing.
            upload.waitForCompletion();
            System.out.println("Object upload complete");
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListObjectsV2Request;
import com.amazonaws.services.s3.model.ListObjectsV2Result;
import com.amazonaws.services.s3.model.S3ObjectSummary;

public class ListKeys {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        
        try {    
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();
    
            System.out.println("Listing objects");
    
            // maxKeys is set to 2 to demonstrate the use of
            // ListObjectsV2Result.getNextContinuationToken()
            ListObjectsV2Request req = new ListObjectsV2Request().withBucketName(bucketName).withMaxKeys(2);
            ListObjectsV2Result result;

            do {
                result = s3Client.listObjectsV2(req);
    
                for (S3ObjectSummary objectSummary : result.getObjectSummaries()) {
                    System.out.printf(" - %s (size: %d)\n", objectSummary.getKey(), objectSummary.getSize());
                }
                // If there are more than maxKeys keys in the bucket, get a continuation token
                // and list the next objects.
                String token = result.getNextContinuationToken();
                System.out.println("Next Continuation Token: " + token);
                req.setContinuationToken(token);
            } while (result.isTruncated());
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.RestoreObjectRequest;

public class RestoreArchivedObject {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Object key ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Create and submit a request to restore an object from Glacier for two days.
            RestoreObjectRequest requestRestore = new RestoreObjectRequest(bucketName, keyName, 2);
            s3Client.restoreObjectV2(requestRestore);
    
            // Check the restoration status of the object.
            ObjectMetadata response = s3Client.getObjectMetadata(bucketName, keyName);
            Boolean restoreFlag = response.getOngoingRestore();
            System.out.format("Restoration status: %s.\n",
                    restoreFlag ? "in progress" : "not in progress (finished or failed)");
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketReplicationConfiguration;
import com.amazonaws.services.s3.model.ReplicationDestinationConfig;
import com.amazonaws.services.s3.model.ReplicationRule;
import com.amazonaws.services.s3.model.ReplicationRuleStatus;
import com.amazonaws.services.s3.model.StorageClass;

public class CrossRegionReplication {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String accountId = "*** Account ID ***";
        String roleName = "*** Role name ***";
        String sourceBucketName = "*** Source bucket name ***";
        String destBucketName = "*** Destination bucket name ***";
        String prefix = "Tax/";
        
        String roleARN = String.format("arn:aws:iam::%s:role/%s", accountId, roleName);
        String destinationBucketARN = "arn:aws:s3:::" + destBucketName;
   
        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Create the replication rule.
            Map<String, ReplicationRule> replicationRules = new HashMap<String, ReplicationRule>();
            replicationRules.put("ReplicationRule1",
                                 new ReplicationRule()
                                     .withStatus(ReplicationRuleStatus.Enabled)
                                     .withPrefix(prefix)
                                     .withDestinationConfig(new ReplicationDestinationConfig()
                                                                     .withBucketARN(destinationBucketARN)
                                                                     .withStorageClass(StorageClass.Standard)));
            
            // Save the replication rule to the source bucket.
            s3Client.setBucketReplicationConfiguration(sourceBucketName,
                                                       new BucketReplicationConfiguration()
                                                               .withRoleARN(roleARN)
                                                               .withRules(replicationRules));
    
            // Retrieve the replication configuration and verify that the configuration
            // matches the rule we just set.
            BucketReplicationConfiguration replicationConfig = s3Client.getBucketReplicationConfiguration(sourceBucketName);
            ReplicationRule rule = replicationConfig.getRule("ReplicationRule1");
            System.out.println("Retrieved destination bucket ARN: " + rule.getDestinationConfig().getBucketARN());
            System.out.println("Retrieved source-bucket replication rule prefix: " + rule.getPrefix());
            System.out.println("Retrieved source-bucket replication rule status: " + rule.getStatus());
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.util.Date;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.transfer.TransferManager;
import com.amazonaws.services.s3.transfer.TransferManagerBuilder;

public class HighLevelAbortMultipartUpload {

    public static void main(String[] args) {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();
            TransferManager tm = TransferManagerBuilder.standard()
                    .withS3Client(s3Client)
                    .build();
            
            // sevenDays is the duration of seven days in milliseconds.
            long sevenDays = 1000 * 60 * 60 * 24 * 7;
            Date oneWeekAgo = new Date(System.currentTimeMillis() - sevenDays);
            tm.abortMultipartUploads(bucketName, oneWeekAgo);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client couldn't 
            // parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;

public class DualStackEndpoints {

    public static void main(String[] args) {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        try {
            // Create an Amazon S3 client with dual-stack endpoints enabled.
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .withDualstackEnabled(true)
                    .build();

            s3Client.listObjects(bucketName);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.ByteArrayInputStream;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.internal.SSEResultBase;
import com.amazonaws.services.s3.model.CopyObjectRequest;
import com.amazonaws.services.s3.model.CopyObjectResult;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.model.PutObjectResult;

public class SpecifyServerSideEncryption {

    public static void main(String[] args) {
           String clientRegion = "*** Client region ***";
           String bucketName = "*** Bucket name ***";
           String keyNameToEncrypt = "*** Key name for an object to upload and encrypt ***";
           String keyNameToCopyAndEncrypt = "*** Key name for an unencrypted object to be encrypted by copying ***";
           String copiedObjectKeyName = "*** Key name for the encrypted copy of the unencrypted object ***";
           
           try {
               AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                                       .withRegion(clientRegion)
                                       .withCredentials(new ProfileCredentialsProvider())
                                       .build();
           
               // Upload an object and encrypt it with SSE.
               uploadObjectWithSSEEncryption(s3Client, bucketName, keyNameToEncrypt);
               
               // Upload a new unencrypted object, then change its encryption state
               // to encrypted by making a copy.
               changeSSEEncryptionStatusByCopying(s3Client, 
                                                  bucketName, 
                                                  keyNameToCopyAndEncrypt, 
                                                  copiedObjectKeyName);
           }
           catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void uploadObjectWithSSEEncryption(AmazonS3 s3Client, String bucketName, String keyName) {
        String objectContent = "Test object encrypted with SSE";
                
        // Specify server-side encryption.
        ObjectMetadata objectMetadata = new ObjectMetadata();
        objectMetadata.setContentLength(objectContent.length());
        objectMetadata.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
        PutObjectRequest putRequest = new PutObjectRequest(bucketName, 
                                                           keyName, 
                                                           new ByteArrayInputStream(objectContent.getBytes()), 
                                                           objectMetadata);

        // Upload the object and check its encryption status.
        PutObjectResult putResult = s3Client.putObject(putRequest);
        System.out.println("Object \"" + keyName + "\" uploaded with SSE.");
        printEncryptionStatus(putResult);
    }
    
    private static void changeSSEEncryptionStatusByCopying(AmazonS3 s3Client, 
                                                           String bucketName, 
                                                           String sourceKey,
                                                           String destKey) {
        // Upload a new, unencrypted object.
        PutObjectResult putResult = s3Client.putObject(bucketName, sourceKey, "Object example to encrypt by copying");
        System.out.println("Unencrypted object \"" + sourceKey + "\" uploaded.");
        printEncryptionStatus(putResult);
        
        // Make a copy of the object and use server-side encryption when storing the copy.
        CopyObjectRequest request = new CopyObjectRequest(bucketName,
                                                          sourceKey,
                                                          bucketName,
                                                          destKey);
        ObjectMetadata objectMetadata = new ObjectMetadata();
        objectMetadata.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);
        request.setNewObjectMetadata(objectMetadata);
        
        // Perform the copy operation and display the copy's encryption status.
        CopyObjectResult response = s3Client.copyObject(request);
        System.out.println("Object \"" + destKey + "\" uploaded with SSE.");
        printEncryptionStatus(response);
        
        // Delete the original, unencrypted object, leaving only the encrypted copy in Amazon S3.
        s3Client.deleteObject(bucketName, sourceKey);
        System.out.println("Unencrypted object \"" + sourceKey + "\" deleted.");
    }
    
    private static void printEncryptionStatus(SSEResultBase response) {
        String encryptionStatus = response.getSSEAlgorithm();
        if(encryptionStatus == null) {
            encryptionStatus = "Not encrypted with SSE"; 
        }
        System.out.println("Object encryption status is: " + encryptionStatus);
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.util.Iterator;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListVersionsRequest;
import com.amazonaws.services.s3.model.ObjectListing;
import com.amazonaws.services.s3.model.S3ObjectSummary;
import com.amazonaws.services.s3.model.S3VersionSummary;
import com.amazonaws.services.s3.model.VersionListing;

public class DeleteBucket {

    public static void main(String[] args) {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();
    
            // Delete all objects from the bucket. This is sufficient
            // for unversioned buckets. For versioned buckets, when you attempt to delete objects, Amazon S3 inserts
            // delete markers for all objects, but doesn't delete the object versions.
            // To delete objects from versioned buckets, delete all of the object versions before deleting
            // the bucket (see below for an example).
            ObjectListing objectListing = s3Client.listObjects(bucketName);
            while (true) {
                Iterator<S3ObjectSummary> objIter = objectListing.getObjectSummaries().iterator();
                while (objIter.hasNext()) {
                    s3Client.deleteObject(bucketName, objIter.next().getKey());
                }
    
                // If the bucket contains many objects, the listObjects() call
                // might not return all of the objects in the first listing. Check to
                // see whether the listing was truncated. If so, retrieve the next page of objects 
                // and delete them.
                if (objectListing.isTruncated()) {
                    objectListing = s3Client.listNextBatchOfObjects(objectListing);
                } else {
                    break;
                }
            }
    
            // Delete all object versions (required for versioned buckets).
            VersionListing versionList = s3Client.listVersions(new ListVersionsRequest().withBucketName(bucketName));
            while (true) {
                Iterator<S3VersionSummary> versionIter = versionList.getVersionSummaries().iterator();
                while (versionIter.hasNext()) {
                    S3VersionSummary vs = versionIter.next();
                    s3Client.deleteVersion(bucketName, vs.getKey(), vs.getVersionId());
                }
    
                if (versionList.isTruncated()) {
                    versionList = s3Client.listNextBatchOfVersions(versionList);
                } else {
                    break;
                }
            }
    
            // After all objects and object versions are deleted, delete the bucket.
            s3Client.deleteBucket(bucketName);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client couldn't
            // parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CompleteMultipartUploadRequest;
import com.amazonaws.services.s3.model.InitiateMultipartUploadRequest;
import com.amazonaws.services.s3.model.InitiateMultipartUploadResult;
import com.amazonaws.services.s3.model.PartETag;
import com.amazonaws.services.s3.model.UploadPartRequest;
import com.amazonaws.services.s3.model.UploadPartResult;

public class LowLevelMultipartUpload {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ***";
        String filePath = "*** Path to file to upload ***";
        
        File file = new File(filePath);
        long contentLength = file.length();
        long partSize = 5 * 1024 * 1024; // Set part size to 5 MB. 

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                                    .withRegion(clientRegion)
                                    .withCredentials(new ProfileCredentialsProvider())
                                    .build();
                       
            // Create a list of ETag objects. You retrieve ETags for each object part uploaded,
            // then, after each individual part has been uploaded, pass the list of ETags to 
            // the request to complete the upload.
            List<PartETag> partETags = new ArrayList<PartETag>();

            // Initiate the multipart upload.
            InitiateMultipartUploadRequest initRequest = new InitiateMultipartUploadRequest(bucketName, keyName);
            InitiateMultipartUploadResult initResponse = s3Client.initiateMultipartUpload(initRequest);

            // Upload the file parts.
            long filePosition = 0;
            for (int i = 1; filePosition < contentLength; i++) {
                // Because the last part could be less than 5 MB, adjust the part size as needed.
                partSize = Math.min(partSize, (contentLength - filePosition));

                // Create the request to upload a part.
                UploadPartRequest uploadRequest = new UploadPartRequest()
                        .withBucketName(bucketName)
                        .withKey(keyName)
                        .withUploadId(initResponse.getUploadId())
                        .withPartNumber(i)
                        .withFileOffset(filePosition)
                        .withFile(file)
                        .withPartSize(partSize);

                // Upload the part and add the response's ETag to our list.
                UploadPartResult uploadResult = s3Client.uploadPart(uploadRequest);
                partETags.add(uploadResult.getPartETag());

                filePosition += partSize;
            }

            // Complete the multipart upload.
            CompleteMultipartUploadRequest compRequest = new CompleteMultipartUploadRequest(bucketName, keyName,
                    initResponse.getUploadId(), partETags);
            s3Client.completeMultipartUpload(compRequest);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.io.OutputStreamWriter;
import java.net.HttpURLConnection;
import java.net.URL;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.HttpMethod;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.GeneratePresignedUrlRequest;
import com.amazonaws.services.s3.model.S3Object;

public class GeneratePresignedUrlAndUploadObject {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String objectKey = "*** Object key ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();
    
            // Set the pre-signed URL to expire after one hour.
            java.util.Date expiration = new java.util.Date();
            long expTimeMillis = expiration.getTime();
            expTimeMillis += 1000 * 60 * 60;
            expiration.setTime(expTimeMillis);

            // Generate the pre-signed URL.
            System.out.println("Generating pre-signed URL.");
            GeneratePresignedUrlRequest generatePresignedUrlRequest = new GeneratePresignedUrlRequest(bucketName, objectKey)
                    .withMethod(HttpMethod.PUT)
                    .withExpiration(expiration);
            URL url = s3Client.generatePresignedUrl(generatePresignedUrlRequest);
            
            // Create the connection and use it to upload the new object using the pre-signed URL.
            HttpURLConnection connection = (HttpURLConnection) url.openConnection();
            connection.setDoOutput(true);
            connection.setRequestMethod("PUT");
            OutputStreamWriter out = new OutputStreamWriter(connection.getOutputStream());
            out.write("This text uploaded as an object via presigned URL.");
            out.close();
    
            // Check the HTTP response code. To complete the upload and make the object available, 
            // you must interact with the connection object in some way.
            connection.getResponseCode();
            System.out.println("HTTP response code: " + connection.getResponseCode());
    
            // Check to make sure that the object was uploaded successfully.
            S3Object object = s3Client.getObject(bucketName, objectKey);
            System.out.println("Object " + object.getKey() + " created in bucket " + object.getBucketName());
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.File;
import java.util.ArrayList;
import java.util.List;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.*;

public class ManagingObjectTags {

    public static void main(String[] args) {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Object key ***";
        String filePath = "*** File path ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Create an object, add two new tags, and upload the object to Amazon S3.
            PutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName, new File(filePath));
            List<Tag> tags = new ArrayList<Tag>();
            tags.add(new Tag("Tag 1", "This is tag 1"));
            tags.add(new Tag("Tag 2", "This is tag 2"));
            putRequest.setTagging(new ObjectTagging(tags));
            PutObjectResult putResult = s3Client.putObject(putRequest);
    
            // Retrieve the object's tags.
            GetObjectTaggingRequest getTaggingRequest = new GetObjectTaggingRequest(bucketName, keyName);
            GetObjectTaggingResult getTagsResult = s3Client.getObjectTagging(getTaggingRequest);
    
            // Replace the object's tags with two new tags.
            List<Tag> newTags = new ArrayList<Tag>();
            newTags.add(new Tag("Tag 3", "This is tag 3"));
            newTags.add(new Tag("Tag 4", "This is tag 4"));
            s3Client.setObjectTagging(new SetObjectTaggingRequest(bucketName, keyName, new ObjectTagging(newTags)));
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.File;
import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.PutObjectRequest;

public class UploadObject {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String stringObjKeyName = "*** String object key name ***";
        String fileObjKeyName = "*** File object key name ***";
        String fileName = "*** Path to file to upload ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();
        
            // Upload a text string as a new object.
            s3Client.putObject(bucketName, stringObjKeyName, "Uploaded String Object");
            
            // Upload a file as a new object with ContentType and title specified.
            PutObjectRequest request = new PutObjectRequest(bucketName, fileObjKeyName, new File(fileName));
            ObjectMetadata metadata = new ObjectMetadata();
            metadata.setContentType("text/plain");
            metadata.addUserMetadata("x-amz-meta-title", "someTitle");
            request.setMetadata(metadata);
            s3Client.putObject(request);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.BucketCrossOriginConfiguration;
import com.amazonaws.services.s3.model.CORSRule;

public class CORS {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        // Create two CORS rules.
        List<CORSRule.AllowedMethods> rule1AM = new ArrayList<CORSRule.AllowedMethods>();
        rule1AM.add(CORSRule.AllowedMethods.PUT);
        rule1AM.add(CORSRule.AllowedMethods.POST);
        rule1AM.add(CORSRule.AllowedMethods.DELETE);
        CORSRule rule1 = new CORSRule().withId("CORSRule1").withAllowedMethods(rule1AM)
                .withAllowedOrigins(Arrays.asList(new String[] { "http://*.example.com" }));

        List<CORSRule.AllowedMethods> rule2AM = new ArrayList<CORSRule.AllowedMethods>();
        rule2AM.add(CORSRule.AllowedMethods.GET);
        CORSRule rule2 = new CORSRule().withId("CORSRule2").withAllowedMethods(rule2AM)
                .withAllowedOrigins(Arrays.asList(new String[] { "*" })).withMaxAgeSeconds(3000)
                .withExposedHeaders(Arrays.asList(new String[] { "x-amz-server-side-encryption" }));

        List<CORSRule> rules = new ArrayList<CORSRule>();
        rules.add(rule1);
        rules.add(rule2);

        // Add the rules to a new CORS configuration.
        BucketCrossOriginConfiguration configuration = new BucketCrossOriginConfiguration();
        configuration.setRules(rules);

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();
            
            // Add the configuration to the bucket.
            s3Client.setBucketCrossOriginConfiguration(bucketName, configuration);
    
            // Retrieve and display the configuration.
            configuration = s3Client.getBucketCrossOriginConfiguration(bucketName);
            printCORSConfiguration(configuration);
    
            // Add another new rule.
            List<CORSRule.AllowedMethods> rule3AM = new ArrayList<CORSRule.AllowedMethods>();
            rule3AM.add(CORSRule.AllowedMethods.HEAD);
            CORSRule rule3 = new CORSRule().withId("CORSRule3").withAllowedMethods(rule3AM)
                    .withAllowedOrigins(Arrays.asList(new String[] { "http://www.example.com" }));
    
            rules = configuration.getRules();
            rules.add(rule3);
            configuration.setRules(rules);
            s3Client.setBucketCrossOriginConfiguration(bucketName, configuration);
    
            // Verify that the new rule was added by checking the number of rules in the configuration.
            configuration = s3Client.getBucketCrossOriginConfiguration(bucketName);
            System.out.println("Expected # of rules = 3, found " + configuration.getRules().size());
    
            // Delete the configuration.
            s3Client.deleteBucketCrossOriginConfiguration(bucketName);
            System.out.println("Removed CORS configuration.");
    
            // Retrieve and display the configuration to verify that it was
            // successfully deleted.
            configuration = s3Client.getBucketCrossOriginConfiguration(bucketName);
            printCORSConfiguration(configuration);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void printCORSConfiguration(BucketCrossOriginConfiguration configuration) {
        if (configuration == null) {
            System.out.println("Configuration is null.");
        } else {
            System.out.println("Configuration has " + configuration.getRules().size() + " rules\n");

            for (CORSRule rule : configuration.getRules()) {
                System.out.println("Rule ID: " + rule.getId());
                System.out.println("MaxAgeSeconds: " + rule.getMaxAgeSeconds());
                System.out.println("AllowedMethod: " + rule.getAllowedMethods());
                System.out.println("AllowedOrigins: " + rule.getAllowedOrigins());
                System.out.println("AllowedHeaders: " + rule.getAllowedHeaders());
                System.out.println("ExposeHeader: " + rule.getExposedHeaders());
                System.out.println();
            }
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CopyObjectRequest;

public class CopyObjectSingleOperation {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String sourceKey = "*** Source object key *** ";
        String destinationKey = "*** Destination object key ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Copy the object into a new object in the same bucket.
            CopyObjectRequest copyObjRequest = new CopyObjectRequest(bucketName, sourceKey, bucketName, destinationKey);
            s3Client.copyObject(copyObjRequest);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListVersionsRequest;
import com.amazonaws.services.s3.model.S3VersionSummary;
import com.amazonaws.services.s3.model.VersionListing;

public class ListKeysVersioningEnabledBucket {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        
        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                                    .withCredentials(new ProfileCredentialsProvider())
                                    .withRegion(clientRegion)
                                    .build();
            
            // Retrieve the list of versions. If the bucket contains more versions
            // than the specified maximum number of results, Amazon S3 returns
            // one page of results per request.
            ListVersionsRequest request = new ListVersionsRequest()
                .withBucketName(bucketName)
                .withMaxResults(2);
            VersionListing versionListing = s3Client.listVersions(request); 
            int numVersions = 0, numPages = 0;
            while(true) {
                numPages++;
                for (S3VersionSummary objectSummary : 
                    versionListing.getVersionSummaries()) {
                    System.out.printf("Retrieved object %s, version %s\n", 
                                            objectSummary.getKey(), 
                                            objectSummary.getVersionId());
                    numVersions++;
                }
                // Check whether there are more pages of versions to retrieve. If
                // there are, retrieve them. Otherwise, exit the loop.
                if(versionListing.isTruncated()) {
                    versionListing = s3Client.listNextBatchOfVersions(versionListing);
                }
                else {
                    break;
                }
            }
            System.out.println(numVersions + " object versions retrieved in " + numPages + " pages");
         } 
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.AccessControlList;
import com.amazonaws.services.s3.model.CanonicalGrantee;
import com.amazonaws.services.s3.model.EmailAddressGrantee;
import com.amazonaws.services.s3.model.Permission;

public class ModifyACLExistingObject {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ***";
        String emailGrantee = "*** user@example.com ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Get the existing object ACL that we want to modify.
            AccessControlList acl = s3Client.getObjectAcl(bucketName, keyName);
            
            // Clear the existing list of grants.
            acl.getGrantsAsList().clear();
            
            // Grant a sample set of permissions, using the existing ACL owner for Full Control permissions.
            acl.grantPermission(new CanonicalGrantee(acl.getOwner().getId()), Permission.FullControl);
            acl.grantPermission(new EmailAddressGrantee(emailGrantee), Permission.WriteAcp);
    
            // Save the modified ACL back to the object.
            s3Client.setObjectAcl(bucketName, keyName, acl);
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.util.List;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.AbortMultipartUploadRequest;
import com.amazonaws.services.s3.model.ListMultipartUploadsRequest;
import com.amazonaws.services.s3.model.MultipartUpload;
import com.amazonaws.services.s3.model.MultipartUploadListing;

public class LowLevelAbortMultipartUpload {

    public static void main(String[] args) {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withRegion(clientRegion)
                    .withCredentials(new ProfileCredentialsProvider())
                    .build();

            // Find all in-progress multipart uploads.
            ListMultipartUploadsRequest allMultipartUploadsRequest = new ListMultipartUploadsRequest(bucketName);
            MultipartUploadListing multipartUploadListing = s3Client.listMultipartUploads(allMultipartUploadsRequest);
    
            List<MultipartUpload> uploads = multipartUploadListing.getMultipartUploads();
            System.out.println("Before deletions, " + uploads.size() + " multipart uploads in progress.");

            // Abort each upload.
            for (MultipartUpload u : uploads) {
                System.out.println("Upload in progress: Key = \"" + u.getKey() + "\", id = " + u.getUploadId());    
                s3Client.abortMultipartUpload(new AbortMultipartUploadRequest(bucketName, u.getKey(), u.getUploadId()));
                System.out.println("Upload deleted: Key = \"" + u.getKey() + "\", id = " + u.getUploadId());
            }
    
            // Verify that all in-progress multipart uploads have been aborted.
            multipartUploadListing = s3Client.listMultipartUploads(allMultipartUploadsRequest);
            uploads = multipartUploadListing.getMultipartUploads();
            System.out.println("After aborting uploads, " + uploads.size() + " multipart uploads in progress.");
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicSessionCredentials;
import com.amazonaws.auth.policy.Policy;
import com.amazonaws.auth.policy.Resource;
import com.amazonaws.auth.policy.Statement;
import com.amazonaws.auth.policy.Statement.Effect;
import com.amazonaws.auth.policy.actions.S3Actions;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.securitytoken.AWSSecurityTokenService;
import com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder;
import com.amazonaws.services.securitytoken.model.Credentials;
import com.amazonaws.services.securitytoken.model.GetFederationTokenRequest;
import com.amazonaws.services.securitytoken.model.GetFederationTokenResult;
import com.amazonaws.services.s3.model.ObjectListing;

public class MakingRequestsWithFederatedTempCredentials {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Specify bucket name ***";
        String federatedUser = "*** Federated user name ***";
        String resourceARN = "arn:aws:s3:::" + bucketName;

        try {
            AWSSecurityTokenService stsClient = AWSSecurityTokenServiceClientBuilder
                    .standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();
            
            GetFederationTokenRequest getFederationTokenRequest = new GetFederationTokenRequest();
            getFederationTokenRequest.setDurationSeconds(7200);
            getFederationTokenRequest.setName(federatedUser);
            
            // Define the policy and add it to the request.
            Policy policy = new Policy();
            policy.withStatements(new Statement(Effect.Allow)
                    .withActions(S3Actions.ListObjects)
                    .withResources(new Resource(resourceARN)));
            getFederationTokenRequest.setPolicy(policy.toJson());
            
            // Get the temporary security credentials.
            GetFederationTokenResult federationTokenResult = stsClient.getFederationToken(getFederationTokenRequest);
            Credentials sessionCredentials = federationTokenResult.getCredentials();
    
            // Package the session credentials as a BasicSessionCredentials
            // object for an Amazon S3 client object to use.
            BasicSessionCredentials basicSessionCredentials = new BasicSessionCredentials(
                                                                    sessionCredentials.getAccessKeyId(), 
                                                                    sessionCredentials.getSecretAccessKey(),
                                                                    sessionCredentials.getSessionToken());
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                                    .withCredentials(new AWSStaticCredentialsProvider(basicSessionCredentials))
                                    .withRegion(clientRegion)
                                    .build();
    
            // To verify that the client works, send a listObjects request using 
            // the temporary security credentials.
            ObjectListing objects = s3Client.listObjects(bucketName);
            System.out.println("No. of Objects = " + objects.getObjectSummaries().size());
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.security.InvalidKeyException;
import java.security.NoSuchAlgorithmException;
import java.security.spec.InvalidKeySpecException;
import java.security.spec.X509EncodedKeySpec;

import javax.crypto.KeyGenerator;
import javax.crypto.SecretKey;
import javax.crypto.spec.SecretKeySpec;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3EncryptionClientBuilder;
import com.amazonaws.services.s3.model.EncryptionMaterials;
import com.amazonaws.services.s3.model.ObjectMetadata;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.model.S3Object;
import com.amazonaws.services.s3.model.StaticEncryptionMaterialsProvider;

public class S3ClientSideEncryptionSymMasterKey {

    public static void main(String[] args) throws Exception {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String objectKeyName = "*** Object key name ***";
        String masterKeyDir = System.getProperty("java.io.tmpdir");
        String masterKeyName = "secret.key";

        // Generate a symmetric 256-bit AES key.
        KeyGenerator symKeyGenerator = KeyGenerator.getInstance("AES");
        symKeyGenerator.init(256);
        SecretKey symKey = symKeyGenerator.generateKey();

        // To see how it works, save and load the key to and from the file system.
        saveSymmetricKey(masterKeyDir, masterKeyName, symKey);
        symKey = loadSymmetricAESKey(masterKeyDir, masterKeyName, "AES");

        try {
            // Create the Amazon S3 encryption client.
            EncryptionMaterials encryptionMaterials = new EncryptionMaterials(symKey);
            AmazonS3 s3EncryptionClient = AmazonS3EncryptionClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withEncryptionMaterials(new StaticEncryptionMaterialsProvider(encryptionMaterials))
                    .withRegion(clientRegion)
                    .build();

            // Upload a new object. The encryption client automatically encrypts it.
            byte[] plaintext = "S3 Object Encrypted Using Client-Side Symmetric Master Key.".getBytes();
            s3EncryptionClient.putObject(new PutObjectRequest(bucketName, 
                                                            objectKeyName, 
                                                            new ByteArrayInputStream(plaintext), 
                                                            new ObjectMetadata()));
    
            // Download and decrypt the object.
            S3Object downloadedObject = s3EncryptionClient.getObject(bucketName, objectKeyName);
            byte[] decrypted = com.amazonaws.util.IOUtils.toByteArray(downloadedObject.getObjectContent());
    
            // Verify that the data that you downloaded is the same as the original data.
            System.out.println("Plaintext: " + new String(plaintext));
            System.out.println("Decrypted text: " + new String(decrypted));
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }

    private static void saveSymmetricKey(String masterKeyDir, String masterKeyName, SecretKey secretKey) throws IOException {
        X509EncodedKeySpec x509EncodedKeySpec = new X509EncodedKeySpec(secretKey.getEncoded());
        FileOutputStream keyOutputStream = new FileOutputStream(masterKeyDir + File.separator + masterKeyName);
        keyOutputStream.write(x509EncodedKeySpec.getEncoded());
        keyOutputStream.close();
    }

    private static SecretKey loadSymmetricAESKey(String masterKeyDir, String masterKeyName, String algorithm)
            throws IOException, NoSuchAlgorithmException, InvalidKeySpecException, InvalidKeyException {
        // Read the key from the specified file.
        File keyFile = new File(masterKeyDir + File.separator + masterKeyName);
        FileInputStream keyInputStream = new FileInputStream(keyFile);
        byte[] encodedPrivateKey = new byte[(int) keyFile.length()];
        keyInputStream.read(encodedPrivateKey);
        keyInputStream.close();

        // Reconstruct and return the master key.
        return new SecretKeySpec(encodedPrivateKey, "AES");
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;
import java.util.List;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListObjectsRequest;
import com.amazonaws.services.s3.model.ObjectListing;
import com.amazonaws.services.s3.model.S3ObjectSummary;

public class MakingRequests {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();
    
            // Get a list of objects in the bucket, two at a time, and 
            // print the name and size of each object.
            ListObjectsRequest listRequest = new ListObjectsRequest().withBucketName(bucketName).withMaxKeys(2);
            ObjectListing objects = s3Client.listObjects(listRequest);
            while(true) {
                List<S3ObjectSummary> summaries = objects.getObjectSummaries();
                for(S3ObjectSummary summary : summaries) {
                    System.out.printf("Object \"%s\" retrieved with size %d\n", summary.getKey(), summary.getSize());
                }
                if(objects.isTruncated()) {
                    objects = s3Client.listNextBatchOfObjects(objects);
                }
                else {
                    break;
                }
            }
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.DeleteObjectRequest;

public class DeleteObjectNonVersionedBucket {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String keyName = "*** Key name ****";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            s3Client.deleteObject(new DeleteObjectRequest(bucketName, keyName));
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.util.List;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.ListMultipartUploadsRequest;
import com.amazonaws.services.s3.model.MultipartUpload;
import com.amazonaws.services.s3.model.MultipartUploadListing;

public class ListMultipartUploads {

    public static void main(String[] args) {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            // Retrieve a list of all in-progress multipart uploads.
            ListMultipartUploadsRequest allMultipartUploadsRequest = new ListMultipartUploadsRequest(bucketName);
            MultipartUploadListing multipartUploadListing = s3Client.listMultipartUploads(allMultipartUploadsRequest);
            List<MultipartUpload> uploads = multipartUploadListing.getMultipartUploads();
            
            // Display information about all in-progress multipart uploads.
            System.out.println(uploads.size() + " multipart upload(s) in progress.");
            for (MultipartUpload u : uploads) {
                System.out.println("Upload in progress: Key = \"" + u.getKey() + "\", id = " + u.getUploadId());
            }
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client  
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.IOException;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CreateBucketRequest;
import com.amazonaws.services.s3.model.GetBucketLocationRequest;

public class CreateBucket {

    public static void main(String[] args) throws IOException {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                    .withCredentials(new ProfileCredentialsProvider())
                    .withRegion(clientRegion)
                    .build();

            if (!s3Client.doesBucketExistV2(bucketName)) {
                // Because the CreateBucketRequest object doesn't specify a region, the
                // bucket is created in the region specified in the client.
                s3Client.createBucket(new CreateBucketRequest(bucketName));
                
                // Verify that the bucket was created by retrieving it and checking its location.
                String bucketLocation = s3Client.getBucketLocation(new GetBucketLocationRequest(bucketName));
                System.out.println("Bucket location: " + bucketLocation);
            }
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it and returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

import java.io.File;
import java.security.SecureRandom;

import javax.crypto.KeyGenerator;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.SdkClientException;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3ClientBuilder;
import com.amazonaws.services.s3.model.CopyObjectRequest;
import com.amazonaws.services.s3.model.PutObjectRequest;
import com.amazonaws.services.s3.model.SSECustomerKey;
import com.amazonaws.services.s3.transfer.Copy;
import com.amazonaws.services.s3.transfer.TransferManager;
import com.amazonaws.services.s3.transfer.TransferManagerBuilder;
import com.amazonaws.services.s3.transfer.Upload;

public class ServerSideEncryptionCopyObjectUsingHLwithSSEC {

    public static void main(String[] args) throws Exception {
        String clientRegion = "*** Client region ***";
        String bucketName = "*** Bucket name ***";
        String fileToUpload = "*** File path ***";
        String keyName = "*** New object key name ***";
        String targetKeyName = "*** Key name for object copy ***";

        try {
            AmazonS3 s3Client = AmazonS3ClientBuilder.standard()
                                    .withRegion(clientRegion)
                                    .withCredentials(new ProfileCredentialsProvider())
                                    .build();
            TransferManager tm = TransferManagerBuilder.standard()
                                    .withS3Client(s3Client)
                                    .build();

            // Create an object from a file.
            PutObjectRequest putObjectRequest = new PutObjectRequest(bucketName, keyName, new File(fileToUpload));
    
            // Create an encryption key.
            KeyGenerator keyGenerator = KeyGenerator.getInstance("AES");
            keyGenerator.init(256, new SecureRandom());
            SSECustomerKey sseCustomerEncryptionKey = new SSECustomerKey(keyGenerator.generateKey());
    
            // Upload the object. TransferManager uploads asynchronously, so this call returns immediately.
            putObjectRequest.setSSECustomerKey(sseCustomerEncryptionKey);
            Upload upload = tm.upload(putObjectRequest);
            
            // Optionally, wait for the upload to finish before continuing.
            upload.waitForCompletion();
            System.out.println("Object created.");
    
            // Copy the object and store the copy using SSE-C with a new key.
            CopyObjectRequest copyObjectRequest = new CopyObjectRequest(bucketName, keyName, bucketName, targetKeyName);
            SSECustomerKey sseTargetObjectEncryptionKey = new SSECustomerKey(keyGenerator.generateKey());
            copyObjectRequest.setSourceSSECustomerKey(sseCustomerEncryptionKey);
            copyObjectRequest.setDestinationSSECustomerKey(sseTargetObjectEncryptionKey);
    
            // Copy the object. TransferManager copies asynchronously, so this call returns immediately.
            Copy copy = tm.copy(copyObjectRequest);
            
            // Optionally, wait for the upload to finish before continuing.
            copy.waitForCompletion();
            System.out.println("Copy complete.");
        }
        catch(AmazonServiceException e) {
            // The call was transmitted successfully, but Amazon S3 couldn't process 
            // it, so it returned an error response.
            e.printStackTrace();
        }
        catch(SdkClientException e) {
            // Amazon S3 couldn't be contacted for a response, or the client
            // couldn't parse the response from Amazon S3.
            e.printStackTrace();
        }
    }
}


  Copyright 2010-2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.

   This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0
   International License (the "License"). You may not use this file except in compliance with the
   License. A copy of the License is located at http://creativecommons.org/licenses/by-nc-sa/4.0/.

   This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
   either express or implied. See the License for the specific language governing permissions and
   limitations under the License.



Amazon S3 Documentation Java Examples
==============================================


These are the Java examples used in the [Amazon S3 developer documentation](https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html).

Prerequisites
=============

To build and run these examples, you'll need:

* [AWS SDK for Java](https://aws.amazon.com/sdk-for-java/) (downloaded and extracted somewhere on
  your machine)
* AWS credentials, either configured in a local AWS credentials file or by setting the
  ``AWS_ACCESS_KEY_ID`` and ``AWS_SECRET_ACCESS_KEY`` environment variables.

For information about how to set AWS credentials for use with the AWS SDK for Java,
see [Set up AWS Credentials and Region for Development](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-credentials.html) in the *AWS
Java Developer Guide*.

Running the Examples
====================

To run the Java examples, you will need to create a Java project in your preferred Java development environment.
For more information, see [Getting Started](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/getting-started.html). You can also use the
[AWS Toolkit for Eclipse](https://docs.aws.amazon.com/toolkit-for-eclipse/v1/user-guide/welcome.html).

**IMPORTANT**

   The examples perform AWS operations for the account and region for which you've specified
   credentials, and you may incur AWS service charges by running them. Please visit the [AWS Pricing](https://aws.amazon.com/pricing/) page for details about the charges you can
   expect for a given service and operation.

   Some of these examples perform *destructive* operations on AWS resources, such as deleting an
   Amazon S3 bucket. **Be very careful** when running an operation that
   may delete or modify AWS resources in your account. It's best to create separate test-only
   resources when experimenting with these examples.

All of the examples require replacing certain configuration values in the source code. These values
are specified as String variables at the beginning of each example, and begin and end with three stars
(for example, "\*\*\* Client region \*\*\*"). The source-code comments and developer guide provide
further information.



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ManagingObjectACLTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string keyName = "*** object key name ***"; 
        private const string emailAddress = "*** email address ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;
        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            TestObjectACLTestAsync().Wait();
        }

        private static async Task TestObjectACLTestAsync()
        {
            try
            {
                    // Retrieve the ACL for the object.
                    GetACLResponse aclResponse = await client.GetACLAsync(new GetACLRequest
                    {
                        BucketName = bucketName,
                        Key = keyName
                    });

                    S3AccessControlList acl = aclResponse.AccessControlList;

                    // Retrieve the owner (we use this to re-add permissions after we clear the ACL).
                    Owner owner = acl.Owner;

                    // Clear existing grants.
                    acl.Grants.Clear();

                    // Add a grant to reset the owner's full permission (the previous clear statement removed all permissions).
                    S3Grant fullControlGrant = new S3Grant
                    {
                        Grantee = new S3Grantee { CanonicalUser = owner.Id },
                        Permission = S3Permission.FULL_CONTROL
                        
                    };

                    // Describe the grant for the permission using an email address.
                    S3Grant grantUsingEmail = new S3Grant
                    {
                        Grantee = new S3Grantee { EmailAddress = emailAddress },
                        Permission = S3Permission.WRITE_ACP
                    };
                    acl.Grants.AddRange(new List<S3Grant> { fullControlGrant, grantUsingEmail });
 
                    // Set a new ACL.
                    PutACLResponse response = await client.PutACLAsync(new PutACLRequest
                    {
                        BucketName = bucketName,
                        Key = keyName,
                        AccessControlList = acl
                    });
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("An AmazonS3Exception was thrown. Exception: " + amazonS3Exception.ToString());
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CopyObjectUsingMPUapiTest
    {
        private const string sourceBucket = "*** provide the name of the bucket with source object ***";
        private const string targetBucket = "*** provide the name of the bucket to copy the object to ***";
        private const string sourceObjectKey = "*** provide the name of object to copy ***";
        private const string targetObjectKey = "*** provide the name of the object copy ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2; 
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            Console.WriteLine("Copying an object");
            MPUCopyObjectAsync().Wait();
        }
        private static async Task MPUCopyObjectAsync()
        {
            // Create a list to store the upload part responses.
            List<UploadPartResponse> uploadResponses = new List<UploadPartResponse>();
            List<CopyPartResponse> copyResponses = new List<CopyPartResponse>();

            // Setup information required to initiate the multipart upload.
            InitiateMultipartUploadRequest initiateRequest =
                new InitiateMultipartUploadRequest
                {
                    BucketName = targetBucket,
                    Key = targetObjectKey
                };

            // Initiate the upload.
            InitiateMultipartUploadResponse initResponse =
                await s3Client.InitiateMultipartUploadAsync(initiateRequest);

            // Save the upload ID.
            String uploadId = initResponse.UploadId;

            try
            {
                // Get the size of the object.
                GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest
                {
                    BucketName = sourceBucket,
                    Key = sourceObjectKey
                };

                GetObjectMetadataResponse metadataResponse =
                    await s3Client.GetObjectMetadataAsync(metadataRequest);
                long objectSize = metadataResponse.ContentLength; // Length in bytes.

                // Copy the parts.
                long partSize = 5 * (long)Math.Pow(2, 20); // Part size is 5 MB.

                long bytePosition = 0;
                for (int i = 1; bytePosition < objectSize; i++)
                {
                    CopyPartRequest copyRequest = new CopyPartRequest
                    {
                        DestinationBucket = targetBucket,
                        DestinationKey = targetObjectKey,
                        SourceBucket = sourceBucket,
                        SourceKey = sourceObjectKey,
                        UploadId = uploadId,
                        FirstByte = bytePosition,
                        LastByte = bytePosition + partSize - 1 >= objectSize ? objectSize - 1 : bytePosition + partSize - 1,
                        PartNumber = i
                    };

                    copyResponses.Add(await s3Client.CopyPartAsync(copyRequest));

                    bytePosition += partSize;
                }

                // Set up to complete the copy.
                CompleteMultipartUploadRequest completeRequest =
                new CompleteMultipartUploadRequest
                {
                    BucketName = targetBucket,
                    Key = targetObjectKey,
                    UploadId = initResponse.UploadId
                };
                completeRequest.AddPartETags(copyResponses);

                // Complete the copy.
                CompleteMultipartUploadResponse completeUploadResponse = 
                    await s3Client.CompleteMultipartUploadAsync(completeRequest);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ListObjectsVersioningEnabledBucketTest
    {
        static string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main(string[] args)
        {
            s3Client = new AmazonS3Client(bucketRegion);
            GetObjectListWithAllVersionsAsync().Wait();
        }

        static async Task GetObjectListWithAllVersionsAsync()
        {
            try
            {
                ListVersionsRequest request = new ListVersionsRequest()
                {
                    BucketName = bucketName,
                    // You can optionally specify key name prefix in the request
                    // if you want list of object versions of a specific object.

                    // For this example we limit response to return list of 2 versions.
                    MaxKeys = 2
                };
                do
                {
                    ListVersionsResponse response = await s3Client.ListVersionsAsync(request); 
                    // Process response.
                    foreach (S3ObjectVersion entry in response.Versions)
                    {
                        Console.WriteLine("key = {0} size = {1}",
                            entry.Key, entry.Size);
                    }

                    // If response is truncated, set the marker to get the next 
                    // set of keys.
                    if (response.IsTruncated)
                    {
                        request.KeyMarker = response.NextKeyMarker;
                        request.VersionIdMarker = response.NextVersionIdMarker;
                    }
                    else
                    {
                        request = null;
                    }
                } while (request != null);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class UploadObjectTest
    {
        private const string bucketName = "*** bucket name ***";
        // Example creates two objects (for simplicity, we upload same file twice).
        // You specify key names for these objects.
        private const string keyName1 = "*** key name for first object created ***";
        private const string keyName2 = "*** key name for second object created ***";
        private const string filePath = @"*** file path ***";
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.EUWest1; 

        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            WritingAnObjectAsync().Wait();
        }

        static async Task WritingAnObjectAsync()
        {
            try
            {
                // 1. Put object-specify only key name for the new object.
                var putRequest1 = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName1,
                    ContentBody = "sample text"
                };

                PutObjectResponse response1 = await client.PutObjectAsync(putRequest1);

                // 2. Put the object-set ContentType and add metadata.
                var putRequest2 = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName2,
                    FilePath = filePath,
                    ContentType = "text/plain"
                };
                putRequest2.Metadata.Add("x-amz-meta-title", "someTitle");
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine(
                        "Error encountered ***. Message:'{0}' when writing an object"
                        , e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine(
                    "Unknown encountered on server. Message:'{0}' when writing an object"
                    , e.Message);
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.IO;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class GetObjectTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string keyName = "*** object key ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            ReadObjectDataAsync().Wait();
        }

        static async Task ReadObjectDataAsync()
        {
            string responseBody = "";
            try
            {
                GetObjectRequest request = new GetObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName
                };
                using (GetObjectResponse response = await client.GetObjectAsync(request))
                using (Stream responseStream = response.ResponseStream)
                using (StreamReader reader = new StreamReader(responseStream))
                {
                    string title = response.Metadata["x-amz-meta-title"]; // Assume you have "title" as medata added to the object.
                    string contentType = response.Headers["Content-Type"];
                    Console.WriteLine("Object metadata, Title: {0}", title);
                    Console.WriteLine("Content type: {0}", contentType);

                    responseBody = reader.ReadToEnd(); // Now you process the response body.
                }
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Transfer;
using System;
using System.IO;
using System.Threading.Tasks;


namespace Amazon.DocSamples.S3
{
    class UploadDirMPUHighLevelAPITest
    {
        private const string existingBucketName = "*** bucket name ***";
        private const string directoryPath = @"*** directory path ***";
        // The example uploads only .txt files.
        private const string wildCard = "*.txt";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            UploadDirAsync().Wait();
        }

        private static async Task UploadDirAsync()
        {
            try
            {
                var directoryTransferUtility =
                    new TransferUtility(s3Client);

                // 1. Upload a directory.
                await directoryTransferUtility.UploadDirectoryAsync(directoryPath,
                    existingBucketName);
                Console.WriteLine("Upload statement 1 completed");

                // 2. Upload only the .txt files from a directory 
                //    and search recursively. 
                await directoryTransferUtility.UploadDirectoryAsync(
                                               directoryPath,
                                               existingBucketName,
                                               wildCard,
                                               SearchOption.AllDirectories);
                Console.WriteLine("Upload statement 2 completed");

                // 3. The same as Step 2 and some optional configuration. 
                //    Search recursively for .txt files to upload.
                var request = new TransferUtilityUploadDirectoryRequest
                {
                    BucketName = existingBucketName,
                    Directory = directoryPath,
                    SearchOption = SearchOption.AllDirectories,
                    SearchPattern = wildCard
                };

                await directoryTransferUtility.UploadDirectoryAsync(request);
                Console.WriteLine("Upload statement 3 completed");
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine(
                        "Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine(
                    "Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CopyObjectTest
    {
        private const string sourceBucket = "*** provide the name of the bucket with source object ***";
        private const string destinationBucket = "*** provide the name of the bucket to copy the object to ***";
        private const string objectKey = "*** provide the name of object to copy ***";
        private const string destObjectKey = "*** provide the destination object key name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2; 
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            Console.WriteLine("Copying an object");
            CopyingObjectAsync().Wait();
        }

        private static async Task CopyingObjectAsync()
        {
            try
            {
                CopyObjectRequest request = new CopyObjectRequest
                {
                    SourceBucket = sourceBucket,
                    SourceKey = objectKey,
                    DestinationBucket = destinationBucket,
                    DestinationKey = destObjectKey
                };
                CopyObjectResponse response = await s3Client.CopyObjectAsync(request);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;

namespace Amazon.DocSamples.S3
{
    class GenPresignedURLTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string objectKey = "*** object key ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            string urlString = GeneratePreSignedURL();
        }
        static string GeneratePreSignedURL()
        {
            string urlString = "";
            try
            {
                GetPreSignedUrlRequest request1 = new GetPreSignedUrlRequest
                {
                    BucketName = bucketName,
                    Key = objectKey,
                    Expires = DateTime.Now.AddMinutes(5)
                };
                urlString = s3Client.GetPreSignedURL(request1);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            return urlString;
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Transfer;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class AbortMPUUsingHighLevelAPITest
    {
        private const string bucketName = "*** provide bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            AbortMPUAsync().Wait();
        }

        private static async Task AbortMPUAsync()
        {
            try
            {
                var transferUtility = new TransferUtility(s3Client);

                // Abort all in-progress uploads initiated before the specified date.
                await transferUtility.AbortMultipartUploadsAsync(
                    bucketName, DateTime.Now.AddDays(-7));
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        } 
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ManageACLsTest
    {
        private const string bucketName    = "*** existing bucket name ***";
        private const string newBucketName = "*** new bucket name ***";
        private const string keyName = "*** object key name ***"; 
        private const string emailAddress = "***  email address ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            TestBucketObjectACLsAsync().Wait();
        }

        private static async Task TestBucketObjectACLsAsync()
        {
            try
            {
                    // Add a bucket (specify canned ACL).
                    await CreateBucketWithCannedACLAsync();

                    // Get the ACL on a bucket.
                    await GetBucketACLAsync(bucketName);

                    // Add (replace) the ACL on an object in a bucket.
                    await AddACLToExistingObjectAsync(bucketName, keyName);
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("An AmazonS3Exception was thrown. Exception: " + amazonS3Exception.ToString());
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
            }
        }

        static async Task CreateBucketWithCannedACLAsync()
        {
            var request = new PutBucketRequest()
            {
                BucketName = newBucketName,
                BucketRegion = S3Region.EUW1, 
                // Add a canned ACL.
                CannedACL = S3CannedACL.LogDeliveryWrite
            };
            var response = await client.PutBucketAsync(request);
        }

        static async Task GetBucketACLAsync(string bucketName)
        {
            GetACLResponse response = await client.GetACLAsync(new GetACLRequest
            {
                BucketName = bucketName
            });
            S3AccessControlList accessControlList = response.AccessControlList;
        }

        static async Task AddACLToExistingObjectAsync(string bucketName, string keyName)
        {
            // Retrieve the ACL for an object.
            GetACLResponse aclResponse = await client.GetACLAsync(new GetACLRequest
            {
                BucketName = bucketName,
                Key = keyName
            });

            S3AccessControlList acl = aclResponse.AccessControlList;

            // Retrieve the owner.
            Owner owner = acl.Owner;

            // Clear existing grants.
            acl.Grants.Clear();

            // Add a grant to reset the owner's full permission 
            // (the previous clear statement removed all permissions).
            S3Grant fullControlGrant = new S3Grant
            {
                Grantee = new S3Grantee { CanonicalUser = acl.Owner.Id }
            };
            acl.AddGrant(fullControlGrant.Grantee, S3Permission.FULL_CONTROL);

            // Specify email to identify grantee for granting permissions.
            S3Grant grantUsingEmail = new S3Grant
            {
                Grantee = new S3Grantee { EmailAddress = emailAddress },
                Permission = S3Permission.WRITE_ACP
            };

            // Specify log delivery group as grantee. 
            S3Grant grantLogDeliveryGroup = new S3Grant
            {
                Grantee = new S3Grantee { URI = "http://acs.amazonaws.com/groups/s3/LogDelivery" },
                Permission = S3Permission.WRITE
            };

            // Create a new ACL.
            S3AccessControlList newAcl = new S3AccessControlList
            {
                Grants = new List<S3Grant> { grantUsingEmail, grantLogDeliveryGroup },
                Owner = owner
            };

            // Set the new ACL.
            PutACLResponse response = await client.PutACLAsync(new PutACLRequest
            {
                BucketName = bucketName,
                Key = keyName,
                AccessControlList = newAcl
            });
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ListObjectsTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;

        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            ListingObjectsAsync().Wait();
        }

        static async Task ListingObjectsAsync()
        {
            try
            {
                ListObjectsV2Request request = new ListObjectsV2Request
                {
                    BucketName = bucketName,
                    MaxKeys = 10
                };
                ListObjectsV2Response response;
                do
                {
                    response = await client.ListObjectsV2Async(request);

                    // Process the response.
                    foreach (S3Object entry in response.S3Objects)
                    {
                        Console.WriteLine("key = {0} size = {1}",
                            entry.Key, entry.Size);
                    }
                    Console.WriteLine("Next Continuation Token: {0}", response.NextContinuationToken);
                    request.ContinuationToken = response.NextContinuationToken;
                } while (response.IsTruncated);
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("S3 error occurred. Exception: " + amazonS3Exception.ToString());
                Console.ReadKey();
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
                Console.ReadKey();
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    public class ObjectTagsTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string keyName = "*** key name for the new object ***";
        private const string filePath = @"*** file path ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            PutObjectWithTagsTestAsync().Wait();
        }

        static async Task PutObjectWithTagsTestAsync()
        {
            try
            {
                // 1. Put an object with tags.
                var putRequest = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName,
                    FilePath = filePath,
                    TagSet = new List<Tag>{
                        new Tag { Key = "Keyx1", Value = "Value1"},
                        new Tag { Key = "Keyx2", Value = "Value2" }
                    }
                };

                PutObjectResponse response = await client.PutObjectAsync(putRequest);
                // 2. Retrieve the object's tags.
                GetObjectTaggingRequest getTagsRequest = new GetObjectTaggingRequest
                {
                    BucketName = bucketName,
                    Key = keyName
                };

                GetObjectTaggingResponse objectTags = await client.GetObjectTaggingAsync(getTagsRequest);
                for (int i = 0; i < objectTags.Tagging.Count; i++)
                    Console.WriteLine("Key: {0}, Value: {1}", objectTags.Tagging[i].Key, objectTags.Tagging[0].Value);


                // 3. Replace the tagset.

                Tagging newTagSet = new Tagging();
                newTagSet.TagSet = new List<Tag>{
                    new Tag { Key = "Key3", Value = "Value3"},
                    new Tag { Key = "Key4", Value = "Value4" }
                };


                PutObjectTaggingRequest putObjTagsRequest = new PutObjectTaggingRequest()
                {
                    BucketName = bucketName,
                    Key = keyName,
                    Tagging = newTagSet
                };
                PutObjectTaggingResponse response2 = await client.PutObjectTaggingAsync(putObjTagsRequest);

                // 4. Retrieve the object's tags.
                GetObjectTaggingRequest getTagsRequest2 = new GetObjectTaggingRequest();
                getTagsRequest2.BucketName = bucketName;
                getTagsRequest2.Key = keyName;
                GetObjectTaggingResponse objectTags2 = await client.GetObjectTaggingAsync(getTagsRequest2);
                for (int i = 0; i < objectTags2.Tagging.Count; i++)
                    Console.WriteLine("Key: {0}, Value: {1}", objectTags2.Tagging[i].Key, objectTags2.Tagging[0].Value);

            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine(
                        "Error encountered ***. Message:'{0}' when writing an object"
                        , e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine(
                    "Encountered an error. Message:'{0}' when writing an object"
                    , e.Message);
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class SpecifyServerSideEncryptionTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string keyName = "*** key name for object created ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            WritingAnObjectAsync().Wait();
        }

        static async Task WritingAnObjectAsync()
        {
            try
            {
                var putRequest = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName,
                    ContentBody = "sample text",
                    ServerSideEncryptionMethod = ServerSideEncryptionMethod.AES256
                };

                var putResponse = await client.PutObjectAsync(putRequest);

                // Determine the encryption state of an object.
                GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest
                {
                    BucketName = bucketName,
                    Key = keyName
                };
                GetObjectMetadataResponse response = await client.GetObjectMetadataAsync(metadataRequest);
                ServerSideEncryptionMethod objectEncryption = response.ServerSideEncryptionMethod;

                Console.WriteLine("Encryption method used: {0}", objectEncryption.ToString());
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}



﻿<?xml version="1.0" encoding="utf-8"?>
<packages>
  <package id="AWSSDK.Core" version="3.3.21.17" targetFramework="net452" />
  <package id="AWSSDK.S3" version="3.3.17.2" targetFramework="net452" />
  <package id="AWSSDK.SecurityToken" version="3.3.4" targetFramework="net452" />
</packages>


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ManagingBucketACLTest
    {
        private const string newBucketName = "*** bucket name ***"; 
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            CreateBucketUseCannedACLAsync().Wait();
        }

        private static async Task CreateBucketUseCannedACLAsync()
        {
            try
            {
                // Add bucket (specify canned ACL).
                PutBucketRequest putBucketRequest = new PutBucketRequest()
                {
                    BucketName = newBucketName,
                    BucketRegion = S3Region.EUW1, // S3Region.US,
                                                  // Add canned ACL.
                    CannedACL = S3CannedACL.LogDeliveryWrite
                };
                PutBucketResponse putBucketResponse = await client.PutBucketAsync(putBucketRequest);

                // Retrieve bucket ACL.
                GetACLResponse getACLResponse = await client.GetACLAsync(new GetACLRequest
                {
                    BucketName = newBucketName
                });
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("S3 error occurred. Exception: " + amazonS3Exception.ToString());
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
            }
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CORSTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2; 
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            CORSConfigTestAsync().Wait();
        }
        private static async Task CORSConfigTestAsync()
        {
            try
            {
                // Create a new configuration request and add two rules    
                CORSConfiguration configuration = new CORSConfiguration
                {
                    Rules = new System.Collections.Generic.List<CORSRule>
                        {
                          new CORSRule
                          {
                            Id = "CORSRule1",
                            AllowedMethods = new List<string> {"PUT", "POST", "DELETE"},
                            AllowedOrigins = new List<string> {"http://*.example.com"}
                          },
                          new CORSRule
                          {
                            Id = "CORSRule2",
                            AllowedMethods = new List<string> {"GET"},
                            AllowedOrigins = new List<string> {"*"},
                            MaxAgeSeconds = 3000,
                            ExposeHeaders = new List<string> {"x-amz-server-side-encryption"}
                          }
                        }
                };

                // Add the configuration to the bucket. 
                await PutCORSConfigurationAsync(configuration);

                // Retrieve an existing configuration. 
                configuration = await RetrieveCORSConfigurationAsync();

                // Add a new rule.
                configuration.Rules.Add(new CORSRule
                {
                    Id = "CORSRule3",
                    AllowedMethods = new List<string> { "HEAD" },
                    AllowedOrigins = new List<string> { "http://www.example.com" }
                });

                // Add the configuration to the bucket. 
                await PutCORSConfigurationAsync(configuration);

                // Verify that there are now three rules.
                configuration = await RetrieveCORSConfigurationAsync();
                Console.WriteLine();
                Console.WriteLine("Expected # of rulest=3; found:{0}", configuration.Rules.Count);
                Console.WriteLine();
                Console.WriteLine("Pause before configuration delete. To continue, click Enter...");
                Console.ReadKey();

                // Delete the configuration.
                await DeleteCORSConfigurationAsync();

                // Retrieve a nonexistent configuration.
                configuration = await RetrieveCORSConfigurationAsync();
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        static async Task PutCORSConfigurationAsync(CORSConfiguration configuration)
        {

            PutCORSConfigurationRequest request = new PutCORSConfigurationRequest
            {
                BucketName = bucketName,
                Configuration = configuration
            };

            var response = await s3Client.PutCORSConfigurationAsync(request);
        }

        static async Task<CORSConfiguration> RetrieveCORSConfigurationAsync()
        {
            GetCORSConfigurationRequest request = new GetCORSConfigurationRequest
            {
                BucketName = bucketName

            };
            var response = await s3Client.GetCORSConfigurationAsync(request);
            var configuration = response.Configuration;
            PrintCORSRules(configuration);
            return configuration;
        }

        static async Task DeleteCORSConfigurationAsync()
        {
            DeleteCORSConfigurationRequest request = new DeleteCORSConfigurationRequest
            {
                BucketName = bucketName
            };
            await s3Client.DeleteCORSConfigurationAsync(request);
        }

        static void PrintCORSRules(CORSConfiguration configuration)
        {
            Console.WriteLine();

            if (configuration == null)
            {
                Console.WriteLine("\nConfiguration is null");
                return;
            }

            Console.WriteLine("Configuration has {0} rules:", configuration.Rules.Count);
            foreach (CORSRule rule in configuration.Rules)
            {
                Console.WriteLine("Rule ID: {0}", rule.Id);
                Console.WriteLine("MaxAgeSeconds: {0}", rule.MaxAgeSeconds);
                Console.WriteLine("AllowedMethod: {0}", string.Join(", ", rule.AllowedMethods.ToArray()));
                Console.WriteLine("AllowedOrigins: {0}", string.Join(", ", rule.AllowedOrigins.ToArray()));
                Console.WriteLine("AllowedHeaders: {0}", string.Join(", ", rule.AllowedHeaders.ToArray()));
                Console.WriteLine("ExposeHeader: {0}", string.Join(", ", rule.ExposeHeaders.ToArray()));
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class TransferAccelerationTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            EnableAccelerationAsync().Wait();
        }

        static async Task EnableAccelerationAsync()
        {
                try
                {
                    var putRequest = new PutBucketAccelerateConfigurationRequest
                    {
                        BucketName = bucketName,
                        AccelerateConfiguration = new AccelerateConfiguration
                        {
                            Status = BucketAccelerateStatus.Enabled
                        }
                    };
                    await s3Client.PutBucketAccelerateConfigurationAsync(putRequest);

                    var getRequest = new GetBucketAccelerateConfigurationRequest
                    {
                        BucketName = bucketName
                    };
                    var response = await s3Client.GetBucketAccelerateConfigurationAsync(getRequest);

                    Console.WriteLine("Acceleration state = '{0}' ", response.Status);
                }
                catch (AmazonS3Exception amazonS3Exception)
                {
                    Console.WriteLine(
                        "Error occurred. Message:'{0}' when setting transfer acceleration",
                        amazonS3Exception.Message);
                }
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class WebsiteConfigTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string indexDocumentSuffix = "*** index object key ***"; // For example, index.html.
        private const string errorDocument = "*** error object key ***"; // For example, error.html.
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;
        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            AddWebsiteConfigurationAsync(bucketName, indexDocumentSuffix, errorDocument).Wait();
        }

        static async Task AddWebsiteConfigurationAsync(string bucketName,
                                            string indexDocumentSuffix,
                                            string errorDocument)
        {
            try
            {
                // 1. Put the website configuration.
                PutBucketWebsiteRequest putRequest = new PutBucketWebsiteRequest()
                {
                    BucketName = bucketName,
                    WebsiteConfiguration = new WebsiteConfiguration()
                    {
                        IndexDocumentSuffix = indexDocumentSuffix,
                        ErrorDocument = errorDocument
                    }
                };
                PutBucketWebsiteResponse response = await client.PutBucketWebsiteAsync(putRequest);

                // 2. Get the website configuration.
                GetBucketWebsiteRequest getRequest = new GetBucketWebsiteRequest()
                {
                    BucketName = bucketName
                };
                GetBucketWebsiteResponse getResponse = await client.GetBucketWebsiteAsync(getRequest);
                Console.WriteLine("Index document: {0}", getResponse.WebsiteConfiguration.IndexDocumentSuffix);
                Console.WriteLine("Error document: {0}", getResponse.WebsiteConfiguration.ErrorDocument);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DeleteObjectNonVersionedBucketTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string keyName = "*** object key ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            DeleteObjectNonVersionedBucketAsync().Wait();
        }

        private static async Task DeleteObjectNonVersionedBucketAsync()
        {
            try
            {
                var deleteObjectRequest = new DeleteObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName
                };

                Console.WriteLine("Deleting an object");
                await client.DeleteObjectAsync(deleteObjectRequest);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.Runtime;
using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.IO;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class UploadFileMPULowLevelAPITest
    {
        private const string bucketName = "*** provide bucket name ***";
        private const string keyName = "*** provide a name for the uploaded object ***";
        private const string filePath = "*** provide the full path name of the file to upload ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            Console.WriteLine("Uploading an object");
            UploadObjectAsync().Wait(); 
        }

        private static async Task UploadObjectAsync()
        {
            // Create list to store upload part responses.
            List<UploadPartResponse> uploadResponses = new List<UploadPartResponse>();

            // Setup information required to initiate the multipart upload.
            InitiateMultipartUploadRequest initiateRequest = new InitiateMultipartUploadRequest
            {
                BucketName = bucketName,
                Key = keyName
            };

            // Initiate the upload.
            InitiateMultipartUploadResponse initResponse =
                await s3Client.InitiateMultipartUploadAsync(initiateRequest);

            // Upload parts.
            long contentLength = new FileInfo(filePath).Length;
            long partSize = 5 * (long)Math.Pow(2, 20); // 5 MB

            try
            {
                Console.WriteLine("Uploading parts");
        
                long filePosition = 0;
                for (int i = 1; filePosition < contentLength; i++)
                {
                    UploadPartRequest uploadRequest = new UploadPartRequest
                        {
                            BucketName = bucketName,
                            Key = keyName,
                            UploadId = initResponse.UploadId,
                            PartNumber = i,
                            PartSize = partSize,
                            FilePosition = filePosition,
                            FilePath = filePath
                        };

                    // Track upload progress.
                    uploadRequest.StreamTransferProgress +=
                        new EventHandler<StreamTransferProgressArgs>(UploadPartProgressEventCallback);

                    // Upload a part and add the response to our list.
                    uploadResponses.Add(await s3Client.UploadPartAsync(uploadRequest));

                    filePosition += partSize;
                }

                // Setup to complete the upload.
                CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest
                    {
                        BucketName = bucketName,
                        Key = keyName,
                        UploadId = initResponse.UploadId
                     };
                completeRequest.AddPartETags(uploadResponses);

                // Complete the upload.
                CompleteMultipartUploadResponse completeUploadResponse =
                    await s3Client.CompleteMultipartUploadAsync(completeRequest);
            }
            catch (Exception exception)
            {
                Console.WriteLine("An AmazonS3Exception was thrown: { 0}", exception.Message);

                // Abort the upload.
                AbortMultipartUploadRequest abortMPURequest = new AbortMultipartUploadRequest
                {
                    BucketName = bucketName,
                    Key = keyName,
                    UploadId = initResponse.UploadId
                };
               await s3Client.AbortMultipartUploadAsync(abortMPURequest);
            }
        }
        public static void UploadPartProgressEventCallback(object sender, StreamTransferProgressArgs e)
        {
            // Process event. 
            Console.WriteLine("{0}/{1}", e.TransferredBytes, e.TotalBytes);
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.IO;
using System.Net;

namespace Amazon.DocSamples.S3
{
    class UploadObjectUsingPresignedURLTest
    {
        private const string bucketName = "*** provide bucket name ***";
        private const string objectKey  = "*** provide the name for the uploaded object ***";
        private const string filePath   = "*** provide the full path name of the file to upload ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2; 
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            var url = GeneratePreSignedURL();
            UploadObject(url);
        }

        private static void UploadObject(string url)
        {
            HttpWebRequest httpRequest = WebRequest.Create(url) as HttpWebRequest;
            httpRequest.Method = "PUT";
            using (Stream dataStream = httpRequest.GetRequestStream())
            {
                var buffer = new byte[8000];
                using (FileStream fileStream = new FileStream(filePath, FileMode.Open, FileAccess.Read))
                {
                    int bytesRead = 0;
                    while ((bytesRead = fileStream.Read(buffer, 0, buffer.Length)) > 0)
                    {
                        dataStream.Write(buffer, 0, bytesRead);
                    }
                }
            }
            HttpWebResponse response = httpRequest.GetResponse() as HttpWebResponse;
        }

        private static string GeneratePreSignedURL()
        {
            var request = new GetPreSignedUrlRequest
            {
                BucketName = bucketName,
                Key        = objectKey,
                Verb       = HttpVerb.PUT,
                Expires    = DateTime.Now.AddMinutes(5)
            };

           string url = s3Client.GetPreSignedURL(request);
           return url;
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class RestoreArchivedObjectTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string objectKey = "** archived object key name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            RestoreObjectAsync(client, bucketName, objectKey).Wait();
        }

        static async Task RestoreObjectAsync(IAmazonS3 client, string bucketName, string objectKey)
        {
            try
            {
                var restoreRequest = new RestoreObjectRequest
                {
                    BucketName = bucketName,
                    Key = objectKey,
                    Days = 2
                };
                RestoreObjectResponse response = await client.RestoreObjectAsync(restoreRequest);

                // Check the status of the restoration.
                await CheckRestorationStatusAsync(client, bucketName, objectKey);
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("An AmazonS3Exception was thrown. Exception: " + amazonS3Exception.ToString());
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
            }
        }

        static async Task CheckRestorationStatusAsync(IAmazonS3 client, string bucketName, string objectKey)
        {
            GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest
            {
                BucketName = bucketName,
                Key = objectKey
            };
            GetObjectMetadataResponse response = await client.GetObjectMetadataAsync(metadataRequest);
            Console.WriteLine("restoration status: {0}", response.RestoreInProgress ? "in-progress" : "finished or failed");
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class MakeS3RequestTest
    {
        private const string bucketName = "*** bucket name ***"; 
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            using (client = new AmazonS3Client(bucketRegion))
            {
                Console.WriteLine("Listing objects stored in a bucket");
                ListingObjectsAsync().Wait();
            }
        }

        static async Task ListingObjectsAsync()
        {
            try
            {
                ListObjectsRequest request = new ListObjectsRequest
                {
                    BucketName = bucketName,
                    MaxKeys = 2
                };
                do
                {
                    ListObjectsResponse response = await client.ListObjectsAsync(request);
                    // Process the response.
                    foreach (S3Object entry in response.S3Objects)
                    {
                        Console.WriteLine("key = {0} size = {1}",
                            entry.Key, entry.Size);
                    }

                    // If the response is truncated, set the marker to get the next 
                    // set of keys.
                    if (response.IsTruncated)
                    {
                        request.Marker = response.NextMarker;
                    }
                    else
                    {
                        request = null;
                    }
                } while (request != null);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class LifecycleTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;
        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            AddUpdateDeleteLifecycleConfigAsync().Wait();
        }

        private static async Task AddUpdateDeleteLifecycleConfigAsync()
        {
            try
            {
                var lifeCycleConfiguration = new LifecycleConfiguration()
                {
                    Rules = new List<LifecycleRule>
                        {
                            new LifecycleRule
                            {
                                 Id = "Archive immediately rule",
                                 Filter = new LifecycleFilter()
                                 {
                                     LifecycleFilterPredicate = new LifecyclePrefixPredicate()
                                     {
                                         Prefix = "glacierobjects/"
                                     }
                                 },
                                 Status = LifecycleRuleStatus.Enabled,
                                 Transitions = new List<LifecycleTransition>
                                 {
                                      new LifecycleTransition
                                      {
                                           Days = 0,
                                           StorageClass = S3StorageClass.Glacier
                                      }
                                  },
                            },
                            new LifecycleRule
                            {
                                 Id = "Archive and then delete rule",
                                  Filter = new LifecycleFilter()
                                 {
                                     LifecycleFilterPredicate = new LifecyclePrefixPredicate()
                                     {
                                         Prefix = "projectdocs/"
                                     }
                                 },
                                 Status = LifecycleRuleStatus.Enabled,
                                 Transitions = new List<LifecycleTransition>
                                 {
                                      new LifecycleTransition
                                      {
                                           Days = 30,
                                           StorageClass = S3StorageClass.StandardInfrequentAccess
                                      },
                                      new LifecycleTransition
                                      {
                                        Days = 365,
                                        StorageClass = S3StorageClass.Glacier
                                      }
                                 },
                                 Expiration = new LifecycleRuleExpiration()
                                 {
                                       Days = 3650
                                 }
                            }
                        }
                };

                // Add the configuration to the bucket. 
                await AddExampleLifecycleConfigAsync(client, lifeCycleConfiguration);

                // Retrieve an existing configuration. 
                lifeCycleConfiguration = await RetrieveLifecycleConfigAsync(client);

                // Add a new rule.
                lifeCycleConfiguration.Rules.Add(new LifecycleRule
                {
                    Id = "NewRule",
                    Filter = new LifecycleFilter()
                    {
                        LifecycleFilterPredicate = new LifecyclePrefixPredicate()
                        {
                            Prefix = "YearlyDocuments/"
                        }
                    },
                    Expiration = new LifecycleRuleExpiration()
                    {
                        Days = 3650
                    }
                });

                // Add the configuration to the bucket. 
                await AddExampleLifecycleConfigAsync(client, lifeCycleConfiguration);

                // Verify that there are now three rules.
                lifeCycleConfiguration = await RetrieveLifecycleConfigAsync(client);
                Console.WriteLine("Expected # of rulest=3; found:{0}", lifeCycleConfiguration.Rules.Count);

                // Delete the configuration.
                await RemoveLifecycleConfigAsync(client);

                // Retrieve a nonexistent configuration.
                lifeCycleConfiguration = await RetrieveLifecycleConfigAsync(client);

            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        static async Task AddExampleLifecycleConfigAsync(IAmazonS3 client, LifecycleConfiguration configuration)
        {

            PutLifecycleConfigurationRequest request = new PutLifecycleConfigurationRequest
            {
                BucketName = bucketName,
                Configuration = configuration
            };
            var response = await client.PutLifecycleConfigurationAsync(request);
        }

        static async Task<LifecycleConfiguration> RetrieveLifecycleConfigAsync(IAmazonS3 client)
        {
            GetLifecycleConfigurationRequest request = new GetLifecycleConfigurationRequest
            {
                BucketName = bucketName
            };
            var response = await client.GetLifecycleConfigurationAsync(request);
            var configuration = response.Configuration;
            return configuration;
        }

        static async Task RemoveLifecycleConfigAsync(IAmazonS3 client)
        {
            DeleteLifecycleConfigurationRequest request = new DeleteLifecycleConfigurationRequest
            {
                BucketName = bucketName
            };
            await client.DeleteLifecycleConfigurationAsync(request);
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Transfer;
using System;
using System.IO;
using System.Threading.Tasks;


namespace Amazon.DocSamples.S3
{
    class UploadFileMPUHighLevelAPITest
    {
        private const string bucketName = "*** provide bucket name ***";
        private const string keyName = "*** provide a name for the uploaded object ***";
        private const string filePath = "*** provide the full path name of the file to upload ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            UploadFileAsync().Wait();
        }

        private static async Task UploadFileAsync()
        {
            try
            {
                var fileTransferUtility =
                    new TransferUtility(s3Client);

                // Option 1. Upload a file. The file name is used as the object key name.
                await fileTransferUtility.UploadAsync(filePath, bucketName);
                Console.WriteLine("Upload 1 completed");

                // Option 2. Specify object key name explicitly.
                await fileTransferUtility.UploadAsync(filePath, bucketName, keyName);
                Console.WriteLine("Upload 2 completed");

                // Option 3. Upload data from a type of System.IO.Stream.
                using (var fileToUpload = 
                    new FileStream(filePath, FileMode.Open, FileAccess.Read))
                {
                    await fileTransferUtility.UploadAsync(fileToUpload,
                                               bucketName, keyName);
                }
                Console.WriteLine("Upload 3 completed");

                // Option 4. Specify advanced settings.
                var fileTransferUtilityRequest = new TransferUtilityUploadRequest
                {
                    BucketName = bucketName,
                    FilePath = filePath,
                    StorageClass = S3StorageClass.StandardInfrequentAccess,
                    PartSize = 6291456, // 6 MB.
                    Key = keyName,
                    CannedACL = S3CannedACL.PublicRead
                };
                fileTransferUtilityRequest.Metadata.Add("param1", "Value1");
                fileTransferUtilityRequest.Metadata.Add("param2", "Value2");

                await fileTransferUtility.UploadAsync(fileTransferUtilityRequest);
                Console.WriteLine("Upload 4 completed");
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }

        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.Runtime;
using Amazon.S3;
using Amazon.S3.Model;
using Amazon.SecurityToken;
using Amazon.SecurityToken.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class TempFederatedCredentialsTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            ListObjectsAsync().Wait();
        }

        private static async Task ListObjectsAsync()
        {
            try
            {
                Console.WriteLine("Listing objects stored in a bucket");
                // Credentials use the default AWS SDK for .NET credential search chain. 
                // On local development machines, this is your default profile.
                SessionAWSCredentials tempCredentials =
                    await GetTemporaryFederatedCredentialsAsync();

                // Create a client by providing temporary security credentials.
                using (client = new AmazonS3Client(bucketRegion))
                {
                    ListObjectsRequest listObjectRequest = new ListObjectsRequest();
                    listObjectRequest.BucketName = bucketName;

                    ListObjectsResponse response = await client.ListObjectsAsync(listObjectRequest);
                    List<S3Object> objects = response.S3Objects;
                    Console.WriteLine("Object count = {0}", objects.Count);

                    Console.WriteLine("Press any key to continue...");
                    Console.ReadKey();
                }
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        private static async Task<SessionAWSCredentials> GetTemporaryFederatedCredentialsAsync()
        {
            AmazonSecurityTokenServiceConfig config = new AmazonSecurityTokenServiceConfig();
            AmazonSecurityTokenServiceClient stsClient =
                new AmazonSecurityTokenServiceClient(
                                             config);

            GetFederationTokenRequest federationTokenRequest =
                                     new GetFederationTokenRequest();
            federationTokenRequest.DurationSeconds = 7200;
            federationTokenRequest.Name = "User1";
            federationTokenRequest.Policy = @"{
               ""Statement"":
               [
                 {
                   ""Sid"":""Stmt1311212314284"",
                   ""Action"":[""s3:ListBucket""],
                   ""Effect"":""Allow"",
                   ""Resource"":""arn:aws:s3:::" + bucketName + @"""
                  }
               ]
             }
            ";

            GetFederationTokenResponse federationTokenResponse =
                        await stsClient.GetFederationTokenAsync(federationTokenRequest);
            Credentials credentials = federationTokenResponse.Credentials;

            SessionAWSCredentials sessionCredentials =
                new SessionAWSCredentials(credentials.AccessKeyId,
                                          credentials.SecretAccessKey,
                                          credentials.SessionToken);
            return sessionCredentials;
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DualStackEndpointTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            var config = new AmazonS3Config
            {
                UseDualstackEndpoint = true,
                RegionEndpoint = bucketRegion
            };
            client = new AmazonS3Client(config);
            Console.WriteLine("Listing objects stored in a bucket");
            ListingObjectsAsync().Wait();
        }

        private static async Task ListingObjectsAsync()
        {
            try
            {
                var request = new ListObjectsV2Request
                {
                    BucketName = bucketName,
                    MaxKeys = 10
                };
                ListObjectsV2Response response;
                do
                {
                    response = await client.ListObjectsV2Async(request);

                    // Process the response.
                    foreach (S3Object entry in response.S3Objects)
                    {
                        Console.WriteLine("key = {0} size = {1}",
                            entry.Key, entry.Size);
                    }
                    Console.WriteLine("Next Continuation Token: {0}", response.NextContinuationToken);
                    request.ContinuationToken = response.NextContinuationToken;
                } while (response.IsTruncated == true);
            }
            catch (AmazonS3Exception amazonS3Exception)
            {
                Console.WriteLine("An AmazonS3Exception was thrown. Exception: " + amazonS3Exception.ToString());
            }
            catch (Exception e)
            {
                Console.WriteLine("Exception: " + e.ToString());
            }
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DeleteMultipleObjectsNonVersionedBucketTest
    {
        private const string bucketName = "*** versioning-enabled bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            MultiObjectDeleteAsync().Wait();
        }

        static async Task MultiObjectDeleteAsync()
        {
            // Create sample objects (for subsequent deletion).
            var keysAndVersions = await PutObjectsAsync(3);

            // a. multi-object delete by specifying the key names and version IDs.
            DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest
            {
                BucketName = bucketName,
                Objects = keysAndVersions // This includes the object keys and null version IDs.
            };
            // You can add specific object key to the delete request using the .AddKey.
            // multiObjectDeleteRequest.AddKey("TickerReference.csv", null);
            try
            {
                DeleteObjectsResponse response = await s3Client.DeleteObjectsAsync(multiObjectDeleteRequest);
                Console.WriteLine("Successfully deleted all the {0} items", response.DeletedObjects.Count);
            }
            catch (DeleteObjectsException e)
            {
                PrintDeletionErrorStatus(e);
            }
        }

        private static void PrintDeletionErrorStatus(DeleteObjectsException e)
        {
            // var errorResponse = e.ErrorResponse;
            DeleteObjectsResponse errorResponse = e.Response;
            Console.WriteLine("x {0}", errorResponse.DeletedObjects.Count);

            Console.WriteLine("No. of objects successfully deleted = {0}", errorResponse.DeletedObjects.Count);
            Console.WriteLine("No. of objects failed to delete = {0}", errorResponse.DeleteErrors.Count);

            Console.WriteLine("Printing error data...");
            foreach (DeleteError deleteError in errorResponse.DeleteErrors)
            {
                Console.WriteLine("Object Key: {0}\t{1}\t{2}", deleteError.Key, deleteError.Code, deleteError.Message);
            }
        }

        static async Task<List<KeyVersion>> PutObjectsAsync(int number)
        {
            List<KeyVersion> keys = new List<KeyVersion>();
            for (int i = 0; i < number; i++)
            {
                string key = "ExampleObject-" + new System.Random().Next();
                PutObjectRequest request = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = key,
                    ContentBody = "This is the content body!",
                };

                PutObjectResponse response = await s3Client.PutObjectAsync(request);
                KeyVersion keyVersion = new KeyVersion
                {
                    Key = key,
                    // For non-versioned bucket operations, we only need object key.
                    // VersionId = response.VersionId
                };
                keys.Add(keyVersion);
            }
            return keys;
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.IO;
using System.Security.Cryptography;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class SSEClientEncryptionKeyObjectOperationsTest
    {
        private const string bucketName = "*** bucket name ***"; 
        private const string keyName = "*** key name for new object created ***"; 
        private const string copyTargetKeyName = "*** key name for object copy ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            ObjectOpsUsingClientEncryptionKeyAsync().Wait();
        }

        private static async Task ObjectOpsUsingClientEncryptionKeyAsync()
        {
            try
            {
                // Create an encryption key.
                Aes aesEncryption = Aes.Create();
                aesEncryption.KeySize = 256;
                aesEncryption.GenerateKey();
                string base64Key = Convert.ToBase64String(aesEncryption.Key);

                // 1. Upload the object.
                PutObjectRequest putObjectRequest = await UploadObjectAsync(base64Key);
                // 2. Download the object and verify that its contents matches what you uploaded.
                await DownloadObjectAsync(base64Key, putObjectRequest);
                // 3. Get object metadata and verify that the object uses AES-256 encryption.
                await GetObjectMetadataAsync(base64Key);
                // 4. Copy both the source and target objects using server-side encryption with 
                //    a customer-provided encryption key.
                await CopyObjectAsync(aesEncryption, base64Key);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered ***. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        private static async Task<PutObjectRequest> UploadObjectAsync(string base64Key)
        {
            PutObjectRequest putObjectRequest = new PutObjectRequest
            {
                BucketName = bucketName,
                Key = keyName,
                ContentBody = "sample text",
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = base64Key
            };
            PutObjectResponse putObjectResponse = await client.PutObjectAsync(putObjectRequest);
            return putObjectRequest;
        }
        private static async Task DownloadObjectAsync(string base64Key, PutObjectRequest putObjectRequest)
        {
            GetObjectRequest getObjectRequest = new GetObjectRequest
            {
                BucketName = bucketName,
                Key = keyName,
                // Provide encryption information for the object stored in Amazon S3.
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = base64Key
            };

            using (GetObjectResponse getResponse = await client.GetObjectAsync(getObjectRequest))
            using (StreamReader reader = new StreamReader(getResponse.ResponseStream))
            {
                string content = reader.ReadToEnd();
                if (String.Compare(putObjectRequest.ContentBody, content) == 0)
                    Console.WriteLine("Object content is same as we uploaded");
                else
                    Console.WriteLine("Error...Object content is not same.");

                if (getResponse.ServerSideEncryptionCustomerMethod == ServerSideEncryptionCustomerMethod.AES256)
                    Console.WriteLine("Object encryption method is AES256, same as we set");
                else
                    Console.WriteLine("Error...Object encryption method is not the same as AES256 we set");

                // Assert.AreEqual(putObjectRequest.ContentBody, content);
                // Assert.AreEqual(ServerSideEncryptionCustomerMethod.AES256, getResponse.ServerSideEncryptionCustomerMethod);
            }
        }
        private static async Task GetObjectMetadataAsync(string base64Key)
        {
            GetObjectMetadataRequest getObjectMetadataRequest = new GetObjectMetadataRequest
            {
                BucketName = bucketName,
                Key = keyName,

                // The object stored in Amazon S3 is encrypted, so provide the necessary encryption information.
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = base64Key
            };

            GetObjectMetadataResponse getObjectMetadataResponse = await client.GetObjectMetadataAsync(getObjectMetadataRequest);
            Console.WriteLine("The object metadata show encryption method used is: {0}", getObjectMetadataResponse.ServerSideEncryptionCustomerMethod);
            // Assert.AreEqual(ServerSideEncryptionCustomerMethod.AES256, getObjectMetadataResponse.ServerSideEncryptionCustomerMethod);
        }
        private static async Task CopyObjectAsync(Aes aesEncryption, string base64Key)
        {
            aesEncryption.GenerateKey();
            string copyBase64Key = Convert.ToBase64String(aesEncryption.Key);

            CopyObjectRequest copyRequest = new CopyObjectRequest
            {
                SourceBucket = bucketName,
                SourceKey = keyName,
                DestinationBucket = bucketName,
                DestinationKey = copyTargetKeyName,
                // Information about the source object's encryption.
                CopySourceServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                CopySourceServerSideEncryptionCustomerProvidedKey = base64Key,
                // Information about the target object's encryption.
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = copyBase64Key
            };
            await client.CopyObjectAsync(copyRequest);
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CrossRegionReplicationTest
    {
        private const string sourceBucket = "*** source bucket ***";
        // Bucket ARN example - arn:aws:s3:::destinationbucket
        private const string destinationBucketArn = "*** destination bucket ARN ***";
        private const string roleArn = "*** IAM Role ARN ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint sourceBucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        public static void Main()
        {
            s3Client = new AmazonS3Client(sourceBucketRegion);
            EnableReplicationAsync().Wait();
        }
        static async Task EnableReplicationAsync()
        {
            try
            {
                ReplicationConfiguration replConfig = new ReplicationConfiguration
                {
                    Role = roleArn,
                    Rules =
                        {
                            new ReplicationRule
                            {
                                Prefix = "Tax",
                                Status = ReplicationRuleStatus.Enabled,
                                Destination = new ReplicationDestination
                                {
                                    BucketArn = destinationBucketArn
                                }
                            }
                        }
                };

                PutBucketReplicationRequest putRequest = new PutBucketReplicationRequest
                {
                    BucketName = sourceBucket,
                    Configuration = replConfig
                };

                PutBucketReplicationResponse putResponse = await s3Client.PutBucketReplicationAsync(putRequest);

                // Verify configuration by retrieving it.
                await RetrieveReplicationConfigurationAsync(s3Client);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
        private static async Task RetrieveReplicationConfigurationAsync(IAmazonS3 client)
        {
            // Retrieve the configuration.
            GetBucketReplicationRequest getRequest = new GetBucketReplicationRequest
            {
                BucketName = sourceBucket
            };
            GetBucketReplicationResponse getResponse = await client.GetBucketReplicationAsync(getRequest);
            // Print.
            Console.WriteLine("Printing replication configuration information...");
            Console.WriteLine("Role ARN: {0}", getResponse.Configuration.Role);
            foreach (var rule in getResponse.Configuration.Rules)
            {
                Console.WriteLine("ID: {0}", rule.Id);
                Console.WriteLine("Prefix: {0}", rule.Prefix);
                Console.WriteLine("Status: {0}", rule.Status);
            }
        }
    }
}


﻿using System.Reflection;
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;

// General Information about an assembly is controlled through the following 
// set of attributes. Change these attribute values to modify the information
// associated with an assembly.
[assembly: AssemblyTitle("S3Examples")]
[assembly: AssemblyDescription("")]
[assembly: AssemblyConfiguration("")]
[assembly: AssemblyCompany("")]
[assembly: AssemblyProduct("S3Examples")]
[assembly: AssemblyCopyright("Copyright ©  2018")]
[assembly: AssemblyTrademark("")]
[assembly: AssemblyCulture("")]

// Setting ComVisible to false makes the types in this assembly not visible 
// to COM components.  If you need to access a type in this assembly from 
// COM, set the ComVisible attribute to true on that type.
[assembly: ComVisible(false)]

// The following GUID is for the ID of the typelib if this project is exposed to COM
[assembly: Guid("6272109b-98dc-4fc0-b821-badb3b51496b")]

// Version information for an assembly consists of the following four values:
//
//      Major Version
//      Minor Version 
//      Build Number
//      Revision
//
// You can specify all the values or you can default the Build and Revision Numbers 
// by using the '*' as shown below:
// [assembly: AssemblyVersion("1.0.*")]
[assembly: AssemblyVersion("1.0.0.0")]
[assembly: AssemblyFileVersion("1.0.0.0")]



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class ServerAccesLoggingTest
    {
        private const string bucketName = "*** bucket name for which to enable logging ***"; 
        private const string targetBucketName = "*** bucket name where you want access logs stored ***"; 
        private const string logObjectKeyPrefix = "Logs";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            EnableLoggingAsync().Wait();
        }

        private static async Task EnableLoggingAsync()
        {
            try
            {
                // Step 1 - Grant Log Delivery group permission to write log to the target bucket.
                await GrantPermissionsToWriteLogsAsync();
                // Step 2 - Enable logging on the source bucket.
                await EnableDisableLoggingAsync();
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        private static async Task GrantPermissionsToWriteLogsAsync()
        {
            var bucketACL = new S3AccessControlList();
            var aclResponse = client.GetACL(new GetACLRequest { BucketName = targetBucketName });
            bucketACL = aclResponse.AccessControlList;
            bucketACL.AddGrant(new S3Grantee { URI = "http://acs.amazonaws.com/groups/s3/LogDelivery" }, S3Permission.WRITE);
            bucketACL.AddGrant(new S3Grantee { URI = "http://acs.amazonaws.com/groups/s3/LogDelivery" }, S3Permission.READ_ACP);
            var setACLRequest = new PutACLRequest
            {
                AccessControlList = bucketACL,
                BucketName = targetBucketName
            };
            await client.PutACLAsync(setACLRequest);
        }

        private static async Task EnableDisableLoggingAsync()
        {
            var loggingConfig = new S3BucketLoggingConfig
            {
                TargetBucketName = targetBucketName,
                TargetPrefix = logObjectKeyPrefix
            };

            // Send request.
            var putBucketLoggingRequest = new PutBucketLoggingRequest
            {
                BucketName = bucketName,
                LoggingConfig = loggingConfig
            };
            await client.PutBucketLoggingAsync(putBucketLoggingRequest);
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DeleteObjectVersion
    {
        private const string bucketName = "*** versioning-enabled bucket name ***";
        private const string keyName = "*** Object Key Name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            CreateAndDeleteObjectVersionAsync().Wait();
        }

        private static async Task CreateAndDeleteObjectVersionAsync()
        {
            try
            {
                // Add a sample object. 
                string versionID = await PutAnObject(keyName);

                // Delete the object by specifying an object key and a version ID.
                DeleteObjectRequest request = new DeleteObjectRequest
                {
                    BucketName = bucketName,
                    Key = keyName,
                    VersionId = versionID
                };
                Console.WriteLine("Deleting an object");
                await client.DeleteObjectAsync(request);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        static async Task<string> PutAnObject(string objectKey)
        {
            PutObjectRequest request = new PutObjectRequest
            {
                BucketName = bucketName,
                Key = objectKey,
                ContentBody = "This is the content body!"
            };
            PutObjectResponse response = await client.PutObjectAsync(request);
            return response.VersionId;
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class EnableNotificationsTest
    {
        private const string bucketName = "*** bucket name ***";
        private const string snsTopic = "*** SNS topic ARN ***";
        private const string sqsQueue = "*** SQS topic ARN ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 client;

        public static void Main()
        {
            client = new AmazonS3Client(bucketRegion);
            EnableNotificationAsync().Wait();
        }

        static async Task EnableNotificationAsync()
        {
            try
            {
                PutBucketNotificationRequest request = new PutBucketNotificationRequest
                {
                    BucketName = bucketName
                };

                TopicConfiguration c = new TopicConfiguration
                {
                    Events = new List<EventType> { EventType.ObjectCreatedCopy },
                    Topic = snsTopic
                };
                request.TopicConfigurations = new List<TopicConfiguration>();
                request.TopicConfigurations.Add(c);
                request.QueueConfigurations = new List<QueueConfiguration>();
                request.QueueConfigurations.Add(new QueueConfiguration()
                {
                    Events = new List<EventType> { EventType.ObjectCreatedPut },
                    Queue = sqsQueue
                });
                
                PutBucketNotificationResponse response = await client.PutBucketNotificationAsync(request);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' ", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown error encountered on server. Message:'{0}' ", e.Message);
            }
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Transfer;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class TrackMPUUsingHighLevelAPITest
    {
        private const string bucketName = "*** provide the bucket name ***";
        private const string keyName = "*** provide the name for the uploaded object ***";
        private const string filePath = " *** provide the full path name of the file to upload **";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;


        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            TrackMPUAsync().Wait();
        }

        private static async Task TrackMPUAsync()
        {
            try
            {
                var fileTransferUtility = new TransferUtility(s3Client);

                // Use TransferUtilityUploadRequest to configure options.
                // In this example we subscribe to an event.
                var uploadRequest =
                    new TransferUtilityUploadRequest
                    {
                        BucketName = bucketName,
                        FilePath = filePath,
                        Key = keyName
                    };

                uploadRequest.UploadProgressEvent +=
                    new EventHandler<UploadProgressArgs>
                        (uploadRequest_UploadPartProgressEvent);

                await fileTransferUtility.UploadAsync(uploadRequest);
                Console.WriteLine("Upload completed");
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }

        static void uploadRequest_UploadPartProgressEvent(object sender, UploadProgressArgs e)
        {
            // Process event.
            Console.WriteLine("{0}/{1}", e.TransferredBytes, e.TotalBytes);
        }
    }
}



// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.Runtime;
using Amazon.S3;
using Amazon.S3.Model;
using Amazon.SecurityToken;
using Amazon.SecurityToken.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class TempCredExplicitSessionStartTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        public static void Main()
        {
            ListObjectsAsync().Wait();
        }

        private static async Task ListObjectsAsync()
        {
            try
            {
                // Credentials use the default AWS SDK for .NET credential search chain. 
                // On local development machines, this is your default profile.
                Console.WriteLine("Listing objects stored in a bucket");
                SessionAWSCredentials tempCredentials = await GetTemporaryCredentialsAsync();

                // Create a client by providing temporary security credentials.
                using (s3Client = new AmazonS3Client(tempCredentials, bucketRegion))
                {
                    var listObjectRequest = new ListObjectsRequest
                    {
                        BucketName = bucketName
                    };
                    // Send request to Amazon S3.
                    ListObjectsResponse response = await s3Client.ListObjectsAsync(listObjectRequest);
                    List<S3Object> objects = response.S3Objects;
                    Console.WriteLine("Object count = {0}", objects.Count);
                }
            }
            catch (AmazonS3Exception s3Exception)
            {
                Console.WriteLine(s3Exception.Message, s3Exception.InnerException);
            }
            catch (AmazonSecurityTokenServiceException stsException)
            {
                Console.WriteLine(stsException.Message, stsException.InnerException);
            }
        }

        private static async Task<SessionAWSCredentials> GetTemporaryCredentialsAsync()
        {
            using (var stsClient = new AmazonSecurityTokenServiceClient())
            {
                var getSessionTokenRequest = new GetSessionTokenRequest
                {
                    DurationSeconds = 7200 // seconds
                };

                GetSessionTokenResponse sessionTokenResponse =
                              await stsClient.GetSessionTokenAsync(getSessionTokenRequest);

                Credentials credentials = sessionTokenResponse.Credentials;

                var sessionCredentials =
                    new SessionAWSCredentials(credentials.AccessKeyId,
                                              credentials.SecretAccessKey,
                                              credentials.SessionToken);
                return sessionCredentials;
            }
        }
    }
}


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class DeleteMultipleObjVersionedBucketTest
    {
        private const string bucketName = "*** versioning-enabled bucket name ***"; 
       // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;

        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            DeleteMultipleObjectsFromVersionedBucketAsync().Wait();
        }

        private static async Task DeleteMultipleObjectsFromVersionedBucketAsync()
        {

            // Delete objects (specifying object version in the request).
            await DeleteObjectVersionsAsync();

            // Delete objects (without specifying object version in the request). 
            var deletedObjects = await DeleteObjectsAsync();

            // Additional exercise - remove the delete markers S3 returned in the preceding response. 
            // This results in the objects reappearing in the bucket (you can 
            // verify the appearance/disappearance of objects in the console).
            await RemoveDeleteMarkersAsync(deletedObjects);
        }

        private static async Task<List<DeletedObject>> DeleteObjectsAsync()
        {
            // Upload the sample objects.
            var keysAndVersions2 = await PutObjectsAsync(3);

            // Delete objects using only keys. Amazon S3 creates a delete marker and 
            // returns its version ID in the response.
            List<DeletedObject> deletedObjects = await NonVersionedDeleteAsync(keysAndVersions2);
            return deletedObjects;
        }

        private static async Task DeleteObjectVersionsAsync()
        {
            // Upload the sample objects.
            var keysAndVersions1 = await PutObjectsAsync(3);

            // Delete the specific object versions.
            await VersionedDeleteAsync(keysAndVersions1);
        }

        private static void PrintDeletionReport(DeleteObjectsException e)
        {
            var errorResponse = e.Response;
            Console.WriteLine("No. of objects successfully deleted = {0}", errorResponse.DeletedObjects.Count);
            Console.WriteLine("No. of objects failed to delete = {0}", errorResponse.DeleteErrors.Count);
            Console.WriteLine("Printing error data...");
            foreach (var deleteError in errorResponse.DeleteErrors)
            {
                Console.WriteLine("Object Key: {0}\t{1}\t{2}", deleteError.Key, deleteError.Code, deleteError.Message);
            }
        }

        static async Task VersionedDeleteAsync(List<KeyVersion> keys)
        {
            // a. Perform a multi-object delete by specifying the key names and version IDs.
            var multiObjectDeleteRequest = new DeleteObjectsRequest
            {
                BucketName = bucketName,
                Objects = keys // This includes the object keys and specific version IDs.
            };
            try
            {
                Console.WriteLine("Executing VersionedDelete...");
                DeleteObjectsResponse response = await s3Client.DeleteObjectsAsync(multiObjectDeleteRequest);
                Console.WriteLine("Successfully deleted all the {0} items", response.DeletedObjects.Count);
            }
            catch (DeleteObjectsException e)
            {
                PrintDeletionReport(e);
            }
        }

        static async Task<List<DeletedObject>> NonVersionedDeleteAsync(List<KeyVersion> keys)
        {
            // Create a request that includes only the object key names.
            DeleteObjectsRequest multiObjectDeleteRequest = new DeleteObjectsRequest();
            multiObjectDeleteRequest.BucketName = bucketName;

            foreach (var key in keys)
            {
                multiObjectDeleteRequest.AddKey(key.Key);
            }
            // Execute DeleteObjects - Amazon S3 add delete marker for each object
            // deletion. The objects disappear from your bucket. 
            // You can verify that using the Amazon S3 console.
            DeleteObjectsResponse response;
            try
            {
                Console.WriteLine("Executing NonVersionedDelete...");
                response = await s3Client.DeleteObjectsAsync(multiObjectDeleteRequest);
                Console.WriteLine("Successfully deleted all the {0} items", response.DeletedObjects.Count);
            }
            catch (DeleteObjectsException e)
            {
                PrintDeletionReport(e);
                throw; // Some deletes failed. Investigate before continuing.
            }
            // This response contains the DeletedObjects list which we use to delete the delete markers.
            return response.DeletedObjects;
        }

        private static async Task RemoveDeleteMarkersAsync(List<DeletedObject> deletedObjects)
        {
            var keyVersionList = new List<KeyVersion>();

            foreach (var deletedObject in deletedObjects)
            {
                KeyVersion keyVersion = new KeyVersion
                {
                    Key = deletedObject.Key,
                    VersionId = deletedObject.DeleteMarkerVersionId
                };
                keyVersionList.Add(keyVersion);
            }
            // Create another request to delete the delete markers.
            var multiObjectDeleteRequest = new DeleteObjectsRequest
            {
                BucketName = bucketName,
                Objects = keyVersionList
            };

            // Now, delete the delete marker to bring your objects back to the bucket.
            try
            {
                Console.WriteLine("Removing the delete markers .....");
                var deleteObjectResponse = await s3Client.DeleteObjectsAsync(multiObjectDeleteRequest);
                Console.WriteLine("Successfully deleted all the {0} delete markers",
                                            deleteObjectResponse.DeletedObjects.Count);
            }
            catch (DeleteObjectsException e)
            {
                PrintDeletionReport(e);
            }
        }

        static async Task<List<KeyVersion>> PutObjectsAsync(int number)
        {
            var keys = new List<KeyVersion>();

            for (var i = 0; i < number; i++)
            {
                string key = "ObjectToDelete-" + new System.Random().Next();
                PutObjectRequest request = new PutObjectRequest
                {
                    BucketName = bucketName,
                    Key = key,
                    ContentBody = "This is the content body!",

                };

                var response = await s3Client.PutObjectAsync(request);
                KeyVersion keyVersion = new KeyVersion
                {
                    Key = key,
                    VersionId = response.VersionId
                };

                keys.Add(keyVersion);
            }
            return keys;
        }
    }
}


﻿<?xml version="1.0" encoding="utf-8"?>
<Project ToolsVersion="14.0" DefaultTargets="Build" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <Import Project="$(MSBuildExtensionsPath)\$(MSBuildToolsVersion)\Microsoft.Common.props" Condition="Exists('$(MSBuildExtensionsPath)\$(MSBuildToolsVersion)\Microsoft.Common.props')" />
  <PropertyGroup>
    <Configuration Condition=" '$(Configuration)' == '' ">Debug</Configuration>
    <Platform Condition=" '$(Platform)' == '' ">AnyCPU</Platform>
    <ProjectGuid>{6272109B-98DC-4FC0-B821-BADB3B51496B}</ProjectGuid>
    <OutputType>Exe</OutputType>
    <AppDesignerFolder>Properties</AppDesignerFolder>
    <RootNamespace>Amazon.DocSamples.S3</RootNamespace>
    <AssemblyName>Amazon.DocSamples.S3</AssemblyName>
    <TargetFrameworkVersion>v4.5.2</TargetFrameworkVersion>
    <FileAlignment>512</FileAlignment>
    <AutoGenerateBindingRedirects>true</AutoGenerateBindingRedirects>
  </PropertyGroup>
  <PropertyGroup Condition=" '$(Configuration)|$(Platform)' == 'Debug|AnyCPU' ">
    <PlatformTarget>AnyCPU</PlatformTarget>
    <DebugSymbols>true</DebugSymbols>
    <DebugType>full</DebugType>
    <Optimize>false</Optimize>
    <OutputPath>bin\Debug\</OutputPath>
    <DefineConstants>DEBUG;TRACE</DefineConstants>
    <ErrorReport>prompt</ErrorReport>
    <WarningLevel>4</WarningLevel>
  </PropertyGroup>
  <PropertyGroup Condition=" '$(Configuration)|$(Platform)' == 'Release|AnyCPU' ">
    <PlatformTarget>AnyCPU</PlatformTarget>
    <DebugType>pdbonly</DebugType>
    <Optimize>true</Optimize>
    <OutputPath>bin\Release\</OutputPath>
    <DefineConstants>TRACE</DefineConstants>
    <ErrorReport>prompt</ErrorReport>
    <WarningLevel>4</WarningLevel>
  </PropertyGroup>
  <PropertyGroup>
    <StartupObject>Amazon.DocSamples.S3.EnableNotificationsTest</StartupObject>
  </PropertyGroup>
  <ItemGroup>
    <Reference Include="AWSSDK.Core, Version=3.3.0.0, Culture=neutral, PublicKeyToken=885c28607f98e604, processorArchitecture=MSIL">
      <HintPath>..\packages\AWSSDK.Core.3.3.21.17\lib\net45\AWSSDK.Core.dll</HintPath>
      <Private>True</Private>
    </Reference>
    <Reference Include="AWSSDK.S3, Version=3.3.0.0, Culture=neutral, PublicKeyToken=885c28607f98e604, processorArchitecture=MSIL">
      <HintPath>..\packages\AWSSDK.S3.3.3.17.2\lib\net45\AWSSDK.S3.dll</HintPath>
      <Private>True</Private>
    </Reference>
    <Reference Include="AWSSDK.SecurityToken, Version=3.3.0.0, Culture=neutral, PublicKeyToken=885c28607f98e604, processorArchitecture=MSIL">
      <HintPath>..\packages\AWSSDK.SecurityToken.3.3.4\lib\net45\AWSSDK.SecurityToken.dll</HintPath>
      <Private>True</Private>
    </Reference>
    <Reference Include="System" />
    <Reference Include="System.Core" />
    <Reference Include="System.Xml.Linq" />
    <Reference Include="System.Data.DataSetExtensions" />
    <Reference Include="Microsoft.CSharp" />
    <Reference Include="System.Data" />
    <Reference Include="System.Net.Http" />
    <Reference Include="System.Xml" />
  </ItemGroup>
  <ItemGroup>
    <Compile Include="AbortMPUUsingHighLevelAPITest.cs" />
    <Compile Include="CopyObjectTest.cs" />
    <Compile Include="CopyObjectUsingMPUapiTest.cs" />
    <Compile Include="CORSTest.cs" />
    <Compile Include="CreateBucketTest.cs" />
    <Compile Include="CrossRegionReplicationTest.cs" />
    <Compile Include="DeleteMultipleObjectsNonVersionedBucketTest.cs" />
    <Compile Include="DeleteMultipleObjVersionedBucketTest.cs" />
    <Compile Include="DeleteObjectNonVersionedBucketTest1.cs" />
    <Compile Include="DeleteObjectVersion.cs" />
    <Compile Include="DualStackEndpointTest.cs" />
    <Compile Include="EnableNotificationsTest.cs" />
    <Compile Include="GenPresignedURLTest.cs" />
    <Compile Include="GetObjectTest.cs" />
    <Compile Include="LifecycleTest.cs" />
    <Compile Include="ListObjectsTest.cs" />
    <Compile Include="MakeS3RequestTest.cs" />
    <Compile Include="ManageingBucketACLTest.cs" />
    <Compile Include="ManagingACLsTest.cs" />
    <Compile Include="ManagingObjectACLTest.cs" />
    <Compile Include="ObjectTagTest.cs" />
    <Compile Include="Properties\AssemblyInfo.cs" />
    <Compile Include="RestoreArchivedObjectTest.cs" />
    <Compile Include="ServerAccesLoggingTest.cs" />
    <Compile Include="SpecifyServerSideEncryptionTest.cs" />
    <Compile Include="SSEClientEncryptionKeyObjectOperationsTest.cs" />
    <Compile Include="SSECLowLevelMPUcopyObjectTest.cs" />
    <Compile Include="TempCredExplicitSessionStartTest.cs" />
    <Compile Include="TempFederatedCredentialsTest.cs" />
    <Compile Include="TrackMPUUsingHighLevelAPITest.cs" />
    <Compile Include="TransferAccelerationTest.cs" />
    <Compile Include="UploadDirMPUHighLevelAPITest.cs" />
    <Compile Include="UploadFileMPUHighLevelAPITest.cs" />
    <Compile Include="UploadFileMPULowLevelAPITest.cs" />
    <Compile Include="UploadObjectTest.cs" />
    <Compile Include="UploadObjectUsingPresignedURLTest.cs" />
    <Compile Include="WebsiteConfigTest.cs" />
  </ItemGroup>
  <ItemGroup>
    <None Include="App.config" />
    <None Include="packages.config" />
  </ItemGroup>
  <ItemGroup>
    <Analyzer Include="..\packages\AWSSDK.S3.3.3.17.2\analyzers\dotnet\cs\AWSSDK.S3.CodeAnalysis.dll" />
    <Analyzer Include="..\packages\AWSSDK.SecurityToken.3.3.4\analyzers\dotnet\cs\AWSSDK.SecurityToken.CodeAnalysis.dll" />
  </ItemGroup>
  <Import Project="$(MSBuildToolsPath)\Microsoft.CSharp.targets" />
  <!-- To modify your build process, add your task inside one of the targets below and uncomment it. 
       Other similar extension points exist, see Microsoft.Common.targets.
  <Target Name="BeforeBuild">
  </Target>
  <Target Name="AfterBuild">
  </Target>
  -->
</Project>


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using Amazon.S3.Util;
using System;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class CreateBucketTest
    {
        private const string bucketName = "*** bucket name ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        public static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            CreateBucketAsync().Wait();
        }

        static async Task CreateBucketAsync()
        {
            try
            {
                if (!(await AmazonS3Util.DoesS3BucketExistAsync(s3Client, bucketName)))
                {
                    var putBucketRequest = new PutBucketRequest
                    {
                        BucketName = bucketName,
                        UseClientRegion = true
                    };

                    PutBucketResponse putBucketResponse = await s3Client.PutBucketAsync(putBucketRequest);
                }
                // Retrieve the bucket location.
                string bucketLocation = await FindBucketLocationAsync(s3Client);
            }
            catch (AmazonS3Exception e)
            {
                Console.WriteLine("Error encountered on server. Message:'{0}' when writing an object", e.Message);
            }
            catch (Exception e)
            {
                Console.WriteLine("Unknown encountered on server. Message:'{0}' when writing an object", e.Message);
            }
        }
        static async Task<string> FindBucketLocationAsync(IAmazonS3 client)
        {
            string bucketLocation;
            var request = new GetBucketLocationRequest()
            {
                BucketName = bucketName
            };
            GetBucketLocationResponse response = await client.GetBucketLocationAsync(request);
            bucketLocation = response.Location.ToString();
            return bucketLocation;
        }
    }
}



﻿<?xml version="1.0" encoding="utf-8" ?>
<configuration>
    <startup> 
        <supportedRuntime version="v4.0" sku=".NETFramework,Version=v4.5.2" />
    </startup>
</configuration>


// Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-s3-developer-guide/blob/master/LICENSE-SAMPLECODE.)

﻿using Amazon.S3;
using Amazon.S3.Model;
using System;
using System.Collections.Generic;
using System.IO;
using System.Security.Cryptography;
using System.Threading.Tasks;

namespace Amazon.DocSamples.S3
{
    class SSECLowLevelMPUcopyObjectTest
    {
        private const string existingBucketName = "*** bucket name ***";
        private const string sourceKeyName      = "*** source object key name ***"; 
        private const string targetKeyName      = "*** key name for the target object ***";
        private const string filePath           = @"*** file path ***";
        // Specify your bucket region (an example region is shown).
        private static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;
        private static IAmazonS3 s3Client;
        static void Main()
        {
            s3Client = new AmazonS3Client(bucketRegion);
            CopyObjClientEncryptionKeyAsync().Wait();
        }

        private static async Task CopyObjClientEncryptionKeyAsync()
        {
            Aes aesEncryption = Aes.Create();
            aesEncryption.KeySize = 256;
            aesEncryption.GenerateKey();
            string base64Key = Convert.ToBase64String(aesEncryption.Key);

            await CreateSampleObjUsingClientEncryptionKeyAsync(base64Key, s3Client);

            await CopyObjectAsync(s3Client, base64Key);
        }
        private static async Task CopyObjectAsync(IAmazonS3 s3Client, string base64Key)
        {
            List<CopyPartResponse> uploadResponses = new List<CopyPartResponse>();

            // 1. Initialize.
            InitiateMultipartUploadRequest initiateRequest = new InitiateMultipartUploadRequest
            {
                BucketName = existingBucketName,
                Key = targetKeyName,
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = base64Key,
            };

            InitiateMultipartUploadResponse initResponse =
                await s3Client.InitiateMultipartUploadAsync(initiateRequest);

            // 2. Upload Parts.
            long partSize = 5 * (long)Math.Pow(2, 20); // 5 MB
            long firstByte = 0;
            long lastByte = partSize;

            try
            {
                // First find source object size. Because object is stored encrypted with
                // customer provided key you need to provide encryption information in your request.
                GetObjectMetadataRequest getObjectMetadataRequest = new GetObjectMetadataRequest()
                {
                    BucketName = existingBucketName,
                    Key = sourceKeyName,
                    ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                    ServerSideEncryptionCustomerProvidedKey = base64Key // " * **source object encryption key ***"
                };

                GetObjectMetadataResponse getObjectMetadataResponse = await s3Client.GetObjectMetadataAsync(getObjectMetadataRequest);

                long filePosition = 0;
                for (int i = 1; filePosition < getObjectMetadataResponse.ContentLength; i++)
                {
                    CopyPartRequest copyPartRequest = new CopyPartRequest
                    {
                        UploadId = initResponse.UploadId,
                        // Source.
                        SourceBucket = existingBucketName,
                        SourceKey = sourceKeyName,
                        // Source object is stored using SSE-C. Provide encryption information.
                        CopySourceServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                        CopySourceServerSideEncryptionCustomerProvidedKey = base64Key, //"***source object encryption key ***",
                        FirstByte = firstByte,
                        // If the last part is smaller then our normal part size then use the remaining size.
                        LastByte = lastByte > getObjectMetadataResponse.ContentLength ?
                            getObjectMetadataResponse.ContentLength - 1 : lastByte,

                        // Target.
                        DestinationBucket = existingBucketName,
                        DestinationKey = targetKeyName,
                        PartNumber = i,
                        // Encryption information for the target object.
                        ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                        ServerSideEncryptionCustomerProvidedKey = base64Key
                    };
                    uploadResponses.Add(await s3Client.CopyPartAsync(copyPartRequest));
                    filePosition += partSize;
                    firstByte += partSize;
                    lastByte += partSize;
                }

                // Step 3: complete.
                CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest
                {
                    BucketName = existingBucketName,
                    Key = targetKeyName,
                    UploadId = initResponse.UploadId,
                };
                completeRequest.AddPartETags(uploadResponses);

                CompleteMultipartUploadResponse completeUploadResponse =
                    await s3Client.CompleteMultipartUploadAsync(completeRequest);
            }
            catch (Exception exception)
            {
                Console.WriteLine("Exception occurred: {0}", exception.Message);
                AbortMultipartUploadRequest abortMPURequest = new AbortMultipartUploadRequest
                {
                    BucketName = existingBucketName,
                    Key = targetKeyName,
                    UploadId = initResponse.UploadId
                };
                s3Client.AbortMultipartUpload(abortMPURequest);
            }
        }
        private static async Task CreateSampleObjUsingClientEncryptionKeyAsync(string base64Key, IAmazonS3 s3Client)
        {
            // List to store upload part responses.
            List<UploadPartResponse> uploadResponses = new List<UploadPartResponse>();

            // 1. Initialize.
            InitiateMultipartUploadRequest initiateRequest = new InitiateMultipartUploadRequest
            {
                BucketName = existingBucketName,
                Key = sourceKeyName,
                ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                ServerSideEncryptionCustomerProvidedKey = base64Key
            };

            InitiateMultipartUploadResponse initResponse =
               await s3Client.InitiateMultipartUploadAsync(initiateRequest);

            // 2. Upload Parts.
            long contentLength = new FileInfo(filePath).Length;
            long partSize = 5 * (long)Math.Pow(2, 20); // 5 MB

            try
            {
                long filePosition = 0;
                for (int i = 1; filePosition < contentLength; i++)
                {
                    UploadPartRequest uploadRequest = new UploadPartRequest
                    {
                        BucketName = existingBucketName,
                        Key = sourceKeyName,
                        UploadId = initResponse.UploadId,
                        PartNumber = i,
                        PartSize = partSize,
                        FilePosition = filePosition,
                        FilePath = filePath,
                        ServerSideEncryptionCustomerMethod = ServerSideEncryptionCustomerMethod.AES256,
                        ServerSideEncryptionCustomerProvidedKey = base64Key
                    };

                    // Upload part and add response to our list.
                    uploadResponses.Add(await s3Client.UploadPartAsync(uploadRequest));

                    filePosition += partSize;
                }

                // Step 3: complete.
                CompleteMultipartUploadRequest completeRequest = new CompleteMultipartUploadRequest
                {
                    BucketName = existingBucketName,
                    Key = sourceKeyName,
                    UploadId = initResponse.UploadId,
                    //PartETags = new List<PartETag>(uploadResponses)

                };
                completeRequest.AddPartETags(uploadResponses);

                CompleteMultipartUploadResponse completeUploadResponse =
                    await s3Client.CompleteMultipartUploadAsync(completeRequest);

            }
            catch (Exception exception)
            {
                Console.WriteLine("Exception occurred: {0}", exception.Message);
                AbortMultipartUploadRequest abortMPURequest = new AbortMultipartUploadRequest
                {
                    BucketName = existingBucketName,
                    Key = sourceKeyName,
                    UploadId = initResponse.UploadId
                };
                await s3Client.AbortMultipartUploadAsync(abortMPURequest);
            }
        }
    }
}


  Copyright 2010-2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.

   This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0
   International License (the "License"). You may not use this file except in compliance with the
   License. A copy of the License is located at http://creativecommons.org/licenses/by-nc-sa/4.0/.

   This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
   either express or implied. See the License for the specific language governing permissions and
   limitations under the License.


Amazon S3 Documentation .NET Examples
==============================================

These are the .NET examples that are used in the [Amazon S3 developer documentation](https://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html).

Prerequisites
=============

To build and run these examples, you need the following:

* The [AWS SDK for .NET](https://aws.amazon.com/sdk-for-net/) (downloaded and extracted somewhere on
  your machine).
* AWS credentials, either configured in a local AWS credentials file or by setting the
  ``AWS_ACCESS_KEY_ID`` and ``AWS_SECRET_ACCESS_KEY`` environment variables. 
* Microsoft Visual Studio installed. The example code is tested using Visual Studio Professional 2015.

For information about how to set AWS credentials for use with the AWS SDK for .NET,
see [Using an AWS Credentials File](https://docs.aws.amazon.com/sdk-for-net/v3/developer-guide/net-dg-config-creds.html#creds-file) in the *AWS
SDK for .NET Developer Guide*.

Running the Code Examples
=========================

To run the .NET examples, you must do the following:

* Download the amazon-s3-developer-guide files from GitHub as a ZIP.
* Unzip the contents to a directory on your machine.
* Open the Visual Studio solution S3Examples.sln from amazon-s3-developer-guide/code_examples/dotnet_examples/.
* Open the S3Examples project properties and set the "start up object:" to the example that you want to run.
* Open the example .cs file and update the code by providing values for class variables such as bucketName, objectKey, and bucketRegion.
* Build and run the example code.

For more information about the AWS SDK for .NET, see [aws-net-developer-guide](https://github.com/awsdocs/aws-net-developer-guide/blob/master/doc_source/welcome.rst).

**IMPORTANT**

   The examples perform AWS operations for the account and Region for which you have specified
   credentials. By running the examples, you might incur AWS service charges. See the [AWS Pricing](https://aws.amazon.com/pricing/) page for details about the charges you can
   expect for a given service and operation.

   Some of these examples perform *destructive* operations on AWS resources, such as deleting an
   Amazon Glacier archive. **Be very careful** when running an operation that
   might delete or modify AWS resources in your account. It's best to create separate test-only
   resources when experimenting with these examples.

All of the examples require replacing certain configuration values in the source code. These values
are specified as string variables at the beginning of each example, and they begin and end with three stars
(for example, "\*\*\* bucket name \*\*\*"). The source code comments and developer guide provide
further information.



﻿
Microsoft Visual Studio Solution File, Format Version 12.00
# Visual Studio 14
VisualStudioVersion = 14.0.25420.1
MinimumVisualStudioVersion = 10.0.40219.1
Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "S3Examples", "S3Examples\S3Examples.csproj", "{6272109B-98DC-4FC0-B821-BADB3B51496B}"
EndProject
Global
	GlobalSection(SolutionConfigurationPlatforms) = preSolution
		Debug|Any CPU = Debug|Any CPU
		Release|Any CPU = Release|Any CPU
	EndGlobalSection
	GlobalSection(ProjectConfigurationPlatforms) = postSolution
		{6272109B-98DC-4FC0-B821-BADB3B51496B}.Debug|Any CPU.ActiveCfg = Debug|Any CPU
		{6272109B-98DC-4FC0-B821-BADB3B51496B}.Debug|Any CPU.Build.0 = Debug|Any CPU
		{6272109B-98DC-4FC0-B821-BADB3B51496B}.Release|Any CPU.ActiveCfg = Release|Any CPU
		{6272109B-98DC-4FC0-B821-BADB3B51496B}.Release|Any CPU.Build.0 = Release|Any CPU
	EndGlobalSection
	GlobalSection(SolutionProperties) = preSolution
		HideSolutionNode = FALSE
	EndGlobalSection
EndGlobal



**/.vs/
**/packages/
**/bin/
**/obj/



*Issue #, if available:*

*Description of changes:*


By submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.



